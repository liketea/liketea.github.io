<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[数据科学：综述（〇）—— 知识框架]]></title>
    <url>%2F%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%2F%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%2F%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%EF%BC%9A%E7%BB%BC%E8%BF%B0%EF%BC%88%E3%80%87%EF%BC%89%E2%80%94%E2%80%94%20%E7%9F%A5%E8%AF%86%E6%A1%86%E6%9E%B6%2F</url>
    <content type="text"><![CDATA[数据科学的核心在于“数据驱动决策”： 数据：指的是数据的收集和整理，典型的就是建立数仓的过程； 驱动：指的是驱动的方法和过程，典型的包括统计描述、统计推断、统计建模等； 决策：指的是决策的场景和问题，典型的包括用户增长、风险控制等； 数据驱动决策的“闭环过程”：虚线代表设计过程，实线代表实现过程 数据驱动决策各环节所涉及知识体系： 数据科学 数据 数据仓库 数仓存储 HDFS HIVE HBase Mysql Impala ClickHouse 数仓ETL 离线计算 MapReduce Spark 实时计算 Flink Spark Streaming 数仓模型 数仓规范 维度建模 数仓编程 SQL Python Scala R 数据治理 元数据管理 标准管理 质量管理 安全管理 共享管理 驱动 统计描述 维度-指标聚合 统计推断 AB 实验 假设检验 统计建模 相关分析 相关性 预测分析 机器学习 因果分析 因果推断 决策 通用知识 用户增长 风险控制 … 领域知识 游戏 电商 社交 金融 医疗 … 问题导向 发现问题 总结过去 过去发生了什么？ 指标描述 趋势分析 监控当下 当前正在发生什么？ 异常监控 异动归因 预测未来 未来可能会发生什么？ 分类 回归 聚类 解决问题 选择 为了…应该选择哪种方案? 开放 为了…应该怎么做?]]></content>
      <categories>
        <category>数据科学</category>
      </categories>
      <tags>
        <tag>数据科学</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据科学：因果推断（二）—— Rubin 因果模型（RCM）]]></title>
    <url>%2F%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%2F%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%2F%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%EF%BC%9A%E5%9B%A0%E6%9E%9C%E6%8E%A8%E6%96%AD%EF%BC%88%E4%BA%8C%EF%BC%89%E2%80%94%E2%80%94%20Rubin%20%E5%9B%A0%E6%9E%9C%E6%A8%A1%E5%9E%8B%EF%BC%88RCM%EF%BC%89%2F</url>
    <content type="text"><![CDATA[鲁宾因果模型（Rubin causal model, RCM），也称内曼-鲁宾因果模型（Neyman–Rubin causal model），是一种基于潜在结果框架（framework of potential outcomes）的因果推断方法，以杰西·内曼（Jerzy Neyman）和唐纳德·鲁宾（Donald Rubin）的名字命名。潜在结果的概念最早是由 Neyman（1923）在研究重复随机化农业实验中提出的，由于该文用波兰语写成，当时没有引起学界的关注。Rubin（1974）重新独立地提出了潜在结果的概念，并将它的使用推广到观测研究领域，从而形成了目前的潜在结果框架。RCM 有三个基本要素：潜在结果、稳定性假设、分配机制。 潜在结果 干预在因果推断中，必须有干预（Intervention），没有干预就没有因果（Rubin，1974）。干预可以是一项政策、一项措施或一项活动等，比如实施 4 万亿财政刺激方案，服用某种新药等。本文主要讨论二值干预变量，两个值分别对应于积极的行动和被动的行动，分别称为干预和控制，受到对应干预的个体分别称为干预组和控制组。 干预和控制只是干预变量的两种状态的标签，具体哪个状态被称为干预，哪个状态称为控制并不重要，两种状态实际上是对称的，可以互换，取决于研究者的目的和偏好。比如，对于药物试验来说，干预是服用药物，控制是不服用药物。 潜在结果在干预状态实现之前，有几个干预状态就有几个潜在结果（Potential outcome），而干预状态实现之后，只有一个潜在结果是可以观测到的。可以将潜在结果看作常数，对于每个特定的个体，他在两种干预状态下的潜在结果是给定的，不依赖于最终实现的干预状态，这一点对于理解 Rubin 因果模型很关键。 比如，考察大学教育对个人收入的影响，干预变量或原因变量是大学教育，那么对于任意个体 $i$ 有两种干预状态，用 $Di$ 来表示，$D_i=1$ 表示个体 $i$ 完成了大学教育，$D_i=0$ 表示个体 $i$ 完成高中教育。无论个体实际是完成大学教育还是高中教育，事前每个个体均有两种可能的状态：完成高中教育或完成大学教育。每一个状态下对应于一个潜在结果，$Y{1i}$ 表示个体 $i$ 在状态$Di=1$ 下的潜在结果，$Y{0i}$ 表示个体 $i$ 在状态 $Di=0$ 下的潜在结果。对个体而言，这两个潜在结果可以看作是确定性的变量，不因个体干预变量的实现状态而改变。比如个体 $i$ 完成大学教育状态下的收入为 $8000$ 元，即 $Y{1i}=8000$，仅完成高中教育状态下收入为 6000 元，即 $Y_{0i}=6000$。如果个体 $i$ 最后实际完成了大学教育，那么其两种干预状态下的潜在结果仍然是（8000，6000），如果个体 $i$ 最后实际完成的是高中教育，其两种干预状态下的潜在结果还是（8000，6000），不因个体最后实现的状态而改变。 观测结果 VS 反事实结果当干预状态实现之后，我们仅能观测到实现状态下的潜在结果，称为观测结果（Observation outcome），没有实现状态下的潜在结果是无法观测的，通常称为反事实结果（Counterfactual outcome）。比如个体 $i$ 最终完成了大学教育，那么观测到的干预状态是 $Di=1$，我们可以观测到潜在结果 $Y{1i}$，即个体 $i$ 完成大学教育后的收入。他完成了大学教育，我们就不能观测到他没有完成大学教育时的潜在结果 $Y_{0i}$，即仅完成高中教育时的收入。一个人不可能同时踏入两条河流，不可能同时处于两种状态，因而，观测研究中，不可能同时看到个体所有的潜在结果。无法同时观测到个体所有潜在结果的现象称为因果推断的基本问题（Holland，1986）。 观测结果 $Y_i$ 与潜在结果之间的关系，可以用下面的公式表示： \begin{align*} Y_i&=D_iY_{1i}+(1-D_i)Y_{0i}\\ &=\left\{\begin{matrix} Y_{1i}, & \text{如果}\ D_i = 1 \\ Y_{0i}, & \text{如果}\ D_i = 0 \end{matrix}\right. \end{align*} \tag {1}潜在结果和观测结果的区分是现代统计学和现代计量经济学的重要标志，是经济学经验研究“可信性革命”的关键，也是区分描述性研究（descriptive study）和因果研究（causal study）的标志。 干预效应/因果效应有了潜在结果的概念，个体因果效应的定义非常直观，不需要对分配机制进行任何内生性或外生性的假设，也不需要对结果变量的函数形式进行任何假设，对于个体 $i$，某项干预的因果效应是两种状态下的潜在结果的比较： \tau_i=Y_{1i}-Y_{0i} \tag {2}关于因果效应的定义有两点说明： 因果效应仅依赖于潜在结果，与观测结果无关：回到大学教育如何影响收入的例子，无论个体 $i$ 是否完成了大学教育，大学教育对其个人的因果影响都取决于其两种状态下的潜在结果，并且是固定不变的，不依赖于个体最终实现的干预状态；如果个体 $i$ 完成了大学教育，大学教育对其收入的影响是每个月收入增加 2000 元；如果个体 $i$ 仅完成高中教育，那么，如果他能完成大学教育，则其收入的影响也是每月增加 2000 元。 因果效应是干预后同一时间、同一物理个体潜在结果的比较：比如考察某种药物对感冒的治疗效果，干预状态是吃药或不吃药，对应的潜在结果是治愈感冒或没有治愈；因果效应应该定义为我现在吃药和不吃药对应潜在结果的比较，而不能用我现在吃药和昨天我没有吃药时的潜在结果比较；因为昨天的我和今天的我不是同一个我，我今天不吃药的潜在结果和昨天不吃药的潜在结果可能是不一样的，所以在评价今天我吃药的因果效应时，应该是今天我吃药和今天我不吃药时潜在结果的比较。 反事实结果估计因果效应的定义仅依赖于不同潜在结果的比较，对于给定个体，研究者只能观察到该个体一个状态下的潜在结果，因而，如果仅有一个个体，我们是没有办法得到个体因果效应的。因果推断的核心内容，实际上是想办法将未观测到的潜在结果估计出来，即反事实结果估计。估计反事实结果必须要用到多个个体，多个个体的选择方式有两种： 同一个体的不同时间：比如，判断一种药物是否对感冒有治疗效果，我们往往根据自己以往的经历。我以前感冒的时候吃药感冒就好了，我今天没吃药，头就很痛，因而，我们认为药物有治疗效果。其实这种推断中，我们进行了很强的假设，我们假设过去的经验可以作为今天吃药的反事实结果。如果这一假设不成立，我们就不能用过去吃药的结果作为今天吃药的反事实结果。因为今天的“我”与过去的“我”是不同的个体，我今天可能心情不好，不吃药头很痛，即使吃药，头仍然是痛的。这并不一定说明药没有治疗效果，而是因为我心情沮丧，使我的头更痛了，即我的头痛还混杂了其他的影响因素。 同一时间的不同个体：很多时候，我们的推断是利用同一时间不同个体的信息来估计反事实结果。比如考虑大学教育对收入的影响。在上大学之前，我们不确定大学能给我们带来什么。我们只知道目前我的结果是什么样子，或收入是什么水平。但不知道大学毕业之后收入会是什么水平。那我们在决定是否上大学时，是怎么作出决定的呢？我们可能会观察那些上了大学的人，可能是亲戚或朋友家的孩子，现在已经大学毕业了，有个很好的工作，获得比较满意的收入。那我们在作决策时是怎么做的呢？我们可能将他们的结果或收入作为我们上大学的潜在收入，从而决定是否上大学。 稳定性假设RCM 的第二个要素是稳定个体干预值假（Stable Unit Treatment Value Assumption, SUTVA），简称稳定性假设（Rubin，1980），SUTVA 有两层含义： 不同个体的潜在结果之间不会交互影响：比如，我们住在同一间宿舍，我们两个都感冒了，如果药物对我头痛的治疗效果依赖于你有没有吃药，那就不满足稳定性假设；在社会科学中，没有交互影响的假设可能不成立，社会科学的研究对象往往是人的行为，个人行为之间往往存在交互影响；但是，在不存在交互影响的假设下，因果推断更加容易，通常假设不同个体之间不存在交互影响。 干预水平对所有个体都是相同的：比如考察药物的治疗效果，那么给所有病人的药物在药效上都应该是一样的，不能有的人有效成分是全额的，有的人是半额的；实际研究中，往往很难完全满足这一要求，通常会忽略掉这种差异，更加关注稳定性假设的第一项要求。 分配机制分配机制是描述为什么有的人在干预组，有的人在控制组的机制。分配机制决定了个体哪个潜在结果会被实现，可以被观测到。在因果推断中，分配机制非常重要，来看一个“手术相对于药物的治疗效果”的例子： 在潜在结果列可以看出，对于病人 1 和病人 3 来说，手术治疗效果优于药物治疗，而对于病人 2 和病人 4 来说，药物治疗优于手术治疗。假设现实中医生具有很好的医术或鉴别力，可以让病人选择对他最有利的治疗方案，从而实现的分配机制如表中第 5 列所示，让 1 和 3 号病人接受手术治疗，让 2 和 4 号病人接受药物治疗，最终我们可以观测到 1、3 病人的 $Y{1i}$ 以及 2、4 病人的 $Y{0i}$，如观测结果列所示。如果不清楚分配机制，直接用两组观测结果进行比较，将会发现手术治疗平均寿命为 6 年，而药物治疗平均寿命为 7 年，从而得出药物治疗更有效的错误结论。而事实上，通过潜在结果计算出的平均因果效应，手术治疗要比药物治疗寿命长 2 年。 根据分配机制是否已知，可以将分配机制分成两类： 随机实验：分配机制是由实验者控制的，是已知的； 观测研究：分配机制是未知的，观测研究的目的就是想办法识别出未知的分配机制，从而估计因果效应； 协变量为了搞清楚分配机制，往往需要一些协变量（Covariates），也称混淆变量（Confusion variable），协变量的基本特征是这些变量不受干预变量的影响，但是却往往决定个体的干预状态，协变量包括两种： 个体属性：不随干预状态变化而变化的变量，比如性别、民族等变量； 干预实施之前取值的变量：比如研究培训的作用时，培训前的收入水平及经济社会特征等； 条件独立性非混杂性（Unconfoundedness），也称为条件独立性（Conditional independence），是指控制协变量 $X_i$ 后，个体干预状态的分配独立于潜在结果，非混杂性可以表示为： (Y_{0i},Y_{1i})\perp D_i|X_i \tag {3}根据分配机制是否满足条件独立性条件，可以将分配机制分成三类： 经典随机化实验：分配机制满足条件独立性，且函数形式已知； 规则分配机制（Regular assignment mechanism）：分配机制满足条件独立性，但函数形式未知； 不规则机制（Irregular assignment mechanism）：分配机制不满足条件独立性； Lord 悖论潜在结果的概念，对理清所要研究的因果问题、定义因果效应非常有帮助。有些因果问题的探讨，必须从潜在结果概念出发才能搞清楚因果效应是否有清晰的定义，从观测结果出发进行建模往往不能清晰地表述所研究的因果效应问题。 这一节介绍一个在统计学中很有名，但是在中文统计教科书中几乎从未介绍过的悖论 —— Lord 悖论（Lord’s Paradox）。这个悖论是由美国教育考试服务中心（EducationalTestingService, ETS）统计学家 FredericLord 于 1967 年提出来的，最终由同在 ETS 工作的另外两位统计学家 Paul Holland 和 Donald Rubin 于 1982 年圆满地找出了这个悖论的根源。 悖论描述 Lord（1967）构造了一个假想的案例，一所大学想考察其食堂膳食对于学生体重是否有差异性的影响，尤其关心食堂对于男女学生体重影响是否相同，为此，收集了学生 9 月份入学时的体重，然后次年 6 月份又获得了学生在校一学年后的体重。两个统计学家分别利用这个数据考察了学校食堂对学生体重的影响，但得到了完全不同的结论： 第一个统计学家用了比较初等的方法，计算了男生和女生入学时的平均体重，分别是 150 磅和 130 磅。然后又计算了入学一学年后男、女生的平均体重，发现仍然是 150 磅和 130 磅。因而，第一位统计学家认为学生食堂膳食对学生体重没有影响。 第二个统计学家采用了更加高等的方法 —— 回归分析，他认为为了考察食堂对学生体重的影响，必须比较两个初始体重相同的人，因而，他构造了一个回归模型，控制了个体入学时的体重，并考察了性别的差异。回归结果表明，同样体重的男生、女生相比，男生的体重增加更大，比女生平均高 7.3 磅。 两个统计学家利用同一数据，采用不同的方法，得到几乎相反的结果，一个说无因果影响，一个说对男生的影响更大，这种矛盾的结果被称为 Lord 悖论。那么，这两个统计学家的分析，哪一个正确呢？ 悖论解释我们首先用 Rubin 因果模型的框架套用到该问题上： 表中的问号（?）是解决 Lord 悖论的关键，尽管积极干预是非常清晰的——学校食堂膳食，它对学生体重的影响是想要研究的问题，但没有清晰的控制干预，不在学校食堂吃饭时是在家吃饭还是在外面下馆子，我们并不清楚，这意味着潜在结果 $Y_0$ 的定义是模糊的，我们权且将 $Y_0$ 看做是假如期间学生没有在学校食堂吃饭时的体重。然而，没有学生在控制组，所有学生都在学校食堂吃饭，为了回答食堂对学生体重的影响，必然要引入一些有关 $Y_0$ 的不可检验的假设，这也正是两位统计学家产生分歧的地方。 食堂膳食对学生体重的个体影响可以写作 $Y_1 - Y_0$，对男女学生的平均影响可以写作: \Delta_i = E[Y_1-Y_0|G=i],\ i=1,2 \tag {4}平均因果影响的性别差异为： \begin{align*} \Delta &= \Delta_1 - \Delta_2\\ &=E[Y_1-Y_0|G=1]-E[Y_1-Y_0|G=2]\\ &=(E[Y_1|G=1]-E[Y_1|G=2])-(E[Y_0|G=1]-E[Y_0|G=2]) \end{align*} \tag {5}第一位统计学家根据男女学生入学前和放假后平均体重的对比，得到学校膳食没有影响的结论。他所依据的假设是“假如学生不在学校食堂吃饭，他们的体重变化相同”，即 $Y_0 = X + C$，其中 $C$ 对男女学生都是相同的常量，基于该假设可以计算平均因果影响的性别差异： \begin{align*} \Delta &= \Delta_1 - \Delta_2\\ &=E[Y_1-Y_0|G=1]-E[Y_1-Y_0|G=2]\\ &=E[Y_1-X-C|G=1]-E[Y_1-X-C|G=2]\\ &=E[Y_1-X|G=1]-E[Y_1-X|G=2]\\ &=0-0\\ &=0 \end{align*} \tag {6}第二位统计学家认为应该控制开学时的体重，比较相同体重的人放假时体重的变化，对于初始体重为 X 的个体，体重的增加为 $\delta_i(X) = E[Y_i-X|X,G=i],\ i=1,2$，增量的性别差异为 $\delta(X) = \delta_1(X)-\delta_2(X)$，为简单起见，Lord 假设条件期望函数均为线性且男女生斜率相同，即 $E[Y_i|X,G=i]=a_i+bX,\ i=1,2$，则 $\delta(X)=a_1-a_2$。$\delta(X)$ 与因果效应参数 $\Delta$ 没有直接关系，但是在一定的假设下二者等价，比如假设“如果学生不在学校食堂吃饭，他们的体重是初始体重的线性函数”，即 $Y_0 = a + bX$，并且对所有性别的学生都一样，在此假设下，有： \begin{align*} \Delta &= \Delta_1 - \Delta_2\\ &=E[Y_1-Y_0|G=1]-E[Y_1-Y_0|G=2]\\ &=E[Y_1-a-bX|G=1]-E[Y_1-a-bX|G=2]\\ &=E[Y_1-bX|G=1]-E[Y_1-bX|G=2]\\ &=E[E[Y_1|X,G=1]-bX|G=1]-E[E[Y_1|X,G=2]-bX|G=2]\\ &=E[a_1+bX-bX|G=1]-E[a_2+bX-bX|G=2]\\ &=a_1-a_2\\ \end{align*}\tag {7}关于 Lord’s Paradox，我们有如下结论： Lord 悖论的根源在于整个研究没有控制组，我们甚至不知道什么是控制组，这导致 $Y_0$ 定义模糊； 统计学家一和二，都可能是对的，他们结论的正确性，依赖于不同的假定，而这些假定本身是不可能被检验的； 统计学家一和二，都是错的，他们有结论，但是却从未清楚地陈述结论回答的是什么问题； 潜在结果的概念，对理清所要研究的因果问题、定义因果效应非常有帮助； 因果效应参数ATE &amp; ATT &amp; ATC 定义实证研究中，我们关心的往往不是某一特定个体的因果效应，而是干预的平均因果效应。假设有 N 个个体，用 i=1,……,N 表示，$D_i \in {0,1}$ 表示干预变量，个体因果效应为： \tau_i=Y_{1i}-Y_{0i},\ i=1,\cdots ,N \tag {8}个体因果效应往往无法估计，因而，我们关注总体平均因果效应（Average Treatment Effect, ATE），它表示从总体中随机抽取一个个体进行干预的平均因果效应： \tau_{ATE}=E[Y_{1i}-Y_{0i}] \tag {9}在政策评价中，我们更关心那些受到政策影响的个体的平均因果效应，称为干预组平均因果效应（Average Treatment Effect for the Treated,ATT）： \tau_{ATT}=E[Y_{1i}-Y_{0i}\mid D_i=1] \tag {10}有些时候，我们关注那些没有受到政策影响的个体如果接受政策干预的话，其平均因果效应是多少，称为控制组平均因果效应（Average Treatment Effect for the Control, ATC）： \tau_{ATC}=E[Y_{1i}-Y_{0i}\mid D_i=0] \tag {11}不同的因果效应参数回答不同的问题，比如考察大学教育对个体收入的影响，将大学教育看作一项积极干预，高中教育看作一项控制干预： ATE：如果想知道大学教育对所有国民的平均影响，估计的参数是总体的平均因果效应（ATE），它反映的是如果全部国民均接受大学教育相对于均接受高中教育全部国民的平均收入增长。 ATT：如果关心的政策问题是大学教育给接受者带来了多大程度的收入增加，需要估计的参数是干预组平均因果效应（ATT）。 ATC：如果想知道那些仅完成高中教育的个人，如果他们能够完成大学教育的话，他们的收入将增长多少，则需要估计的参数是控制组平均因果效应（ATC）。 ATE &amp; ATT &amp; ATC 计算下面通过一个简单的例子来示范三个因果效应参数的计算，假设有四个个体，并且我们可以同时看到两种干预状态下的潜在结果（现实中只能看到一种状态下的结果）: 理论上，我们可以根据表中的潜在结果数据分别计算 ATE、ATT、ATC： \begin{align*} \tau_{ATE}&=E[Y_{1i}-Y_{0i}]=3\cdot 1/4 + 0 \cdot 1/4 + 1 \cdot 1/4 + 0 \cdot 1/4=1.0\\ \tau_{ATT}&=E[Y_{1i}-Y_{0i}\mid D_i=1]=3\cdot 1/2 + 0 \cdot 1/2=1.5\\ \tau_{ATC}&=E[Y_{1i}-Y_{0i}\mid D_i=0]=1\cdot 1/2 + 0 \cdot 1/2=0.5 \end{align*}实际上，我们仅能观测到每个个体在其中一种状态下的潜在结果。对于前两个个体，他们在干预组，我们可以观测到他们在积极干预状态下的潜在结果 $Y{1i}=Y_i$，但观测不到他们在控制状态下的潜在结果 $Y{0i}$；相反对于后两个个体，他们在控制组，我们可以观测到他们在被动控制状态下的潜在结果 $Y{0i}=Y_i$，但却观测不到他们在干预状态下的潜在结果 $Y{1i}$。从而，前面计算的三个因果效应参数也就没有办法计算出来了，现在我们再来看各个因果效应参数的定义： \begin{align*} \tau_{ATE}&=E[Y_{1i}-Y_{0i}]\\ &=E[E[Y_{1i}-Y_{0i}|D_i]]\\ &=E[Y_{1i}-Y_{0i}|D_i=1]\cdot P(D_i=1)+E[Y_{1i}-Y_{0i}|D_i=0]\cdot P(D_i=0) \\ &=\tau_{ATT}\cdot P_t+\tau_{ATC}\cdot P_c\\ \tau_{ATT}&=E[Y_{1i}-Y_{0i}|D_i=1]=E[Y_i|D_i=1]-E[Y_{0i}|D_i=1]\\ \tau_{ATC}&=E[Y_{1i}-Y_{0i}|D_i=0]=E[Y_{1i}|D_i=0]-E[Y_i|D_i=0] \end{align*}\tag {12}其中，反事实结果 $E[Y{0i}|D_i=1]$ 和 $E[Y{1i}|D_i=0]$ 是观测不到的，必须通过一定的方法将其估计出来，才能得到以上干预效应。 回归分析与因果效应学过回归分析的学生可能禁不住想用 $Y_i$ 对 $D_i$ 回归，这也是计量经济学的基本建模方式，但是这种回归并不能识别出任何因果效应参数。比如我们建立一个简单的双变量回归模型： Y_i=\alpha + \tau D_i + \varepsilon_i \tag {13}根据初等计量经济学的知识，用一个容量为 N 的随机样本去估计上述简单回归模型，$D_i$ 的回归系数为： \hat{\tau}^{ols}=\frac{\sum_{i=1}^{N}(Y_i-\bar{Y})(D_i-\bar{D})}{\sum_{i=1}^{N}(D_i-\bar{D})^2} \tag {14}当干预变量是 $0-1$ 二值变量时，可以证明 $Y_i$ 对 $D_i$ 的回归系数 $\hat{\tau}^{ols}$ 等于干预组和控制组样本均值之差，在大样本的情况下： \hat{\tau}^{ols}=\bar{Y_t}-\bar{Y_c}\overset{p}{\rightarrow}E[Y_i|D_i=1]-E[Y_i|D_i=0]=\tau^{ols} \tag {15}$\tau^{ols}$ 是总体回归系数，一般不能反映因果效应参数，除非施加一定的假设。 首先，考察总体回归系数和干预组平均因果效应（ATT）之间的关系： \begin{align*} \tau^{ols}&=E[Y_i|D_i=1]-E[Y_i|D_i=0]\\ &=E[Y_{1i}|D_i=1]-E[Y_{0i}|D_i=0]\\ &=E[Y_{1i}-Y_{0i}|D_i=1]+(E[Y_{0i}|D_i=1]-E[Y_{0i}|D_i=0])\\ &=\tau_{ATT}+\Delta \tau_0 \end{align*} \tag {16}回归系数和因果效应参数 ATT 之间相差 $E[Y{0i}｜D_i=1]-E[Y{0i}｜Di=0]$，它表示干预组和控制组个体在控制状态下的潜在结果差异，也称为基线潜在结果差异（difference in baseline potential outcomes），这一偏差通常称为选择偏差（selection bias）。$E[Y{0i}｜Di=1]$ 表示干预组个体在控制状态下的潜在结果，是观测不到的，但是在选择偏差为 0 的假设下，可以用控制组的观测结果 $E[Y{0i}｜Di=0]$ 来代替干预组的反事实结果 $E[Y{0i}|D_i=1]$。比如教育收益率的例子，如果潜在收入高的人倾向于选择上大学，那么，上大学的人即使仅完成了高中教育，他们的收入也会比高中组高，那么大学组合高中组观测到的收入均值差就不能解释为大学教育对个人收入的因果影响，选择偏差为正，回归系数将高估教育对收入的影响。 类似地，总体回归系数也不是控制组平均因果效应（ATC），只有假设干预组和控制组的干预潜在结果相同，即 $E[Y{1i}|D_i=1]=E[Y{1i}|D_i=0]$，回归系数才等于 ATC： \begin{align*} \tau^{ols}&=E[Y_i|D_i=1]-E[Y_i|D_i=0]\\ &=E[Y_{1i}|D_i=1]-E[Y_{0i}|D_i=0]\\ &=E[Y_{1i}-Y_{0i}|D_i=0]+(E[Y_{1i}|D_i=1]-E[Y_{1i}|D_i=0])\\ &=\tau_{ATC}+\Delta \tau_1 \end{align*} \tag {17}最后，总体回归系数通常也不是平均因果效应，只有同时施加假设 $\Delta \tau0=0$ 和$\Delta \tau_1=0$ 时，总体回归系数才可解释为总体平均因果效应。将式 $(16)$ 和 $(17)$ 带入到 $\tau{ATE}=\tau{ATT}\cdot P_t+\tau{ATC}\cdot P_c$，易得： \tau^{ols}=\tau_{ATE}+\Delta \tau_0+P_c\cdot (\tau_{ATT}-\tau_{ATC}) \tag {18}我们可以得到分配机制、潜在结果、干预效应和回归系数之间的一般关系，如下图所示： 需要注意的是，潜在结果框架仅关注因果效应，不能说明变量之间的影响机制，因果效应是一个“黑箱”，只能给出因果效应的大小，不能给出产生这一因果效应的内在机制。 参考 赵西亮. 基本有用的计量经济学 (高等院校经济学管理学系列教材) 因果推断简介之七：Lord’s Paradox]]></content>
      <categories>
        <category>数据科学</category>
      </categories>
      <tags>
        <tag>数据科学</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据科学：因果推断（一）—— 辛普森悖论]]></title>
    <url>%2F%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%2F%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%2F%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%EF%BC%9A%E5%9B%A0%E6%9E%9C%E6%8E%A8%E6%96%AD%EF%BC%88%E4%B8%80%EF%BC%89%E2%80%94%E2%80%94%20%E8%BE%9B%E6%99%AE%E6%A3%AE%E6%82%96%E8%AE%BA%2F</url>
    <content type="text"><![CDATA[There are three kinds of lies: lies, damned lies, and statistics. ——Mark Twain 辛普森悖论——描述辛普森悖论（Simpson’s paradox）是概率统计中的一种现象：在变量 Z 的每一个分层上，变量 X 和变量 Y 都表现出一致的相关性，但是在 Z 的整体上，X 和 Y 却呈现出与之相反的相关性。该现象于 20 世纪初就有人讨论，但一直到 1951 年 E.H.辛普森在他发表的论文中阐述此一现象后，该现象才算正式地被描述解释，辛普森悖论这个名字是由柯林·布莱斯（Colin R. Blyth）在 1972 年提出的。 以 BBG 药物（Bad/Bad/Good Drug）之谜为例，假设有一种新药 D，这种新药似乎可以降低心脏病发作的风险，我们通过临床观测收集到了如下数据（数据来自观测实验而非随机化实验）： 整体来看，服药组和未服药组各有 60 人，男性和女性各有 60 人，不同人群的心脏发病率表现如下： 对于女性患者：未服药组的心脏病发病率 5% &lt; 服药组的心脏病发病率 8%； 对于男性患者：未服药组的心脏病发病率 30% &lt; 服药组的心脏病发病率 40%； 对于所有患者：未服药组的心脏病发病率 22% &gt; 服药组的心脏病发病率 18%； 这种药物似乎对女性有害，对男性也有害，但却对整个人类有益！一个表面的解决方案是，当我们知道病人的性别是男性或者是女性时，我们不采用这种药物疗法，但如果病人的性别是未知的，我们就应该采用这种疗法！但显然，这个结论是荒谬的。这三句话中一定有一句是错的，但错的是哪一句？为什么？这种令人迷惑不解的情况究竟是如何发生的呢？ 辛普森悖论——解决任何声称能够解决悖论的方法都应该能够回答一些关于悖论的基本问题： 解释悖论让人困惑的原因； 确定悖论出现的场景类别； 挖掘悖论掩盖的正确结论； 辛普森逆转“辛普森逆转”是指在合并样本时，两个或多个不同样本关于某一特定事件的相对频率出现反转的现象。在上面的例子中，我们可以看到两组相对频率：1/20 &lt; 3/40，12/40 &lt; 8/20，然而 (1 + 12)/(20 + 40) &gt; (3 + 8)/(40 + 20)。 为了直观理解辛普森逆转机制，我们通过混合不同浓度的溶液来类比混合不同性别心脏发病率的场景，其中容器的形状代表性别，女性用圆形容器来表示，男性则用方形容器来表示，患者发病率用黑色阴影来表示，混合前圆形容器和方形容器干预组液体浓度都要大于对照组，混合后干预组液体浓度却高于对照组： 辛普森逆转通常满足两个前提： 不同 Z 分层对应的 Y 值相差很大：男性患者的发病率(33.3%)远高于女性患者的发病率(6.7%)； Z 在干预组和对照组的分布有明显差异：男性在对照组占比(66.7%)远高于在干预组占比(33.3%)； 辛普森悖论辛普森逆转只是一个纯粹的数字事实，本身并无新奇之处，它最多只是纠正了人们对“平均表现”的错误概念。而悖论的含义不止于此，它应该能够引起两种为绝大部分人深信不疑的信念之间的冲突。在 BBG 药物悖论中，当“对男性有害”“对女性有害”“对人类有益”这三个陈述被简单理解为比例增减时，它们在数学上并不矛盾，但是你可能认为这种情况在现实世界中不可能存在，因为一种药物不可能既导致心脏病发作又防止心脏病发作。幸运的是，你的直觉是对的，BBG 药物确实不存在！ 确凿性原则：假如无论事件 C 是否发生，某个行动都会增加某一结果的可能性，则该行动也将在我们不知道事件 C 是否发生的情况下增加这个结果的可能性，前提是该行动不会改变 C 的概率。 根据确凿性原则，以下三种陈述之一必定为假： 药物 D 增加了男性患者和女性患者的心脏病发作的概率； 药物 D 降低了整个总体的心脏病发作的概率； 药物 D 不会改变男性和女性的数量； 因为药物改变病人性别的事不太可能发生，所以前两句陈述中一定有一句为假。那么，哪句陈述是假的？要回答这个问题，我们必须在数据之外探寻数据生成的过程。我们可以通过以下因果图对 BBG 药物数据的产生过程建模，这张图对性别对心脏病发作风险的影响（男性患者的风险更大），以及性别对患者是否选择服用药物 D 的影响（女性更倾向于服用药物 D）进行了编码，性别因素构成了是否服用药物和心脏病发作的混淆因子： 为了客观估计药物对心脏病的影响，我们必须对混淆因子进行控制，或按照一般总体中性别分布对不同性别下药物效果进行加权： 对于女性患者：未服药组的心脏病发病率 5% &lt; 服药组的心脏病发病率 8%； 对于男性患者：未服药组的心脏病发病率 30% &lt; 服药组的心脏病发病率 40%； 对于所有患者：未服药组的心脏病发病率 5% × 0.5 + 30% × 0.5 = 17.5% &lt; 服药组的心脏病发病率 8% × 0.5 + 40% × 0.5 = 24%； 至此，我们找到了关于 BBG 药物最清晰、明确的答案：药物 D 不是 BBG 药物，而是 BBB 药物，对女性有害、对男性有害、对人类有害。 至此，我们回答了 BBG 药物悖论中的基本问题： 解释悖论让人困惑的原因：从心脏病发病率来看，BBG 药物似乎对男性有害、对女性有害、对全体人类有益，这是荒谬的，违反了绝大多数人的直觉； 确定悖论出现的场景类别：BBG 药物悖论产生的前提有三，① 性别因素既是影响是否服用药物的原因，又是影响患者发病率的原因，即性别是药物服用和发病的混淆因子；② 不同性别下，发病率差别较大；③ 不同性别下，药物服用比例差别较大； 挖掘悖论掩盖的正确结论：控制混淆因子，分层看数据或者按混淆因子在一般总体中的分布情况对统计数据进行修正，可得正确结论，BBG 药物对男性有害、对女性有害、对人类有害； 分合取决于因果而非数据关于辛普森悖论，还应明确： 辛普森悖论的存在并不意味着聚合数据总是错的：是分是合取决于数据的生成过程，而非数据本身； 辛普森悖论没有出现也不意味着混淆因子不存在：潜在的混淆因子仍然会干扰统计推断，只是没有达到辛普森悖论的极端表现； 假设高血压是心脏病发作的可能原因，而药物 B 能降低血压，研究人员向看看这种药物是否也能降低心脏病发作的风险，因此他们在病人服药后测量了病人的血压，并观察病人是否会出现心脏病发作的情况： 这些数据看起来非常熟悉，其中的数字和 BBG 药物的统计数据是完全一致的。我们可以通过以下因果图对服用药物 B、血压、心脏病发作三者建模，与 BBG 因果图不同的是，血压不再是药物服用和心脏病发作的混淆因子，而是二者之间的中介物： “服用药物 B -&gt; 心脏病发作”这一因果关系中没有混杂因子，所以数据分层是不必要的。事实上，如果控制血压会使其中一条因果路径失效（而且可能是最重要的那条因果路径），导致药物无法通过这条路径发挥作用。鉴于此，我们得出的结论与在 BBG 药物的例子中得到的结论完全相反：药物 B 能有效预防心脏病发作。 辛普森悖论——实例肾结石疗法1996 年发表的一篇观察性研究报告表明，对于摘除小型肾结石而言，开腹手术比内窥镜手术的恢复率高，对于摘除较大的肾结石而言，开腹手术也有更高的恢复率。然而就总体而言，开腹手术的恢复率反而较低。 小肾结石被认为是不太严重的病例，开腹手术比内窥镜手术更加激进，因此对于小肾结石，医生更有可能推荐保守内窥镜手术，因为病情不太严重，患者也更有可能首先成功恢复。对于严重的大肾结石，医生往往选择更激进的开腹手术，较大肾结石的病人本身的恢复率较低。 吸烟者存活率更高？在 1995 年发表的一份关于甲状腺疾病的研究报告中，数据显示吸烟者的存活率（76%）比不吸烟者的存活率（69%）更高，寿命平均多出20年。然而，在样本的7个年龄组中，有6个年龄组中不吸烟者的存活率更高，而第7个年龄组中二者的差异微乎其微。年龄显然是吸烟和存活率的混杂因子：吸烟者的平均年龄比不吸烟者小（很可能是因为年老的吸烟者已经死了）。根据年龄来分割数据，我们就可以得出正确的结论：吸烟对存活率有负面影响。 运动水平与体内胆固醇水平逆转也可能发生在包含连续变量的情况，假设有一项关于各年龄段群体每周的运动时间与其体内胆固醇水平之关系的研究。如左图所示，我们以 x 轴表示运动时间，以 y 轴表示胆固醇水平。一方面，我们在每个年龄组中都看到了向下的趋势，表明运动可能的确有降低人体胆固醇水平的效果。另一方面，如果我们使用相同的散点图，但不按年龄对数据进行分层，如右图所示，那么我们就会看到一个明显向上的趋势，表明运动得越多，人体胆固醇水平就越高。看起来我们再次遇到了 BBG 药物的情况，其中运动就是那个药物：它似乎对每个年龄组都产生了有益的影响，却对整个总体有害。 像往常一样，要决定运动是有益的还是有害的，我们需要考察数据背后的故事。数据显示，总体中年龄越大的人运动得越多。因为更可能发生的是年龄影响运动，而不是反过来。同时，年龄可能对胆固醇水平也有因果效应。因此我们得出结论，年龄可能是运动时间和胆固醇水平的混杂因子，我们应该对年龄进行变量控制。换言之，我们应该看的是按照年龄组别进行分层后的数据，并据其得出结论：无论年龄大小，运动都是有益的。 参考 为什么（美）朱迪亚·珀尔，（美）达纳·麦肯齐著；江生，于华译.北京：中信出版社，2019.7 辛普森悖论·维基百科 JUDEA PEARL, MADELYN GLYMOUR, NICHOLAS P. JEWELL CAUSAL INFERENCE IN STATISTICS: A PRIMER 因果推断简介之一：从 Yule-Simpson’s Paradox 讲起]]></content>
      <categories>
        <category>数据科学</category>
      </categories>
      <tags>
        <tag>数据科学</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据科学：因果推断（〇）—— 综述]]></title>
    <url>%2F%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%2F%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%2F%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%EF%BC%9A%E5%9B%A0%E6%9E%9C%E6%8E%A8%E6%96%AD%EF%BC%88%E3%80%87%EF%BC%89%E2%80%94%E2%80%94%20%E7%BB%BC%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[我们生活在一个相信大数据能够解决所有问题的时代，然而数据远非万能，数据可以告诉你服药的病人比不服药的病人康复得快，却不能告诉你原因何在。也许，那些服药的人只是因为他们支付得起，即使不服用这种药，他们也能恢复得更快。正如 Kendall 和 Stuart 所说，统计关系无论有多强，有多紧密，也决不能建立起因果关系，因果关系的概念来自统计学之外的某个理论。 因果关系（causality）因果观念是人类认知事物的重要方式，我们相信，世界并非是由简单的事实堆砌而成，相反，这些事实是通过错综复杂的因果网络联系在一起的，科学正是建立在因果律的基础之上的。关于因果的讨论，已经持续了上千年，至今仍没有统一定论，在正式讨论“因果推断”之前，我们有必要搞清楚，当我们提到“因果”时，究竟是在谈论着什么。 神话时代在神话思维时代，人类对诸如雷电、地震等自然现象都会归结为某个神灵的意志。这种拟人化的目的归因，是人类试图捕捉现象背后本质因果思维的最初尝试，并发展出交感巫术、祈祷等手段与神灵沟通，从而对自然过程进行干预。 希腊时代人类文明的轴心时代，是古希腊人最早发扬了理性精神。哲学和科学的诞生，不仅来自经验知识，更因为是有数学和几何。古希腊最早的哲学家，包括泰勒斯、毕达哥拉斯等，都同时也是数学家和自然科学家。数学对象之间的必然关系，放到经验世界，就产生了让哲学脱胎于神话的第一次天问：“世界是如何起源的？从此人类以理性思维探讨世界秩序成为了可能。 希腊哲学家对世界起源的回答，无论是水、气、火、数、逻各斯或无定形，最后都被亚里士多德总结为四种原因： 质料因（Matter - material cause）：构成事物的材料，例如木材就是桌子的质料因； 形式因（Form - formal cause）：构成事物的样式，例如木工心中桌子的样式，就是桌子的形式因； 动力因（Agent - efficient cause）：构成事物的过程，例如木工制作桌子的过程就是桌子的动力因； 目的因（Purpose - final cause）：构成事物的目的，例如放置物品就是桌子的目的因； 理性主义17 世纪，德国数学家和哲学家莱布尼茨，将自己的哲学建立在两个逻辑前提之上：矛盾律（在同一时刻，某个事物不可能在同一方面既是这样又不是这样）和充分理由律（任何事物都有其存在的充足理由）。这两个前提又都建立在一种“分析”命题的概念之上，而所谓的分析命题就是谓项被包含在主项之中的命题 —— 例如，“所有的白种人都是人”。矛盾律所陈述的是“所有分析命题都是真命题”，充分理由律所陈述的则是“所有的真命题都是分析命题”。这一点不仅适用于逻辑陈述，甚至对于那些我们必须当作关于实际问题的经验性陈述也适用。如果“我”做一次旅行，“我”的概念一定自永恒以来就将这次旅行的概念包括在内了，这次旅行就是“我”的谓项。 19 世纪德国哲学家、唯意志论创始人叔本华，在博士论文《充足理由律的四重根》中给出了莱布尼茨的充足理由律的四种表现形式： 因果关系（Becoming）：生成/变化的充足理由律，适用于现实对象； 逻辑推论（Knowing）：认识的充足理由律，适用于逻辑对象； 数学证明（Being）：存在的充足理由律，解释时间和空间的必然性； 行为动机（Willing）：行动的充足理由律，解释动机和行为之间的必然性。 经验主义18 世纪，英国经验主义哲学家休谟将因果关系限定在了经验世界的具体对象中，先后在《人性论》和《人类理智研究》中给出了因果关系两个定义： 我们无从得知因果之间的关系，只能得知某些事物总是会连结在一起，而这些事物在过去的经验里又是从不曾分开过的。我们并不能看透连结这些事物背后的理性为何，我们只能观察到这些事物的本身，并且发现这些事物总是透过一种恒常的连结而被我们在想像中归类。 —— 休谟.人性论.1739 我们可以给一个因下定义说，它是先行于、接近于另一个对象的一个对象，而且在这里，凡与前一个对象类似的一切对象都和与后一个对象类似的那些对象处在类似的先行关系和接近关系中。或者，换言之，假如没有前一个对象，那么后一个对象就不可能存在。 —— 休谟.人类理解研究.1748 在《人性论》中，休谟对因果关系的客观性提出了怀疑，认为我们只能观察到事物本身及其恒常相继发生，并不能观察到事物背后的因果链接。在《人类理解研究》中，休谟提到了反事实推理的必要因，也即“若非因”。 经典力学17 世纪，牛顿创立经典力学之后，决定论占据了所有学科领域的核心：万事万物都被包含在确定性的因果链条之中。法国数学家皮埃尔-西蒙·拉普拉斯在他的概率论导论中说： 我们可以把宇宙现在的状态视为其过去的果以及未来的因，假若一位智者知道在某一时刻所有促使自然运动的力和所有物体的位置，假若他也能够对这些数据进行分析，则在宇宙里，从最大的物体到最小的粒子，它们的运动都包含在一条简单公式里。对于这位智者来说，没有任何事物会是含糊的，并且未来只会像过去般出现在他眼前。 拉普拉斯这里所说的“智者”（intelligence）便是后人所称的拉普拉斯妖。 概率论从赖欣巴哈和萨普斯开始，哲学家们开始使用“概率提高”的概念来定义因果关系：如果 X 提高了 Y 的概率，那么我们就说 X 导致了 Y，即 $P(Y|X) &gt; P(X) =&gt; X \rightarrow Y$。这个概念也存在于我们的直觉中，并且根深蒂固。但是这种解释是错的，因为“提高”是一个因果概念，意味着 X 对 Y 的因果效应。但是，这种概率提高完全可能是其他因素造成的，比如 Y 是 X 的原因，或者其他变量是它们二者的原因。 18 世纪，一位英国长老会牧师和业余数学家托马斯·贝叶斯（Thomas Bayes），将概率现象解释为主观信念程度的变化和更新，让概率本身也失去了客观性。但自 19 世纪中叶起，随着频率学派（经典统计学派）的兴起，贝叶斯解释逐渐被统计学主流所拒绝。现代贝叶斯统计学的复兴肇始于 Jeffreys(1939)，从 1950 年代开始，经过众多统计学家的努力，贝叶斯统计学才逐渐发展壮大。 P(h|D)=P(h)\times \frac{P(D|h)}{P(D)}在形式上，贝叶斯定理只是条件概率定义的一个初等推论，但在认识论上，它远远超出了初等的范畴。事实上，它作为一种规范性规则，能够用于根据证据更新信念这一重要操作。从许多层面来说，贝叶斯定理都是对科学方法的提炼：1. 提出一个假设 $h$；2. 推断假设的可检验结果；3. 进行实验并收集证据 $D$；4. 更新对假设的信念 $P(h|D)$。 贝叶斯定理所描述的仍然是“证据”和“假设”之间的相关性，证据所带来的“信念增强”并不意味着“证据”是“假设”的原因。 统计学然而“除了物理学之外，都是集邮”（卢瑟福），纷纷效法物理学的其他自然和社会科学并没有取得想象中确定性的成功。到了19 世纪，统计学创始人高尔顿在研究“遗传均值回归”现象的过程中，以寻找因果关系为起点，最终却发现了相关性 —— 一种无视因果的关系。高尔顿的学生，作为统计学之父的卡尔·皮尔逊，则干脆用相关关系（Correlation）取代了因果关系，认为因果关系只是相关关系的一个特例。 我认为……高尔顿的本意是，存在一个比因果关系更广泛的范畴，即相关性，而因果关系只是被囊括于其中的一个有限的范畴。这种关于相关性的新概念在很大程度上将心理学、人类学、医学和社会学引向了数学处理的领域。 —— 皮尔逊.1934 一个特定的事件序列在过去已经发生并且重复发生，这只是一个经验问题，对此我们可以借助因果关系的概念给出其表达式……在任何情况下，科学都不能证明该特定事件序列中存在任何内在的必然性，也不能绝对肯定地证明它必定会重复发生。 —— 皮尔逊.科学语法.1892 皮尔逊将因果关系从统计学中剔除，取而代之的是相关关系。统计学告诉我们“相关关系不等于因果关系”，但并没有告诉我们因果关系是什么。在统计学教科书的索引里查找“因果”这个词是徒劳的。统计学不允许学生们说 X 是 Y 的原因，只允许他们说 X 与 Y “相关”或“存在关联”。统计学唯一关注的是如何总结数据，而不关注如何解释数据。 继高尔顿和皮尔逊之后，罗纳德·艾尔默·费舍尔成为当时统计学界无可争议的领袖，他简洁地描述了这种差异： 一旦你从统计学中删除因果关系，那么剩下的就只有数据约简了。 量子力学进入 20 世纪，就连在物理学中人们也发现了更多不确定性现象。量子力学对微观世界的描述，让很多人确信，世界在根基上就是不确定性的。混沌理论革命则让人们意识到，对复杂系统即使存在确定的关系，也会因为初始敏感导致计算不可约性。 在这些科学发展的背景下，不确定性完全占据了上风，大多数人认为可能只存在相关性，在科学实践和决策上也广泛采取统计学方法。科学反映客观实在的观念已一去不复返，物理定律也降格为基于某种观测数据拟合的理论模型。 因果革命2020 年 6 月 21 日，在第二届北京智源大会开幕式及全体会议上，图灵奖得主、贝叶斯网络奠基人Judea Pearl 做了名为《The New Science of Cause and Effect with reflections on data science and artificial intelligence》的主题演讲。 在演讲中，Judea Pearl 站在整个数据科学的视角，简单回顾了过去的“大数据革命”，指出数据科学正在从当前以数据为中心的范式向以科学为中心的范式偏移，现在正在发生一场席卷各个研究领域的“因果革命”。 To Build Truly Intelligent Machines, Teach Them Cause and Effect 。 ——Judea Pearl 因果革命和以数据为中心的第一次数据科学革命，也就是大数据革命（涉及机器学习，深度学习机器应用，例如 Alpha-Go、语音识别、机器翻译、自动驾驶等等）的不同之处在于，它以科学为中心，涉及从数据到政策、可解释性、机制的泛化，再到一些社会科学中的基础概念信用、责备和公平性， 甚至哲学中的创造性和自由意志 。可以说， 因果革命彻底改变了科学家处理因果问题的方式。 Judea Pearl 认为，统计学的其他分支，以及那些依赖统计学工具的学科仍然停留在禁令时代，错误地相信所有科学问题的答案都藏于数据之中，有待巧妙的数据挖掘手段将其揭示出来。因果分析绝不只是针对数据的分析，在因果分析中，我们必须将我们对数据生成过程的理解体现出来，并据此得出初始数据不包含的内容。与相关性分析和大多数主流统计学不同，因果分析要求研究者做出主观判断。研究者必须绘制出一个因果图，其反映的是他对于某个研究课题所涉及的因果过程拓扑结构的定性判断，或者更理想的是，他所属的专业领域的研究者对于该研究课题的共识。为了确保客观性，他反而必须放弃传统的客观性教条。在因果关系方面，睿智的主观性比任何客观性都更能阐明我们所处的这个真实世界。 因果定义数据科学所研究的因果关系是经验世界中事件之间的因果关系，正如休谟所言，在经验世界中，我们实际所能观测到的只是事件本身，而无法观测到隐藏在事件背后的“因果机理”，事件间的因果关系本质上是对事件序列间特定关系的概括性称谓。目前，一个被广泛接受的因果关系的定义是由 Lazarsfeld（1959）给出的： 如果变量 A 和变量 B 满足以下三个条件，则称 A 和 B 之间存在因果关系“A 导致 B”，其中 A 被称为 B 原因，B 被称为 A 的结果： A 在时间上必须先于 B； A 和 B 应当在经验上相互关联； A 和 B 之间观测到的经验相关不能被第三个导致 A 和 B 两者的变量所解释； 相关性只是因果性的一个必要非充分条件，即“相关性不一定意味着因果性”，A 和 B 相关可能是以下情形的结果： A 和 B 都由第三个变量 C 决定：如果通过控制 C，A 和 B 之间的相关性会消失，则说此相关是虚假的（spurious）；比如“是否携带打火机”与“癌症发病率”之间的相关性，本质上是因为抽烟的人通常会携带打火机，并且癌症发病率更高所导致的； A 导致 B：我们对干扰变量进行了控制，但我们仍然观测到 A 和 B 之间高度相关； B 导致 A：相关性本身并没有告诉我们因果关系的方向；比如“公鸡打鸣”和“太阳升起”有高度相关性，但是统计数据本身并不能告诉我们到底是公鸡打鸣导致了太阳升起，还是太阳升起导致了公鸡打鸣； 至此，我们已经查勘了因果观念的全景，现在可以对数据科学所涉及到的因果关系概括如下： 在经验世界中，我们所能观察到的只是事件（数据）本身，而如果仅凭数据间的关联，我们只能得到事件间的相关性，事件间的因果关系是对事件序列特定关系的概括：如果 A 和 B 同时满足以下条件 ① A 在时间上先于 B；② A 和 B 在经验上相关；③ A 和 B 间的相关性不能被其他变量所解释；则称 A 是 B 的原因，或称 A 导致了 B。 因果推断（Causal Inference）因果推断是研究变量间因果关系的学科，作为一门学科，因果推断目前仍然处于大众视野之外。朱迪亚·珀尔（Judea Pearl） 认为，一旦我们真正理解了因果思维背后的逻辑，就可以在现代计算机上模拟它，进而创造出一个“人工科学家”。这个智能机器人将会为我们发现未知的现象，解开悬而未决的科学之谜，设计新的实验，并不断从环境中提取更多的因果知识。 关于因果推断的讨论，可以有两个方向： 考察结果的原因：看到结果，寻找结果背后的原因，这种研究往往是科学的起点，但寻找结果背后的原因，非常复杂。某一种结果产生的原因可能有很多，需要通过详细的调查、深入的分析才能找到。 考察原因的结果：主要关注某一干预对结果的影响，一项干预对结果变量产生的影响，通常称为因果效应（causal effects）或干预效应（treatment effects）。 问题定义按照所能回答问题的类型，Judea Pearl 将因果信息划分成了三个层级，其中，高层级信息可以回答低层级问题，但是低层级信息无法回答高层级问题： 层级 任务 活动 符号 问题 例子 评价 关联 基于被动观察做出预测 观察 $P(Y\mid X)$ 如果观察到X，如何预测Y？ 购买啤酒的用户多大可能会购买尿布？ 好的预测无需好的解释（因果） 当前机器学习/深度学习/统计学几乎完全是在关联层级下，由一系列观察数据拟合出一个函数 干预 基于主动干预做出评估 行动 $P(Y\mid do(X))$ 如果改变X，Y会怎样？ 如果价格提高两倍，销量会怎么变化？ 预测干预结果的方法是在严格控制的条件下进行实验 反事实 通过因果模型做出预测 想象 $P(y_x \mid X’,Y’)$ 假如观察到的不是X’，Y会怎样? 假如过去没有抽烟，现在身体会更好吗？ 预测在尚未经历甚至未曾设想过的情况下会发生什么——这是所有科学的圣杯 Judea Pearl 在《The Book of Why》一书中对以上三种因果层级进行了详细描述，并将其称为“因果关系之梯”： Judea Pearl 认为，人类的大脑拥有某种简洁的信息表示方式，同时还拥有某种十分有效的程序用以正确解释每个问题，并从存储的信息表示中提取正确答案，这就是因果图。Judea Pearl 通过一个被他称作“迷你图灵测试”的例子，借助因果图语言介绍了以上三种因果层级之间的差异。 如下图所示，假设一个犯人将要被执行枪决，这件事的发生必然会以一连串的事件发生为前提：首先，法院方面要下令处决犯人；命令下达到行刑队长后，他将指示行刑队的士兵（A 和 B）执行枪决；我们假设他们是服从命令的专业抢手，只听命令射击，并且只要其中任何一个抢手开了枪，囚犯都必死无疑。借助这个因果图，我们就可以回答来自因果关系之梯不同层级的因果问题了。 （1）首先，我们可以回答关联问题（一个事实告诉我们有关另一事实的什么信息）。一个可能的问题是，如果犯人死了，那么这是否意味着法院已下令处决犯人？我们（或一台计算机）可以通过核查因果图，追踪每个箭头背后的规则，并根据标准逻辑得出结论：如果没有行刑队队长的命令，两名士兵就不会射击。同样，如果行刑队队长没有接到法院的命令，他就不会发出执行枪决的命令。因此，这个问题的答案是肯定的。另一个可能的问题是，假设我们发现士兵 A 射击了，它告诉了我们关于 B 的什么信息？通过追踪箭头，计算机将断定B一定也射击了。（原因在于，如果行刑队队长没有发出射击命令，士兵A就不会射击，因此接收到同样命令的士兵B也一定射击了。）即使士兵 A 的行为不是士兵 B 做出某一行为的原因（因为从 A 到 B 没有箭头），该判断依然为真。 （2）沿着因果关系之梯向上攀登，我们可以提出有关干预的问题。如果士兵 A 决定按自己的意愿射击，而不等待队长的命令，情况会怎样？犯人会不会死？如果我们希望计算机能理解因果关系，我们就必须教会它如何打破规则，让它懂得“观察到某事件”和“使某事件发生”之间的区别。我们需要告诉计算机：“无论何时，如果你想使某事发生，那就删除指向该事的所有箭头，之后继续根据逻辑规则进行分析，就好像那些箭头从未出现过一样。”如此一来，对于这个问题，我们就需要删除所有指向被干预变量（A）的箭头，并且还要将该变量手动设置为规定值（真）。这种特殊的“外科手术”的基本原理很简单：使某事发生就意味着将它从所有其他影响因子中解放出来，并使它受限于唯一的影响因子——能强制其发生的那个因子。下图表示出了根据这个例子生成的因果图，显然，这种干预会不可避免地导致犯人的死亡，这就是箭头 A 到 D 背后的因果作用。同时，我们还能判断出：B（极有可能）没有开枪，A 的决定不会影响模型中任何不受 A 的行为的影响的其他变量。需要注意的是，仅凭收集大数据无助于我们登上因果关系之梯去回答上面的问题。假设你是一个记者，每天的工作就是记录行刑场中的处决情况，那么你的数据会由两种事件组成：要么所有 5 个变量都为真，要么所有都为假。在未掌握“谁听从于谁”的相关知识的情况下，这种数据根本无法让你（或任何机器学习算法）预测“说服枪手 A 不射击”的结果。 （3）最后，为了说明因果关系之梯的第三层级，我们提出一个反事实问题。假设犯人现在已倒地身亡，从这一点我们（借助第一层级的知识）可以得出结论：A射击了，B射击了，行刑队队长发出了指令，法院下了判决。但是，假如 A 决定不开枪，犯人是否还活着？这个问题需要我们将现实世界和一个与现实世界相矛盾的虚构世界进行比较。在虚构世界中，A 没有射击，指向 A 的箭头被去除，这进而又解除了 A 与 C 的听命关系。现在，我们将A的值设置为假，并让A行动之前的所有其他变量的水平与现实世界保持一致。如此一来，这一虚构世界就如下图所示。为通过迷你图灵测试，计算机一定会得出这样的结论：在虚构世界里犯人也会死，因为B会开枪击毙他。所以，A勇敢改变主意的做法也救不了犯人的命。 看起来，我们刚刚像是花了很大一番力气回答了一些答案显而易见的小问题。的确，因果推理对你来说很容易，其原因在于你是人类，在你还是三岁儿童时，你所拥有的功能神奇的大脑就比任何动物或计算机都更能理解因果关系。“迷你图灵问题”的重点就是要让计算机也能够进行因果推理，而我们能从人类进行因果推断的做法中得到启示。如上述三个例子所示，我们必须教会计算机如何有选择地打破逻辑规则。计算机不擅长打破规则，而这是儿童的强项。 数据来源用于因果推断的数据来源一般有三种： 控制实验：对于实验组和控制组，严格控制混淆变量，结果的差异可以归因于原因变量的差异；控制实验对实验条件要求苛刻，一般用于自然科学研究领域； 随机实验：Fisher 认为我们不必控制其他变量差异，现实中也没有办法完全控制所有的其他变量，只要让随机机制决定干预变量的分配，就可以获得正确的因果效应；随机试验被称为因果推断的黄金标准； 观察实验：在很多场景下，尤其是在社会科学领域，我们既没有办法实施控制实验，也没有办法实施随机实验，只能获取到被动观察的自然数据；此时，可以通过一些近似手段模拟随机试验过程，进行因果推断； 理论基础识别策略参考 Judea Pearl.The Book of Why: the new science of cause and effect 赵西亮.基本有用的计量经济学 罗素.西方哲学史 因果观念新革命？万字长文，解读复杂系统背后的暗因果 Judea Pearl.The Seven Tools of Causal Inference, with Reflections on Machine Learning 图灵奖得主Judea Pearl：从“大数据革命”到“因果革命” Judea Pearl.The New Science of Cause and Effect with reflections on data science and AI 视频 Foundations and new horizons for causal inference 研讨会, 2019（因果推断始于经济和生物统计等学科，它刚刚才开始成为人工智能的一个重要工具，数学基础依旧很零碎，该研讨会聚集了来自人工智能，生物统计学，计算机科学，经济学，流行病学，机器学习，数学和统计学的顶尖研究人员，研讨会上的报告和讨论将有助于在未来几年内塑造和改变这一领域的发展） Causality for Machine Learning, Bernhard Schölkopf, 2019（这是一篇刚刚挂 arxiv 就被 Pearl 亲自 twitter 点赞的论文，是马普智能所所长 Bernhard Scholkopf 最引以为傲的论文之一，他将被 Pearl 点赞这事情写在其个人主页自我介绍的第一段中。Scholkopf 及其团队在因果结合机器学习方面做了最多的工作，此文总结和升华了提出了信息革命时代下因果结合机器学习的一般理论和深刻思考） A Second Chance to Get Causal Inference Right: A Classification of Data Science Tasks, Miguel A. Hernán, John Hsu &amp;Brian Healy, 2019（来自哈佛教授 Migual A. Hernan 对当前数据科学的深刻反思，澄清了数据科学任务如何分类的基本问题：prediction, deion and counterfactual prediction.） Causal Inference and Data-Fusion in Econometrics, P. Hünermund, E. Bareinboim.Dec, 2019.(该论文是因果革命，Pearl 的因果图模型框架如何影响某一个特定领域—计量经济学的范例)]]></content>
      <categories>
        <category>数据科学</category>
      </categories>
      <tags>
        <tag>数据科学</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据科学：综述（一）—— 工作内容]]></title>
    <url>%2F%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%2F%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%2F%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%EF%BC%9A%E7%BB%BC%E8%BF%B0%EF%BC%88%E4%B8%80%EF%BC%89%E2%80%94%E2%80%94%20%E5%B7%A5%E4%BD%9C%E5%86%85%E5%AE%B9%2F</url>
    <content type="text"><![CDATA[本文转载自 One Data Science Job Doesn’t Fit All 在一家高速增长的公司里，当一名领导者的乐趣之一就是你不仅有机会去改变一些事情 —— 你还必须主动驱动变革以跟上步伐。而在数据科学（DS）这个新的、快速发展的领域工作，我们将同时置身于公司和行业的快速变化之中。 在 Airbnb，我们把数据看作是用户的声音，我们的目标是让数据科学家最大程度地发挥他们的影响，并对自己的工作充满期待。我们正在朝着这个方向努力，也一直在寻找改进的方法。作为这一演变的一部分，我们最近建立了一个角色定义框架，我希望我们在此过程中学到的知识可以对其他公司在定义数据科学角色方面具有参考意义。 我要分享的主要结论是：为了满足业务的需求，公司会考虑数据科学工作的三个通道 —— 分析、推断、算法。下面我将描述我们是如何发展到这三条工作通道上的，以及它是如何帮助我们的。 数据科学家的其他名称我们从“分析团队”开始，最初雇用的是“分析专家”。 2012年，我被聘为“数据科学家”。后来，我们聘请了“数据架构师”来处理数据质量，然后聘请了“数据分析专家”来帮助解决数据访问和工具方面的空白。然后，我们看到了机器学习方面的其他需求，因此我们聘请了“机器学习数据科学家”。这些头衔的演变既是对团队需求的反应，也是对竞争格局的反应。我们在2015年成为了“数据科学”部门，尽管我们仍然使用“ A-team”，因为它很有趣并且拥有我们重视的历史。 当我在2017年中担任数据科学职能部门的负责人时，我们大约有80位数据科学家分布在各个团队中。一些正在构建报表，一些正在构建NLP（自然语言处理）模型，另一些正在构建用于决策和设计实验的模型。 新兴学科快速发展这种变化并不完全出乎意料，数据科学相对较新，而且发展迅速。我们在数据中看到了这一点。首先，从内部来看，我们发现 Airbnb 数据科学角色在2015-2018年间增长了4倍： 而且，根据谷歌趋势数据，对数据科学的查询也在增长： 数据科学不仅是一个新的领域，人们所说的“数据科学”的含义也千差万别，有时候，这纯粹是机器学习。有时是科技公司的商业智能。它是新的，而且在进化。 认识到科学技能的多样性我们发现人们对数据科学的预期并不明确。在一个给定的公司中，这种多样性的缺点是，它可能导致组织混乱和人员流失，因为合作伙伴团队不知道从数据科学家那里得到什么，而数据科学家自己可能也不清楚他们的角色。那些来自 DS 只做建模的地方的人可能不认为数据科学技能能很好地用于更简单的分析。其他来自 DS 只做分析的地方的人可能会觉得最好让工程师做建模。 我们还有一个额外的挑战：从事分析工作的团队成员觉得他们的工作没有机器学习工作那么重要，但他们的工作对业务至关重要。业务合作伙伴渴望更具可操作性的见解，以推动决策，并扩展工具以了解数据本身。我们通过我们非常受欢迎的数据大学对数据教育进行了投资，但我们仍然需要专家。我们确定的一个原因是，虽然团队成员是“数据科学”职能的一部分，但我们使用的是“数据分析专家”的头衔，而且我们谈论“数据科学工作”的方式中有一些暗示，给人的印象是，分析工作并不同等重要。 我与同行公司的领导进行了交谈，以了解其他团队是如何处理这一问题的——有一次，我甚至创建了一个与不同组织结构共享的电子表格。我听说过新的分析团队从零开始创建，团队从机器学习中分离出来，工具团队被整合到数据科学中，等等。 很明显，没有一刀切的方法，但在定义我们是谁以及如何增加价值方面，具有战略性和有意识的态度将是至关重要的。我们知道我们的目标是“捍卫使命”，即完成公司最需要的工作。因此，我们需要符合当前业务需求的角色，同时也允许个性化和明确的期望。 解决办法：数据科学工作的三种风味我们决定沿着三个方向来重构数据科学，这三个方向描述了我们正在追寻的东西，也是我们想要吸引人才的领域：The Algorithms track would be the home for those with expertise in machine learning, passionate about creating business value by infusing data in our product and processes. And the Inference track would be perfect for our statisticians, economists, and social scientists using statistics to improve our decision making and measure the impact of our work. 对于那些善于提出好的问题、善于以揭示性的方式探索数据、善于通过报表和可视化工具进行自动分析、善于通过建议来驱动业务变化的人来说，Analytics 通道是理想的选择； 对于那些在机器学习方面具有专业知识，热衷于通过在我们的产品和流程中注入数据来创造业务价值的人来说，Algorithms 通道将成为他们的家； 对于我们的统计学家，经济学家和社会科学家来说，Inference 通道将是完美的选择，他们可以使用统计信息来改善我们的决策制定并衡量工作的影响； 团队中的每一位数据科学家都应具备这些领域的专业知识，并根据业务需求和自身兴趣获得这些领域的技能。在每一个通道中都可以有进一步的专业化，但是每个人都有“数据科学家”的头衔，然后下面的描述提供了更清晰的描述。 如果我们看另一门学科，比如工程学，这里有“前端”和“后端”工程学的简写，它可以帮助你了解某人的技能或关注的领域。我意识到这是一个不完美的区别，但它比简单的“工程”更能让人感觉到某人的专业知识。数据科学离这一点还很远；这是我们正在朝着的方向发展。 明确预期我们还修改了我们的绩效评估标准，以反映我们的新结构。我们有多层次的数据科学家和管理者，我们通过观察对业务的影响来定义成功。对于那些在技术通道上的人，我们修改了我们的评估框架，使之与这些主要领域保持一致。 技术方面： 分析：定义并监控指标，创建数据描述，并构建工具来推动决策； 算法：构建并解释驱动数据产品的算法； 推理：利用统计数据建立因果关系； 基础：展示数据质量和代码的所有权和责任（所有通道都需要）； 业务方面（适用于所有通道）： Ownership：能够推动项目取得成功，帮助他人，拥有影响力； 影响力：清晰沟通，展示团队合作精神，建立人际关系； Enrichment：通过指导、文化、招聘和多元化努力促进团队建设； 我们可以在这里写很多东西，但主要的收获是，我们也明确改变了我们评估绩效的方式，以反映工作的三个方面，并明确了期望。 何时专业化Airbnb 足够大，拥有所有这些区别和细微差别是有意义的。当与那些想知道是否应该用专家组建团队的小公司交谈时，我建议他们从通用性开始。在早期，我们能够处理任何最紧迫的项目，而不是坐在一个僵硬的专业里，这真的很有帮助。随着时间的推移，专业化是有意义的，但最好是从通用开始，除非你能更早地看到它的商业案例。我们直到 2015 年左右才开始专攻，那时我们的团队只有 30 人。 我们还希望随着业务需求的变化，继续改变职能部门的角色。 从中获益即使是在我们的专业领域，每个领域的数据科学家也会从事其他类型的工作，我们鼓励团队成员也成为多面手（有时这是一个混乱的问题）。总体而言，进行此更改后，我们所听到的混乱少了很多。我也开始听到合作伙伴说诸如“我们需要具有推理和算法专业知识的人”之类的东西。因此，该语言对于传达业务需求非常有用。 这有助于我们找出差距。我最近联系了一位产品经理，她表示担心团队没有想出创新的方法来在她富有挑战性的产品领域进行实验。我立刻诊断出了这个问题：在那个特定的数据科学团队中，没有一个具备推理专业知识的人。这是我们下一次招聘时可以解决的问题，或者鼓励团队成员向其他推理专家学习。 我们很高兴听到从事分析工作的团队成员不再感到疏远或自卑。分析专家了解，如果他们尝试将机器学习应用于他们正在处理的业务问题，那么它们的影响将较小。 Where we go from here我希望与大家分享我们的故事，希望其他公司也能采用这个框架！当应聘者带着一个模糊的“数据科学”的名字，这可能意味着很多不同的东西，招聘就变得复杂起来。如果所有公司都使用类似的框架，这将使数据科学作为一个整体更容易传达我们的价值观。 如果您喜欢这个概念，请告诉您的数据科学领导者，或者如果您是数据科学的领导者，请自己进行更改。或者，如果你有一个更好的模型，我也很乐意听到这个-请伸出援手(data-science-org-ideas@airbnb.com). 考虑到数据科学领域是多么的新和快速发展，最好的命名约定将随着时间的推移而演变。在数据科学领域，我们越能联合起来制定规范，我们的行业就越快成熟，我们作为个人就越有能力驾驭它。 参考 5篇必读的数据科学论文 One Data Science Job Doesn’t Fit All]]></content>
      <categories>
        <category>数据科学</category>
      </categories>
      <tags>
        <tag>数据科学</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark 指南：Spark 原理（三）—— 内存管理]]></title>
    <url>%2FSpark%2FSpark%2FSpark%20%E6%8C%87%E5%8D%97%EF%BC%9ASpark%20%E5%8E%9F%E7%90%86%EF%BC%88%E4%B8%89%EF%BC%89%E2%80%94%E2%80%94%20%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%2F</url>
    <content type="text"><![CDATA[原文最初由 IBM developerWorks 中国网站发表，本文在此基础上进行了总结梳理，仅作为个人学习使用。 Spark 作为一个基于内存的分布式计算引擎，其内存管理模块至关重要。理解 Spark 内存管理的基本原理，有助于更好地开发 Spark 应用程序和性能调优。本文基于 Spark 2.1 版本，旨在梳理 Spark 内存管理的基本脉络。 在执行 Spark 应用程序时，Spark 集群会启动 Driver 和 Executor 两种 JVM 进程： Driver 为主控进程，主要负责： 创建 Spark 上下文； 提交 Spark 作业（Job）； 在各 Executor 进程间分配、协调任务（Task）调度； Executor 主要负责： 在工作节点上执行具体的计算任务（Task）； 将结果返回给 Driver； 为需要持久化的 RDD 提供存储功能； 由于 Driver 的内存管理相对简单，本文主要对 Executor 的内存管理进行分析，下文中 Spark 内存均指 Executor 内存。 内存规划作为一个 JVM 进程，Executor 的内存管理建立在 JVM 的内存管理之上，Spark 对 JVM 的堆内（On-heap）空间进行了更为详细的分配，以充分利用内存。同时，Spark 引入了堆外（Off-heap）内存，使之可以直接在工作节点的系统内存中开辟空间，进一步优化了内存的使用。 堆内内存（On-Heap）Executor 内运行的并发任务共享 JVM 堆内内存，堆内内存的大小由 Spark 应用程序启动时的 –executor-memory 或 spark.executor.memory 参数配置（默认 1g），Spark 对堆内内存进行了详细的规划： 统一内存（Unified）：Spark 1.6 之后引入了统一内存管理机制，存储内存和执行内存共享该块空间，可以动态占用对方的空闲区域，统一内存的大小（占堆内内存比例）可以通过 Spark 参数 spark.memory.fraction 来设置（默认 0.6） 存储内存（Storage）：在缓存 RDD 或广播（Broadcast）数据时占用的内存被规划为存储内存，存储内存的大小（占统一内存比例）可以通过 Spark 参数 spark.memory.storagefraction 来设置（默认 0.5）； 执行内存（Execution）：在执行 Shuffle、Join、Sort、Aggregation 等转换时占用的内存被规划为执行内存； 剩余内存（Other）：Spark 内部的对象实例，或者用户定义的 Spark 应用程序中的对象实例，元数据占用剩余内存，Spark 对剩余内存不做特殊规划； 预留内存（Reserved）：默认 300M 的系统预留内存，主要用于程序运行，参见SPARK-12081； 总结堆内内存的规划大小计算公式如下： 规划项 计算公式 默认值 堆内内存（On-Heap） spark.executor.memory 1g 统一内存（Unified） spark.executor.memory * spark.memory.fraction 1g * 0.6 = 600M 存储内存（Storage） spark.executor.memory * spark.memory.fraction * spark.memory.storagefraction 1g 0.6 0.5 = 300M 执行内存（Execution） spark.executor.memory * spark.memory.fraction * (1 - spark.memory.storagefraction) 1g 0.6 (1-0.5) = 300M 剩余内存（Other） spark.executor.memory * (1 - spark.memory.fraction) 1g * (1-0.6) = 400M 预留内存（Reserved） 300M 300M Spark 对堆内内存的管理只是一种”规划式“的管理，因为对象实例占用内存的申请和释放都由 JVM 完成，Spark 只能在申请和释放前记录这些内存，其具体流程为： 申请内存： Spark 在代码中创建一个对象实例； JVM 从堆内内存分配空间，创建对象并返回对象引用； Spark 保存该对象的引用，记录该对象占用的内存； 释放内存： Spark 记录该对象释放的内存，删除该对象的引用； 等待 JVM 的垃圾回收机制释放该对象占用的堆内内存； JVM 对象可以以序列化（将对象转化为二进制字节流）的方式存储，本质上可以理解为将非连续的链式存储转化为连续存储，在访问时则需要进行反序列化（将字节流转化为对象），这种方式节省了空间，但是增加了存储和读取的计算开销。对于序列化对象，由于是字节流的形式，其占用的内存大小可以直接计算，而对于非序列化对象，其占用的内存则通过周期采样近似估算，这种方式降低了时间开销但是可能误差较大，导致某一时刻的实际内存有可能远远超出预期。此外，在被 Spark 标记为释放的对象实例，很有可能在实际上并没有被 JVM 回收，导致实际可用的内存小于 Spark 记录的可用内存。所以 Spark 并不能准确记录实际可用的堆内内存，从而也就无法完全避免内存溢出（OOM, Out of Memory）的异常。 统一内存管理Spark 1.6 之后引入了统一内存管理机制，存储内存和执行内存共享同一块空间，可以动态占用对方的空闲区域，如图所示： 统一内存的动态占用机制： 当存储内存和执行内存都不足时，则存储到磁盘；当己方空间不足而对方空间空余时，可借用对方空间； 执行内存被存储占用时，可以让对方将占用的部分转存到硬盘，归还借用的空间； 存储内存被执行占用时，无法让对方归还，因为考虑 Shuffle 过程的很多因素，不好实现； 凭借统一内存管理机制，Spark 在一定程度上提高了堆内和堆外内存资源的利用率，降低了开发者维护 Spark 内存的难度，但并不意味着开发者可以高枕无忧。譬如，所以如果存储内存的空间太大或者说缓存的数据过多，反而会导致频繁的全量垃圾回收，降低任务执行时的性能，因为缓存的 RDD 数据通常都是长期驻留内存的。所以要想充分发挥 Spark 的性能，需要开发者进一步了解存储内存和执行内存各自的管理方式和实现原理。 存储内存管理RDD 持久化机制弹性分布式数据集（RDD）作为 Spark 最根本的数据抽象，是只读的分区记录（Partition）的集合，只能基于在稳定物理存储中的数据集上创建，或者在其他已有的 RDD 上执行转换（Transformation）操作产生一个新的 RDD。转换后的 RDD 与原始的 RDD 之间产生的依赖关系，构成了血统（Lineage）。凭借血统，Spark 保证了每一个 RDD 都可以被重新恢复。但 RDD 的所有转换都是惰性的，即只有当一个返回结果给 Driver 的行动（Action）发生时，Spark 才会创建任务读取 RDD，然后真正触发转换的执行。 Task 在启动之初读取一个分区时，会先判断这个分区是否已经被持久化，如果没有则需要检查 Checkpoint 或按照血统重新计算。所以如果一个 RDD 上要执行多次 Action，可以在第一次 Action 中使用 persist 或 cache 方法，在内存或磁盘中持久化或缓存这个 RDD，从而在后面的行动时提升计算速度。事实上，cache 方法是使用默认的 MEMORY_ONLY 的存储级别将 RDD 持久化到内存，故缓存是一种特殊的持久化。 堆内和堆外存储内存的设计，便可以对缓存 RDD 时使用的内存做统一的规划和管理。 RDD 的持久化由 Spark 的 Storage 模块负责，实现了 RDD 与物理存储的解耦合。Storage 模块负责管理 Spark 在计算过程中产生的数据，将那些在内存或磁盘、在本地或远程存取数据的功能封装了起来。在具体实现时 Driver 端和 Executor 端的 Storage 模块构成了主从式的架构，即 Driver 端的 BlockManager 为 Master，Executor 端的 BlockManager 为 Slave。Storage 模块在逻辑上以 Block 为基本存储单位，RDD 的每个 Partition 经过处理后唯一对应一个 Block（BlockId 的格式为 rdd_RDD-ID_PARTITION-ID ）。Master 负责整个 Spark 应用程序的 Block 的元数据信息的管理和维护，而 Slave 需要将 Block 的更新等状态上报到 Master，同时接收 Master 的命令，例如新增或删除一个 RDD。 在对 RDD 持久化时，Spark 规定了 MEMORY_ONLY、MEMORY_AND_DISK 等 7 种不同的 存储级别 ，而存储级别是以下 5 个变量的组合： 1234567class StorageLevel private( private var _useDisk: Boolean, // 磁盘 private var _useMemory: Boolean, // 堆内内存 private var _useOffHeap: Boolean, // 堆外内存 private var _deserialized: Boolean, // 是否为非序列化 private var _replication: Int = 1 // 副本个数) 通过对数据结构的分析，可以看出存储级别从三个维度定义了 RDD 的 Partition（同时也就是 Block）的存储方式： 存储位置：磁盘／堆内内存／堆外内存。如 MEMORY_AND_DISK 是同时在磁盘和堆内内存上存储，实现了冗余备份。OFF_HEAP 则是只在堆外内存存储，目前选择堆外内存时不能同时存储到其他位置； 存储形式：Block 缓存到存储内存后，是否为非序列化的形式。如 MEMORY_ONLY 是非序列化方式存储，OFF_HEAP 是序列化方式存储； 副本数量：大于 1 时需要远程冗余备份到其他节点。如 DISK_ONLY_2 需要远程备份 1 个副本； RDD 缓存过程RDD 缓存的过程是将对象从 other 内存区迁移至 Storage 区或 Disk 的过程： RDD 在缓存到存储内存之前：Partition 中的数据一般以迭代器（Iterator）的数据结构来访问，这是 Scala 语言中一种遍历数据集合的方法。通过 Iterator 可以获取分区中每一条序列化或者非序列化的数据项(Record)，这些 Record 的对象实例在逻辑上占用了 JVM 堆内内存的 other 部分的空间，同一 Partition 的不同 Record 的空间并不连续； RDD 在缓存到存储内存之后：Partition 被转换成 Block，Record 在堆内或堆外存储内存中占用一块连续的空间，当存储空间不足时会根据动态占用机制进行处理。将 Partition 由不连续的存储空间转换为连续存储空间的过程，Spark 称之为”展开”（Unroll）。Block 有序列化和非序列化两种存储格式，具体以哪种方式取决于该 RDD 的存储级别 非序列化的 Block 以一种 DeserializedMemoryEntry 的数据结构定义，用一个数组存储所有的对象实例； 序列化的 Block 则以 SerializedMemoryEntry的数据结构定义，用字节缓冲区（ByteBuffer）来存储二进制数据。每个 Executor 的 Storage 模块用一个链式 Map 结构（LinkedHashMap）来管理堆内和堆外存储内存中所有的 Block 对象的实例[6]，对这个 LinkedHashMap 新增和删除间接记录了内存的申请和释放； 因为不能保证存储空间可以一次容纳 Iterator 中的所有数据，当前的计算任务在 Unroll 时要向 MemoryManager 申请足够的 Unroll 空间来临时占位，空间不足则 Unroll 失败，空间足够时可以继续进行。对于序列化的 Partition，其所需的 Unroll 空间可以直接累加计算，一次申请。而非序列化的 Partition 则要在遍历 Record 的过程中依次申请，即每读取一条 Record，采样估算其所需的 Unroll 空间并进行申请，空间不足时可以中断，释放已占用的 Unroll 空间。如果最终 Unroll 成功，当前 Partition 所占用的 Unroll 空间被转换为正常的缓存 RDD 的存储空间，如下图所示： 淘汰和落盘由于同一个 Executor 的所有的计算任务共享有限的存储内存空间，当有新的 Block 需要缓存但是剩余内存空间不足且无法动态占用时，就要对 LinkedHashMap 中的旧 Block 进行淘汰（Eviction），而被淘汰的 Block 如果其存储级别中同时包含存储到磁盘的要求，则要对其进行落盘（Drop），否则直接删除该 Block。 存储内存的淘汰规则为： 被淘汰的旧 Block 要与新 Block 的 MemoryMode 相同，即同属于堆外或堆内内存； 新旧 Block 不能属于同一个 RDD，避免循环淘汰； 旧 Block 所属 RDD 不能处于被读状态，避免引发一致性问题； 遍历 LinkedHashMap 中 Block，按照最近最少使用（LRU）的顺序淘汰，直到满足新 Block 所需的空间。其中 LRU 是 LinkedHashMap 的特性； 存储内存的落盘规则为：如果其存储级别符合 _useDisk 为 true 的条件，再根据其_deserialized 判断是否是非序列化的形式，若是则对其进行序列化，最后将数据存储到磁盘，在 Storage 模块中更新其信息。 执行内存管理多任务间内存分配Executor 内运行的任务同样共享执行内存，Spark 用一个 HashMap 结构保存了任务到内存耗费的映射。每个任务可占用的执行内存大小的范围为 1/2N ~ 1/N，其中 N 为当前 Executor 内正在运行的任务的个数。每个任务在启动之时，要向 MemoryManager 请求申请最少为 1/2N 的执行内存，如果不能被满足要求则该任务被阻塞，直到有其他任务释放了足够的执行内存，该任务才可以被唤醒。 Shuffle 内存占用执行内存主要用来存储任务在执行 Shuffle 时占用的内存，Shuffle 是按照一定规则对 RDD 数据重新分区的过程，我们来看 Shuffle 的 Write 和 Read 两阶段对执行内存的使用： Shuffle Write 若在 map 端选择普通的排序方式，会采用 ExternalSorter 进行外排，在内存中存储数据时主要占用堆内执行空间； 若在 map 端选择 Tungsten 的排序方式，则采用 ShuffleExternalSorter 直接对以序列化形式存储的数据排序，在内存中存储数据时可以占用堆外或堆内执行空间，取决于用户是否开启了堆外内存以及堆外执行内存是否足够； Shuffle Read 在对 reduce 端的数据进行聚合时，要将数据交给 Aggregator 处理，在内存中存储数据时占用堆内执行空间； 如果需要进行最终结果排序，则要再次将数据交给 ExternalSorter 处理，占用堆内执行空间； 在 ExternalSorter 和 Aggregator 中，Spark 会使用一种叫 AppendOnlyMap 的哈希表在堆内执行内存中存储数据，但在 Shuffle 过程中所有数据并不能都保存到该哈希表中，当这个哈希表占用的内存会进行周期性地采样估算，当其大到一定程度，无法再从 MemoryManager 申请到新的执行内存时，Spark 就会将其全部内容存储到磁盘文件中，这个过程被称为溢存(Spill)，溢存到磁盘的文件最后会被归并(Merge)。 Shuffle Write 阶段中用到的 Tungsten 是 Databricks 公司提出的对 Spark 优化内存和 CPU 使用的计划，解决了一些 JVM 在性能上的限制和弊端。Spark 会根据 Shuffle 的情况来自动选择是否采用 Tungsten 排序。Tungsten 采用的页式内存管理机制建立在 MemoryManager 之上，即 Tungsten 对执行内存的使用进行了一步的抽象，这样在 Shuffle 过程中无需关心数据具体存储在堆内还是堆外。每个内存页用一个 MemoryBlock 来定义，并用 Object obj 和 long offset 这两个变量统一标识一个内存页在系统内存中的地址。堆内的 MemoryBlock 是以 long 型数组的形式分配的内存，其 obj 的值为是这个数组的对象引用，offset 是 long 型数组的在 JVM 中的初始偏移地址，两者配合使用可以定位这个数组在堆内的绝对地址；堆外的 MemoryBlock 是直接申请到的内存块，其 obj 为 null，offset 是这个内存块在系统内存中的 64 位绝对地址。Spark 用 MemoryBlock 巧妙地将堆内和堆外内存页统一抽象封装，并用页表(pageTable)管理每个 Task 申请到的内存页。Tungsten 页式管理下的所有内存用 64 位的逻辑地址表示，由页号和页内偏移量组成： 页号：占 13 位，唯一标识一个内存页，Spark 在申请内存页之前要先申请空闲页号。 页内偏移量：占 51 位，是在使用内存页存储数据时，数据在页内的偏移地址。 有了统一的寻址方式，Spark 可以用 64 位逻辑地址的指针定位到堆内或堆外的内存，整个 Shuffle Write 排序的过程只需要对指针进行排序，并且无需反序列化，整个过程非常高效，对于内存访问效率和 CPU 使用效率带来了明显的提升。 Spark 的存储内存和执行内存有着截然不同的管理方式：对于存储内存来说，Spark 用一个 LinkedHashMap 来集中管理所有的 Block，Block 由需要缓存的 RDD 的 Partition 转化而成；而对于执行内存，Spark 用 AppendOnlyMap 来存储 Shuffle 过程中的数据，在 Tungsten 排序中甚至抽象成为页式内存管理，开辟了全新的 JVM 内存管理机制。 堆外内存（Off-Heap）为了进一步优化内存的使用以及提高 Shuffle 时排序的效率，Spark 引入了堆外（Off-heap）内存，使之可以直接在工作节点的系统内存中开辟空间，存储经过序列化的二进制数据。利用 JDK Unsafe API（从 Spark 2.0 开始，在管理堆外的存储内存时不再基于 Tachyon，而是与堆外的执行内存一样，基于 JDK Unsafe API 实现），Spark 可以直接操作系统堆外内存，减少了不必要的内存开销，以及频繁的 GC 扫描和回收，提升了处理性能。堆外内存可以被精确地申请和释放，而且序列化的数据占用的空间可以被精确计算，所以相比堆内内存来说降低了管理的难度，也降低了误差。 在默认情况下堆外内存并不启用，可通过配置 spark.memory.offHeap.enabled 参数启用，并由 spark.memory.offHeap.size 参数设定堆外空间的大小。除了没有 other 空间，堆外内存与堆内内存的划分方式相同，所有运行中的并发任务共享存储内存和执行内存。 运行实例假设 Spark 应用程序运行参数设置如下： Spark 应用程序运行过程中，我们可以在 Web UI -&gt; Executors 中查看 Excutor 内存实际使用大小/内存规划大小： 从该实例可以看出 Executor 的统一内存为 5.9G，与理论计算出来的值相近（10G * 0.6 = 6G），存储内存为 5.8 G，动态占用机制使得存储内存占用了绝大部分统一内存，导致只有很少的内存用于 Shuffle，这也是影响本任务执行效率的关键问题。此外，Driver 的内存基本没有被存储占用，有充足的内存可以用于执行 Spark 程序，可以适当减少 Driver 端内存分配。 进一步考察存储内存占用过高的原因，可以看到该程序缓存了非常大的中间结果，可以选择把缓存数据全部存储到磁盘，在这个场景下不会对缓存过程有太大影响，却可以保证充足的执行内存： 参考Apache Spark 内存管理详解Spark 配置yarn 资源管理参数设置Spark 性能优化指南(官网文档)]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark 指南：Spark 原理（二）—— Partition 和 Shuffle]]></title>
    <url>%2FSpark%2FSpark%2FSpark%20%E6%8C%87%E5%8D%97%EF%BC%9ASpark%20%E5%8E%9F%E7%90%86%EF%BC%88%E4%BA%8C%EF%BC%89%E2%80%94%E2%80%94%20partition%20%E5%92%8C%20Shuffle%2F</url>
    <content type="text"><![CDATA[分区分区（Partition）是控制 RDD 在各节点上分布情况的高级特性，RDD 的存储和计算都是基于分区来进行的。为分布式数据集选择正确的分区方式和为本地数据选择合适的数据结构很相似 —— 数据分布都会极其明显地影响程序的性能。有时使用可控的分区方式把常被一起访问的数据放到同一个节点上，可以大大减少应用的通信开销，带来明显的性能提升。 分区的特性RDD、分区、TASK、节点、核之间的关系： 一个 RDD 会被划分为一个或多个分区； 这些分区会被保存到多个节点，每个节点可能存储一个或多个分区，但是一个分区只能位于同一个节点，不能跨节点保存，分区是决定 RDD 分布的最小单位； RDD 的分区数是可以配置的，默认会等于所有 executor 的核数； Spark 会为每个分区分配一个 TASK，每个核一次处理一个 TASK； 默认分区RDD 创建方式不同，会产生不同的默认分区行为。比如，从 HDFS 中读取文件来创建 RDD 和通过一个 RDD 转换操作生成另一个新的 RDD 的分区行为是不同的。 分布式化一个本地数据集： 调用 API 默认分区数 分区器类 sc.parallelize(...) sc.defaultParallelism 无 从 HDFS 读取数据： 调用 API 默认分区数 分区器类 sc.textFile(...) sc.defaultParallelism 和文件 block 数中较大值 无 转换操作：由于 map、flatMap 操作结果可能会改变原 RDD 的 KEY，结果 RDD 会丢失分区器，如果希望继承父 RDD，可以使用 mapValues、flatMapValues，后两者会针对于 (K,V) 形式的类型只对 V 进行操作 调用 API 默认分区数 分区器类 filter,map,flatMap,distinct 同父 RDD filter同父 RDD，其他无分区器 mapValues, flatMapValues 同父 RDD 同父 RDD union union 的两个 RDD 分区数之和 无 subtract 同第一个RDD 无 cartesian 两个 RDD 分区数乘积 无 聚合操作： 调用 API 默认分区数 分区器类 reduceByKey,foldByKey,combineByKey 同父 RDD HashPartitioner sortByKey 同父 RDD RangePartitioner cogroup,groupByKey,join,leftOuterJoin,rightOuterJoin 取决于 RDD 的输入属性 HashPartitioner 分区器Partitioner（分区器）定义了 RDD 的分区分布，决定了一个 RDD 可以被分成多少个分区，以及每个分区的数据量有多大，进而决定了每个 Task 将处理哪些数据。一般来说，分区器是针对 key-value 值 RDD 的，并通过对 key 的运算来划分分区，非 key-value 形式的 RDD 无法根据数据特征来进行分区，也就没有设置分区器，此时 Spark 会把数据均匀的分配到执行节点上。 目前的版本提供了三种分区器: HashPartitioner（哈希分区器）: HashPartitioner 是基于 Java 的 Object.hashCode 来实现的分区器，根据 Object.hashCode 来对 key 进行计算得到一个整数，再通过公式Object.hashCode % numPartitions 计算某个 key 该分到哪个分区，当 RDD 没有 Partitioner 时，会把 HashPartitioner 作为默认的 Partitioner； RangePartitioner（范围分区器）: RangePartitioner 将 key 位于相同范围内的记录分配给给定分区，排序需要 RangePartitioner，因为 RangePartitioner 能够确保通过对给定分区内的记录进行排序，最终完成整个RDD的排序； 自定义分区器: 通过继承 Partitioner 抽象类，可以定制自己的分区器； 获取分区在 Scala 和 Java 中，你可以使用 RDD 的 partitioner 属性（Java 中使用 partitioner() 方法）来获取 RDD 的分区方式。它会返回一个 scala.Option 对象，这是 Scala 中用来存放可能存在的对象的容器类。你可以对这个 Option 对象调用 isDefined() 来检查其中是否有值，调用 get() 来获取其中的值。如果存在值的话，这个值会是一个 spark.Partitioner 对象。这本质上是一个告诉我们 RDD 中各个键分别属于哪个分区的函数。 12pairs.groupByKey().partitioner.getres8: org.apache.spark.Partitioner = org.apache.spark.HashPartitioner@c 设置分区有三种方式可以用于设置 RDD 的分区数，但要注意，若改变分区数量或分区器通常会导致 Shuffle 操作，务必在调整分区后进行缓存： 调用 partitionBy 方法：下面代码，我们自定义了一个分区器，并根据自定义的分区器对 RDD 进行重新分区，需要特别注意的是，在每次调用 partitionBy 之后，务必对结果进行缓存，否则后续每次惰性执行时都会重新执行分区动作，严重影响程序性能； 1234567891011import org.apache.spark.Partitionerclass CustTwoPartitioner(override val numPartitions: Int) extends Partitioner &#123; def getPartition(key: Any): Int = key match &#123; case s: String =&gt; &#123; if (s(0).toUpper &gt; &#x27;C&#x27;) 1 else 0 &#125; &#125;&#125;var x = sc.parallelize(Array((&quot;aa&quot;,1),(&quot;bb&quot;,1),(&quot;cc&quot;,1),(&quot;dd&quot;,1),(&quot;ee&quot;,1)), 3)var y = x.partitionBy(new CustTwoPartitioner(2)).persist() 通过转换操作返回带有特定分区的 RDD：这部分（读取数据源、转换继承）在上面默认分区器部分已讲过； 调用 repartition 或 coalesec 方法： coalesce(numPartitions: Int, shuffle: Boolean = false)：对 RDD 进行重分区，使用 HashPartitioner，第一个参数为重分区的数目，第二个为是否进行 shuffle，默认为false（此时是合并分区，父 RDD 和子 RDD 是窄依赖，不会产生 Shuffle）；如果重分区的数目大于原来的分区数，那么必须指定 shuffle 参数为 true； repartition(numPartitions: Int, partitionExprs: Column*)：repartition 是 coalesce shuffle 参数为 true 的简易实现，返回一个按 partitionExprs 将原 RDD 划分为 numPartitions 个分区的新 RDD，过程中会发生 Shuffle，父 RDD 和子 RDD 之间构成宽依赖； 分区并不是对所有应用都有好处的，如果给定 RDD 只需要被扫描一次，我们完全没有必要对其预先进行分区处理，只有当数据集多次在诸如 JOIN 这种基于键的操作中使用时，分区才会有帮助。 ShuffleShuffle 定义你永远不会调用一个名为 shuffle 的方法，但是有很多方法会导致 shuffle 的发生，比如在 RDD 上调用 groupByKey() 方法时，会返回一个 ShuffledRDD： 12345val pairs = sc.parallelize(List((1, &quot;one&quot;), (2, &quot;two&quot;), (3, &quot;three&quot;)))pairs.groupByKey()pairs: org.apache.spark.rdd.RDD[(Int, String)] = ParallelCollectionRDD[0] at parallelize at &lt;console&gt;:35res1: org.apache.spark.rdd.RDD[(Int, Iterable[String])] = ShuffledRDD[1] at groupByKey at &lt;console&gt;:38 要执行分布式 groupByKey 操作，我们通常必须在节点之间移动数据，以便数据可以按照它的 KEY 收集到单个机器上： 数据通过网络在节点之间移动的过程，称为 Shuffle（洗牌或混洗）。 Shuffle 过程以 Shuffle 为边界，Spark 将一个 Job 划分为不同的 Stage，这些 Stage 构成了一个大粒度的 DAG。Spark 的 Shuffle 过程分为 Write 和 Read 两个阶段，分属于两个不同的 Stage，前者是 Parent Stage 的最后一步，后者是 Child Stage 的第一步，如下图所示: Shuffle 过程首先会将前置 Stage 的 Map Task 结果写入本地磁盘（Shuffle Write），然后后续 Stage 的 reduce Task 再从磁盘中读取这些文件（Shuffle Read）来执行计算，这有两点好处： 将 Shuffle 文件写入磁盘（称为 Shuffle 持久化），使得 Spark 能够在时间上串行地执行不同的 Stage； 出现故障时，只需要重启 Reduce Task ，而不用重新运行所有的任务。 Spark 在 Shuffle 的实现上做了很多优化改进，Spark Shuffle 的演进过程如下（最早实现是 Hash Based Shuffle，2.0 以后就只有 Sort Based Shuffle 了）： Spark 0.8及以前 Hash Based Shuffle Spark 0.8.1 为 Hash Based Shuffle 引入 File Consolidation机制 Spark 0.9 引入 ExternalAppendOnlyMap Spark 1.1 引入 Sort Based Shuffle，但默认仍为Hash Based Shuffle Spark 1.2 默认的 Shuffle 方式改为 Sort Based Shuffle Spark 1.4 引入 Tungsten-Sort Based Shuffle Spark 1.6 Tungsten-sort 并入 Sort Based Shuffle Spark 2.0 Hash Based Shuffle 退出历史舞台 Hash Based Shuffle Hash Based Shuffle 的基本流程： Shuffle Write: 每个 Map Task 将计算结果数据分成多份（bucket），每一份对应到下游 stage 的每个 Partition 中，写入当前节点的本地磁盘，bucket 的数量就是 $M\times R$，这样会产生大量的小文件，对文件系统压力很大，而且不利于 IO 吞吐量，后面 Spark 做了优化，把在统一 Core 上运行的多个 Mapper 输出合并到同一个文件，这样 bucket 的数量就是 $Cores\times R$； Shuffle Read: 每个 Reduce Task 通过网络拉取属于当前任务的 bucket 数据，根据数据的 Key 进行聚合，然后判断是否需要排序，最后生成新的 RDD； Sort Based Shuffle Sort Based Shuffle 的基本流程： Shuffle Write: 不会为每个 Reduce Task 生成一个单独的文件，相反会把每个 Map Task 的结果数据写到一个 Data 文件中，并使用 Index 文件存储具体 Map Task 输出数据在同一个 Data 文件中是如何分类的信息；Shuffle Write 过程对每个 Map Task 生成两个文件 —— Data 文件和 Index 文件，因此生成的总文件数为 2M；Shuffle Write 阶段会按照 Reduce Task 的 PartitionId 和记录本身的 Key 进行排序，方便 Reducer 获取数据； Shuffle Read: Reduce Task 首先找 Driver 获取每个 Map Task 输出的位置信息，根据位置信息获取 Index 文件，解析 Index 文件获取 Data 文件中属于自己的那部分数据； Shuffle 规避和内存计算相比，网络通信和磁盘读写是非常耗时的过程，会严重影响程序执行效率，因此如非必要，应该尽可能避免数据 Shuffle。 宽窄依赖宽窄依赖定义为了更好地理解什么时候可能发生 Shuffle，我们需要先看看 RDD 是如何表示的： RDD 由四部分组成： Partitions（分区）: 数据的原子性片段，每个节点有一个或多个分区； Dependencies（依赖）: RDD 转化过程可以表示为一个 DAG，父 RDD 和子 RDD 之间的分区衍生关系； Function（函数）: 基于父 RDD 的计算； Metadata（元数据）: 分区 Schema 和数据位置； 事实上，RDD 之间的依赖关系定义了数据何时需要在网络中进行移动，根据父 RDD 和子 RDD 之间的依赖关系，可以将 Transformation 划分为两种： Narrow Dependencies（窄依赖）: 父 RDD 的每个分区只被子 RDD 中的一个分区依赖，窄依赖不会发生 Shuffle，执行非常块，可以按照 pipeline 进行优化； Wide Dependencies（宽依赖）: 父 RDD 的每个分区被子 RDD 中的多个分区依赖，宽依赖会导致 Shuffle，执行非常慢，是 Spark 用来划分 Stage 的依据； 宽窄依赖算子 总结 Spark 中常见的宽窄依赖 Transformation: 窄依赖: map、mapValues、flatMap、mapPartitions filter union co-partitioned join: 两个 RDD 分区方式相同的 JOIN 操作 coalesce: shuffle=false 窄依赖: groupByKey、reduceByKey、combineByKey、cogroup、groupWith join、leftOuterJoin、rightOuterJoin intersection、distinct repartition 容错机制通过追踪分区间的依赖关系可以从血缘图中重新计算丢失的分区数据： 重新计算窄依赖中丢失的分区数据很快，但是要重新计算宽依赖中丢失的分区数据很慢： 使用分区器减少 Shuffle有一些方法可以让你在使用宽依赖算子的同时尽量避免或减少 shuffle 的发生，其核心思想是通过重分区在集群中合理地组织数据。 分组前预分区在使用 groupByKey 之类的算子之前先对 RDD 进行预分区（预 Shuffle），之后所有工作都可以在工作节点上的本地分区上完成，无需将数据重新 shuffle 到另一个节点上，在这种情况下，必须移动数据的唯一时间是将最终的 reduce 值从工作节点发送会 Driver 节点： 可以通过 toDebugString 方法查看执行计划： JOIN 前预分区在执行 JOIN 前，使用相同的的分区器对连接的两个 RDD 进行预分区，可以避免 Shuffle，因为需要连接的两个 RDD 的数据已经被重新定位到同一分区中的相同节点上，不需要移动数据。 通过一个实际的例子来看，假设我们想统计有多少用户访问了他们没有订阅的主题，这可以通过用户订阅表和用户点击事件表进行 JOIN 得到： 1234567891011121314val sc = new SparkContext( ... )// 大表：用户ID-用户订阅列表val userData = sc.sequenceFile[UserID, Userlnfo](&quot;hdfs:// ... &quot;).persist()def processNewlogs(logFileName: String) &#123; // 小表：用户点击事件表 val events = sc.sequenceFile[UserID, Linklnfo](logFileName) val joined = userData.join(events) val offTopicVisits = joined.filter &#123; case (userld, (userlnfo, linklnfo)) =&gt; !userlnfo.topics.contains(linklnfo.topic) &#125;.count() println(&#x27;&#x27;Number of visits to non-subscribed topics: &#x27;&#x27; + offTopicVisi ts)&#125; “htt上面的 JOIN 操作会非常耗时，因为 JOIN 操作不知道任何关于数据的分区信息。JOIN 操作默认会 hash 两个数据集所有的 key，并将具有相同 hash 值的记录发送到同一个节点上进行 JOIN。解决办法很简单，就是在 JOIN 之前使用 partitionBy 对大表 RDD 进行重分区： 123val userData = sc.sequenceFile[UserID, Userlnfo](&quot;hdfs:// ... &quot;) .partitionBy(new HashPartitioner(100)) // Create 100 partitions .persist() 我们在读入 userData 时调用了 partitionBy，Spark 会知道它被 hash 分区了，在后面调用 userData.join(events) 时会利用这一点，按照每个特定的 UserID 将 events RDD shuffle 到包含 userData 对应 hash 分区的节点上。 参考 Coursera: Big Data Analysis with Scala and Spark Shuffle Operation in Hadoop and Spark 彻底搞懂spark的shuffle过程（shuffle write） Spark 2.1.0 中 Sort-Based Shuffle 产生的内幕]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark 指南：Spark 原理（一）—— Spark 程序如何在集群上运行]]></title>
    <url>%2FSpark%2FSpark%2FSpark%20%E6%8C%87%E5%8D%97%EF%BC%9ASpark%20%E5%8E%9F%E7%90%86%EF%BC%88%E4%B8%80%EF%BC%89%E2%80%94%E2%80%94%20Spark%20%E7%A8%8B%E5%BA%8F%E5%A6%82%E4%BD%95%E5%9C%A8%E9%9B%86%E7%BE%A4%E4%B8%8A%E8%BF%90%E8%A1%8C%2F</url>
    <content type="text"><![CDATA[本文主要讨论 Spark 在执行代码时会发生什么，我们以一种忽略具体实现的方式来讨论这个问题，既不依赖于所使用的集群管理器，也不依赖于正在运行的代码。 Spark 运行时架构基本组件Spark 运行时架构包含以下三种基本组件： Driver：是 Spark 程序的主控进程，主要负责： 创建 Spark 上下文； 提交 Spark 作业（job）； 在各 Executor 进程间分配、协调任务（Task）调度； Executor：是执行具体任务的进程，主要负责： 执行计算任务（Task）； 将结果返回给 Driver； 为需要持久化的 RDD 提供存储功能； 集群管理器：负责维护运行 Spark 程序的机器集群，集群管理器也有自己的 driver（称为主节点 master）和工作者（称为工作节点 worker），但是它们与物理机器而不是进程相关联。下图显示了一个基本的集群设置，左侧机器是群集管理器的 master 节点，右侧机器是集群管理器的 worker 节点，圆圈表示相应进程，目前为止，还没有运行 Spark 应用程序，这些只是来自集群管理器的进程。Spark 目前支持三个集群管理器：一个简单的内置独立集群管理器、Apache Mesos 和 Hadoop Yarn，但是，这个列表将继续增长； 执行模式执行模式使您能够在运行应用程序时确定上述资源的物理位置，有三种模式可供选择（在下面的部分中，带实心边框的矩形表示 driver 进程，而带虚线边框的矩形表示 executor 进程）： 集群模式（Cluster mode）：集群模式是运行 Spark 应用程序最常见的方式，在集群模式下，用户向集群管理器提交预编译的 JAR、Python 脚本或 R 脚本。然后，除了 executor 之外，集群管理员在集群内的 worker 节点上启动 driver 进程。 客户端模式（Client mode）：客户端模式与集群模式几乎相同，只是 Spark driver 程序保留在提交应用程序的客户端上，这意味着客户端负责维护 Spark driver 进程，集群管理器维护 executor 进程。 本地模式（Local mode）：本地模式与前两种模式有很大不同，它在一台机器上运行整个 Spark 应用程序，它通过单个机器上的线程实现并行性。这是学习 Spark、测试应用程序或使用本地开发进行迭代实验的常用方法，但是，我们不建议在运行生产应用程序时使用本地模式。 Spark 程序的生命周期Spark 外部生命周期从 Spark 代码外部来看 Spark 应用程序的整个生命周期： 客户端请求： 第一步是在本地计算机上执行代码（预编译的 JAR），并向集群管理器 master 节点发出请求，为 Spark driver 进程提供资源； 集群管理器接受请求，并将 driver 程序放在集群的一个 worker 节点上； 提交原始作业的客户端进程退出； 启动程序： Spark driver 开始运行用户代码，此代码必须包含初始化 Spark 集群的 SparkSession； SparkSession 随后将与集群管理器（较暗的线）通信，要求它在集群中启动 Spark executor 进程（较亮的线），执行器（executor）的数量及其相关配置由用户通过原始 Spark-submit 调用中的命令行参数设置； 集群管理器通过启动 executor 进行响应，并将有关其位置的相关信息发送到 driver 进程，在所有的东西都连接正确之后，我们就有了一个 Spark 集群； 执行：driver 和 executor 之间进行通信，执行代码并移动数据，driver 将任务分配到每个 executor，每个 executor 执行接收的具体任务，并将执行状态以及结果反馈给 driver； 完成：Spark 程序完成后，Driver 以成功或失败退出，然后，集群管理器为 driver 关闭该 Spark 集群中的 executor； Spark 内部生命周期相比 Spark 的外部生命周期，Spark 内部（用户代码）生命周期更加重要： 创建 SparkSession； 按照 Action 划分 Job； 按照 Shuffle 划分 Stage； 按照 Partition 划分 Task； SparkSession（会话）任何 Spark 应用程序的第一步都是创建 SparkSession，在许多交互模式中，这是为您完成的，但在应用程序中，您必须手动完成。一些遗留代码可能使用新的 SparkContext 模式。应该避免这样做，因为 SparkSession 上的 builder 方法更能有力地实例化 Spark 和 SQL 上下文，并确保没有上下文冲突，因为可能有多个库试图在同一Spark应用程序中创建会话。 1234// Creating a SparkSession in Scalaimport org.apache.Spark.sql.SparkSessionval Spark = SparkSession.builder().appName(&quot;Databricks Spark Example&quot;).config(&quot;Spark.sql.warehouse.dir&quot;, &quot;/user/hive/warehouse&quot;).getOrCreate() 在进行 SparkSession 之后，您应该能够运行 Spark 代码。通过 SparkSession，您还可以相应地访问所有低阶和遗留上下文和配置。请注意，SparkSession 类只添加在 Spark 2.x 中。您可能会发现，较旧的代码将直接为结构化API创建 SparkContext 和 sqlContext。 Job（作业）—— 划分标准：ActionSpark 代码基本上由转换（transformation）和动作（action）组成，在 Spark 中，所有的 transformation 类型操作都是延迟计算的，Spark 只是记录了将要对数据集进行的操作，只有需要将数据返回到 Driver 程序时（即触发 Action 类型操作），所有已记录的 transformation 才会执行，这被称为“惰性计算”。通常，Spark 会按照动作（action）将 Spark 程序划分为不同的 Job。 transformation 种类繁多，我们只需要记住那些会将数据返回到 Driver 程序的那些操作即可： 函数名 目的 示例 结果 collect() 所有元素 rdd.collect() {1,2,3,3} count() 元素个数 rdd.count() 4 countByValue() 各元素在rdd中出现的次数 rdd.countByValue() {(1,1),(2,1),(3,2)} take(num) 从rdd中返回num个元素 rdd.take(2) {1,2} top(num) 从rdd中返回最前面的num个元素 rdd.top(2) {3,3} takeOrdered(num)(ordering) 按提供的顺序，返回最前面的 num 个元素 rdd.takeOrdered(2)(myOrdering) {3,3} takeSample(withReplacement,num,[seed]) 从rdd中返回任意一些元素 rdd.takeSample(false,1) 非确定的 reduce(func) 整合RDD中的所有数据 rdd.reduce((x,y)=&gt;x+y) 9 fold(zero)(func) 和reduce一样，但是需要初始值 rdd.fold(0)((x,y)=&gt;x+y) 9 aggregate(zeroValue)(seqOp,combOp) 和reduce()相似，但是通常返回不同类型的函数 rdd.aggregate((0,0))((x,y)=&gt;(x,y)=&gt;(x._1+y,x._2+1),(x,y)=&gt;(x._1+y._1,x._2+y._2)) (9,4) foreach(func) 对RDd中的每个元素使用给定的元素 rdd.foreach(func) 无 Stage（阶段）—— 划分标准：ShuffleSpark 中的阶段（stage）表示可以一起执行以在多台计算机上并行计算相同操作的任务（task）组。一般来说，Spark 会尝试将尽可能多的工作（即工作中尽可能多的转换）打包到同一个阶段（stage），但引擎会在称为洗牌（Shuffle）的操作后启动新的阶段（stage）。 在“Spark 指南：Spark 原理（一）—— Partition 和 Shuffle”一文中我们讲过宽依赖算子会导致 Shuffle，这里重温一下那些会导致 Shuffle 的算子： groupByKey、reduceByKey、combineByKey、cogroup、groupWith join、leftOuterJoin、rightOuterJoin intersection、distinct repartition Shuffle 过程首先会将前置 Stage 的 Map Task 结果写入本地磁盘（Shuffle Write），然后后续 Stage 的 reduce Task 会从磁盘中读取这些文件（Shuffle Read）来执行计算，这有两点好处： 将 Shuffle 文件写入磁盘（称为 Shuffle 持久化），使得 Spark 能够在时间上串行地执行不同的 Stage； 出现故障时，只需要重启 Reduce Task ，而不用重新运行所有的任务。 Task（任务）划分标准：Partition每个任务（task）对应于将在单个执行器（executor）上运行的数据块（Partition）和一组转换的组合。Task 只是应用于数据单元（Partition）的计算单位，将数据划分为更多数量的分区意味着可以并行执行更多数据。如果我们的数据集中有一个大分区，我们将有一个任务；如果有1000个小分区，我们将有 1,000 个可以并行执行的任务。 使 Spark 成为“内存计算工具”的一个重要原因是，与之前的工具（如 MapReduce）不同，Spark 在将数据写入内存或磁盘前会尝试执行尽可能多的步骤。Spark 执行的关键优化之一是 pipelining，它发生在 RDD 及以下级别。使用流水线技术，任何可以将数据直接传递给彼此而无需在节点间移动的操作序列，都会被折叠成单个任务阶段，阶段内的所有操作会一起执行。例如，如果您编写一个基于 RDD 的程序，该程序执行一个 map，一个 filter，然后是另一个 map，则这些将导致单阶段任务，这些任务立即读取每个输入记录，将其传递给第一个 map，再将其传递给 filter，并在需要时将其传递给最后一个 map 函数。这种流水线式的计算比在每个步骤之后将中间结果写入内存或磁盘要快得多。 参考 How Spark Runs on a Cluster Spark_online/)]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark 指南：Spark SQL（五）—— SQL]]></title>
    <url>%2FSpark%2FSpark%2FSpark%20%E6%8C%87%E5%8D%97%EF%BC%9ASpark%20SQL%EF%BC%88%E4%BA%94%EF%BC%89%E2%80%94%E2%80%94%20SQL%2F</url>
    <content type="text"><![CDATA[SQL（Structured Query Language） 是一种领域特定语言，用于表达对数据的关系型操作。SQL 无处不在，即使技术专家预言了它的消亡，它还是许多企业所依赖的及其灵活的数据工具。Spark 实现了 ANSI SQL:2003 的一个子集，该标准是大多数 SQL 数据库中可用的标准。Spark SQL 旨在用作联机分析处理（OLAP）数据库，而不是联机事务处理（OLTP）数据库，这意味着它不打算执行极低延迟的查询，即使将来肯定会支持原地修改，但是目前还不支持。 Spark SQL &amp; HiveSpark SQL 的前身是 Shark。为了给熟悉 RDBMS 但又不理解 MapReduce 的技术人员提供快速上手的工具，hive 应运而生，它是当时唯一运行在 Hadoop 上的 SQL-on-hadoop 工具。但是MapReduce 计算过程中大量的中间磁盘落地过程消耗了大量的 I/O，降低的运行效率，为了提高 SQL-on-Hadoop 的效率，Shark 应运而生，但又因为 Shark 对于 Hive 的太多依赖（如采用 Hive 的语法解析器、查询优化器等等)，2014 年 Spark 团队停止对 Shark 的开发，将所有资源放 Spark SQL 项目上。其中 Spark SQL 作为 Spark 生态的一员继续发展，而不再受限于 Hive，只是兼容 Hive；而 Hive on Spark 是一个 Hive 的发展计划，该计划将 Spark 作为 Hive 的底层引擎之一，也就是说，Hive 将不再受限于一个引擎，可以采用 Map-Reduce、Tez、Spark 等引擎。 执行 SQLSpark 提供了几个接口来执行 SQL 查询： Spark SQL CLI：你可以使用 Spark SQL CLI 从命令行在本地模式下进行基本的 Spark SQL 查询， Spark SQL CLI 无法与 Thrift JDBC 服务器通信，要启动 Spark SQL CLI，请在 Spark 目录下运行以下命令 1./bin/spark-sql Spark 编程接口：你可以通过任意 Spark 语言 API 以临时方式执行 SQL，你可以通过 SparkSession 对象上的 sql 方法执行此操作，这将返回一个 DataFrame 1spark.sql(sql_statement) CatalogCatalog 是 Spark SQL 中最高级别的抽象，用于对数据库、表、视图、缓存、列、函数（UDF/UDAF）的元数据进行操作，其 API 可以在 org.apache.spark.sql.catalog 中查看。 示例数据： 1234567891011121314151617181920212223242526272829303132333435363738394041val data = Seq( Row(&quot;M&quot;, 3000, Row(&quot;James &quot;,&quot;&quot;,&quot;Smith&quot;), Seq(1,2), Map(&quot;1&quot;-&gt;&quot;a&quot;, &quot;11&quot;-&gt;&quot;aa&quot;)), Row(&quot;F&quot;, 4000, Row(&quot;Maria &quot;,&quot;Anne&quot;,&quot;Jones&quot;), Seq(3,3), Map(&quot;4&quot;-&gt;&quot;d&quot;, &quot;44&quot;-&gt;&quot;dd&quot;)), Row(&quot;F&quot;, -1, Row(&quot;Jen&quot;,&quot;Mary&quot;,&quot;Brown&quot;), Seq(5,2), Map(&quot;5&quot;-&gt;&quot;e&quot;)) )val schema = new StructType() .add(&quot;gender&quot;,StringType) .add(&quot;salary&quot;,IntegerType) .add(&quot;f_struct&quot;, new StructType() .add(&quot;firstname&quot;,StringType) .add(&quot;middlename&quot;,StringType) .add(&quot;lastname&quot;,StringType) ) .add(&quot;f_array&quot;, ArrayType(IntegerType)) .add(&quot;f_map&quot;, MapType(StringType, StringType))val df = spark.createDataFrame(spark.sparkContext.parallelize(data),schema)df.show()df.printSchema+------+------+--------------------+-------+------------------+|gender|salary| f_struct|f_array| f_map|+------+------+--------------------+-------+------------------+| M| 3000| [James , , Smith]| [1, 2]|[1 -&gt; a, 11 -&gt; aa]|| F| 4000|[Maria , Anne, Jo...| [3, 3]|[4 -&gt; d, 44 -&gt; dd]|| F| -1| [Jen, Mary, Brown]| [5, 2]| [5 -&gt; e]|+------+------+--------------------+-------+------------------+root |-- gender: string (nullable = true) |-- salary: integer (nullable = true) |-- f_struct: struct (nullable = true) | |-- firstname: string (nullable = true) | |-- middlename: string (nullable = true) | |-- lastname: string (nullable = true) |-- f_array: array (nullable = true) | |-- element: integer (containsNull = true) |-- f_map: map (nullable = true) | |-- key: string | |-- value: string (valueContainsNull = true) 获取 catalog 对象： 1val c = spark.catalog 操作数据库 API： 12345678910// 返回当前使用的数据库，相当于select database()currentDatabase: String// 设置当前使用的数据库，相当于use database_name;setCurrentDatabase(dbName: String): Unit// 查看所有数据库，相当于show databases;listDatabases(): Dataset[Database]// 获取某数据库的元数据，返回值是Database类型的，如果指定的数据库不存在则会@throws[AnalysisException](&quot;database does not exist&quot;)getDatabase(dbName: String): Database// 判断某个数据库是否已经存在，返回boolean值databaseExists(dbName: String): Boolean 示例： 12345678910111213c.listDatabases().show(false)+-------+----------------+-----------------------------------------------+|name |description |locationUri |+-------+----------------+-----------------------------------------------+|default|default database|file:/Users/likewang/ilab/Spark/spark-warehouse|+-------+----------------+-----------------------------------------------+val d = c.getDatabase(&quot;default&quot;)println(s&quot;name:$&#123;d.name&#125; path:$&#123;d.locationUri&#125;&quot;)name:default path:file:/Users/likewang/ilab/Spark/spark-warehousec.databaseExists(&quot;default&quot;)res4: Boolean = true 操作表/视图 API： 123456789101112131415161718192021222324252627282930313233343536373839// 表/视图的属性name：表的名字database：表所属的数据库的名字description：表的描述信息tableType：用于区分是表还是视图，两个取值：table或viewisTemporary：是否是临时表或临时视图，解释一下啥是临时表，临时表就是使用 Dataset 或DataFrame 的 createOrReplaceTempView 等类似的 API 注册的视图或表，当此次 Spark 任务结束后这些表就没了，再次使用的话还要再进行注册，而非临时表就是在 Hive 中真实存在的，开启Hive支持就能够直接使用的，本次 Spark 任务结束后表仍然能存在，下次启动不需要重新做任何处理就能够使用，表是持久的，这种不是临时表// 查看所有表或视图，相当于show tableslistTables(): Dataset[Table]// 返回指定数据库下的表或视图，如果指定的数据库不存在则会抛出@throws[AnalysisException](&quot;database does not exist&quot;)表示数据库不存在。listTables(dbName: String): Dataset[Table]// 获取表的元信息，不存在则会抛出异常getTable(tableName: String): TablegetTable(dbName: String, tableName: String): Table// 判断表或视图是否存在，返回boolean值tableExists(tableName: String): BooleantableExists(dbName: String, tableName: String): Boolean// 使用createOrReplaceTempView类似API注册的临时视图可以使用此方法删除，如果这个视图已经被缓存过的话会自动清除缓存dropTempView(viewName: String): BooleandropGlobalTempView(viewName: String): Boolean// 用于判断一个表否已经缓存过了isCached(tableName: String): Boolean// 用于缓存表cacheTable(tableName: String): UnitcacheTable(tableName: String, storageLevel: StorageLevel): Unit// 对表取消缓存uncacheTable(tableName: String): Unit// 清空所有缓存clearCache(): Unit// Spark为了性能考虑，对表的元数据做了缓存，所以当被缓存的表已经改变时也必须刷新元数据重新缓存refreshTable(tableName: String): UnitrefreshByPath(path: String): Unit// 根据给定路径创建表，并返回相关的 DataFramecreateTable(tableName: String, path: String): DataFramecreateTable(tableName: String, path: String, source: String): DataFramecreateTable(tableName: String, source: String, options: java.util.Map[String, String]): DataFramecreateTable(tableName: String, source: String, options: Map[String, String]): DataFramecreateTable(tableName: String, source: String, schema: StructType, options: java.util.Map[String, String]): DataFramecreateTable(tableName: String, source: String, schema: StructType, options: Map[String, String]): DataFrame 示例： 123456789101112131415161718192021222324252627282930313233c.listTables(&quot;default&quot;).show()+----+--------+-----------+---------+-----------+|name|database|description|tableType|isTemporary|+----+--------+-----------+---------+-----------++----+--------+-----------+---------+-----------+df.createOrReplaceTempView(&quot;df&quot;)c.listTables(&quot;default&quot;).show()+----+--------+-----------+---------+-----------+|name|database|description|tableType|isTemporary|+----+--------+-----------+---------+-----------+| df| null| null|TEMPORARY| true|+----+--------+-----------+---------+-----------+val t = c.getTable(&quot;df&quot;)println(s&quot;name:$&#123;t.name&#125; tableType:$&#123;t.tableType&#125; isTemporary:$&#123;t.isTemporary&#125;&quot;)name:df tableType:TEMPORARY isTemporary:truec.tableExists(&quot;df&quot;)res10: Boolean = truec.isCached(&quot;df&quot;)res11: Boolean = falsedf.cache()c.isCached(&quot;df&quot;)res13: Boolean = truec.uncacheTable(&quot;df&quot;)c.isCached(&quot;df&quot;)res14: Boolean = falsec.refreshTable(&quot;df&quot;) 函数相关 API： 12345678910111213141516// 函数的属性database：函数注册在哪个数据库下，函数是跟数据库绑定的description：对函数的描述信息，可以理解成注释className：函数其实就是一个class，调用函数就是调用类的方法，className表示函数对应的class的全路径类名isTemporary：是否是临时函数// 列出当前数据库下的所有函数，包括注册的临时函数listFunctions(): Dataset[Function]// 列出指定数据库下注册的所有函数，包括临时函数，如果指定的数据库不存在的话则会抛出@throws[AnalysisException](&quot;database does not exist&quot;)表示数据库不存在listFunctions(dbName: String): Dataset[Function]// 获取函数的元信息，函数不存在则会抛出异常getFunction(functionName: String): FunctiongetFunction(dbName: String, functionName: String): Function// 判断函数是否存在，返回boolean值functionExists(functionName: String): BooleanfunctionExists(dbName: String, functionName: String): Boolean 示例： 123456789101112131415161718192021c.listFunctions.show(10, false)+----+--------+-----------+---------------------------------------------------------+-----------+|name|database|description|className |isTemporary|+----+--------+-----------+---------------------------------------------------------+-----------+|! |null |null |org.apache.spark.sql.catalyst.expressions.Not |true ||% |null |null |org.apache.spark.sql.catalyst.expressions.Remainder |true ||&amp; |null |null |org.apache.spark.sql.catalyst.expressions.BitwiseAnd |true ||* |null |null |org.apache.spark.sql.catalyst.expressions.Multiply |true ||+ |null |null |org.apache.spark.sql.catalyst.expressions.Add |true ||- |null |null |org.apache.spark.sql.catalyst.expressions.Subtract |true ||/ |null |null |org.apache.spark.sql.catalyst.expressions.Divide |true ||&lt; |null |null |org.apache.spark.sql.catalyst.expressions.LessThan |true ||&lt;= |null |null |org.apache.spark.sql.catalyst.expressions.LessThanOrEqual|true ||&lt;=&gt; |null |null |org.apache.spark.sql.catalyst.expressions.EqualNullSafe |true |+----+--------+-----------+---------------------------------------------------------+-----------+c.functionExists(&quot;!&quot;)res21: Boolean = truec.getFunction(&quot;!&quot;)res22: org.apache.spark.sql.catalog.Function = Function[name=&#x27;!&#x27;, className=&#x27;org.apache.spark.sql.catalyst.expressions.Not&#x27;, isTemporary=&#x27;true&#x27;] 操作表/视图的列 API： 12345678910// 列的属性name：列的名字description：列的描述信息，与注释差不多dataType：列的数据类型nullable：列是否允许为nullisPartition：是否是分区列isBucket：是否是桶列// 列出指定的表或视图有哪些列，表不存在则抛异常listColumns(tableName: String): Dataset[Column]listColumns(dbName: String, tableName: String): Dataset[Column] 示例： 12345678910c.listColumns(&quot;df&quot;).show()+--------+-----------+--------------------+--------+-----------+--------+| name|description| dataType|nullable|isPartition|isBucket|+--------+-----------+--------------------+--------+-----------+--------+| gender| null| string| true| false| false|| salary| null| int| true| false| false||f_struct| null|struct&lt;firstname:...| true| false| false|| f_array| null| array&lt;int&gt;| true| false| false|| f_map| null| map&lt;string,string&gt;| true| false| false|+--------+-----------+--------------------+--------+-----------+--------+ Tables要用 Spark SQL 做任何有用的事情，首先要定义表，表在逻辑上等效于 DataFrame，因为他们是运行命令所依据的数据结构，我们可以对表进行关联、过滤、汇总等操作，表和 DataFame 之间的核心区别在于：在编程语言范围内定义 DataFrame，在数据库中定义表。 创建表Spark 相当独特的功能是可以在 SQL 中重用整个数据源 API： 12345678910111213141516171819202122232425262728293031323334// 从数据源读取数据，创建表，定义了一个非托管表val sql = &quot;&quot;&quot;CREATE TABLE if not exists flights( a string comment &quot;name&quot;, b int comment &quot;level&quot;, c int comment &quot;age&quot;) using csv options (path &#x27;job.csv&#x27;)&quot;&quot;&quot;spark.sql(sql)// 从查询创建表，定义了一个托管表，Spark 会为其跟踪所有相关信息val sql = &quot;&quot;&quot;CREATE TABLE if not exists df_copyUSING parquet AS SELECT * from df&quot;&quot;&quot;spark.sql(sql)c.listTables().show()+-------+--------+-----------+---------+-----------+| name|database|description|tableType|isTemporary|+-------+--------+-----------+---------+-----------+|df_copy| default| null| MANAGED| false||flights| default| null| EXTERNAL| false|| df| null| null|TEMPORARY| true|+-------+--------+-----------+---------+-----------+spark.sql(&quot;select * from df_copy&quot;).show()+------+------+--------------------+-------+------------------+|gender|salary| f_struct|f_array| f_map|+------+------+--------------------+-------+------------------+| M| 3000| [James , , Smith]| [1, 2]|[1 -&gt; a, 11 -&gt; aa]|| F| 4000|[Maria , Anne, Jo...| [3, 3]|[4 -&gt; d, 44 -&gt; dd]|| F| -1| [Jen, Mary, Brown]| [5, 2]| [5 -&gt; e]|+------+------+--------------------+-------+------------------+ 插入表1234567891011121314151617val sql = &quot;&quot;&quot;insert into df_copySELECT * from df limit 3&quot;&quot;&quot;spark.sql(sql)spark.sql(&quot;select * from flights&quot;).show()+------+------+--------------------+-------+------------------+|gender|salary| f_struct|f_array| f_map|+------+------+--------------------+-------+------------------+| M| 3000| [James , , Smith]| [1, 2]|[1 -&gt; a, 11 -&gt; aa]|| F| 4000|[Maria , Anne, Jo...| [3, 3]|[4 -&gt; d, 44 -&gt; dd]|| F| -1| [Jen, Mary, Brown]| [5, 2]| [5 -&gt; e]|| F| 4000|[Maria , Anne, Jo...| [3, 3]|[4 -&gt; d, 44 -&gt; dd]|| F| -1| [Jen, Mary, Brown]| [5, 2]| [5 -&gt; e]|| M| 3000| [James , , Smith]| [1, 2]|[1 -&gt; a, 11 -&gt; aa]|+------+------+--------------------+-------+------------------+ 描述表12345678910spark.sql(&quot;describe df_copy&quot;).show()+--------+--------------------+-------+|col_name| data_type|comment|+--------+--------------------+-------+| gender| string| null|| salary| int| null||f_struct|struct&lt;firstname:...| null|| f_array| array&lt;int&gt;| null|| f_map| map&lt;string,string&gt;| null|+--------+--------------------+-------+ 刷新表REFRESH TALE 刷新与该表的所有缓存条目（实质上是文件），如果该表先前已被缓存，则下次扫描时将被延迟缓存： 1spark.sql(&quot;refresh table df_copy&quot;) 删除表删除表会删除托管表中的数据，因此执行此操作时需要非常小心。 12345678spark.sql(&quot;drop table if exists df_copy&quot;)c.listTables().show()+-------+--------+-----------+---------+-----------+| name|database|description|tableType|isTemporary|+-------+--------+-----------+---------+-----------+|flights| default| null| EXTERNAL| false|| df| null| null|TEMPORARY| true|+-------+--------+-----------+---------+-----------+ 缓存表和 DataFrame 一样，你可以缓存表或者取消缓存表: 1234567spark.sql(&quot;uncache table flights&quot;)c.isCached(&quot;flights&quot;)res60: Boolean = falsespark.sql(&quot;cache table flights&quot;)c.isCached(&quot;flights&quot;)res59: Boolean = true Views视图是保存的查询计划，可以方便地组织或重用查询逻辑。 创建视图Spark 有几种不同的视图概念，视图可以是全局视图、数据库视图或会话视图： 123456789101112131415161718192021222324252627282930313233343536373839// 常规/数据库视图：在所属数据库可见，不能基于视图再创建常规视图val sql = &quot;&quot;&quot;create view view_f as select * from flights&quot;&quot;&quot;spark.sql(sql)// 会话临时视图：仅在当前会话期间可用，且未注册到数据库val sql = &quot;&quot;&quot;create temp view temp_view_f as select * from flights&quot;&quot;&quot;spark.sql(sql)// 全局临时视图：仅在当前会话期间可用，无论用哪个数据库都可见val sql = &quot;&quot;&quot;create global temp view global_temp_view_f as select * from flights&quot;&quot;&quot;spark.sql(sql)// 覆盖临时视图：如果临时视图已存在则覆盖val sql = &quot;&quot;&quot;create or replace temp view replace_temp_view_f as select * from flights&quot;&quot;&quot;spark.sql(sql)// 视图会在表列表中列出spark.sql(&quot;show tables&quot;).show()+--------+-------------------+-----------+|database| tableName|isTemporary|+--------+-------------------+-----------+| default| flights| false|| default| view_f| false|| | df| true|| |replace_temp_view_f| true|| | temp_view_f| true|+--------+-------------------+-----------+ 访问视图定义好视图，就可以像访问表一样在 SQL 中访问视图了： 12345678910spark.sql(&quot;select * from replace_temp_view_f&quot;).show()+------+---+---+| a| b| c|+------+---+---+| a| b| c||caster| 0| 26|| like| 1| 30|| leo| 2| 30||rayray| 3| 27|+------+---+---+ 删除视图12345678910spark.sql(&quot;drop view if exists replace_temp_view_f&quot;)spark.sql(&quot;show tables&quot;).show()+--------+-----------+-----------+|database| tableName|isTemporary|+--------+-----------+-----------+| default| flights| false|| default| view_f| false|| | df| true|| |temp_view_f| true|+--------+-----------+-----------+ Databases数据库是用于组织表的工具，如果你没有定义数据库，Spark 将使用默认的数据库，在 Spark 中运行的所有 SQL 语句（包括 DataFrame 命令）都是在数据库的上下文中执行的，如果你更改数据库，则任何用户定义的表都将保留在先前的数据库中，并且要以其他方式查询。 123456789101112131415161718192021// 创建数据库spark.sql(&quot;create database if not exists some_db&quot;)// 查看所有数据库spark.sql(&quot;show databases&quot;).show()+------------+|databaseName|+------------+| default|| some_db|+------------+// 切换数据库spark.sql(&quot;use some_db&quot;)spark.sql(&quot;show tables&quot;).show()// 删除数据库spark.sql(&quot;drop database if exists some_db&quot;)spark.sql(&quot;show databases&quot;).show()+------------+|databaseName|+------------+| default|+------------+ 查询语句Spark 中的查询支持以下 ANSI SQL 要求（此处列出了 SELECT 表达式的布局）： 123456789101112131415161718192021222324SELECT [ALL|DISTINCT] named_expression[, named_expression, ...]FROM relation[, relation, ...][lateral_view[, lateral_view, ...]][WHERE boolean_expression][aggregation [HAVING boolean_expression]][ORDER BY sort_expressions][CLUSTER BY expressions][DISTRIBUTE BY expressions][SORT BY sort_expressions][WINDOW named_window[, WINDOW named_window, ...]]named_expression: :expression [AS alias]relation: | join_relation | (table_name|query|relation)[sample][AS alias] : VALUES(expressions)[, (expressions), ...] [AS (column_name[, column_name, ...])]expressions: : expressions[, expressions, ...]sort_expressions: :expressions [ASC|DESC][, expressions [ASC|DESC], ...] SQL 配置查看当前环境 SQL 参数的配置: 123456789101112131415161718192021222324252627spark.sql(&quot;SET -v&quot;).show(false)+-----------------------------------------------------+---------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+|key |value |meaning |+-----------------------------------------------------+---------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+|spark.sql.adaptive.enabled |false |When true, enable adaptive query execution. ||spark.sql.adaptive.shuffle.targetPostShuffleInputSize|67108864b |The target post-shuffle input size in bytes of a task. ||spark.sql.autoBroadcastJoinThreshold |10485760 |Configures the maximum size in bytes for a table that will be broadcast to all worker nodes when performing a join. By setting this value to -1 broadcasting can be disabled. Note that currently statistics are only supported for Hive Metastore tables where the command &lt;code&gt;ANALYZE TABLE &amp;lt;tableName&amp;gt; COMPUTE STATISTICS noscan&lt;/code&gt; has been run, and file-based data source tables where the statistics are computed directly on the files of data.||spark.sql.avro.compression.codec |snappy |Compression codec used in writing of AVRO files. Supported codecs: uncompressed, deflate, snappy, bzip2 and xz. Default codec is snappy. ||spark.sql.avro.deflate.level |-1 |Compression level for the deflate codec used in writing of AVRO files. Valid value must be in the range of from 1 to 9 inclusive or -1. The default value is -1 which corresponds to 6 level in the current implementation. ||spark.sql.broadcastTimeout |300000ms |Timeout in seconds for the broadcast wait time in broadcast joins. ||spark.sql.cbo.enabled |false |Enables CBO for estimation of plan statistics when set true. ||spark.sql.cbo.joinReorder.dp.star.filter |false |Applies star-join filter heuristics to cost based join enumeration. ||spark.sql.cbo.joinReorder.dp.threshold |12 |The maximum number of joined nodes allowed in the dynamic programming algorithm. ||spark.sql.cbo.joinReorder.enabled |false |Enables join reorder in CBO. ||spark.sql.cbo.starSchemaDetection |false |When true, it enables join reordering based on star schema detection. ||spark.sql.columnNameOfCorruptRecord |_corrupt_record|The name of internal column for storing raw/un-parsed JSON and CSV records that fail to parse. ||spark.sql.crossJoin.enabled |false |When false, we will throw an error if a query contains a cartesian product without explicit CROSS JOIN syntax. ||spark.sql.execution.arrow.enabled |false |When true, make use of Apache Arrow for columnar data transfers. Currently available for use with pyspark.sql.DataFrame.toPandas, and pyspark.sql.SparkSession.createDataFrame when its input is a Pandas DataFrame. The following data types are unsupported: BinaryType, MapType, ArrayType of TimestampType, and nested StructType. ||spark.sql.execution.arrow.fallback.enabled |true |When true, optimizations enabled by &#x27;spark.sql.execution.arrow.enabled&#x27; will fallback automatically to non-optimized implementations if an error occurs. ||spark.sql.execution.arrow.maxRecordsPerBatch |10000 |When using Apache Arrow, limit the maximum number of records that can be written to a single ArrowRecordBatch in memory. If set to zero or negative there is no limit. ||spark.sql.extensions |&lt;undefined&gt; |Name of the class used to configure Spark Session extensions. The class should implement Function1[SparkSessionExtension, Unit], and must have a no-args constructor. ||spark.sql.files.ignoreCorruptFiles |false |Whether to ignore corrupt files. If true, the Spark jobs will continue to run when encountering corrupted files and the contents that have been read will still be returned. ||spark.sql.files.ignoreMissingFiles |false |Whether to ignore missing files. If true, the Spark jobs will continue to run when encountering missing files and the contents that have been read will still be returned. ||spark.sql.files.maxPartitionBytes |134217728 |The maximum number of bytes to pack into a single partition when reading files. |+-----------------------------------------------------+---------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ 配置项123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110#Job ID /Namespark.app.name=clsfd_ad_attr_map_w_mvca_ins#yarn 进行调度，也可以是mesos，yarn，以及standalone#一个spark application，是一个spark应用。一个应用对应且仅对应一个sparkContext。每一个应用，运行一组独立的executor processes。一个应用，可以以多线程的方式提交多个作业job。spark可以运行在多种集群管理器上如：mesos，yarn，以及standalone，每种集群管理器都会提供跨应用的资源调度策略。spark.master=yarn#激活外部shuffle服务。服务维护executor写的文件，因而executor可以被安全移除。#需要设置spark.dynamicAllocation.enabled 为true，同事指定外部shuffle服务。#对shuffle来说，executor现将自己的map输出写入到磁盘，然后，自己作为一个server，向其他executor提供这些map输出文件的数据。而动态资源调度将executor返还给集群后，这个shuffle数据服务就没有了。因此，如果要使用动态资源策略，解决这个问题的办法就是，将保持shuffle文件作为一个外部服务，始终运行在spark集群的每个节点上，独立于应用和executorspark.shuffle.service.enabled=true#在默认情况下，三种集群管理器均不使用动态资源调度模式。所以要使用动态资源调度需要提前配置。spark.dynamicAllocation.enabled=true# 如果所有的executor都移除了，重新请求时启动的初始executor数spark.dynamicAllocation.initialExecutors=20# 最少保留的executor数spark.dynamicAllocation.minExecutors=10# 最多使用的executor数，默认为你申请的最大executor数spark.dynamicAllocation.maxExecutors=100# 可以是cluster也可以是Clientspark.submit.deployMode=cluster# 指定提交到Yarn的资源池spark.yarn.queue=hdlq-data-batch-low# 在yarn-cluster模式下，申请Yarn App Master（包括Driver）所用的内存。spark.driver.memory=8g# excutor的核心数spark.executor.cores=16# 一个Executor对应一个JVM进程。Executor占用的内存分为两部分：ExecutorMemory和MemoryOverheadspark.executor.memory=32gspark.yarn.executor.memoryOverhead=2g# shuffle分区数100，根据数据量进行调控，这儿配置了Join时shuffle的分区数和聚合数据时的分区数。spark.sql.shuffle.partitions=100# 如果用户没有指定并行度，下面这个参数将是RDD中的分区数，它是由join,reducebykey和parallelize # 这个参数只适用于未加工的RDD不适用于dataframe# 没有join和聚合计算操作，这个参数将是无效设置spark.default.parallelism# 打包传入一个分区的最大字节，在读取文件的时候。spark.sql.files.maxPartitionBytes=128MB# 用相同时间内可以扫描的数据的大小来衡量打开一个文件的开销。当将多个文件写入同一个分区的时候该参数有用。# 该值设置大一点有好处，有小文件的分区会比大文件分区处理速度更快（优先调度）。spark.sql.files.openCostInBytes=4MB# Spark 事件总线是SparkListenerEvent事件的阻塞队列大小spark.scheduler.listenerbus.eventqueue.size=100000# 是否启动推测机制spark.speculation=false# 开启spark的推测机制，开启推测机制后如果某一台机器的几个task特别慢，推测机制会将任务分配到其他机器执行，最后Spark会选取最快的作为最终结果。# 2表示比其他task慢两倍时，启动推测机制spark.speculation.multiplier=2# 推测机制的检测周期spark.speculation.interval=5000ms# 完成task的百分比时启动推测spark.speculation.quantile=0.6# 最多允许失败的Executor数量。spark.task.maxFailures=10# spark序列化 对于优化&lt;网络性能&gt;极为重要，将RDD以序列化格式来保存减少内存占用.spark.serializer=org.apache.spark.serializer.KryoSerializer# 因为spark是基于内存的机制，所以默认是开启RDD的压缩spark.rdd.compress=true# Spark的安全管理#https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/SecurityManager.scalaspark.ui.view.acls=*spark.ui.view.acls.groups=*# 表示配置GC线程数为3spark.executor.extraJavaOptions=&quot;-XX:ParallelGCThreads=3&quot;# 最大广播表的大小。设置为-1可以禁止该功能。当前统计信息仅支持Hive Metastore表。这里设置的是10MBspark.sql.autoBroadcastJoinThreshold=104857600# 广播等待超时，这里单位是秒spark.sql.broadcastTimeout=300# 心跳检测间隔spark.yarn.scheduler.heartbeat.interval-ms=10000spark.sql.broadcastTimeout#缓存表问题#spark2.+采用：#spark.catalog.cacheTable(&quot;tableName&quot;)缓存表，spark.catalog.uncacheTable(&quot;tableName&quot;)解除缓存。#spark 1.+采用：#sqlContext.cacheTable(&quot;tableName&quot;)缓存，sqlContext.uncacheTable(&quot;tableName&quot;) 解除缓存#Sparksql仅仅会缓存必要的列，并且自动调整压缩算法来减少内存和GC压力。#假如设置为true，SparkSql会根据统计信息自动的为每个列选择压缩方式进行压缩。spark.sql.inMemoryColumnarStorage.compressed=true#控制列缓存的批量大小。批次大有助于改善内存使用和压缩，但是缓存数据会有OOM的风险spark.sql.inMemoryColumnarStorage.batchSize=10000 配置方法可以在应用程序初始化时或在应用程序执行过程中进行设置： 1spark.conf.set(&quot;spark.sql.crossJoin.enabled&quot;, &quot;true&quot;) 参考 《Spark 权威指南：Chapter 10》 什么是Catalog https://spark.apache.org/docs/2.3.0/api/sql/]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark 指南：Spark SQL（四）—— 结构化函数]]></title>
    <url>%2FSpark%2FSpark%2FSpark%20%E6%8C%87%E5%8D%97%EF%BC%9ASpark%20SQL%EF%BC%88%E5%9B%9B%EF%BC%89%E2%80%94%E2%80%94%20%E7%BB%93%E6%9E%84%E5%8C%96%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[Spark SQL 结构化函数一般都在 functions 模块，要使用这些函数，需要先导入该模块： 123import org.apache.spark.sql.functions._import org.apache.spark.sql.Rowimport org.apache.spark.sql.types._ 普通函数Spark SQL 函数众多，最好的做法就是当需要某个具体功能时在以下列表中检索，或者直接百度谷歌: 字符串函数: Spark SQL String Functions 日期时间函数: Spark SQL Date and Time Functions 数组函数: Spark SQL Array functions complete list 字典函数: Spark SQL Array functions complete list 排序函数: Spark SQL Sort functions 聚合函数: Spark SQL Aggregate Functions 聚合函数在聚合中，您将指定一个分组和一个聚合函数，该函数必须为每个分组产生一个结果。Spark 的聚合功能是复杂巧妙且成熟的，具有各种不同的用例和可能性。通常，通过分组使用聚合函数去汇总数值型数据，也可以将任何类型的值聚合到 array、list 或 map 中。 Spark 支持以下分组类型，每个分组都会返回一个 RelationalGroupedDataset，可以在上面指定聚合函数： 最简单的分组是通过在 select 语句中执行聚合来汇总一个完整的 DataFrame； group by 允许指定一个或多个 key 以及一个或多个聚合函数来转换列值； window 可以指定一个或多个 key 以及一个或多个聚合函数来转换列值，但是输入到函数的行以某种方式与当前行有关； grouping set 可用于在多个不同级别进行聚合，grouping set 可以作为 SQL 原语或通过 DataFrame 中的 rollup 和 cube 使用；group by A, B grouping sets(A, B) 等价于 group by A union group by B； rollup 可以指定一个或多个 key 以及一个或多个聚合函数来转换列值，这些列将按照层次进行聚合；group by A,B,C with rollup 首先会对 A,B,C 进行 group by，然后对 A,B 进行 group by，最后对 A 进行 group by，再对全表进行 group by，最后将结构进行 union，缺少字段补 null； cube 可以指定一个或多个 key 以及一个或多个聚合函数来转换列值，这些列将在所有列的组合中进行聚合；group by A,B,C with cube，会对 A, B, C 的所有可能组合进行 group by，最后再将结果 union； 除了可以在 DataFrame 上或通过 .stat 出现的特殊情况之外，所有聚合都可用作函数，你可以在 org.apache.spark.sql.functions 包中找到大多数聚合函数。 统计聚合 DataFrame 级聚合： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657// count(&quot;*&quot;) 会显示 count(1)，但是直接写 count(1) 却会报错// 在整个 DataFrame 上使用 count 会把结果拉回 Driver，是 action，但是用在 select 中是 transformationdf.select(count(&quot;stockCode&quot;), count(&quot;*&quot;)).show()+----------------+--------+|count(stockCode)|count(1)|+----------------+--------+| 541909| 541914|+----------------+--------+// 去重，近似去重（为加速），第二个参数指定允许的最大估计误差df.select(countDistinct(&quot;StockCode&quot;), approx_count_distinct(&quot;StockCode&quot;, 0.05)).show()+-------------------------+--------------------------------+|count(DISTINCT StockCode)|approx_count_distinct(StockCode)|+-------------------------+--------------------------------+| 4070| 3804|+-------------------------+--------------------------------+// 第一行、最后一行df.select(first(&quot;StockCode&quot;), last(&quot;StockCode&quot;)).show()+-----------------------+----------------------+|first(StockCode, false)|last(StockCode, false)|+-----------------------+----------------------+| 85123A| null|+-----------------------+----------------------+// 最大、最小值df.select(min(&quot;Quantity&quot;), max(&quot;Quantity&quot;)).show()+-------------+-------------+|min(Quantity)|max(Quantity)|+-------------+-------------+| -80995| 80995|+-------------+-------------+// 求和、去重求和df.select(sum(&quot;Quantity&quot;), sumDistinct(&quot;Quantity&quot;)).show()+-------------+----------------------+|sum(Quantity)|sum(DISTINCT Quantity)|+-------------+----------------------+| 5176450| 29310|+-------------+----------------------+// 均值、方差、标准差df.select(avg(&quot;Quantity&quot;), var_pop(&quot;Quantity&quot;), stddev_pop(&quot;Quantity&quot;)).show()+----------------+------------------+--------------------+| avg(Quantity)| var_pop(Quantity)|stddev_pop(Quantity)|+----------------+------------------+--------------------+|9.55224954743324|47559.303646609325| 218.08095663447858|+----------------+------------------+--------------------+// 偏度、峰度df.select(skewness(&quot;Quantity&quot;), kurtosis(&quot;Quantity&quot;)).show()+-------------------+------------------+| skewness(Quantity)|kurtosis(Quantity)|+-------------------+------------------+|-0.2640755761052948| 119768.0549553411|+-------------------+------------------+// 相关系数、协方差df.select(corr(&quot;InvoiceNo&quot;, &quot;Quantity&quot;), covar_pop(&quot;InvoiceNo&quot;, &quot;Quantity&quot;)).show()+-------------------------+------------------------------+|corr(InvoiceNo, Quantity)|covar_pop(InvoiceNo, Quantity)|+-------------------------+------------------------------+| 4.912186085641252E-4| 1052.7260778752557|+-------------------------+------------------------------+ 分组聚合：分组通常是针对分类数据完成的，我们先将数据按照某些列中的值进行分组，然后对被归入同一组的其他列执行聚合计算；事实上，DataFrame 级聚合只是分组聚合的一种特例； 12345678910111213141516171819202122232425262728293031323334// 分组语法groupBy(col1: String, cols: String*)groupBy(cols: Column*)// 示例，RelationalGroupedDataset 对象也有 count 方法，但是和 DataFrame 的 count 方法会将结果收集到 Driver 不同，这还是一个 transformationdf.groupBy(&quot;InvoiceNo&quot;, &quot;CustomerID&quot;).count().show(3)+---------+----------+-----+|InvoiceNo|CustomerID|count|+---------+----------+-----+| 536846| 14573| 76|| 537026| 12395| 12|| 537883| 14437| 5|+---------+----------+-----+// 分组聚合最常用的形式df.groupBy(&quot;InvoiceNo&quot;).agg( count(&quot;Quantity&quot;).as(&quot;quan&quot;), expr(&quot;count(Quantity)&quot;)).show(3)+---------+----+---------------+|InvoiceNo|quan|count(Quantity)|+---------+----+---------------+| 536596| 6| 6|| 536938| 14| 14|| 537252| 1| 1|+---------+----+---------------+// map 形式df.groupBy(&quot;InvoiceNo&quot;).agg(&quot;Quantity&quot;-&gt;&quot;avg&quot;, &quot;Quantity&quot;-&gt;&quot;stddev_pop&quot;).show(3)+---------+------------------+--------------------+|InvoiceNo| avg(Quantity)|stddev_pop(Quantity)|+---------+------------------+--------------------+| 536596| 1.5| 1.1180339887498947|| 536938|33.142857142857146| 20.698023172885524|| 537252| 31.0| 0.0|+---------+------------------+--------------------+ 多维分析 grouping sets：group by keys grouping sets(combine1(keys), ..., combinen(keys))，其中，keys 包含了所有可能用于分组的字段，combine(keys) 是 keys 的一个子集，聚合函数会分别基于每组 combine(keys) 进行聚合，最后再把所有聚合结果按字段进行 union，不同类型的分组缺失字段补 null；可以通过 null 值在各列上的分布来判断各结果行所属的聚合类型，进一步地，我们可以用 grouping_id() 聚合函数值来标识每一结果行的聚合类型，grouping_id() 首先用二进制表示各个 key 是否为 null，如 (a, null, null) 对应二进制 011，然后再将该二进制数转化为对应的十进制数（在这个例子中，十进制数为 3）得到 grouping_id() 的值；grouping sets 仅在 SQL 中可用，是 group by 子句的扩展，要在 DataFrame 中执行相同的操作，请使用 rollup 和 cube 算子； 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758val sql = &quot;&quot;&quot;select area, grade, honor, sum(value) as total_value, grouping_id() as groupIdfrom df group by area, grade, honor grouping sets(area, grade, honor)order by 5&quot;&quot;&quot;spark.sql(sql).show()+----+-----+-----+-----------+-------+|area|grade|honor|total_value|groupId|+----+-----+-----+-----------+-------+| a| null| null| 915| 3|| c| null| null| 155| 3|| b| null| null| 155| 3||null| ac| null| 345| 5||null| ab| null| 360| 5||null| aa| null| 520| 5||null| null| aaf| 30| 6||null| null| aaa| 150| 6||null| null| aah| 180| 6||null| null| aac| 300| 6||null| null| aad| 240| 6||null| null| aae| 120| 6||null| null| aab| 70| 6||null| null| aag| 135| 6|+----+-----+-----+-----------+-------+// (area, grade) 代表按照 `area, grade` 进行 group by，() 代表在整个 DataFrame 上 group byval sql = &quot;&quot;&quot;select area, grade, honor, sum(value) as total_value, grouping_id() as groupIdfrom df group by area, grade, honor grouping sets(area, grade, honor, (area, grade), ())order by 5&quot;&quot;&quot;spark.sql(sql).show()+----+-----+-----+-----------+-------+|area|grade|honor|total_value|groupId|+----+-----+-----+-----------+-------+| a| aa| null| 420| 1|| c| aa| null| 50| 1|| c| ac| null| 45| 1|| a| ab| null| 240| 1|| a| ac| null| 255| 1|| c| ab| null| 60| 1|| b| ac| null| 45| 1|| b| ab| null| 60| 1|| b| aa| null| 50| 1|| a| null| null| 915| 3|| c| null| null| 155| 3|| b| null| null| 155| 3||null| ab| null| 360| 5||null| ac| null| 345| 5||null| aa| null| 520| 5||null| null| aaa| 150| 6||null| null| aah| 180| 6||null| null| aad| 240| 6||null| null| aag| 135| 6||null| null| aab| 70| 6|+----+-----+-----+-----------+-------+ rollup：group by A,B,C with rollup 首先会对 A,B,C 进行 group by，然后对 A,B 进行 group by，最后对 A 进行 group by，再对全表进行 group by，最后将结构进行 union，缺少字段补 null； 123456789101112131415161718192021222324252627282930313233343536373839404142val sql = &quot;&quot;&quot;select area,grade,honor,sum(value) as total_value from df group by area,grade,honor with rollup&quot;&quot;&quot;spark.sql(sql)df.rollup(&quot;area&quot;, &quot;grade&quot;, &quot;honor&quot;) .agg(grouping_id().as(&quot;groupId&quot;), sum(&quot;value&quot;).alias(&quot;total_value&quot;)) .orderBy(&quot;groupId&quot;) .show(100)+----+-----+-----+-------+-----------+|area|grade|honor|groupId|total_value|+----+-----+-----+-------+-----------+| c| ab| aad| 0| 60|| a| ac| aah| 0| 180|| b| ab| aad| 0| 60|| a| ac| aag| 0| 45|| a| ac| aaf| 0| 30|| a| aa| aaa| 0| 50|| b| aa| aaa| 0| 50|| c| aa| aaa| 0| 50|| a| aa| aab| 0| 70|| c| ac| aag| 0| 45|| a| ab| aae| 0| 120|| b| ac| aag| 0| 45|| a| aa| aac| 0| 300|| a| ab| aad| 0| 120|| a| ac| null| 1| 255|| c| ac| null| 1| 45|| c| aa| null| 1| 50|| c| ab| null| 1| 60|| b| aa| null| 1| 50|| b| ab| null| 1| 60|| b| ac| null| 1| 45|| a| ab| null| 1| 240|| a| aa| null| 1| 420|| a| null| null| 3| 915|| b| null| null| 3| 155|| c| null| null| 3| 155||null| null| null| 7| 1225|+----+-----+-----+-------+-----------+ cube：group by A,B,C with cube，会对 A, B, C 的所有可能组合进行 group by，最后再将结果 union； 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475val sql = &quot;&quot;&quot;select area,grade,honor,sum(value) as total_value from dfgroup by area,grade,honor with cube&quot;&quot;&quot;spark.sql(sql)df.cube(&quot;area&quot;, &quot;grade&quot;, &quot;honor&quot;) .agg(grouping_id().as(&quot;groupId&quot;),sum(&quot;value&quot;).alias(&quot;total_value&quot;)) .orderBy(&quot;groupId&quot;) .show(100)+----+-----+-----+-------+-----------+|area|grade|honor|groupId|total_value|+----+-----+-----+-------+-----------+| c| ab| aad| 0| 60|| a| aa| aab| 0| 70|| c| ac| aag| 0| 45|| b| aa| aaa| 0| 50|| b| ab| aad| 0| 60|| c| aa| aaa| 0| 50|| a| aa| aac| 0| 300|| b| ac| aag| 0| 45|| a| ac| aag| 0| 45|| a| ac| aaf| 0| 30|| a| ac| aah| 0| 180|| a| ab| aad| 0| 120|| a| aa| aaa| 0| 50|| a| ab| aae| 0| 120|| b| aa| null| 1| 50|| a| ab| null| 1| 240|| c| ac| null| 1| 45|| b| ab| null| 1| 60|| a| ac| null| 1| 255|| c| ab| null| 1| 60|| b| ac| null| 1| 45|| a| aa| null| 1| 420|| c| aa| null| 1| 50|| a| null| aaf| 2| 30|| a| null| aag| 2| 45|| a| null| aac| 2| 300|| a| null| aaa| 2| 50|| b| null| aad| 2| 60|| a| null| aab| 2| 70|| a| null| aah| 2| 180|| a| null| aae| 2| 120|| a| null| aad| 2| 120|| c| null| aaa| 2| 50|| c| null| aad| 2| 60|| b| null| aag| 2| 45|| b| null| aaa| 2| 50|| c| null| aag| 2| 45|| b| null| null| 3| 155|| c| null| null| 3| 155|| a| null| null| 3| 915||null| ab| aad| 4| 240||null| aa| aab| 4| 70||null| ac| aah| 4| 180||null| aa| aaa| 4| 150||null| ac| aag| 4| 135||null| ab| aae| 4| 120||null| aa| aac| 4| 300||null| ac| aaf| 4| 30||null| ab| null| 5| 360||null| ac| null| 5| 345||null| aa| null| 5| 520||null| null| aae| 6| 120||null| null| aaa| 6| 150||null| null| aaf| 6| 30||null| null| aad| 6| 240||null| null| aac| 6| 300||null| null| aab| 6| 70||null| null| aah| 6| 180||null| null| aag| 6| 135||null| null| null| 7| 1225|+----+-----+-----+-------+-----------+ 聚合为复杂类型可以通过 collect_list 和 collect_set 收集某列中的值，前者保留原始顺序，后者不保证顺序但会去重。 1234567891011121314val res = df.select(collect_list(&quot;Country&quot;), collect_set(&quot;Country&quot;))res.show()res.printSchema+---------------------+--------------------+|collect_list(Country)|collect_set(Country)|+---------------------+--------------------+| [United Kingdom, ...|[Portugal, Italy,...|+---------------------+--------------------+root |-- collect_list(Country): array (nullable = true) | |-- element: string (containsNull = true) |-- collect_set(Country): array (nullable = true) | |-- element: string (containsNull = true) 窗口函数Spark 窗口函数对一组行（如frame、partition）进行操作，并为每个输入行返回一个值。窗口函数是一种特殊的聚合函数，但是输入到函数的行以某种方式与当前行有关，函数会为每一行返回一个值。Spark SQL支持三种窗口函数： 排序函数：row_number() rank() dense_rank() percent_rank() ntile() 分析函数: cume_dist() lag() lead() 聚合函数: sum() first() last() max() min() mean() stddev() 语法： 1234// 定义窗口val window = Window...// 在窗口上应用窗口函数，返回列对象windowFunc.over(Window) 示例数据： 1234567891011121314151617181920212223242526272829303132import spark.implicits._import org.apache.spark.sql.functions._import org.apache.spark.sql.expressions.Windowval simpleData = Seq((&quot;James&quot;, &quot;Sales&quot;, 3000), (&quot;Michael&quot;, &quot;Sales&quot;, 4600), (&quot;Robert&quot;, &quot;Sales&quot;, 4100), (&quot;Maria&quot;, &quot;Finance&quot;, 3000), (&quot;James&quot;, &quot;Sales&quot;, 3000), (&quot;Scott&quot;, &quot;Finance&quot;, 3300), (&quot;Jen&quot;, &quot;Finance&quot;, 3900), (&quot;Jeff&quot;, &quot;Marketing&quot;, 3000), (&quot;Kumar&quot;, &quot;Marketing&quot;, 2000), (&quot;Saif&quot;, &quot;Sales&quot;, 4100) )val df = simpleData.toDF(&quot;employee_name&quot;, &quot;department&quot;, &quot;salary&quot;)df.show()+-------------+----------+------+|employee_name|department|salary|+-------------+----------+------+| James| Sales| 3000|| Michael| Sales| 4600|| Robert| Sales| 4100|| Maria| Finance| 3000|| James| Sales| 3000|| Scott| Finance| 3300|| Jen| Finance| 3900|| Jeff| Marketing| 3000|| Kumar| Marketing| 2000|| Saif| Sales| 4100|+-------------+----------+------+ 排序窗口函数用于排序的窗口定义： 12// 按照指定字段分组，在分组内按照另一字段排序，得到排序窗口，如果需要降序，可以使用col(&quot;salary&quot;).desc val windowSpec = Window.partitionBy(&quot;department&quot;).orderBy(&quot;salary&quot;) row_number: 返回每行排序字段在窗口内的行号； 1234567891011121314151617df.withColumn(&quot;row_number&quot;,row_number.over(windowSpec)).show()+-------------+----------+------+----------+|employee_name|department|salary|row_number|+-------------+----------+------+----------+| James| Sales| 3000| 1|| James| Sales| 3000| 2|| Robert| Sales| 4100| 3|| Saif| Sales| 4100| 4|| Michael| Sales| 4600| 5|| Maria| Finance| 3000| 1|| Scott| Finance| 3300| 2|| Jen| Finance| 3900| 3|| Kumar| Marketing| 2000| 1|| Jeff| Marketing| 3000| 2|+-------------+----------+------+----------+ rank: 返回每行排序字段在窗口内的排名，rank=n+1，n 代表窗口内比当前行小的行数； 1234567891011121314151617df.withColumn(&quot;rank&quot;,rank().over(windowSpec)).show()+-------------+----------+------+----+|employee_name|department|salary|rank|+-------------+----------+------+----+| James| Sales| 3000| 1|| James| Sales| 3000| 1|| Robert| Sales| 4100| 3|| Saif| Sales| 4100| 3|| Michael| Sales| 4600| 5|| Maria| Finance| 3000| 1|| Scott| Finance| 3300| 2|| Jen| Finance| 3900| 3|| Kumar| Marketing| 2000| 1|| Jeff| Marketing| 3000| 2|+-------------+----------+------+----+ dense_rank: 返回每行排序字段在窗口内的稠密排名，rank=n+1，n 代表窗口内比当前行小的不同取值数； 1234567891011121314151617df.withColumn(&quot;dense_rank&quot;,dense_rank().over(windowSpec)).show()+-------------+----------+------+----------+|employee_name|department|salary|dense_rank|+-------------+----------+------+----------+| James| Sales| 3000| 1|| James| Sales| 3000| 1|| Robert| Sales| 4100| 2|| Saif| Sales| 4100| 2|| Michael| Sales| 4600| 3|| Maria| Finance| 3000| 1|| Scott| Finance| 3300| 2|| Jen| Finance| 3900| 3|| Kumar| Marketing| 2000| 1|| Jeff| Marketing| 3000| 2|+-------------+----------+------+----------+ percent_rank: 返回每行排序字段在窗口内的百分位排名； 123456789101112131415161718//percent_rankdf.withColumn(&quot;percent_rank&quot;,percent_rank().over(windowSpec)).show()+-------------+----------+------+------------+|employee_name|department|salary|percent_rank|+-------------+----------+------+------------+| James| Sales| 3000| 0.0|| James| Sales| 3000| 0.0|| Robert| Sales| 4100| 0.5|| Saif| Sales| 4100| 0.5|| Michael| Sales| 4600| 1.0|| Maria| Finance| 3000| 0.0|| Scott| Finance| 3300| 0.5|| Jen| Finance| 3900| 1.0|| Kumar| Marketing| 2000| 0.0|| Jeff| Marketing| 3000| 1.0|+-------------+----------+------+------------+ ntile: 返回窗口分区中结果行的相对排名，在下面的示例中，我们使用2作为ntile的参数，因此它返回介于2个值（1和2）之间的排名； 1234567891011121314151617df.withColumn(&quot;ntile&quot;,ntile(2).over(windowSpec)).show()+-------------+----------+------+-----+|employee_name|department|salary|ntile|+-------------+----------+------+-----+| James| Sales| 3000| 1|| James| Sales| 3000| 1|| Robert| Sales| 4100| 1|| Saif| Sales| 4100| 2|| Michael| Sales| 4600| 2|| Maria| Finance| 3000| 1|| Scott| Finance| 3300| 1|| Jen| Finance| 3900| 2|| Kumar| Marketing| 2000| 1|| Jeff| Marketing| 3000| 2|+-------------+----------+------+-----+ 分析窗口函数 cume_dist: 窗口函数用于获取窗口分区内值的累积分布，和 SQL 中的 DENSE_RANK 作用相同 12345678910111213141516df.withColumn(&quot;cume_dist&quot;,cume_dist().over(windowSpec)).show()+-------------+----------+------+------------------+|employee_name|department|salary| cume_dist|+-------------+----------+------+------------------+| James| Sales| 3000| 0.4|| James| Sales| 3000| 0.4|| Robert| Sales| 4100| 0.8|| Saif| Sales| 4100| 0.8|| Michael| Sales| 4600| 1.0|| Maria| Finance| 3000|0.3333333333333333|| Scott| Finance| 3300|0.6666666666666666|| Jen| Finance| 3900| 1.0|| Kumar| Marketing| 2000| 0.5|| Jeff| Marketing| 3000| 1.0|+-------------+----------+------+------------------+ lag: 和 SQL 中的 LAG 函数相同，返回值为当前行之前的 offset 行，如果当前行之前的行少于 offset，则返回“ null”。 12345678910111213141516df.withColumn(&quot;lag&quot;,lag(&quot;salary&quot;,2).over(windowSpec)).show()+-------------+----------+------+----+|employee_name|department|salary| lag|+-------------+----------+------+----+| James| Sales| 3000|null|| James| Sales| 3000|null|| Robert| Sales| 4100|3000|| Saif| Sales| 4100|3000|| Michael| Sales| 4600|4100|| Maria| Finance| 3000|null|| Scott| Finance| 3300|null|| Jen| Finance| 3900|3000|| Kumar| Marketing| 2000|null|| Jeff| Marketing| 3000|null|+-------------+----------+------+----+ lead: 和 SQL 中的 LEAD 函数相同，返回值为当前行之后的 offset 行，如果当前行之后的行少于 offset，则返回“ null”。 12345678910111213141516df.withColumn(&quot;lead&quot;,lead(&quot;salary&quot;,2).over(windowSpec)).show()+-------------+----------+------+----+|employee_name|department|salary|lead|+-------------+----------+------+----+| James| Sales| 3000|4100|| James| Sales| 3000|4100|| Robert| Sales| 4100|4600|| Saif| Sales| 4100|null|| Michael| Sales| 4600|null|| Maria| Finance| 3000|3900|| Scott| Finance| 3300|null|| Jen| Finance| 3900|null|| Kumar| Marketing| 2000|null|| Jeff| Marketing| 3000|null|+-------------+----------+------+----+ 聚合窗口函数在本部分中，我将解释如何使用 Spark SQL Aggregate 窗口函数和 WindowSpec 计算每个分组的总和，最小值，最大值，使用聚合函数时，order by 子句特别重要，影响着最后聚合的具体范围。 1234567891011121314151617181920212223242526272829303132val windowSpec = Window.partitionBy(&quot;department&quot;).orderBy(&quot;salary&quot;)val res = df.withColumn(&quot;row&quot;,row_number.over(windowSpec))// 不排序: 每一行都是基于全组做聚合，默认所有行有相同的次序val windowSpecAgg = Window.partitionBy(&quot;department&quot;)// 通过某个字段 f 排序，每一行对全组所有 &lt;= 当前行该字段值的做聚合val windowSpecSalaryAgg = Window.partitionBy(&quot;department&quot;).orderBy(&quot;salary&quot;)// 以 row 排序，每一行对全组所有 row &lt;= 当前 row 值的做聚合，等价于累积聚合val windowSpecRowAgg = Window.partitionBy(&quot;department&quot;).orderBy(&quot;row&quot;)// 以 row 排序，每一行对附近偏移范围内的数据做聚合val windowSpecBetweenAgg = Window.partitionBy(&quot;department&quot;).orderBy(&quot;row&quot;).rowsBetween(-2, -1)res.withColumn(&quot;sum&quot;, sum(col(&quot;salary&quot;)).over(windowSpecAgg)) .withColumn(&quot;salarysum&quot;, sum(col(&quot;salary&quot;)).over(windowSpecSalaryAgg)) .withColumn(&quot;rowsum&quot;, sum(col(&quot;salary&quot;)).over(windowSpecRowAgg)) .withColumn(&quot;betweensum&quot;, sum(col(&quot;salary&quot;)).over(windowSpecBetweenAgg)) .show()+-------------+----------+------+---+-----+---------+------+----------+|employee_name|department|salary|row| sum|salarysum|rowsum|betweensum|+-------------+----------+------+---+-----+---------+------+----------+| James| Sales| 3000| 1|18800| 6000| 3000| null|| James| Sales| 3000| 2|18800| 6000| 6000| 3000|| Robert| Sales| 4100| 3|18800| 14200| 10100| 6000|| Saif| Sales| 4100| 4|18800| 14200| 14200| 7100|| Michael| Sales| 4600| 5|18800| 18800| 18800| 8200|| Maria| Finance| 3000| 1|10200| 3000| 3000| null|| Scott| Finance| 3300| 2|10200| 6300| 6300| 3000|| Jen| Finance| 3900| 3|10200| 10200| 10200| 6300|| Kumar| Marketing| 2000| 1| 5000| 2000| 2000| null|| Jeff| Marketing| 3000| 2| 5000| 5000| 5000| 2000|+-------------+----------+------+---+-----+---------+------+----------+ 自定义函数自定义函数是 Spark SQL 最有用的特性之一，它扩展了 Spark 的内置函数，允许用户实现更加复杂的计算逻辑。但是，自定义函数是 Spark 的黑匣子，无法利用 Spark SQL 的优化器，自定义函数将失去 Spark 在 Dataframe / Dataset 上所做的所有优化，通常性能和安全性较差。如果可能，应尽量选用 Spark SQL 内置函数，因为这些函数提供了优化。 根据自定义函数是作用于单行还是多行，可以将其划分为两类： UDF：User Defined Function，即用户自定义函数，接收一行输入并返回一个输出； UDAF：User Defined Aggregate Function，即用户自定义的聚合函数，接收多行输入并返回一个输出； UDF使用 UDF 的一般步骤： 定义普通函数：与定义一般函数的方式完全相同，但是需要额外注意 UDF 中参数和返回值类型并不是我们可以随意定义的，因为涉及到数据的序列化和反序列化，详情参考“传递复杂数据类型”一节； null 值的处理，如果设计不当，UDF 很容易出错，最好的做法是在函数内部检查 null，而不是在外部检查 null； 注册 UDF：在 DataFrame API 和 SQL 表达式中使用的 UDF 注册方式有所差异 如果要在 DataFrame API 中使用：val 函数名 = org.apache.spark.sql.functions.udf(函数值)； 如果要在 SQL 表达式中使用：sparkSession.udf.register(函数名, 函数值)； 应用 UDF：与应用 Spark 内置函数的方法完全相同，只不过原始函数中的变长参数会被注册为 ArrayType 类型，实际传参时也要传入 ArrayType 类型的实参； 传递简单数据类型1234567891011121314151617// 示例数据import spark.implicits._val columns = Seq(&quot;Seqno&quot;,&quot;Quote&quot;)val data = Seq((&quot;1&quot;, &quot;Be the change that you wish to see in the world&quot;), (&quot;2&quot;, &quot;Everyone thinks of changing the world, but no one thinks of changing himself.&quot;), (&quot;3&quot;, &quot;The purpose of our lives is to be happy.&quot;) )val df = data.toDF(columns:_*)df.show(false)+-----+-----------------------------------------------------------------------------+|Seqno|Quote |+-----+-----------------------------------------------------------------------------+|1 |Be the change that you wish to see in the world ||2 |Everyone thinks of changing the world, but no one thinks of changing himself.||3 |The purpose of our lives is to be happy. |+-----+-----------------------------------------------------------------------------+ 创建一个普通函数: 12345// convertCase 是一个函数值，将句子中每个单词首字母改为大写val convertCase = (strQuote:String) =&gt; &#123; val arr = strQuote.split(&quot; &quot;) arr.map(f=&gt; f.substring(0,1).toUpperCase + f.substring(1,f.length)).mkString(&quot; &quot;)&#125; 在 DataFrame 中使用 UDF: 1234567891011121314import org.apache.spark.sql.functions.udf// 1. 创建 Spark UDF，传给 udf 的是一个函数值，如果 x 只是一个普通函数名，则需传入 x _val convertUDF = udf(convertCase)convertUDF: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(&lt;function1&gt;,StringType,Some(List(StringType)))// 2. 在 DataFrame 中使用 UDFdf.select(col(&quot;Seqno&quot;), convertUDF(col(&quot;Quote&quot;)).as(&quot;Quote&quot;) ).show(false)+-----+-----------------------------------------------------------------------------+|Seqno|Quote |+-----+-----------------------------------------------------------------------------+|1 |Be The Change That You Wish To See In The World ||2 |Everyone Thinks Of Changing The World, But No One Thinks Of Changing Himself.||3 |The Purpose Of Our Lives Is To Be Happy. | 在 SQL 中使用 UDF: 12345// 1. 注册 UDFspark.udf.register(&quot;convertUDF&quot;, convertCase)// 2. 在 SQL 中使用 UDF，得到同样的结果输出df.createOrReplaceTempView(&quot;QUOTE_TABLE&quot;)spark.sql(&quot;select Seqno, convertUDF(Quote) from QUOTE_TABLE&quot;).show(false) 传递复杂数据类型在 “Spark SQL 数据类型”一文曾介绍过 Spark 类型和 Scala 类型之间的对应关系，当 UDF 在 Spark 和 Scala 之间传递参数和返回值时也遵循同样的对应关系，下面列出了 Spark 中复杂类型与 Scala 本地类型之间的对应关系： Spark 类型 udf 参数类型 udf 返回值类型 StructType Row Tuple/case class ArrayType Seq Seq/Array/List MapType Map Map 本部分将使用如下示例数据来演示以上各种场景： 123456789101112131415161718192021222324252627282930313233343536373839404142434445val data = Seq( Row(&quot;M&quot;, 3000, Row(&quot;James &quot;,&quot;&quot;,&quot;Smith&quot;), Seq(1,2), Map(&quot;1&quot;-&gt;&quot;a&quot;, &quot;11&quot;-&gt;&quot;aa&quot;)), Row(&quot;M&quot;, 4000, Row(&quot;Michael &quot;,&quot;Rose&quot;,&quot;&quot;), Seq(3,2), Map(&quot;2&quot;-&gt;&quot;b&quot;, &quot;22&quot;-&gt;&quot;bb&quot;)), Row(&quot;M&quot;, 4000, Row(&quot;Robert &quot;,&quot;&quot;,&quot;Williams&quot;), Seq(1,2), Map(&quot;3&quot;-&gt;&quot;c&quot;, &quot;33&quot;-&gt;&quot;cc&quot;)), Row(&quot;F&quot;, 4000, Row(&quot;Maria &quot;,&quot;Anne&quot;,&quot;Jones&quot;), Seq(3,3), Map(&quot;4&quot;-&gt;&quot;d&quot;, &quot;44&quot;-&gt;&quot;dd&quot;)), Row(&quot;F&quot;, -1, Row(&quot;Jen&quot;,&quot;Mary&quot;,&quot;Brown&quot;), Seq(5,2), Map(&quot;5&quot;-&gt;&quot;e&quot;)) )val schema = new StructType() .add(&quot;gender&quot;,StringType) .add(&quot;salary&quot;,IntegerType) .add(&quot;f_struct&quot;, new StructType() .add(&quot;firstname&quot;,StringType) .add(&quot;middlename&quot;,StringType) .add(&quot;lastname&quot;,StringType) ) .add(&quot;f_array&quot;, ArrayType(IntegerType)) .add(&quot;f_map&quot;, MapType(StringType, StringType))val df = spark.createDataFrame(spark.sparkContext.parallelize(data),schema)df.show()df.printSchema+------+------+--------------------+-------+------------------+|gender|salary| f_struct|f_array| f_map|+------+------+--------------------+-------+------------------+| M| 3000| [James , , Smith]| [1, 2]|[1 -&gt; a, 11 -&gt; aa]|| M| 4000| [Michael , Rose, ]| [3, 2]|[2 -&gt; b, 22 -&gt; bb]|| M| 4000|[Robert , , Willi...| [1, 2]|[3 -&gt; c, 33 -&gt; cc]|| F| 4000|[Maria , Anne, Jo...| [3, 3]|[4 -&gt; d, 44 -&gt; dd]|| F| -1| [Jen, Mary, Brown]| [5, 2]| [5 -&gt; e]|+------+------+--------------------+-------+------------------+root |-- gender: string (nullable = true) |-- salary: integer (nullable = true) |-- f_struct: struct (nullable = true) | |-- firstname: string (nullable = true) | |-- middlename: string (nullable = true) | |-- lastname: string (nullable = true) |-- f_array: array (nullable = true) | |-- element: integer (containsNull = true) |-- f_map: map (nullable = true) | |-- key: string | |-- value: string (valueContainsNull = true) StructType如果传给 udf 的是 StructType 类型，udf 参数类型应该定义为 Row类型；如果需要 udf 返回 StructType 类型，udf 返回值类型应该定义为 Tuple 或 case class； udf 返回值类型可以是 Tuple：Tuple 返回值会被转化为 struct，Tuple 的各个元素分别对应 struct 的各个子域 _1、_2…… 123456789101112131415161718192021222324252627282930313233343536373839404142// 数据类型转化过程：Struct =&gt; Row =&gt; Tuple =&gt; Structdef myF(gender:String, r:Row):(String, String) = &#123; r match &#123; case Row(firstname:String, middlename: String, lastname: String) =&gt; &#123; val x = if (firstname.isEmpty) &quot;&quot; else (firstname + &quot;:&quot; + gender) (x, firstname) &#125; &#125;&#125;val myUdf = udf(myF _)// udf 签名：&lt;function2&gt; 代表 udf 包含两个参数；StructType(StructField(_1,StringType,true), StructField(_2,StringType,true)) 代表 udf 返回的是一个 struct，且该 struuct 包含了两个子域 _1、_2；None 是 udf 的入参类型，入参有 Row 就会变成 None，尚不清楚其中机理myUdf: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(&lt;function2&gt;,StructType(StructField(_1,StringType,true), StructField(_2,StringType,true)),None)val res = df.withColumn(&quot;f_udf&quot;, myUdf(col(&quot;gender&quot;), col(&quot;f_struct&quot;)))res.show()res.printSchema+------+------+--------------------+-------+------------------+--------------------+|gender|salary| f_struct|f_array| f_map| f_udf|+------+------+--------------------+-------+------------------+--------------------+| M| 3000| [James , , Smith]| [1, 2]|[1 -&gt; a, 11 -&gt; aa]| [James :M, James ]|| M| 4000| [Michael , Rose, ]| [3, 2]|[2 -&gt; b, 22 -&gt; bb]|[Michael :M, Mich...|| M| 4000|[Robert , , Willi...| [1, 2]|[3 -&gt; c, 33 -&gt; cc]|[Robert :M, Robert ]|| F| 4000|[Maria , Anne, Jo...| [3, 3]|[4 -&gt; d, 44 -&gt; dd]| [Maria :F, Maria ]|| F| -1| [Jen, Mary, Brown]| [5, 2]| [5 -&gt; e]| [Jen:F, Jen]|+------+------+--------------------+-------+------------------+--------------------+root |-- gender: string (nullable = true) |-- salary: integer (nullable = true) |-- f_struct: struct (nullable = true) | |-- firstname: string (nullable = true) | |-- middlename: string (nullable = true) | |-- lastname: string (nullable = true) |-- f_array: array (nullable = true) | |-- element: integer (containsNull = true) |-- f_map: map (nullable = true) | |-- key: string | |-- value: string (valueContainsNull = true) |-- f_udf: struct (nullable = true) | |-- _1: string (nullable = true) | |-- _2: string (nullable = true) udf 的返回值可以是样例类：样例类型返回值会以一种更加自然的方式转化为 struct，样例类的不同属性构成了 struct 的各个子域； 1234567891011121314151617181920212223242526272829303132333435363738394041case class P(x:String, y:Int)def myF(gender:String, r:Row):P = &#123; r match &#123; case Row(firstname:String, middlename: String, lastname: String) =&gt; &#123; val x = if (firstname.isEmpty) &quot;&quot; else (firstname + &quot;:&quot; + gender) P(x, 1) &#125; &#125;&#125;val myUdf = udf(myF _)myUdf: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(&lt;function2&gt;,StructType(StructField(x,StringType,true), StructField(y,IntegerType,false)),None)val res = df.withColumn(&quot;f_udf&quot;, myUdf(col(&quot;gender&quot;), col(&quot;f_struct&quot;)))res.show()res.printSchema+------+------+--------------------+-------+------------------+---------------+|gender|salary| f_struct|f_array| f_map| f_udf|+------+------+--------------------+-------+------------------+---------------+| M| 3000| [James , , Smith]| [1, 2]|[1 -&gt; a, 11 -&gt; aa]| [James :M, 1]|| M| 4000| [Michael , Rose, ]| [3, 2]|[2 -&gt; b, 22 -&gt; bb]|[Michael :M, 1]|| M| 4000|[Robert , , Willi...| [1, 2]|[3 -&gt; c, 33 -&gt; cc]| [Robert :M, 1]|| F| 4000|[Maria , Anne, Jo...| [3, 3]|[4 -&gt; d, 44 -&gt; dd]| [Maria :F, 1]|| F| -1| [Jen, Mary, Brown]| [5, 2]| [5 -&gt; e]| [Jen:F, 1]|+------+------+--------------------+-------+------------------+---------------+root |-- gender: string (nullable = true) |-- salary: integer (nullable = true) |-- f_struct: struct (nullable = true) | |-- firstname: string (nullable = true) | |-- middlename: string (nullable = true) | |-- lastname: string (nullable = true) |-- f_array: array (nullable = true) | |-- element: integer (containsNull = true) |-- f_map: map (nullable = true) | |-- key: string | |-- value: string (valueContainsNull = true) |-- f_udf: struct (nullable = true) | |-- x: string (nullable = true) | |-- y: integer (nullable = false) ArrayType 返回值类型也可以是 Seq、Array 或 List，不会影响到 udf 签名 12345678910111213141516171819202122232425262728293031323334def myF(gender:String, a:Seq[Int]):Seq[String] = a.map(x =&gt; gender * x.toInt)def myF(gender:String, a:Seq[Int]):Array[String] = a.map(x =&gt; gender * x.toInt).toArraydef myF(gender:String, a:Seq[Int]):List[String] = a.map(x =&gt; gender * x.toInt).toListval myUdf = udf(myF _)myUdf: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(&lt;function2&gt;,ArrayType(StringType,true),Some(List(StringType, ArrayType(IntegerType,false))))val res = df.withColumn(&quot;f_udf&quot;, myUdf(col(&quot;gender&quot;), col(&quot;f_array&quot;)))res.show()res.printSchema+------+------+--------------------+-------+------------------+-----------+|gender|salary| f_struct|f_array| f_map| f_udf|+------+------+--------------------+-------+------------------+-----------+| M| 3000| [James , , Smith]| [1, 2]|[1 -&gt; a, 11 -&gt; aa]| [M, MM]|| M| 4000| [Michael , Rose, ]| [3, 2]|[2 -&gt; b, 22 -&gt; bb]| [MMM, MM]|| M| 4000|[Robert , , Willi...| [1, 2]|[3 -&gt; c, 33 -&gt; cc]| [M, MM]|| F| 4000|[Maria , Anne, Jo...| [3, 3]|[4 -&gt; d, 44 -&gt; dd]| [FFF, FFF]|| F| -1| [Jen, Mary, Brown]| [5, 2]| [5 -&gt; e]|[FFFFF, FF]|+------+------+--------------------+-------+------------------+-----------+root |-- gender: string (nullable = true) |-- salary: integer (nullable = true) |-- f_struct: struct (nullable = true) | |-- firstname: string (nullable = true) | |-- middlename: string (nullable = true) | |-- lastname: string (nullable = true) |-- f_array: array (nullable = true) | |-- element: integer (containsNull = true) |-- f_map: map (nullable = true) | |-- key: string | |-- value: string (valueContainsNull = true) |-- f_udf: array (nullable = true) | |-- element: string (containsNull = true) 参数不能是 Array 或 List，否则会报无法进行类型转换的错误 1scala.collection.mutable.WrappedArray$ofRef cannot be cast to scala.collection.immutable.List` 变长参数会被注册为 ArrayType 类型：使用变长参数和使用 Seq 参数效果是一样的 12345678910111213141516171819202122232425262728293031323334def myF(gender:String, a:String *):Seq[String] = &#123; a.map(x =&gt; gender * x.toInt)&#125;val myUdf = udf(myF _)myUdf: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(&lt;function2&gt;,ArrayType(StringType,true),Some(List(StringType, ArrayType(StringType,true))))val res = df.withColumn(&quot;f_udf&quot;, myUdf(col(&quot;gender&quot;), col(&quot;f_array&quot;)))res.show()res.printSchema+------+------+--------------------+-------+------------------+-----------+|gender|salary| f_struct|f_array| f_map| f_udf|+------+------+--------------------+-------+------------------+-----------+| M| 3000| [James , , Smith]| [1, 2]|[1 -&gt; a, 11 -&gt; aa]| [M, MM]|| M| 4000| [Michael , Rose, ]| [3, 2]|[2 -&gt; b, 22 -&gt; bb]| [MMM, MM]|| M| 4000|[Robert , , Willi...| [1, 2]|[3 -&gt; c, 33 -&gt; cc]| [M, MM]|| F| 4000|[Maria , Anne, Jo...| [3, 3]|[4 -&gt; d, 44 -&gt; dd]| [FFF, FFF]|| F| -1| [Jen, Mary, Brown]| [5, 2]| [5 -&gt; e]|[FFFFF, FF]|+------+------+--------------------+-------+------------------+-----------+root |-- gender: string (nullable = true) |-- salary: integer (nullable = true) |-- f_struct: struct (nullable = true) | |-- firstname: string (nullable = true) | |-- middlename: string (nullable = true) | |-- lastname: string (nullable = true) |-- f_array: array (nullable = true) | |-- element: integer (containsNull = true) |-- f_map: map (nullable = true) | |-- key: string | |-- value: string (valueContainsNull = true) |-- f_udf: array (nullable = true) | |-- element: string (containsNull = true) MapType1234567891011121314151617181920212223242526272829303132333435def myF(gender:String, m:Map[String, String]):Map[String, String] = &#123; m.filter(kv =&gt; kv._1.toInt &lt; 10)&#125;val myUdf = udf(myF _)myUdf: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(&lt;function2&gt;,MapType(StringType,StringType,true),Some(List(StringType, MapType(StringType,StringType,true))))val res = df.withColumn(&quot;f_udf&quot;, myUdf(col(&quot;gender&quot;), col(&quot;f_map&quot;)))res.show()res.printSchema+------+------+--------------------+-------+------------------+--------+|gender|salary| f_struct|f_array| f_map| f_udf|+------+------+--------------------+-------+------------------+--------+| M| 3000| [James , , Smith]| [1, 2]|[1 -&gt; a, 11 -&gt; aa]|[1 -&gt; a]|| M| 4000| [Michael , Rose, ]| [3, 2]|[2 -&gt; b, 22 -&gt; bb]|[2 -&gt; b]|| M| 4000|[Robert , , Willi...| [1, 2]|[3 -&gt; c, 33 -&gt; cc]|[3 -&gt; c]|| F| 4000|[Maria , Anne, Jo...| [3, 3]|[4 -&gt; d, 44 -&gt; dd]|[4 -&gt; d]|| F| -1| [Jen, Mary, Brown]| [5, 2]| [5 -&gt; e]|[5 -&gt; e]|+------+------+--------------------+-------+------------------+--------+root |-- gender: string (nullable = true) |-- salary: integer (nullable = true) |-- f_struct: struct (nullable = true) | |-- firstname: string (nullable = true) | |-- middlename: string (nullable = true) | |-- lastname: string (nullable = true) |-- f_array: array (nullable = true) | |-- element: integer (containsNull = true) |-- f_map: map (nullable = true) | |-- key: string | |-- value: string (valueContainsNull = true) |-- f_udf: map (nullable = true) | |-- key: string | |-- value: string (valueContainsNull = true) UDAFUDAF（User Defined Aggregate Function，即用户自定义的聚合函数）相比 UDF 要复杂很多，UDF 接收一行输入并产生一个输出，UDAF 则是接收一组（一般是多行）输入并产生一个输出，Spark 维护了一个 AggregationBuffer 来存储每组输入数据的中间结果。使用 UDAF 的一般步骤： 自定义类继承 UserDefinedAggregateFunction，对每个阶段方法做实现； 在 spark 中注册 UDAF，为其绑定一个名字； 然后就可以在sql语句中使用上面绑定的名字调用； 定义 UDAF我们通过一个计算平均值的 UDAF 实际例子来了解定义 UDAF 的过程： 1234567891011121314151617181920212223242526272829303132333435363738394041import org.apache.spark.sql.Rowimport org.apache.spark.sql.expressions.&#123;MutableAggregationBuffer, UserDefinedAggregateFunction&#125;import org.apache.spark.sql.types._ object AverageUserDefinedAggregateFunction extends UserDefinedAggregateFunction &#123; // 聚合函数的输入数据结构 override def inputSchema: StructType = StructType(StructField(&quot;input&quot;, LongType) :: Nil) // 缓存区数据结构 override def bufferSchema: StructType = StructType(StructField(&quot;sum&quot;, LongType) :: StructField(&quot;count&quot;, LongType) :: Nil) // 聚合函数返回值数据结构 override def dataType: DataType = DoubleType // 聚合函数是否是幂等的，即相同输入是否总是能得到相同输出 override def deterministic: Boolean = true // 初始化缓冲区 override def initialize(buffer: MutableAggregationBuffer): Unit = &#123; buffer(0) = 0L buffer(1) = 0L &#125; // 给聚合函数传入一条新数据进行处理 override def update(buffer: MutableAggregationBuffer, input: Row): Unit = &#123; if (input.isNullAt(0)) return buffer(0) = buffer.getLong(0) + input.getLong(0) buffer(1) = buffer.getLong(1) + 1 &#125; // 合并聚合函数缓冲区 override def merge(buffer1: MutableAggregationBuffer, buffer2: Row): Unit = &#123; buffer1(0) = buffer1.getLong(0) + buffer2.getLong(0) buffer1(1) = buffer1.getLong(1) + buffer2.getLong(1) &#125; // 计算最终结果 override def evaluate(buffer: Row): Any = buffer.getLong(0).toDouble / buffer.getLong(1) &#125; 注册-使用 UDAF1234567891011121314151617import org.apache.spark.sql.SparkSession object SparkSqlUDAFDemo_001 &#123; def main(args: Array[String]): Unit = &#123; val spark = SparkSession.builder().master(&quot;local[*]&quot;).appName(&quot;SparkStudy&quot;).getOrCreate() spark.read.json(&quot;data/user&quot;).createOrReplaceTempView(&quot;v_user&quot;) spark.udf.register(&quot;u_avg&quot;, AverageUserDefinedAggregateFunction) // 将整张表看做是一个分组对求所有人的平均年龄 spark.sql(&quot;select count(1) as count, u_avg(age) as avg_age from v_user&quot;).show() // 按照性别分组求平均年龄 spark.sql(&quot;select sex, count(1) as count, u_avg(age) as avg_age from v_user group by sex&quot;).show() &#125; &#125; 参考 《Spark 权威指南 Chapter 7.Aggregations》]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark 指南：Spark SQL（三）—— 结构化类型]]></title>
    <url>%2FSpark%2FSpark%2FSpark%20%E6%8C%87%E5%8D%97%EF%BC%9ASpark%20SQL%EF%BC%88%E4%B8%89%EF%BC%89%E2%80%94%E2%80%94%20%E7%BB%93%E6%9E%84%E5%8C%96%E7%B1%BB%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[Spark TypesSpark-Scala 数据类型Spark SQL 具有大量内部类型表示形式，下表列出了 Scala 绑定的类型信息： id Data Type Value type in Scala API to create a data Type 1 ByteType Byte ByteType 2 ShortType Short ShortType 3 IntegerType Int IntegerType 4 LongType Long LongType 5 FloatType Float FloatType 6 DoubleType Double DoubleType 7 DecimalType java.math.BigDecimal DecimalType 8 StringType String StringType 9 BinaryType Array[Byte] BinaryType 10 BooleanType Boolean BooleanType 11 TimestampType java.Timestamp TimestampType 12 DateType java.sql.Date DateType 13 ArrayType scala.collection.Seq ArrayType(elementType,[containsNull]) 14 MapType scala.collection.Map MapType(keyType,valueType,[valueContainsNull]) 15 StructType org.apache.spark.sql.Row tructType(fields: Array[StructField]) 16 StructField Scala中此字段的数据类型的值类型 StructField(name,dataType,[nullable]) 在 Scala 中，要使用 Spark 类型，需要先导入 org.apache.spark.sql.types._： 12345678910111213141516171819202122232425262728293031323334353637383940414243import org.apache.spark.sql.types._import org.apache.spark.sql.functions._val data = Seq( Row(Row(&quot;James &quot;,&quot;&quot;,&quot;Smith&quot;),&quot;36636&quot;,&quot;M&quot;,&quot;3000&quot;), Row(Row(&quot;Michael &quot;,&quot;Rose&quot;,&quot;&quot;),&quot;40288&quot;,&quot;M&quot;,&quot;4000&quot;), Row(Row(&quot;Robert &quot;,&quot;&quot;,&quot;Williams&quot;),&quot;42114&quot;,&quot;M&quot;,&quot;4000&quot;), Row(Row(&quot;Maria &quot;,&quot;Anne&quot;,&quot;Jones&quot;),&quot;39192&quot;,&quot;F&quot;,&quot;4000&quot;), Row(Row(&quot;Jen&quot;,&quot;Mary&quot;,&quot;Brown&quot;),&quot;&quot;,&quot;F&quot;,&quot;-1&quot;))val schema = new StructType() .add(&quot;name&quot;,new StructType() .add(&quot;firstname&quot;,StringType) .add(&quot;middlename&quot;,StringType) .add(&quot;lastname&quot;,StringType) ) .add(&quot;dob&quot;,StringType) .add(&quot;gender&quot;,StringType) .add(&quot;salary&quot;,StringType)val df = spark.createDataFrame(spark.sparkContext.parallelize(data),schema)df.show()df.printSchema+--------------------+-----+------+------+| name| dob|gender|salary|+--------------------+-----+------+------+| [James , , Smith]|36636| M| 3000|| [Michael , Rose, ]|40288| M| 4000||[Robert , , Willi...|42114| M| 4000||[Maria , Anne, Jo...|39192| F| 4000|| [Jen, Mary, Brown]| | F| -1|+--------------------+-----+------+------+root |-- name: struct (nullable = true) | |-- firstname: string (nullable = true) | |-- middlename: string (nullable = true) | |-- lastname: string (nullable = true) |-- dob: string (nullable = true) |-- gender: string (nullable = true) |-- salary: string (nullable = true) 数据类型转换本地类型 &amp; Spark 类型我们经常需要在本地类型和 Spark 类型之间进行转换，以利用各自在数据处理不同方面的优势，在转化过程中本地类型和 Spark 类型要符合上表中列出的对应关系，如果无法进行隐式转换就会报错： 本地类型 -&gt; Spark 类型： 通过本地对象创建 DataFrame：toDF()、createDataFrame()； 将本地基本类型转化为 Spark 基本类型：lit()； udf 返回值会被隐式地转化为 Spark 对应的类型； Spark 类型 -&gt; 本地类型： 将 DataFrame 收集到 driver端：collect()； 向 udf 传递参数时，会将 Spark 类型隐式地转化为对应的本地类型； 12import org.apache.spark.sql.functions.litdf.select(lit(5).as(&quot;f_integer&quot;), lit(&quot;five&quot;).as(&quot;f_string&quot;), lit(5.0).as(&quot;f_double&quot;)) 需要注意的是，如果传给 lit() 的参数本身就是 Column 对象，lit() 将原样返回该 Column 对象： 1234567891011121314151617181920/** * Creates a [[Column]] of literal value. * * The passed in object is returned directly if it is already a [[Column]]. * If the object is a Scala Symbol, it is converted into a [[Column]] also. * Otherwise, a new [[Column]] is created to represent the literal value. * * @group normal_funcs * @since 1.3.0 */def lit(literal: Any): Column = &#123; literal match &#123; case c: Column =&gt; return c case s: Symbol =&gt; return new ColumnName(literal.asInstanceOf[Symbol].name) case _ =&gt; // continue &#125; val literalExpr = Literal(literal) Column(literalExpr)&#125; Spark 类型 &amp; Spark 类型将 DataFrame 列类型从一种类型转换到另一种类型有很多种方法：withColumn()、cast()、selectExpr、SQL 表达式，需要注意的是目标类型必须是 DataType 的子类。 123456789101112131415161718192021222324252627282930313233343536// 示例数据import org.apache.spark.sql.Rowimport org.apache.spark.sql.types._val simpleData = Seq(Row(&quot;James&quot;,34,&quot;2006-01-01&quot;,&quot;true&quot;,&quot;M&quot;,3000.60), Row(&quot;Michael&quot;,33,&quot;1980-01-10&quot;,&quot;true&quot;,&quot;F&quot;,3300.80), Row(&quot;Robert&quot;,37,&quot;06-01-1992&quot;,&quot;false&quot;,&quot;M&quot;,5000.50) )val simpleSchema = StructType(Array( StructField(&quot;firstName&quot;,StringType,true), StructField(&quot;age&quot;,IntegerType,true), StructField(&quot;jobStartDate&quot;,StringType,true), StructField(&quot;isGraduated&quot;, StringType, true), StructField(&quot;gender&quot;, StringType, true), StructField(&quot;salary&quot;, DoubleType, true)))val df = spark.createDataFrame(spark.sparkContext.parallelize(simpleData),simpleSchema)df.printSchema()df.show(false)root |-- firstName: string (nullable = true) |-- age: integer (nullable = true) |-- jobStartDate: string (nullable = true) |-- isGraduated: string (nullable = true) |-- gender: string (nullable = true) |-- salary: double (nullable = true)+---------+---+------------+-----------+------+------+|firstName|age|jobStartDate|isGraduated|gender|salary|+---------+---+------------+-----------+------+------+|James |34 |2006-01-01 |true |M |3000.6||Michael |33 |1980-01-10 |true |F |3300.8||Robert |37 |06-01-1992 |false |M |5000.5|+---------+---+------------+-----------+------+------+ 通过 withColumn()、cast()： 123456789101112val df2 = df .withColumn(&quot;age&quot;,col(&quot;age&quot;).cast(StringType)) .withColumn(&quot;isGraduated&quot;,col(&quot;isGraduated&quot;).cast(BooleanType)) .withColumn(&quot;jobStartDate&quot;,col(&quot;jobStartDate&quot;).cast(DateType))df2.printSchema()root |-- firstName: string (nullable = true) |-- age: string (nullable = true) |-- jobStartDate: date (nullable = true) |-- isGraduated: boolean (nullable = true) |-- gender: string (nullable = true) |-- salary: double (nullable = true) 通过 select： 1234567891011121314151617val cast_df = df.select(df.columns.map &#123; case column@&quot;age&quot; =&gt; col(column).cast(&quot;String&quot;).as(column) case column@&quot;salary&quot; =&gt; col(column).cast(&quot;String&quot;).as(column) case column =&gt; col(column) &#125;: _*)cast_df.printSchema()root |-- firstName: string (nullable = true) |-- age: string (nullable = true) |-- jobStartDate: string (nullable = true) |-- isGraduated: string (nullable = true) |-- gender: string (nullable = true) |-- salary: string (nullable = true) 通过 selectExpr： 12345val df3 = df2.selectExpr(&quot;cast(age as int) age&quot;, &quot;cast(isGraduated as string) isGraduated&quot;, &quot;cast(jobStartDate as string) jobStartDate&quot;)df3.printSchema()df3.show(false) 布尔类型布尔类型是所有过滤的基础： 1234567891011121314151617181920212223242526272829303132333435363738394041df.where(col(&quot;salary&quot;) &lt; 4000).show()+------------------+-----+------+------+| name| dob|gender|salary|+------------------+-----+------+------+| [James , , Smith]|36636| M| 3000||[Jen, Mary, Brown]| | F| -1|+------------------+-----+------+------+// Scala 中判断列是否相等使用 ===，=!=df.where(col(&quot;salary&quot;) === 4000).show()+--------------------+-----+------+------+| name| dob|gender|salary|+--------------------+-----+------+------+| [Michael , Rose, ]|40288| M| 4000||[Robert , , Willi...|42114| M| 4000||[Maria , Anne, Jo...|39192| F| 4000|+--------------------+-----+------+------+df.where(col(&quot;salary&quot;) =!= 4000).show()+------------------+-----+------+------+| name| dob|gender|salary|+------------------+-----+------+------+| [James , , Smith]|36636| M| 3000||[Jen, Mary, Brown]| | F| -1|+------------------+-----+------+------+df.select((col(&quot;salary&quot;) =!= 4000).as(&quot;equal_400&quot;)).show()+---------+|equal_400|+---------+| true|| false|| false|| false|| true|+---------+df.select((col(&quot;salary&quot;) =!= 4000).as(&quot;equal_400&quot;)).printSchemaroot |-- equal_400: boolean (nullable = true)// 布尔表达式更简洁的表达方式是使用 SQL 表达式df.where(&quot;salary=4000 and gender=&#x27;M&#x27;&quot;).show() 数字类型摘要12345678910df.describe().show()+-------+------------------+------+------------------+|summary| dob|gender| salary|+-------+------------------+------+------------------+| count| 5| 5| 5|| mean| 39557.5| null| 2999.8|| stddev|2290.4202671125668| null|1732.4838238783068|| min| | F| -1|| max| 42114| M| 4000|+-------+------------------+------+------------------+ 运算123456789101112131415161718192021val df2 = df.withColumn(&quot;f_diff&quot;, (col(&quot;dob&quot;) - col(&quot;salary&quot;))/col(&quot;salary&quot;)) .withColumn(&quot;f_round&quot;, round(col(&quot;f_diff&quot;),2)) .withColumn(&quot;f_pow&quot;, pow(col(&quot;salary&quot;), 2))df2.show()+--------------------+-----+------+------+------+-------+---------+| name| dob|gender|salary|f_diff|f_round| f_pow|+--------------------+-----+------+------+------+-------+---------+| [James , , Smith]|36636| M| 3000|11.212| 11.21|9000000.0|| [Michael , Rose, ]|40288| M| 4000| 9.072| 9.07| 1.6E7||[Robert , , Willi...|42114| M| 4000|9.5285| 9.53| 1.6E7||[Maria , Anne, Jo...|39192| F| 4000| 8.798| 8.8| 1.6E7|| [Jen, Mary, Brown]| | F| -1| null| null| 1.0|+--------------------+-----+------+------+------+-------+---------+// 计算两列的协方差df2.select(corr(&quot;salary&quot;,&quot;f_pow&quot;)).show()+-------------------+|corr(salary, f_pow)|+-------------------+| 0.9817491111765669|+-------------------+ 统计StatFunctions 程序包中提供了许多统计功能，可以通过 df.stat 访问。 123456789101112131415// 交叉表df.stat.crosstab(&quot;gender&quot;, &quot;salary&quot;).show()+-------------+---+----+----+|gender_salary| -1|3000|4000|+-------------+---+----+----+| M| 0| 1| 2|| F| 1| 0| 1|+-------------+---+----+----+// 频次最高的值df.stat.freqItems(Seq(&quot;gender&quot;, &quot;salary&quot;)).show()+----------------+----------------+|gender_freqItems|salary_freqItems|+----------------+----------------+| [M, F]|[3000, 4000, -1]|+----------------+----------------+ 自增 IDmonotonically_increasing_id 生成一个单调递增并且是唯一的 ID。 1df.withColumn(&quot;f_id&quot;, monotonically_increasing_id()).show() 字符串类型截取12345678910111213// 语法：pos 从 1 开始substring(str: Column, pos: Int, len: Int)// 示例df.withColumn(&quot;f_substring&quot;, substring(col(&quot;dob&quot;), 2, 3)).show()+--------------------+-----+------+------+-----------+| name| dob|gender|salary|f_substring|+--------------------+-----+------+------+-----------+| [James , , Smith]|36636| M| 3000| 663|| [Michael , Rose, ]|40288| M| 4000| 028||[Robert , , Willi...|42114| M| 4000| 211||[Maria , Anne, Jo...|39192| F| 4000| 919|| [Jen, Mary, Brown]| | F| -1| |+--------------------+-----+------+------+-----------+ 拆分12345678910111213// 语法：pattern 是一个正则表达式，返回一个 Arraysplit(str: Column, pattern: String)// 示例df.withColumn(&quot;f_split&quot;, split(col(&quot;dob&quot;), &quot;6&quot;)).show()+--------------------+-----+------+------+----------+| name| dob|gender|salary| f_split|+--------------------+-----+------+------+----------+| [James , , Smith]|36636| M| 3000|[3, , 3, ]|| [Michael , Rose, ]|40288| M| 4000| [40288]||[Robert , , Willi...|42114| M| 4000| [42114]||[Maria , Anne, Jo...|39192| F| 4000| [39192]|| [Jen, Mary, Brown]| | F| -1| []|+--------------------+-----+------+------+----------+ 拼接1234567891011121314151617// 语法concat(exprs: Column*)concat_ws(sep: String, exprs: Column*)// 示例，第二个参数是变长参数，可以接收一个 array() 或者多个 Columndf.withColumn(&quot;f_concat&quot;, concat(col(&quot;gender&quot;), lit(&quot;-&quot;), col(&quot;dob&quot;))) .withColumn(&quot;f_concat_ws1&quot;, concat_ws(&quot;~&quot;, col(&quot;gender&quot;), col(&quot;dob&quot;))) .withColumn(&quot;f_concat_ws2&quot;, concat_ws(&quot;~&quot;, array(col(&quot;gender&quot;), col(&quot;dob&quot;)))) .show()+--------------------+-----+------+------+--------+------------+------------+| name| dob|gender|salary|f_concat|f_concat_ws1|f_concat_ws2|+--------------------+-----+------+------+--------+------------+------------+| [James , , Smith]|36636| M| 3000| M-36636| M~36636| M~36636|| [Michael , Rose, ]|40288| M| 4000| M-40288| M~40288| M~40288||[Robert , , Willi...|42114| M| 4000| M-42114| M~42114| M~42114||[Maria , Anne, Jo...|39192| F| 4000| F-39192| F~39192| F~39192|| [Jen, Mary, Brown]| | F| -1| F-| F~| F~|+--------------------+-----+------+------+--------+------------+------------+ 增删两侧12345678910111213141516// 语法trim(e: Column)trim(e: Column, trimString: String)// 示例df.select( ltrim(lit(&quot; HELLO &quot;)).as(&quot;f_ltrim&quot;), rtrim(lit(&quot; HELLO &quot;)).as(&quot;f_rtrim&quot;), trim(lit(&quot;---HELLO+++&quot;), &quot;+&quot;).as(&quot;f_trim&quot;), lpad(lit(&quot;HELLO&quot;), 10, &quot;+&quot;).as(&quot;f_lpad&quot;), rpad(lit(&quot;HELLO&quot;), 10, &quot;+&quot;).as(&quot;f_rpad&quot;)).show(1)+-------+-------+--------+----------+----------+|f_ltrim|f_rtrim| f_trim| f_lpad| f_rpad|+-------+-------+--------+----------+----------+|HELLO | HELLO|---HELLO|+++++HELLO|HELLO+++++|+-------+-------+--------+----------+----------+ 字符替换12345678910df.withColumn(&quot;f_translate&quot;, translate(col(&quot;dob&quot;), &quot;36&quot;, &quot;+-&quot;)).show()+--------------------+-----+------+------+-----------+| name| dob|gender|salary|f_translate|+--------------------+-----+------+------+-----------+| [James , , Smith]|36636| M| 3000| +--+-|| [Michael , Rose, ]|40288| M| 4000| 40288||[Robert , , Willi...|42114| M| 4000| 42114||[Maria , Anne, Jo...|39192| F| 4000| +9192|| [Jen, Mary, Brown]| | F| -1| |+--------------------+-----+------+------+-----------+ 子串查询12345678910111213// 语法，other 可以是 Column 对象，将逐行判断contains(other: Any)// 示例df.withColumn(&quot;f_contain&quot;, col(&quot;dob&quot;).contains(66)).show()+--------------------+-----+------+------+---------+| name| dob|gender|salary|f_contain|+--------------------+-----+------+------+---------+| [James , , Smith]|36636| M| 3000| true|| [Michael , Rose, ]|40288| M| 4000| false||[Robert , , Willi...|42114| M| 4000| false||[Maria , Anne, Jo...|39192| F| 4000| false|| [Jen, Mary, Brown]| | F| -1| false|+--------------------+-----+------+------+---------+ 正则替换正则详细规则参见这里。 1234567891011121314// 语法regexp_replace(e: Column, pattern: String, replacement: String)regexp_replace(e: Column, pattern: Column, replacement: Column)// 示例df.withColumn(&quot;f_regex_replace&quot;, regexp_replace(col(&quot;dob&quot;), &quot;6|3&quot;, &quot;+&quot;)).show()+--------------------+-----+------+------+---------------+| name| dob|gender|salary|f_regex_replace|+--------------------+-----+------+------+---------------+| [James , , Smith]|36636| M| 3000| +++++|| [Michael , Rose, ]|40288| M| 4000| 40288||[Robert , , Willi...|42114| M| 4000| 42114||[Maria , Anne, Jo...|39192| F| 4000| +9192|| [Jen, Mary, Brown]| | F| -1| |+--------------------+-----+------+------+---------------+ 正则抽取12345678910111213// 语法regexp_extract(e: Column, exp: String, groupIdx: Int)// 示例：重复连续出现两次的子串，(\\d) 作为编号为 1 的分组，整体正则串默认标号为0，\\1 使用分组 1 的内容df.withColumn(&quot;f_regex_extract&quot;, regexp_extract(col(&quot;dob&quot;), &quot;(\\d)\\1&#123;1&#125;&quot;, 0)).show()+--------------------+-----+------+------+---------------+| name| dob|gender|salary|f_regex_extract|+--------------------+-----+------+------+---------------+| [James , , Smith]|36636| M| 3000| 66|| [Michael , Rose, ]|40288| M| 4000| 88||[Robert , , Willi...|42114| M| 4000| 11||[Maria , Anne, Jo...|39192| F| 4000| || [Jen, Mary, Brown]| | F| -1| |+--------------------+-----+------+------+---------------+ 日期类型在 Spark 中，有四种日期相关的数据类型： DateType：日期，专注于日历日期； TimestampType：时间戳，包括日期和时间信息，仅支持秒级精度，如果要使用毫秒或微秒则需要进行额外处理； StringType：经常将日期和时间戳存储为字符串，并在其运行时转换为日期类型； LongType：Long 型时间戳，注意当通过 Spark SQL 内置函数返回整型时间戳时单位为秒； 本部分只介绍 Spark 内置的日期处理工具，更复杂的操作可以借助 java.text.SimpleDateFormat 和 java.util.&#123;Calendar, Date&#125; 使用 UDF 来解决。 日期获取获取当前日期123456789101112131415161718192021val df = spark.range(3) .withColumn(&quot;date&quot;, current_date()) .withColumn(&quot;timestamp&quot;, current_timestamp()) .withColumn(&quot;dateStr&quot;,lit(&quot;2020-11-07&quot;)) .withColumn(&quot;timestampLong&quot;, unix_timestamp())df.show(false)df.printSchema+---+----------+-----------------------+----------+-------------+|id |date |timestamp |dateStr |timestampLong|+---+----------+-----------------------+----------+-------------+|0 |2020-11-07|2020-11-07 18:55:38.947|2020-11-07|1604746538 ||1 |2020-11-07|2020-11-07 18:55:38.947|2020-11-07|1604746538 ||2 |2020-11-07|2020-11-07 18:55:38.947|2020-11-07|1604746538 |+---+----------+-----------------------+----------+-------------+root |-- id: long (nullable = false) |-- date: date (nullable = false) |-- timestamp: timestamp (nullable = false) |-- dateStr: string (nullable = false) |-- timestampLong: long (nullable = true) 从日期中提取字段1234567891011121314151617181920212223val tmp = spark.range(1).select(lit(&quot;2020-11-07 19:45:12&quot;).as(&quot;date&quot;)) .withColumn(&quot;year&quot;, year(col(&quot;date&quot;))) .withColumn(&quot;month&quot;, month(col(&quot;date&quot;))) .withColumn(&quot;day&quot;, dayofmonth(col(&quot;date&quot;))) .withColumn(&quot;hour&quot;, hour(col(&quot;date&quot;))) .withColumn(&quot;minute&quot;, minute(col(&quot;date&quot;))) .withColumn(&quot;second&quot;, second(col(&quot;date&quot;)))tmp.show(1)tmp.printSchema+-------------------+----+-----+---+----+------+------+| date|year|month|day|hour|minute|second|+-------------------+----+-----+---+----+------+------+|2020-11-07 19:45:12|2020| 11| 7| 19| 45| 12|+-------------------+----+-----+---+----+------+------+root |-- date: string (nullable = false) |-- year: integer (nullable = true) |-- month: integer (nullable = true) |-- day: integer (nullable = true) |-- hour: integer (nullable = true) |-- minute: integer (nullable = true) |-- second: integer (nullable = true) 获取特殊日期1234567891011121314151617181920212223242526272829val tmp = spark.range(1).select(lit(&quot;2020-11-07 19:45:12&quot;).as(&quot;date&quot;)) .withColumn(&quot;dayofyear&quot;, dayofyear(col(&quot;date&quot;))) .withColumn(&quot;dayofmonth&quot;, dayofmonth(col(&quot;date&quot;))) .withColumn(&quot;dayofweek&quot;, dayofweek(col(&quot;date&quot;))) .withColumn(&quot;weekofyear&quot;, weekofyear(col(&quot;date&quot;))) // date_sub 第二个参数不支持 Column 只能用表达式，解决此问题更好的方式是使用 next_day .withColumn(&quot;monday_expr&quot;, expr(&quot;date_sub(date, (dayofweek(date) -2) % 7)&quot;)) // next_day 获取相对指定日期下一周某天的日期，dayOfWeek 参数对大小写不敏感，而且接受以下简写 // &quot;Mon&quot;, &quot;Tue&quot;, &quot;Wed&quot;, &quot;Thu&quot;, &quot;Fri&quot;, &quot;Sat&quot;, &quot;Sun&quot; .withColumn(&quot;monday&quot;, date_sub(next_day(col(&quot;date&quot;), &quot;monday&quot;), 7)) // trunc截取某部分的日期，其他部分默认为01 .withColumn(&quot;trunc&quot;, trunc(col(&quot;date&quot;), &quot;MONTH&quot;))tmp.show(1)tmp.printSchema+-------------------+---------+----------+---------+----------+-----------+----------+----------+| date|dayofyear|dayofmonth|dayofweek|weekofyear|monday_expr| monday| trunc|+-------------------+---------+----------+---------+----------+-----------+----------+----------+|2020-11-07 19:45:12| 312| 7| 7| 45| 2020-11-02|2020-11-02|2020-11-01|+-------------------+---------+----------+---------+----------+-----------+----------+----------+root |-- date: string (nullable = false) |-- dayofyear: integer (nullable = true) |-- dayofmonth: integer (nullable = true) |-- dayofweek: integer (nullable = true) |-- weekofyear: integer (nullable = true) |-- monday_expr: date (nullable = true) |-- monday: date (nullable = true) |-- trunc: date (nullable = true) 类型转换日期相关的四种数据类型之间的转换方法如下图所示，其中，格式串遵守 Java SimpleDateFormat 标准。 Long &amp; Stringfrom_unixtime 函数可以将 Long 型时间戳转化为 String 类型的日期，unix_timestamp 函数可以将 String 类型的日期转化为 Long 型时间戳。 语法： 1234567891011// 默认返回当前秒级时间戳，在同一个查询中对 unix_timestamp 的所有调用都会返回相同值，unix_timestamp 会在查询开始时进行计算unix_timestamp()// 将 yyyy-MM-dd HH:mm:ss 格式的时间字符串转化为秒级时间戳，如果失败则会返回 nullunix_timestamp(s: Column)// 按照指定格式将时间字符串转化为秒级时间戳，格式串可参考 http://docs.oracle.com/javase/tutorial/i18n/format/simpleDateFormat.htmlunix_timestamp(s: Column, p: String)// 将秒级时间戳转化为 yyyy-MM-dd HH:mm:ss 格式的时间字符串from_unixtime(ut: Column)// 按指定格式将秒级时间戳转化为时间字符串from_unixtime(ut: Column, f: String) 示例： 123456789101112131415161718192021222324val tmp = df.withColumn(&quot;long_string&quot;, from_unixtime(col(&quot;timestampLong&quot;))) .withColumn(&quot;long_string2&quot;, from_unixtime(col(&quot;timestampLong&quot;), &quot;yyyyMMdd&quot;)) .withColumn(&quot;string_long&quot;, unix_timestamp(col(&quot;dateStr&quot;), &quot;yyyy-MM-dd&quot;)) .withColumn(&quot;date_long&quot;, unix_timestamp(col(&quot;date&quot;), &quot;yyyy-MM-dd&quot;))tmp.show()tmp.printSchema+---+----------+--------------------+----------+-------------+-------------------+------------+-----------+----------+| id| date| timestamp| dateStr|timestampLong| long_string|long_string2|string_long| date_long|+---+----------+--------------------+----------+-------------+-------------------+------------+-----------+----------+| 0|2020-11-07|2020-11-07 19:10:...|2020-11-07| 1604747436|2020-11-07 19:10:36| 20201107| 1604678400|1604678400|| 1|2020-11-07|2020-11-07 19:10:...|2020-11-07| 1604747436|2020-11-07 19:10:36| 20201107| 1604678400|1604678400|| 2|2020-11-07|2020-11-07 19:10:...|2020-11-07| 1604747436|2020-11-07 19:10:36| 20201107| 1604678400|1604678400|+---+----------+--------------------+----------+-------------+-------------------+------------+-----------+----------+root |-- id: long (nullable = false) |-- date: date (nullable = false) |-- timestamp: timestamp (nullable = false) |-- dateStr: string (nullable = false) |-- timestampLong: long (nullable = true) |-- long_string: string (nullable = true) |-- long_string2: string (nullable = true) |-- string_long: long (nullable = true) |-- date_long: long (nullable = true) String &amp; Dateto_date 函数可以将时间字符串转化为 date 类型，如果不指定具体的格式串，则等价于 cast(&quot;date&quot;)；date_format 函数可以将 date/timestamp/string 类型的日期时间转化为指定格式的时间字符串，如果只是希望将他们按原样转化为字符串，也可直接通过 cast(&quot;string&quot;) 来实现。 语法： 12345678// 等价于 col(e: Column).cast(&quot;date&quot;)to_date(e: Column)// 按照指定格式将时间字符串转化为dateto_date(e: Column, fmt: String)// 将 date/timestamp/string 按照指定格式转化为时间字符串date_format(dateExpr: Column, format: String) 示例： 123456789101112131415161718192021val tmp = df.withColumn(&quot;date_string&quot;, date_format(col(&quot;date&quot;), &quot;yyyyMMdd&quot;)) .withColumn(&quot;string_date&quot;, to_date(col(&quot;dateStr&quot;), &quot;yyyy-MM-dd&quot;))tmp.show()tmp.printSchema+---+----------+--------------------+----------+-------------+-----------+-----------+| id| date| timestamp| dateStr|timestampLong|date_string|string_date|+---+----------+--------------------+----------+-------------+-----------+-----------+| 0|2020-11-07|2020-11-07 19:15:...|2020-11-07| 1604747711| 20201107| 2020-11-07|| 1|2020-11-07|2020-11-07 19:15:...|2020-11-07| 1604747711| 20201107| 2020-11-07|| 2|2020-11-07|2020-11-07 19:15:...|2020-11-07| 1604747711| 20201107| 2020-11-07|+---+----------+--------------------+----------+-------------+-----------+-----------+root |-- id: long (nullable = false) |-- date: date (nullable = false) |-- timestamp: timestamp (nullable = false) |-- dateStr: string (nullable = false) |-- timestampLong: long (nullable = true) |-- date_string: string (nullable = false) |-- string_date: date (nullable = true) String &amp; Timestamp和 string &amp; date 之间的转换基本一致，不再赘述，这里只通过几个示例来做说明： 1234567891011121314151617181920val tmp = df.withColumn(&quot;timestamp_string&quot;, date_format(col(&quot;timestamp&quot;), &quot;yyyyMMdd&quot;)) .withColumn(&quot;string_timestamp&quot;, to_timestamp(col(&quot;dateStr&quot;), &quot;yyyy-MM-dd&quot;))tmp.show()tmp.printSchema+---+----------+--------------------+----------+-------------+----------------+-------------------+| id| date| timestamp| dateStr|timestampLong|timestamp_string| string_timestamp|+---+----------+--------------------+----------+-------------+----------------+-------------------+| 0|2020-11-07|2020-11-07 19:24:...|2020-11-07| 1604748297| 20201107|2020-11-07 00:00:00|| 1|2020-11-07|2020-11-07 19:24:...|2020-11-07| 1604748297| 20201107|2020-11-07 00:00:00|| 2|2020-11-07|2020-11-07 19:24:...|2020-11-07| 1604748297| 20201107|2020-11-07 00:00:00|+---+----------+--------------------+----------+-------------+----------------+-------------------+root |-- id: long (nullable = false) |-- date: date (nullable = false) |-- timestamp: timestamp (nullable = false) |-- dateStr: string (nullable = false) |-- timestampLong: long (nullable = true) |-- timestamp_string: string (nullable = false) |-- string_timestamp: timestamp (nullable = true) Date &amp; Timestampdate &amp; timestamp 之间的转换直接通过 cast 即可实现，无需赘言： 1234567891011121314151617181920val tmp = df.withColumn(&quot;timestamp_date&quot;, col(&quot;timestamp&quot;).cast(&quot;date&quot;)) .withColumn(&quot;date_timestamp&quot;, col(&quot;date&quot;).cast(&quot;timestamp&quot;))tmp.show()tmp.printSchema+---+----------+--------------------+----------+-------------+--------------+-------------------+| id| date| timestamp| dateStr|timestampLong|timestamp_date| date_timestamp|+---+----------+--------------------+----------+-------------+--------------+-------------------+| 0|2020-11-07|2020-11-07 19:27:...|2020-11-07| 1604748466| 2020-11-07|2020-11-07 00:00:00|| 1|2020-11-07|2020-11-07 19:27:...|2020-11-07| 1604748466| 2020-11-07|2020-11-07 00:00:00|| 2|2020-11-07|2020-11-07 19:27:...|2020-11-07| 1604748466| 2020-11-07|2020-11-07 00:00:00|+---+----------+--------------------+----------+-------------+--------------+-------------------+root |-- id: long (nullable = false) |-- date: date (nullable = false) |-- timestamp: timestamp (nullable = false) |-- dateStr: string (nullable = false) |-- timestampLong: long (nullable = true) |-- timestamp_date: date (nullable = false) |-- date_timestamp: timestamp (nullable = false) 日期运算用到的时候搜索 API 即可，这里还是有必要列出最常用到的： 日期 ± 天数123456789101112131415161718192021222324252627282930313233// 原型，start 必须是date或者可以隐式地通过 cast(&quot;date&quot;) 转化为 date (timestamp 或 yyyy-MM-dd HH:ss 格式的字符串)// 奇怪的是 days 是 int 类型，而不是 Column，导致days 参数不能传入另一列，但是 SQL 表达式可以date_add(start: Column, days: Int)date_sub(start: Column, days: Int)// 示例val tmp = df .withColumn(&quot;n&quot;, lit(1)) .withColumn(&quot;date_add&quot;, date_add(col(&quot;date&quot;), 2)) .withColumn(&quot;timestamp_add&quot;, date_add(col(&quot;timestamp&quot;), 2)) .withColumn(&quot;string_add&quot;, date_add(col(&quot;dateStr&quot;), 2))// .withColumn(&quot;string_sub&quot;, date_sub(col(&quot;dateStr&quot;), col(&quot;n&quot;))) .withColumn(&quot;string_sub&quot;, expr(&quot;date_sub(dateStr, n)&quot;))tmp.show()tmp.printSchema+---+----------+--------------------+----------+-------------+---+----------+-------------+----------+----------+| id| date| timestamp| dateStr|timestampLong| n| date_add|timestamp_add|string_add|string_sub|+---+----------+--------------------+----------+-------------+---+----------+-------------+----------+----------+| 0|2020-11-07|2020-11-07 20:14:...|2020-11-07| 1604751268| 1|2020-11-09| 2020-11-09|2020-11-09|2020-11-06|| 1|2020-11-07|2020-11-07 20:14:...|2020-11-07| 1604751268| 1|2020-11-09| 2020-11-09|2020-11-09|2020-11-06|| 2|2020-11-07|2020-11-07 20:14:...|2020-11-07| 1604751268| 1|2020-11-09| 2020-11-09|2020-11-09|2020-11-06|+---+----------+--------------------+----------+-------------+---+----------+-------------+----------+----------+root |-- id: long (nullable = false) |-- date: date (nullable = false) |-- timestamp: timestamp (nullable = false) |-- dateStr: string (nullable = false) |-- timestampLong: long (nullable = true) |-- n: integer (nullable = false) |-- date_add: date (nullable = false) |-- timestamp_add: date (nullable = false) |-- string_add: date (nullable = true) |-- string_sub: date (nullable = true) 日期 - 日期123456789101112131415161718192021// 返回 end - start 的天数datediff(end: Column, start: Column)val tmp = df.withColumn(&quot;date_diff&quot;, datediff(col(&quot;date&quot;), lit(&quot;2020-11-01&quot;)))tmp.show()tmp.printSchema+---+----------+--------------------+----------+-------------+---------+| id| date| timestamp| dateStr|timestampLong|date_diff|+---+----------+--------------------+----------+-------------+---------+| 0|2020-11-07|2020-11-07 19:39:...|2020-11-07| 1604749181| 6|| 1|2020-11-07|2020-11-07 19:39:...|2020-11-07| 1604749181| 6|| 2|2020-11-07|2020-11-07 19:39:...|2020-11-07| 1604749181| 6|+---+----------+--------------------+----------+-------------+---------+root |-- id: long (nullable = false) |-- date: date (nullable = false) |-- timestamp: timestamp (nullable = false) |-- dateStr: string (nullable = false) |-- timestampLong: long (nullable = true) |-- date_diff: integer (nullable = true) 月份运算1234567891011121314151617181920val tmp = df.withColumn(&quot;month_diff&quot;, months_between(col(&quot;date&quot;), lit(&quot;2020-09-01&quot;))) .withColumn(&quot;add_months&quot;, add_months(col(&quot;date&quot;), 1))tmp.show()tmp.printSchema+---+----------+--------------------+----------+-------------+----------+----------+| id| date| timestamp| dateStr|timestampLong|month_diff|add_months|+---+----------+--------------------+----------+-------------+----------+----------+| 0|2020-11-07|2020-11-07 19:41:...|2020-11-07| 1604749312|2.19354839|2020-12-07|| 1|2020-11-07|2020-11-07 19:41:...|2020-11-07| 1604749312|2.19354839|2020-12-07|| 2|2020-11-07|2020-11-07 19:41:...|2020-11-07| 1604749312|2.19354839|2020-12-07|+---+----------+--------------------+----------+-------------+----------+----------+root |-- id: long (nullable = false) |-- date: date (nullable = false) |-- timestamp: timestamp (nullable = false) |-- dateStr: string (nullable = false) |-- timestampLong: long (nullable = true) |-- month_diff: double (nullable = true) |-- add_months: date (nullable = false) 处理空值最佳实践是，你应该始终使用 null 来表示 DataFrame 中缺失或为空的数据，与使用空字符串或其他值相比，Spark 可以优化使用 null 的工作。对于空值的处理，要么删除要么填充，与 null 交互的主要方式是在 DataFrame 上调用 .na 子包。 填充空值 ifnull(expr1, expr2)：默认返回 expr1，如果 expr1 值为 null 则返回 expr2；只用于 SQL 表达式；nullif(expr1, expr2)：如果条件为真则返回 null，否则返回 expr1；只用于 SQL 表达式；nvl(expr1, expr2)：同 ifnull；nvl2(expr1, expr2, expr3)：如果 expr1 为 null 则返回 expr2，否则返回 expr3； 1234567891011121314df.createOrReplaceTempView(&quot;df&quot;)spark.sql(&quot;&quot;&quot;selectifnull(null, &#x27;return_value&#x27;) as a,nullif(&#x27;value&#x27;, &#x27;value&#x27;) as b,nvl(null, &#x27;return_value&#x27;) as c,nvl2(&#x27;not_null&#x27;, &#x27;return_value&#x27;, &#x27;else_value&#x27;) as dfrom df limit 1&quot;&quot;&quot;).show()+------------+----+------------+------------+| a| b| c| d|+------------+----+------------+------------+|return_value|null|return_value|return_value|+------------+----+------------+------------+ coalesce(e: Column*)：从左向右，返回第一个不为 null 的值； 123456df.select(coalesce(lit(null), lit(null), lit(1)).as(&quot;coalesce&quot;)).show(1)+--------+|coalesce|+--------+| 1|+--------+ na.fill：用法比较灵活：只有 value 的类型和所在列的原有类型可隐式转换时才会填充 如果对所有列都用相同的值填充空值，可以用 df.na.fill(value)； 如果对几个列都用相同的值填充空值，可以用 df.na.fill(value, Seq(cols_name*))； 如果对几个列分别用不同的值填充空值，可以用 df.na.fill(Map(col-&gt;value)) 12345678910111213141516171819202122232425262728293031323334353637383940414243val df = spark.range(1).select( lit(null).cast(&quot;string&quot;).as(&quot;f_string1&quot;), lit(&quot;x&quot;).cast(&quot;string&quot;).as(&quot;f_string2&quot;), lit(null).cast(&quot;int&quot;).as(&quot;f_int&quot;), lit(null).cast(&quot;double&quot;).as(&quot;f_double&quot;), lit(null).cast(&quot;boolean&quot;).as(&quot;f_bool&quot;))df.show()df.printSchema+---------+---------+-----+--------+------+|f_string1|f_string2|f_int|f_double|f_bool|+---------+---------+-----+--------+------+| null| x| null| null| null|+---------+---------+-----+--------+------+root |-- f_string1: string (nullable = true) |-- f_string2: string (nullable = false) |-- f_int: integer (nullable = true) |-- f_double: double (nullable = true) |-- f_bool: boolean (nullable = true)df.na.fill(1).show()+---------+---------+-----+--------+------+|f_string1|f_string2|f_int|f_double|f_bool|+---------+---------+-----+--------+------+| null| x| 1| 1.0| null|+---------+---------+-----+--------+------+df.na.fill(1, Seq(&quot;f_int&quot;)).show()+---------+---------+-----+--------+------+|f_string1|f_string2|f_int|f_double|f_bool|+---------+---------+-----+--------+------+| null| x| 1| null| null|+---------+---------+-----+--------+------+df.na.fill(Map(&quot;f_int&quot;-&gt;1, &quot;f_string1&quot;-&gt;&quot;&quot;)).show()+---------+---------+-----+--------+------+|f_string1|f_string2|f_int|f_double|f_bool|+---------+---------+-----+--------+------+| | x| 1| null| null|+---------+---------+-----+--------+------+ 删除空值删除空值可以分为以下几种情况： 删除某列为空的行：直接通过 .where(&quot;col is not null&quot;) 即可完成； 删除包含空值的行：na.drop(); 删除所有列均为空的行：na.drop(&quot;all&quot;) 仅当改行所有列均为 null 或 NaN 时，才会删除； 123456789101112df.na.drop().show()+---------+---------+-----+--------+------+|f_string1|f_string2|f_int|f_double|f_bool|+---------+---------+-----+--------+------++---------+---------+-----+--------+------+df.na.drop(&quot;all&quot;).show()+---------+---------+-----+--------+------+|f_string1|f_string2|f_int|f_double|f_bool|+---------+---------+-----+--------+------+| null| x| null| null| null|+---------+---------+-----+--------+------+ 处理复杂类型复杂类型可以帮助你以对问题更有意义的方式组织和构造数据，Spark SQL 中复杂类型共有三种： id Data Type Scala Type API to create a data Type 1 StructType org.apache.spark.sql.Row tructType(fields: Array[StructField]) 2 ArrayType scala.collection.Seq ArrayType(elementType,[containsNull]) 3 MapType scala.collection.Map MapType(keyType,valueType,[valueContainsNull]) 示例数据：创建 DataFrame 时，显式定义 struct/array/map 类型 123456789101112131415161718192021222324252627282930313233343536373839404142434445val data = Seq( Row(&quot;M&quot;, 3000, Row(&quot;James &quot;,&quot;&quot;,&quot;Smith&quot;), Seq(1,2), Map(&quot;1&quot;-&gt;&quot;a&quot;, &quot;11&quot;-&gt;&quot;aa&quot;)), Row(&quot;M&quot;, 4000, Row(&quot;Michael &quot;,&quot;Rose&quot;,&quot;&quot;), Seq(3,2), Map(&quot;2&quot;-&gt;&quot;b&quot;, &quot;22&quot;-&gt;&quot;bb&quot;)), Row(&quot;M&quot;, 4000, Row(&quot;Robert &quot;,&quot;&quot;,&quot;Williams&quot;), Seq(1,2), Map(&quot;3&quot;-&gt;&quot;c&quot;, &quot;33&quot;-&gt;&quot;cc&quot;)), Row(&quot;F&quot;, 4000, Row(&quot;Maria &quot;,&quot;Anne&quot;,&quot;Jones&quot;), Seq(3,3), Map(&quot;4&quot;-&gt;&quot;d&quot;, &quot;44&quot;-&gt;&quot;dd&quot;)), Row(&quot;F&quot;, -1, Row(&quot;Jen&quot;,&quot;Mary&quot;,&quot;Brown&quot;), Seq(5,2), Map(&quot;5&quot;-&gt;&quot;e&quot;)) )val schema = new StructType() .add(&quot;gender&quot;,StringType) .add(&quot;salary&quot;,IntegerType) .add(&quot;f_struct&quot;, new StructType() .add(&quot;firstname&quot;,StringType) .add(&quot;middlename&quot;,StringType) .add(&quot;lastname&quot;,StringType) ) .add(&quot;f_array&quot;, ArrayType(IntegerType)) .add(&quot;f_map&quot;, MapType(StringType, StringType))val df = spark.createDataFrame(spark.sparkContext.parallelize(data),schema)df.show()df.printSchema+------+------+--------------------+-------+------------------+|gender|salary| f_struct|f_array| f_map|+------+------+--------------------+-------+------------------+| M| 3000| [James , , Smith]| [1, 2]|[1 -&gt; a, 11 -&gt; aa]|| M| 4000| [Michael , Rose, ]| [3, 2]|[2 -&gt; b, 22 -&gt; bb]|| M| 4000|[Robert , , Willi...| [1, 2]|[3 -&gt; c, 33 -&gt; cc]|| F| 4000|[Maria , Anne, Jo...| [3, 3]|[4 -&gt; d, 44 -&gt; dd]|| F| -1| [Jen, Mary, Brown]| [5, 2]| [5 -&gt; e]|+------+------+--------------------+-------+------------------+root |-- gender: string (nullable = true) |-- salary: integer (nullable = true) |-- f_struct: struct (nullable = true) | |-- firstname: string (nullable = true) | |-- middlename: string (nullable = true) | |-- lastname: string (nullable = true) |-- f_array: array (nullable = true) | |-- element: integer (containsNull = true) |-- f_map: map (nullable = true) | |-- key: string | |-- value: string (valueContainsNull = true) StructType可以将 struct 视为 DataFrame 中的 DataFrame，struct 是一个拥有命名子域的结构体。 基于现有列生成 struct: 在 Column 对象上使用 struct 函数，或者在表达式中使用一对括号 12345678910df.select(struct(col(&quot;gender&quot;), col(&quot;salary&quot;)), expr(&quot;(gender, salary)&quot;)).show()+--------------------------------------------+--------------------------------------------+|named_struct(gender, gender, salary, salary)|named_struct(gender, gender, salary, salary)|+--------------------------------------------+--------------------------------------------+| [M, 3000]| [M, 3000]|| [M, 4000]| [M, 4000]|| [M, 4000]| [M, 4000]|| [F, 4000]| [F, 4000]|| [F, -1]| [F, -1]|+--------------------------------------------+--------------------------------------------+ 提取 struct 中的值：点操作会直接提取子域的值，列名为子域名，特别的，.* 可以提取 struct 中所有的子域；getField 方法也可以提取子域的值，但列名为完整带点号的名称 12345678910df.select(coldf.select(col(&quot;f_struct.firstname&quot;), expr(&quot;f_struct.firstname&quot;), col(&quot;f_struct&quot;).getField(&quot;firstname&quot;), col(&quot;f_struct.*&quot;)).show()+---------+---------+------------------+---------+----------+--------+|firstname|firstname|f_struct.firstname|firstname|middlename|lastname|+---------+---------+------------------+---------+----------+--------+| James | James | James | James | | Smith|| Michael | Michael | Michael | Michael | Rose| || Robert | Robert | Robert | Robert | |Williams|| Maria | Maria | Maria | Maria | Anne| Jones|| Jen| Jen| Jen| Jen| Mary| Brown|+---------+---------+------------------+---------+----------+--------+ ArrayType 基于现有列生成 array：列对象和表达式用法相同，都是在多列外使用 array 函数；split、collect_list 等函数也会返回 array； 1234567891011121314151617df.select(array(col(&quot;gender&quot;), col(&quot;salary&quot;)), expr(&quot;array(gender, salary)&quot;)).show()+---------------------+-------------------------------------+|array(gender, salary)|array(gender, CAST(salary AS STRING))|+---------------------+-------------------------------------+| [M, 3000]| [M, 3000]|| [M, 4000]| [M, 4000]|| [M, 4000]| [M, 4000]|| [F, 4000]| [F, 4000]|| [F, -1]| [F, -1]|+---------------------+-------------------------------------+df.groupBy().agg(collect_list(col(&quot;gender&quot;)).as(&quot;collect_list&quot;)).show()+---------------+| collect_list|+---------------+|[M, M, M, F, F]|+---------------+ 提取 array 中的元素：通过 [index] 按索引提取数组中的值； 12345678910df.select(col(&quot;f_array&quot;).getItem(0), expr(&quot;f_array[0]&quot;)).show()+----------+----------+|f_array[0]|f_array[0]|+----------+----------+| 1| 1|| 3| 3|| 1| 1|| 3| 3|| 5| 5|+----------+----------+ 处理 array 的函数：参考 org.apache.spark.functions 1234567891011121314151617181920212223242526272829303132333435df.select( size(col(&quot;f_array&quot;)).as(&quot;f_array_size&quot;), array_contains(col(&quot;f_array&quot;), 1).as(&quot;f_array_contain&quot;), array_max(col(&quot;f_array&quot;)).as(&quot;f_array_max&quot;), array_distinct(col(&quot;f_array&quot;)).as(&quot;f_array_distinct&quot;), array_position(col(&quot;f_array&quot;), 3).as(&quot;f_array_pos&quot;), array_sort(col(&quot;f_array&quot;)).as(&quot;f_array_sort&quot;), array_remove(col(&quot;f_array&quot;), 2).as(&quot;f_array_remove&quot;)).show()+------------+---------------+-----------+----------------+-----------+------------+--------------+|f_array_size|f_array_contain|f_array_max|f_array_distinct|f_array_pos|f_array_sort|f_array_remove|+------------+---------------+-----------+----------------+-----------+------------+--------------+| 2| true| 2| [1, 2]| 0| [1, 2]| [1]|| 2| false| 3| [3, 2]| 1| [2, 3]| [3]|| 2| true| 2| [1, 2]| 0| [1, 2]| [1]|| 2| false| 3| [3]| 1| [3, 3]| [3, 3]|| 2| false| 5| [5, 2]| 0| [2, 5]| [5]|+------------+---------------+-----------+----------------+-----------+------------+--------------+// explode 会将数组中的所有元素取出，为每个值创建一个行，其他字段保持原样不变，默认忽略空数组df.withColumn(&quot;f_array_val&quot;, explode(col(&quot;f_array&quot;))).show()+------+------+--------------------+-------+------------------+-----------+|gender|salary| f_struct|f_array| f_map|f_array_val|+------+------+--------------------+-------+------------------+-----------+| M| 3000| [James , , Smith]| [1, 2]|[1 -&gt; a, 11 -&gt; aa]| 1|| M| 3000| [James , , Smith]| [1, 2]|[1 -&gt; a, 11 -&gt; aa]| 2|| M| 4000| [Michael , Rose, ]| [3, 2]|[2 -&gt; b, 22 -&gt; bb]| 3|| M| 4000| [Michael , Rose, ]| [3, 2]|[2 -&gt; b, 22 -&gt; bb]| 2|| M| 4000|[Robert , , Willi...| [1, 2]|[3 -&gt; c, 33 -&gt; cc]| 1|| M| 4000|[Robert , , Willi...| [1, 2]|[3 -&gt; c, 33 -&gt; cc]| 2|| F| 4000|[Maria , Anne, Jo...| [3, 3]|[4 -&gt; d, 44 -&gt; dd]| 3|| F| 4000|[Maria , Anne, Jo...| [3, 3]|[4 -&gt; d, 44 -&gt; dd]| 3|| F| -1| [Jen, Mary, Brown]| [5, 2]| [5 -&gt; e]| 5|| F| -1| [Jen, Mary, Brown]| [5, 2]| [5 -&gt; e]| 2|+------+------+--------------------+-------+------------------+-----------+ MapType 基于现有列生成 map：Column 和表达式用法相同，map(key1, value1, key2, value2, ...)；其中，输入列必须可以被分组为 key-value 对，所有 key 列必须具有相同类型且不能为 null，value 列也必须具有相同类型（或者可以通过 cast 转化为相同类型）； 1234567891011121314151617181920212223val dfmap = df.select( map(col(&quot;gender&quot;), lit(1), col(&quot;salary&quot;), lit(&quot;2&quot;)), expr(&quot;map(gender, 1, salary, 2)&quot;))dfmap.show()dfmap.printSchema+-------------------------+-----------------------------------------+|map(gender, 1, salary, 2)|map(gender, 1, CAST(salary AS STRING), 2)|+-------------------------+-----------------------------------------+| [M -&gt; 1, 3000 -&gt; 2]| [M -&gt; 1, 3000 -&gt; 2]|| [M -&gt; 1, 4000 -&gt; 2]| [M -&gt; 1, 4000 -&gt; 2]|| [M -&gt; 1, 4000 -&gt; 2]| [M -&gt; 1, 4000 -&gt; 2]|| [F -&gt; 1, 4000 -&gt; 2]| [F -&gt; 1, 4000 -&gt; 2]|| [F -&gt; 1, -1 -&gt; 2]| [F -&gt; 1, -1 -&gt; 2]|+-------------------------+-----------------------------------------+root |-- map(gender, 1, salary, 2): map (nullable = false) | |-- key: string | |-- value: string (valueContainsNull = false) |-- map(gender, 1, CAST(salary AS STRING), 2): map (nullable = false) | |-- key: string | |-- value: integer (valueContainsNull = false) 处理 map 的函数： 12345678910111213141516171819202122232425262728293031dfmap .withColumn(&quot;map_keys&quot;, map_keys(col(&quot;f_map&quot;))) .withColumn(&quot;map_values&quot;, map_values(col(&quot;f_map&quot;))) // 返回 map 中指定 key 对应的 value，如果没有找到对应的 key 则返回 null .withColumn(&quot;f_value&quot;, expr(&quot;f_map[&#x27;M&#x27;]&quot;)) .show()+-------------------+-----------------------------------------+---------+----------+-------+| f_map|map(gender, 1, CAST(salary AS STRING), 2)| map_keys|map_values|f_value|+-------------------+-----------------------------------------+---------+----------+-------+|[M -&gt; 1, 3000 -&gt; 2]| [M -&gt; 1, 3000 -&gt; 2]|[M, 3000]| [1, 2]| 1||[M -&gt; 1, 4000 -&gt; 2]| [M -&gt; 1, 4000 -&gt; 2]|[M, 4000]| [1, 2]| 1||[M -&gt; 1, 4000 -&gt; 2]| [M -&gt; 1, 4000 -&gt; 2]|[M, 4000]| [1, 2]| 1||[F -&gt; 1, 4000 -&gt; 2]| [F -&gt; 1, 4000 -&gt; 2]|[F, 4000]| [1, 2]| null|| [F -&gt; 1, -1 -&gt; 2]| [F -&gt; 1, -1 -&gt; 2]| [F, -1]| [1, 2]| null|+-------------------+-----------------------------------------+---------+----------+-------+dfmap.select(col(&quot;*&quot;), explode(col(&quot;f_map&quot;))).show()+-------------------+-----------------------------------------+----+-----+| f_map|map(gender, 1, CAST(salary AS STRING), 2)| key|value|+-------------------+-----------------------------------------+----+-----+|[M -&gt; 1, 3000 -&gt; 2]| [M -&gt; 1, 3000 -&gt; 2]| M| 1||[M -&gt; 1, 3000 -&gt; 2]| [M -&gt; 1, 3000 -&gt; 2]|3000| 2||[M -&gt; 1, 4000 -&gt; 2]| [M -&gt; 1, 4000 -&gt; 2]| M| 1||[M -&gt; 1, 4000 -&gt; 2]| [M -&gt; 1, 4000 -&gt; 2]|4000| 2||[M -&gt; 1, 4000 -&gt; 2]| [M -&gt; 1, 4000 -&gt; 2]| M| 1||[M -&gt; 1, 4000 -&gt; 2]| [M -&gt; 1, 4000 -&gt; 2]|4000| 2||[F -&gt; 1, 4000 -&gt; 2]| [F -&gt; 1, 4000 -&gt; 2]| F| 1||[F -&gt; 1, 4000 -&gt; 2]| [F -&gt; 1, 4000 -&gt; 2]|4000| 2|| [F -&gt; 1, -1 -&gt; 2]| [F -&gt; 1, -1 -&gt; 2]| F| 1|| [F -&gt; 1, -1 -&gt; 2]| [F -&gt; 1, -1 -&gt; 2]| -1| 2|+-------------------+-----------------------------------------+----+-----+ 处理 JSONSpark 对 JSON 数据提供了一些独特的支持，可以直接在 Spark 中对 JSON 字符串进行处理，并从 JSON 字符串解析或提取 JSON 对象（返回字符串）。 创建一个 JSON 列： 12345val df = spark.range(1).selectExpr(&quot;&quot;&quot; &#x27;&#123;&quot;myJSONKey&quot;: &#123;&quot;myJSONValue&quot;: [1,2,3]&#125;&#125;&#x27; as f_json&quot;&quot;&quot;)df.show(false)df.printSchema 提取 JSON 字符串中的值：可以使用 get_json_object 内联查询 JSON 对象，如果只有一层嵌套，也可以使用 json_tuple 12345678910111213141516171819202122val res = df .withColumn(&quot;f_myJSONKey&quot;, get_json_object(col(&quot;f_json&quot;), &quot;$.myJSONKey&quot;)) .withColumn(&quot;f_myJSONKey2&quot;, json_tuple(col(&quot;f_json&quot;), &quot;myJSONKey&quot;)) .withColumn(&quot;myJSONValue&quot;, get_json_object(col(&quot;f_json&quot;), &quot;$.myJSONKey.myJSONValue&quot;)) .withColumn(&quot;f_value&quot;, get_json_object(col(&quot;f_json&quot;), &quot;$.myJSONKey.myJSONValue[0]&quot;))res.show(false)res.printSchema+---------------------------------------+-----------------------+-----------------------+-----------+-------+|f_json |f_myJSONKey |f_myJSONKey2 |myJSONValue|f_value|+---------------------------------------+-----------------------+-----------------------+-----------+-------+|&#123;&quot;myJSONKey&quot;: &#123;&quot;myJSONValue&quot;: [1,2,3]&#125;&#125;|&#123;&quot;myJSONValue&quot;:[1,2,3]&#125;|&#123;&quot;myJSONValue&quot;:[1,2,3]&#125;|[1,2,3] |1 |+---------------------------------------+-----------------------+-----------------------+-----------+-------+root |-- f_json: string (nullable = false) |-- f_myJSONKey: string (nullable = true) |-- f_myJSONKey2: string (nullable = true) |-- myJSONValue: string (nullable = true) |-- f_value: string (nullable = true) 将 struct/map 列转化为 json 列：to_json 函数可以将 StructType 或 MapType 列转化为 JSON 字符串； 12345678910111213141516171819202122232425val dfjson = df.select(&quot;f_struct&quot;, &quot;f_map&quot;) .withColumn(&quot;f_struct_json&quot;, to_json(col(&quot;f_struct&quot;))) .withColumn(&quot;f_map_json&quot;, to_json(col(&quot;f_map&quot;)))dfjson.show(false)dfjson.printSchema+---------------------+------------------+-------------------------------------------------------------+-------------------+|f_struct |f_map |f_struct_json |f_map_json |+---------------------+------------------+-------------------------------------------------------------+-------------------+|[James , , Smith] |[1 -&gt; a, 11 -&gt; aa]|&#123;&quot;firstname&quot;:&quot;James &quot;,&quot;middlename&quot;:&quot;&quot;,&quot;lastname&quot;:&quot;Smith&quot;&#125; |&#123;&quot;1&quot;:&quot;a&quot;,&quot;11&quot;:&quot;aa&quot;&#125;||[Michael , Rose, ] |[2 -&gt; b, 22 -&gt; bb]|&#123;&quot;firstname&quot;:&quot;Michael &quot;,&quot;middlename&quot;:&quot;Rose&quot;,&quot;lastname&quot;:&quot;&quot;&#125; |&#123;&quot;2&quot;:&quot;b&quot;,&quot;22&quot;:&quot;bb&quot;&#125;||[Robert , , Williams]|[3 -&gt; c, 33 -&gt; cc]|&#123;&quot;firstname&quot;:&quot;Robert &quot;,&quot;middlename&quot;:&quot;&quot;,&quot;lastname&quot;:&quot;Williams&quot;&#125;|&#123;&quot;3&quot;:&quot;c&quot;,&quot;33&quot;:&quot;cc&quot;&#125;||[Maria , Anne, Jones]|[4 -&gt; d, 44 -&gt; dd]|&#123;&quot;firstname&quot;:&quot;Maria &quot;,&quot;middlename&quot;:&quot;Anne&quot;,&quot;lastname&quot;:&quot;Jones&quot;&#125;|&#123;&quot;4&quot;:&quot;d&quot;,&quot;44&quot;:&quot;dd&quot;&#125;||[Jen, Mary, Brown] |[5 -&gt; e] |&#123;&quot;firstname&quot;:&quot;Jen&quot;,&quot;middlename&quot;:&quot;Mary&quot;,&quot;lastname&quot;:&quot;Brown&quot;&#125; |&#123;&quot;5&quot;:&quot;e&quot;&#125; |+---------------------+------------------+-------------------------------------------------------------+-------------------+root |-- f_struct: struct (nullable = true) | |-- firstname: string (nullable = true) | |-- middlename: string (nullable = true) | |-- lastname: string (nullable = true) |-- f_map: map (nullable = true) | |-- key: string | |-- value: string (valueContainsNull = true) |-- f_struct_json: string (nullable = true) |-- f_map_json: string (nullable = true) 将 json 列解析回 struct/map 列：from_json 函数可以将 json 列解析回 struct/map 列，但是要求制定一个 Schema 1234567891011121314151617181920212223242526272829303132333435363738394041val structSchema = new StructType() .add(&quot;firstname&quot;,StringType) .add(&quot;middlename&quot;,StringType) .add(&quot;lastname&quot;,StringType)val mapSchema = MapType(StringType, StringType)val dffromjson = dfjson .withColumn(&quot;json_strcut&quot;, from_json(col(&quot;f_struct_json&quot;), structSchema)) .withColumn(&quot;json_map&quot;, from_json(col(&quot;f_map_json&quot;), mapSchema))dffromjson.show()dffromjson.printSchema+--------------------+------------------+--------------------+-------------------+--------------------+------------------+| f_struct| f_map| f_struct_json| f_map_json| json_strcut| json_map|+--------------------+------------------+--------------------+-------------------+--------------------+------------------+| [James , , Smith]|[1 -&gt; a, 11 -&gt; aa]|&#123;&quot;firstname&quot;:&quot;Jam...|&#123;&quot;1&quot;:&quot;a&quot;,&quot;11&quot;:&quot;aa&quot;&#125;| [James , , Smith]|[1 -&gt; a, 11 -&gt; aa]|| [Michael , Rose, ]|[2 -&gt; b, 22 -&gt; bb]|&#123;&quot;firstname&quot;:&quot;Mic...|&#123;&quot;2&quot;:&quot;b&quot;,&quot;22&quot;:&quot;bb&quot;&#125;| [Michael , Rose, ]|[2 -&gt; b, 22 -&gt; bb]||[Robert , , Willi...|[3 -&gt; c, 33 -&gt; cc]|&#123;&quot;firstname&quot;:&quot;Rob...|&#123;&quot;3&quot;:&quot;c&quot;,&quot;33&quot;:&quot;cc&quot;&#125;|[Robert , , Willi...|[3 -&gt; c, 33 -&gt; cc]||[Maria , Anne, Jo...|[4 -&gt; d, 44 -&gt; dd]|&#123;&quot;firstname&quot;:&quot;Mar...|&#123;&quot;4&quot;:&quot;d&quot;,&quot;44&quot;:&quot;dd&quot;&#125;|[Maria , Anne, Jo...|[4 -&gt; d, 44 -&gt; dd]|| [Jen, Mary, Brown]| [5 -&gt; e]|&#123;&quot;firstname&quot;:&quot;Jen...| &#123;&quot;5&quot;:&quot;e&quot;&#125;| [Jen, Mary, Brown]| [5 -&gt; e]|+--------------------+------------------+--------------------+-------------------+--------------------+------------------+root |-- f_struct: struct (nullable = true) | |-- firstname: string (nullable = true) | |-- middlename: string (nullable = true) | |-- lastname: string (nullable = true) |-- f_map: map (nullable = true) | |-- key: string | |-- value: string (valueContainsNull = true) |-- f_struct_json: string (nullable = true) |-- f_map_json: string (nullable = true) |-- json_strcut: struct (nullable = true) | |-- firstname: string (nullable = true) | |-- middlename: string (nullable = true) | |-- lastname: string (nullable = true) |-- json_map: map (nullable = true) | |-- key: string | |-- value: string (valueContainsNull = true) 参考 《Spark 权威指南 Chapter 6》_online/) org.apache.spark.functions]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark 指南：Spark SQL（二）—— 结构化操作]]></title>
    <url>%2FSpark%2FSpark%2FSpark%20%E6%8C%87%E5%8D%97%EF%BC%9ASpark%20SQL%EF%BC%88%E4%BA%8C%EF%BC%89%E2%80%94%E2%80%94%20%E7%BB%93%E6%9E%84%E5%8C%96%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[从定义上讲，DataFrame 由一系列行和列组成，行的类型为 Row，列可以表示成每个单独行的计算表达式。Schema 定义了每个列的名称和类型，Partition 定义了整个集群中 DataFrame 的物理分布。 SchemaSchema 定义了 DataFrame 的列名和类型，我们可以让数据源定义 Schema（schema-on-read），也可以自己明确地进行定义。对于临时分析，schema-on-read 通常效果很好，但是这也可能导致精度问题，例如在读取文件时将 Long 型错误地设置为整型，在生产环境中手动定义 Schema 通常是更好的选择，尤其是在使用 CSV 和 JSON 等无类型数据源时。 Schema 是一种 structType，由很多 StructFields 组成，每个 StructField 具有名称、类型和布尔值标识（用于指示该列是否可以为 null），最后用户可以选择指定与该列关联的元数据，元数据是一种存储有关此列的信息的方式（Spark 在其机器学习库中使用此信息）。如果数据中的类型与 Schema 不匹配，Spark 将引发错误。 123456789101112131415161718192021222324252627import org.apache.spark.sql.types._import org.apache.spark.sql.Rowval schema = StructType( Array( StructField(&quot;name&quot;, StringType, true), StructField(&quot;age&quot;, IntegerType, false) ))val data = spark.sparkContext.parallelize(Seq( Row(&quot;like&quot;, 18), Row(&quot;arya&quot;, 24)))val df = spark.createDataFrame(data, schema)df.show()+----+---+|name|age|+----+---+|like| 18||arya| 24|+----+---+// 打印 DataFrame 的 Schemadf.printSchemaroot |-- name: string (nullable = true) |-- age: integer (nullable = false) Columns and Expressions列只是表达式（Columns are just Expressions）：列以及列上的转换与经过解析的表达式拥有相同的逻辑计划。这是极为重要的一点，这意味着你可以将表达式编写为 DataFrame 代码或 SQL 表达式，并获得完全相同的性能特性。 Columns对 Spark 而言，Columns 是一种逻辑构造，仅表示通过表达式在每条记录上所计算出来的值。这意味着要有一个列的实际值，我们就需要有一个行，要有一个行，我们就需要有一个 DataFrame，你不能在 DataFrame 上下文之外操作单个列，你必须在 DataFrame 中使用 Spark 转换来修改列的内容。 在 DataFrame 中引用列的方式有很多，以下几种语法形式是等价的： 123456789101112df.columnsArray[String] = Array(name, dob, gender, salary)df.select(&#x27;dob, $&quot;dob&quot;, df(&quot;dob&quot;), col(&quot;dob&quot;), df.col(&quot;dob&quot;), expr(&quot;dob&quot;)).show()+-----+-----+-----+-----+-----+| dob| dob| dob| dob| dob|+-----+-----+-----+-----+-----+|36636|36636|36636|36636|36636||40288|40288|40288|40288|40288||42114|42114|42114|42114|42114||39192|39192|39192|39192|39192|+-----+-----+-----+-----+-----+ ExpressionsExpressions 是对 DataFrame 记录中一个或多个值的一组转换，可以将其视为一个函数，该函数将一个或多个列名作为输入，进行解析，然后可能应用更多表达式为数据集中每个记录创建单个值（可以是诸如 Map 或 Array 之类的复杂类型）。在最简单的情况下，通过 expr() 函数创建的表达式仅仅是 DataFrame 列引用，expr(&quot;col_name&quot;) 等价于 col(&quot;col_name&quot;)。 列提供了表达式功能的子集，如果使用 col() 并想在该列上执行转换，则必须在该列引用上执行那些转换，使用表达式时， expr 函数实际上可以解析字符串中的转换和列引用，例如：expr(&quot;col_name - 5&quot;) 等价于 col(&quot;col_name&quot;) - 5，甚至等价于 expr(&quot;col_name&quot;) - 5。 123import org.apache.spark.sql.functions.expr(((col(&quot;col_name&quot;) + 5) * 200) - 6) &lt; col(&quot;other_col&quot;)expr(&quot;(((col_name + 5) * 200) - 6) &lt; other_col&quot;) Records and RowsDataFrame 中的每一行都是一条记录，Spark 将此记录表示为 Row 类型的对象，Spark 使用列表达式操纵 Row 对象，以产生可用的值。Row 对象在内部表示为字节数组，但是字节数组接口从未显示给用户，因为我们仅使用列表达式来操作它们。 可以通过手动实例化具有每个列中的值的 Row 对象来创建行，但是务必注意只有 DataFrame 有 Schema，Row 本身没有模式。 12import org.apache.spark.sql.Rowval myRow = Row(&quot;hello&quot;, null, 1, false) 访问行中的数据很容易，只需要指定位置或列名： 12345df.collect().foreach(row=&gt;&#123; val name = row(0).asInstanceOf[String] val age = row.getAs[Integer](&quot;age&quot;) println(s&quot;name:$name age:$age&quot;)&#125;) DataFrame 转换DataFrame 转换不会改变原有的 DataFrame，而是生成一个新的 DataFrame。很多 DataFrame 转换/函数被包含在 org.apache.spark.sql.functions 模块，使用前推荐先导入相关模块： 123import org.apache.spark.sql.functions._import org.apache.spark.sql.types._import org.apache.spark.sql.Row 本文主要用到的示例数据： 123456789101112131415161718192021val data = Seq( Row(Row(&quot;James &quot;,&quot;&quot;,&quot;Smith&quot;),&quot;36636&quot;,&quot;M&quot;,&quot;3000&quot;), Row(Row(&quot;Michael &quot;,&quot;Rose&quot;,&quot;&quot;),&quot;40288&quot;,&quot;M&quot;,&quot;4000&quot;), Row(Row(&quot;Robert &quot;,&quot;&quot;,&quot;Williams&quot;),&quot;42114&quot;,&quot;M&quot;,&quot;4000&quot;), Row(Row(&quot;Maria &quot;,&quot;Anne&quot;,&quot;Jones&quot;),&quot;39192&quot;,&quot;F&quot;,&quot;4000&quot;), Row(Row(&quot;Jen&quot;,&quot;Mary&quot;,&quot;Brown&quot;),&quot;&quot;,&quot;F&quot;,&quot;-1&quot;))val schema = new StructType() .add(&quot;name&quot;,new StructType() .add(&quot;firstname&quot;,StringType) .add(&quot;middlename&quot;,StringType) .add(&quot;lastname&quot;,StringType) ) .add(&quot;dob&quot;,StringType) .add(&quot;gender&quot;,StringType) .add(&quot;salary&quot;,StringType)val df = spark.createDataFrame(spark.sparkContext.parallelize(data),schema)df.show()df.printSchema 123456789101112131415161718+--------------------+-----+------+------+| name| dob|gender|salary|+--------------------+-----+------+------+| [James , , Smith]|36636| M| 3000|| [Michael , Rose, ]|40288| M| 4000||[Robert , , Willi...|42114| M| 4000||[Maria , Anne, Jo...|39192| F| 4000|| [Jen, Mary, Brown]| | F| -1|+--------------------+-----+------+------+root |-- name: struct (nullable = true) | |-- firstname: string (nullable = true) | |-- middlename: string (nullable = true) | |-- lastname: string (nullable = true) |-- dob: string (nullable = true) |-- gender: string (nullable = true) |-- salary: string (nullable = true) 列操作select —— 筛选列 功能：select() 用于筛选/操作列； 语法：有两种语法形式，但是两种形式不能混用； 1234// 传入列名字符串select(col : scala.Predef.String, cols : scala.Predef.String*) : org.apache.spark.sql.DataFrame// 传入多个列对象select(cols : org.apache.spark.sql.Column*) : org.apache.spark.sql.DataFrame 示例： 1234567891011121314151617181920212223242526// 可以是列名字符串、*代表所有列、a.b 代表 struct 中的子域、不可用 asdf.select(&quot;name.firstname&quot;, &quot;dob&quot;, &quot;*&quot;).show()+---------+-----+--------------------+-----+------+------+|firstname| dob| name| dob|gender|salary|+---------+-----+--------------------+-----+------+------+| James |36636| [James , , Smith]|36636| M| 3000|| Michael |40288| [Michael , Rose, ]|40288| M| 4000|| Robert |42114|[Robert , , Willi...|42114| M| 4000|| Maria |39192|[Maria , Anne, Jo...|39192| F| 4000|| Jen| | [Jen, Mary, Brown]| | F| -1|+---------+-----+--------------------+-----+------+------+// 列对象有多种表示方法：$&quot;col_name&quot;、col(&quot;col_name&quot;)、df(&quot;col_name&quot;)// 列可以通过.as(col_name) 起别名// 列可以通过.cast() 改变列的类型// 列字面量用 lit(c) 表示df.select($&quot;name.firstname&quot;.cast(&quot;String&quot;), col(&quot;dob&quot;).as(&quot;f_dob&quot;), df(&quot;*&quot;), lit(1).as(&quot;new_col&quot;)).show()+---------+-----+--------------------+-----+------+------+-------+|firstname|f_dob| name| dob|gender|salary|new_col|+---------+-----+--------------------+-----+------+------+-------+| James |36636| [James , , Smith]|36636| M| 3000| 1|| Michael |40288| [Michael , Rose, ]|40288| M| 4000| 1|| Robert |42114|[Robert , , Willi...|42114| M| 4000| 1|| Maria |39192|[Maria , Anne, Jo...|39192| F| 4000| 1|| Jen| | [Jen, Mary, Brown]| | F| -1| 1|+---------+-----+--------------------+-----+------+------+-------+ selectExpr —— 通过 SQL 语句筛选列 功能：selectExpr 和 select 作用相同，只是 selectExpr 更加简洁、灵活、强大； 语法：可以通过构造任意有效的非聚合 SQL 语句来生成列（如果使用了聚合函数，则只能应用于整个 DataFrame）；这释放了 Spark 的真正力量，我们可以将 selectExpr 视为构建复杂表达式以生成新的 DataFrame 的简单方法；如果列名中包含了保留字或关键字，例如空格或破折号，可以通过反引号（`）字符引用列名； 12selectExpr(exprs : scala.Predef.String*) : org.apache.spark.sql.DataFrameselect(cols : org.apache.spark.sql.Column*, expr()) 示例： 12345678910111213141516171819df.selectExpr(&quot;name.firstname&quot;, &quot;dob as f_dob&quot;, &quot;*&quot;, &quot;dob + salary as new_col&quot;).show()df.select(col(&quot;name.firstname&quot;), expr(&quot;dob as f_dob&quot;), df(&quot;*&quot;), expr(&quot;dob + salary as new_col&quot;), lit(1).as(&quot;f_one&quot;)).show()+---------+-----+--------------------+-----+------+------+-------+-----+|firstname|f_dob| name| dob|gender|salary|new_col|f_one|+---------+-----+--------------------+-----+------+------+-------+-----+| James |36636| [James , , Smith]|36636| M| 3000|39636.0| 1|| Michael |40288| [Michael , Rose, ]|40288| M| 4000|44288.0| 1|| Robert |42114|[Robert , , Willi...|42114| M| 4000|46114.0| 1|| Maria |39192|[Maria , Anne, Jo...|39192| F| 4000|43192.0| 1|| Jen| | [Jen, Mary, Brown]| | F| -1| null| 1|+---------+-----+--------------------+-----+------+------+-------+-----+df.selectExpr(&quot;max(salary) as max_salary&quot;, &quot;avg(salary) as `avg salary`&quot;).show()+----------+----------+|max_salary|avg salary|+----------+----------+| 4000| 2999.8|+----------+----------+ selectExpr 的灵活用法使其可以替代大部分的列操作算子，但是考虑到代码的简洁性，对于一些具体的操作，往往会有更简单直接的算子。事实上，DataFrame 操作使用最多的算子是 withColumn，withColumn 算子将单列处理逻辑封装到独立的子句中，更具可读性，也方便了代码维护。 withColumn —— 添加或更新列 功能：withColumn() 可以用来添加新列、改变已有列的值、改变列的类型； 语法：withColumn 有两个参数，列名和将为 DataFrame 各行创建值的表达式； 1withColumn(colName: String, col: Column): DataFrame 示例： 123456// 添加新的列df.withColumn(&quot;CopiedColumn&quot;,col(&quot;salary&quot;)* -1)// 改变列类型df.withColumn(&quot;salary&quot;,col(&quot;salary&quot;).cast(&quot;Integer&quot;))// 改变已有列的值df.withColumn(&quot;salary&quot;,col(&quot;salary&quot;)*100) withColumnRenamed —— 重命名列 功能：withColumnRenamed 用于重命名列； 语法： 1withColumnRenamed(existingName: String, newName: String): DataFrame 示例：有多种方式可以用于重命名单个列、多个列、所有列、嵌套列 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162// 重命名单个列，withColumnRenamed(x, y) 将 y 列重名为 xdf.withColumnRenamed(&quot;dob&quot;,&quot;DateOfBirth&quot;)// 重命名嵌套列，col(&quot;name&quot;).cast(schema2) 将嵌套列重命名为 schema2 中定义的列名val schema2 = new StructType() .add(&quot;fname&quot;,StringType) .add(&quot;middlename&quot;,StringType) .add(&quot;lname&quot;,StringType)df.select(col(&quot;name&quot;).cast(schema2), col(&quot;dob&quot;), col(&quot;gender&quot;), col(&quot;salary&quot;)).printSchemaroot |-- name: struct (nullable = true) | |-- fname: string (nullable = true) | |-- middlename: string (nullable = true) | |-- lname: string (nullable = true) |-- dob: string (nullable = true) |-- gender: string (nullable = true) |-- salary: string (nullable = true)// 重命名嵌套列，col(&quot;x.y&quot;).as(&quot;z&quot;) 可以将 x 中的 y 抽离出来作为单独的列val df4 = df.select(col(&quot;name.firstname&quot;).as(&quot;fname&quot;), col(&quot;name.middlename&quot;).as(&quot;mname&quot;), col(&quot;name.lastname&quot;).as(&quot;lname&quot;), col(&quot;dob&quot;),col(&quot;gender&quot;),col(&quot;salary&quot;))df4.show()+--------+-----+--------+-----+------+------+| fname|mname| lname| dob|gender|salary|+--------+-----+--------+-----+------+------+| James | | Smith|36636| M| 3000||Michael | Rose| |40288| M| 4000|| Robert | |Williams|42114| M| 4000|| Maria | Anne| Jones|39192| F| 4000|| Jen| Mary| Brown| | F| -1|+--------+-----+--------+-----+------+------+// 重命名多列，col() 函数val old_columns = Seq(&quot;dob&quot;,&quot;gender&quot;,&quot;salary&quot;,&quot;fname&quot;,&quot;mname&quot;,&quot;lname&quot;)val new_columns = Seq(&quot;DateOfBirth&quot;,&quot;Sex&quot;,&quot;salary&quot;,&quot;firstName&quot;,&quot;middleName&quot;,&quot;lastName&quot;)val columnsList = old_columns.zip(new_columns).map(f=&gt;&#123;col(f._1).as(f._2)&#125;)val df5 = df4.select(columnsList:_*)df5.show()+-----------+---+------+---------+----------+--------+|DateOfBirth|Sex|salary|firstName|middleName|lastName|+-----------+---+------+---------+----------+--------+| 36636| M| 3000| James | | Smith|| 40288| M| 4000| Michael | Rose| || 42114| M| 4000| Robert | |Williams|| 39192| F| 4000| Maria | Anne| Jones|| | F| -1| Jen| Mary| Brown|+-----------+---+------+---------+----------+--------+// 重命名所有列，toDF() 方法val newColumns = Seq(&quot;newCol1&quot;,&quot;newCol2&quot;,&quot;newCol3&quot;,&quot;newCol4&quot;)df.toDF(newColumns:_*).show()+--------------------+-------+-------+-------+| newCol1|newCol2|newCol3|newCol4|+--------------------+-------+-------+-------+| [James , , Smith]| 36636| M| 3000|| [Michael , Rose, ]| 40288| M| 4000||[Robert , , Willi...| 42114| M| 4000||[Maria , Anne, Jo...| 39192| F| 4000|| [Jen, Mary, Brown]| | F| -1|+--------------------+-------+-------+-------+ drop —— 删除列 功能：drop() 用于删除 DataFrame 中单个或多个列，如果指定列不存在则忽略，在两个表进行 join 时通常可以利用这一点来保证两个表除了关联键之外不存在同名字段。 语法： 1234// drop 有三种不同的形式：1) drop(colName : scala.Predef.String) : org.apache.spark.sql.DataFrame2) drop(colNames : scala.Predef.String*) : org.apache.spark.sql.DataFrame3) drop(col : org.apache.spark.sql.Column) : org.apache.spark.sql.DataFrame 示例： 123456789101112131415161718192021222324252627282930313233343536373839404142val df = spark.range(3) .withColumn(&quot;today_str&quot;,lit(&quot;2020-11-04&quot;)) .withColumn(&quot;today&quot;, current_date()) .withColumn(&quot;now&quot;, current_timestamp()) .orderBy(rand())df.show(false)+---+----------+----------+-----------------------+|id |today_str |today |now |+---+----------+----------+-----------------------+|0 |2020-11-04|2020-11-04|2020-11-04 20:57:06.515||1 |2020-11-04|2020-11-04|2020-11-04 20:57:06.515||2 |2020-11-04|2020-11-04|2020-11-04 20:57:06.515|+---+----------+----------+-----------------------+// 删除单列df.drop(&quot;today_str&quot;).show()+---+----------+--------------------+| id| today| now|+---+----------+--------------------+| 0|2020-11-04|2020-11-04 22:19:...|| 1|2020-11-04|2020-11-04 22:19:...|| 2|2020-11-04|2020-11-04 22:19:...|+---+----------+--------------------+// 删除多列df.drop(&quot;today_str&quot;, &quot;today&quot;).show()+---+--------------------+| id| now|+---+--------------------+| 0|2020-11-04 22:19:...|| 1|2020-11-04 22:19:...|| 2|2020-11-04 22:19:...|+---+--------------------+// 删除不存在的列df.drop(&quot;xxx&quot;).show()+---+----------+----------+--------------------+| id| today_str| today| now|+---+----------+----------+--------------------+| 0|2020-11-04|2020-11-04|2020-11-04 22:19:...|| 1|2020-11-04|2020-11-04|2020-11-04 22:19:...|| 2|2020-11-04|2020-11-04|2020-11-04 22:19:...|+---+----------+----------+--------------------+ 行操作where | filter —— 筛选行 功能：where 和 filter 是完全等价的，用于按照指定条件筛选 DataFrame 中满足条件的行； 语法：传入一个布尔表达式，过滤掉 false 所对应的行； 12345// 有四种形式1) filter(condition: Column): Dataset[T]2) filter(conditionExpr: String): Dataset[T]3) filter(func: T =&gt; Boolean): Dataset[T]4) filter(func: FilterFunction[T]): Dataset[T] 示例： 123456789101112131415161718192021df.show()+--------------------+------------------+-----+------+| name| languages|state|gender|+--------------------+------------------+-----+------+| [James, , Smith]|[Java, Scala, C++]| OH| M|| [Anna, Rose, ]|[Spark, Java, C++]| NY| F|| [Julia, , Williams]| [CSharp, VB]| OH| F||[Maria, Anne, Jones]| [CSharp, VB]| NY| M|| [Jen, Mary, Brown]| [CSharp, VB]| NY| M||[Mike, Mary, Will...| [Python, VB]| OH| M|+--------------------+------------------+-----+------+// Column 形式表达式df.filter(df(&quot;state&quot;) === &quot;OH&quot; &amp;&amp; df(&quot;gender&quot;) === &quot;M&quot;)// String 形式表达式 == 和 = 等价df.filter(&quot;gender == &#x27;M&#x27;&quot;)df.filter(&quot;gender = &#x27;M&#x27;&quot;)// Filtering on an Array columndf.filter(array_contains(df(&quot;languages&quot;),&quot;Java&quot;))// Filtering on Nested Struct columnsdf.filter(df(&quot;name.lastname&quot;) === &quot;Williams&quot;) distinct —— 行去重 功能：distinct() 方法可以移除 DataFrame 中重复的行，dropDuplicates() 方法用于移除 DataFrame 中在某几个字段上重复的行（默认保留重复行中的第一行）。 语法： 12distinct(): Dataset[T] = dropDuplicates()dropDuplicates(colNames: Seq[String]): Dataset[T] 示例： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354df.show()+-------------+----------+------+|employee_name|department|salary|+-------------+----------+------+| James| Sales| 3000|| Michael| Sales| 4600|| Robert| Sales| 4100|| Maria| Finance| 3000|| James| Sales| 3000|| Scott| Finance| 3300|| Jen| Finance| 3900|| Jeff| Marketing| 3000|| Kumar| Marketing| 2000|| Saif| Sales| 4100|+-------------+----------+------+// Distinct all columnsval distinctDF = df.distinct()println(&quot;Distinct count: &quot;+distinctDF.count())distinctDF.show(false)Distinct count: 9+-------------+----------+------+|employee_name|department|salary|+-------------+----------+------+|James |Sales |3000 ||Michael |Sales |4600 ||Maria |Finance |3000 ||Robert |Sales |4100 ||Saif |Sales |4100 ||Scott |Finance |3300 ||Jeff |Marketing |3000 ||Jen |Finance |3900 ||Kumar |Marketing |2000 |+-------------+----------+------+// Distinct using dropDuplicatesval dropDisDF = df.dropDuplicates(&quot;department&quot;,&quot;salary&quot;)println(&quot;Distinct count of department &amp; salary : &quot;+dropDisDF.count())dropDisDF.show(false)Distinct count of department &amp; salary : 8+-------------+----------+------+|employee_name|department|salary|+-------------+----------+------+|Jen |Finance |3900 ||Maria |Finance |3000 ||Scott |Finance |3300 ||Michael |Sales |4600 ||Kumar |Marketing |2000 ||Robert |Sales |4100 ||James |Sales |3000 ||Jeff |Marketing |3000 |+-------------+----------+------+ groupBy —— 行分组 功能：和 SQL 中的 group by 语句类似，groupBy() 函数用于将 DataFrame/Dataset 按照指定字段分组，返回一个 RelationalGroupedDataset 对象 语法：RelationalGroupedDataset 对象包含以下几种聚合方法： count()/max()/min()/mean()/avg()/sum(): 返回每个分组的行数/最大/最小/平均值/和； agg(): 可以同时计算多个聚合； pivot(): 用于行转列； 示例： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980import spark.implicits._val simpleData = Seq((&quot;James&quot;,&quot;Sales&quot;,&quot;NY&quot;,90000,34,10000), (&quot;Michael&quot;,&quot;Sales&quot;,&quot;NY&quot;,86000,56,20000), (&quot;Robert&quot;,&quot;Sales&quot;,&quot;CA&quot;,81000,30,23000), (&quot;Maria&quot;,&quot;Finance&quot;,&quot;CA&quot;,90000,24,23000), (&quot;Raman&quot;,&quot;Finance&quot;,&quot;CA&quot;,99000,40,24000), (&quot;Scott&quot;,&quot;Finance&quot;,&quot;NY&quot;,83000,36,19000), (&quot;Jen&quot;,&quot;Finance&quot;,&quot;NY&quot;,79000,53,15000), (&quot;Jeff&quot;,&quot;Marketing&quot;,&quot;CA&quot;,80000,25,18000), (&quot;Kumar&quot;,&quot;Marketing&quot;,&quot;NY&quot;,91000,50,21000) )val df = simpleData.toDF(&quot;employee_name&quot;,&quot;department&quot;,&quot;state&quot;,&quot;salary&quot;,&quot;age&quot;,&quot;bonus&quot;)df.show()+-------------+----------+-----+------+---+-----+|employee_name|department|state|salary|age|bonus|+-------------+----------+-----+------+---+-----+| James| Sales| NY| 90000| 34|10000|| Michael| Sales| NY| 86000| 56|20000|| Robert| Sales| CA| 81000| 30|23000|| Maria| Finance| CA| 90000| 24|23000|| Raman| Finance| CA| 99000| 40|24000|| Scott| Finance| NY| 83000| 36|19000|| Jen| Finance| NY| 79000| 53|15000|| Jeff| Marketing| CA| 80000| 25|18000|| Kumar| Marketing| NY| 91000| 50|21000|+-------------+----------+-----+------+---+-----+// 计算每个分组的行数df.groupBy(&quot;department&quot;).count().show()+----------+-----+|department|count|+----------+-----+| Sales| 3|| Finance| 4|| Marketing| 2|+----------+-----+// 在某个列上聚合df.groupBy(&quot;department&quot;).sum(&quot;salary&quot;).show(false)+----------+-----------+|department|sum(salary)|+----------+-----------+|Sales |257000 ||Finance |351000 ||Marketing |171000 |+----------+-----------+// 同时在多列应用相同的聚合函数df.groupBy(&quot;department&quot;,&quot;state&quot;) .sum(&quot;salary&quot;,&quot;bonus&quot;) .show(false)+----------+-----+-----------+----------+|department|state|sum(salary)|sum(bonus)|+----------+-----+-----------+----------+|Finance |NY |162000 |34000 ||Marketing |NY |91000 |21000 ||Sales |CA |81000 |23000 ||Marketing |CA |80000 |18000 ||Finance |CA |189000 |47000 ||Sales |NY |176000 |30000 |+----------+-----+-----------+----------+// agg() 可以同时在多个列上应用不同聚合函数，并为每个聚合结果起别名import org.apache.spark.sql.functions._df.groupBy(&quot;department&quot;) .agg( sum(&quot;salary&quot;).as(&quot;sum_salary&quot;), avg(&quot;salary&quot;).as(&quot;avg_salary&quot;), sum(&quot;bonus&quot;).as(&quot;sum_bonus&quot;), max(&quot;bonus&quot;).as(&quot;max_bonus&quot;)) .show(false)+----------+----------+-----------------+---------+---------+|department|sum_salary|avg_salary |sum_bonus|max_bonus|+----------+----------+-----------------+---------+---------+|Sales |257000 |85666.66666666667|53000 |23000 ||Finance |351000 |87750.0 |81000 |24000 ||Marketing |171000 |85500.0 |39000 |21000 |+----------+----------+-----------------+---------+---------+ sort —— 行排序 功能：在 Spark 中，可以使用 sort() 或 orderBy() 方法来根据某几个字段的值对 DataFrame/Dataset 进行排序。 语法： 123456// sortsort(sortCol : scala.Predef.String, sortCols : scala.Predef.String*) : Dataset[T]sort(sortExprs : org.apache.spark.sql.Column*) : Dataset[T]// orderByorderBy(sortCol : scala.Predef.String, sortCols : scala.Predef.String*) : Dataset[T]orderBy(sortExprs : org.apache.spark.sql.Column*) : Dataset[T] 示例： 1234567891011121314df.sort(&quot;department&quot;,&quot;state&quot;).show(false)df.sort(col(&quot;department&quot;),col(&quot;state&quot;)).show(false)df.orderBy(&quot;department&quot;,&quot;state&quot;).show(false)df.orderBy(col(&quot;department&quot;),col(&quot;state&quot;)).show(false)// 默认即为升序 ascdf.sort(col(&quot;department&quot;).asc,col(&quot;state&quot;).desc).show(false)df.orderBy(col(&quot;department&quot;).asc,col(&quot;state&quot;).desc).show(false)// Spark SQL 函数提供了 asc desc asc_nulls_first asc_nulls_last 函数df.select($&quot;employee_name&quot;,asc(&quot;department&quot;),desc(&quot;state&quot;),$&quot;salary&quot;,$&quot;age&quot;,$&quot;bonus&quot;).show(false)df.createOrReplaceTempView(&quot;EMP&quot;)spark.sql(&quot; select employee_name,asc(&#x27;department&#x27;),desc(&#x27;state&#x27;),salary,age,bonus from EMP&quot;).show(false) map —— 映射 功能：map() 和 mapPartitions() 转换将函数应用于 DataFrame/Dataset 的每个元素/记录/行，并返回新的 DataFrame/Dataset，需要注意的是这两个转换都返回 Dataset[U] 而不是 DataFrame（在Spark 2.0中，DataFrame = Dataset [Row]）。 语法: Spark 提供了 2 个映射转换签名，一个以 scala.function1 作为参数，另一个以 Spark MapFunction 作为签名，注意到这两个函数都返回 Dataset [U]，但不返回DataFrame，即Dataset [Row]。如果希望将 DataFrame 作为输出，则需要使用 toDF() 函数将 Dataset 转换为 DataFrame。 12341) map[U](func : scala.Function1[T, U])(implicit evidence$6 : org.apache.spark.sql.Encoder[U]) : org.apache.spark.sql.Dataset[U]2) map[U](func : org.apache.spark.api.java.function.MapFunction[T, U], encoder : org.apache.spark.sql.Encoder[U]) : org.apache.spark.sql.Dataset[U] 示例: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586// 示例数据val structureData = Seq( Row(&quot;James&quot;,&quot;&quot;,&quot;Smith&quot;,&quot;36636&quot;,&quot;NewYork&quot;,3100), Row(&quot;Michael&quot;,&quot;Rose&quot;,&quot;&quot;,&quot;40288&quot;,&quot;California&quot;,4300), Row(&quot;Robert&quot;,&quot;&quot;,&quot;Williams&quot;,&quot;42114&quot;,&quot;Florida&quot;,1400), Row(&quot;Maria&quot;,&quot;Anne&quot;,&quot;Jones&quot;,&quot;39192&quot;,&quot;Florida&quot;,5500), Row(&quot;Jen&quot;,&quot;Mary&quot;,&quot;Brown&quot;,&quot;34561&quot;,&quot;NewYork&quot;,3000) )val structureSchema = new StructType() .add(&quot;firstname&quot;,StringType) .add(&quot;middlename&quot;,StringType) .add(&quot;lastname&quot;,StringType) .add(&quot;id&quot;,StringType) .add(&quot;location&quot;,StringType) .add(&quot;salary&quot;,IntegerType)val df2 = spark.createDataFrame( spark.sparkContext.parallelize(structureData),structureSchema)df2.printSchema()df2.show(false)root |-- firstname: string (nullable = true) |-- middlename: string (nullable = true) |-- lastname: string (nullable = true) |-- id: string (nullable = true) |-- location: string (nullable = true) |-- salary: integer (nullable = true)+---------+----------+--------+-----+----------+------+|firstname|middlename|lastname|id |location |salary|+---------+----------+--------+-----+----------+------+|James | |Smith |36636|NewYork |3100 ||Michael |Rose | |40288|California|4300 ||Robert | |Williams|42114|Florida |1400 ||Maria |Anne |Jones |39192|Florida |5500 ||Jen |Mary |Brown |34561|NewYork |3000 |+---------+----------+--------+-----+----------+------+// 为了通过实例解释 map() 和 mapPartitions()，我们再创建一个 Util 类，这个类具有一个 combine() 方法，该方法接收三个字符串参数，通过逗号合并三个参数并输出。class Util extends Serializable &#123; def combine(fname:String,mname:String,lname:String):String = &#123; fname+&quot;,&quot;+mname+&quot;,&quot;+lname &#125;&#125;// map 是在 worker 节点上执行的，而我们在 map 函数内部创建了 Util 实例，初始化将发生在 DataFrame 中的每一行，当您进行了大量复杂的初始化时，这会导致性能问题import spark.implicits._val df3 = df2.map(row=&gt;&#123; val util = new Util() val fullName = util.combine(row.getString(0),row.getString(1),row.getString(2)) (fullName, row.getString(3),row.getInt(5))&#125;)val df3Map = df3.toDF(&quot;fullName&quot;,&quot;id&quot;,&quot;salary&quot;)df3Map.printSchema()df3Map.show(false)root |-- fullName: string (nullable = true) |-- id: string (nullable = true) |-- salary: integer (nullable = false)+----------------+-----+------+|fullName |id |salary|+----------------+-----+------+|James,,Smith |36636|3100 ||Michael,Rose, |40288|4300 ||Robert,,Williams|42114|1400 ||Maria,Anne,Jones|39192|5500 ||Jen,Mary,Brown |34561|3000 |+----------------+-----+------+// mapPartitions() 提供了一种功能，可以对每个分区进行一次初始化（例如，数据库连接），而不是对每个行进行一次初始化，这有助于提提高效率，下面代码将得到和上例相同的结果val df4 = df2.mapPartitions(iterator =&gt; &#123; val util = new Util() val res = iterator.map(row=&gt;&#123; val fullName = util.combine(row.getString(0),row.getString(1),row.getString(2)) (fullName, row.getString(3),row.getInt(5)) &#125;) res&#125;)val df4part = df4.toDF(&quot;fullName&quot;,&quot;id&quot;,&quot;salary&quot;)df4part.printSchema()df4part.show(false) foreach —— 遍历 功能：foreach() 方法用于在 RDD/DataFrame/Dataset 的每个元素上应用函数，主要用于操作累积器共享变量，也可以用于将 RDD/DataFrame 结果写入数据库，生产消息到 kafka topic 等。foreachPartition() 方法用于在 RDD/DataFrame/Dataset 的每个分区上应用函数，主要用于在每个分区进行复杂的初始化操作（比如连接数据库），也可以用于操作累加器变量。foreach() 和 foreachPartition() 方法都是不会返回值的 action。 语法: 12foreachPartition(f : scala.Function1[scala.Iterator[T], scala.Unit]) : scala.Unit 示例: 12345678910111213141516171819202122232425// foreach 操作累加器val longAcc = spark.sparkContext.longAccumulator(&quot;SumAccumulator&quot;)df.foreach(f=&gt; &#123; longAcc.add(f.getInt(1)) &#125;)println(&quot;Accumulator value:&quot;+longAcc.value)// foreachPartition 写入数据val df = spark.createDataFrame(data).toDF(&quot;Product&quot;,&quot;Amount&quot;,&quot;Country&quot;)df.foreachPartition(partition =&gt; &#123; //Initialize database connection or kafka partition.foreach(fun=&gt;&#123; //apply the function to insert the database // or produce kafka topic &#125;) //If you have batch inserts, do here. &#125;)// rdd foreach 和 DataFrame foreach 是等价的 actionval rdd2 = spark.sparkContext.parallelize(Seq(1,2,3,4,5,6,7,8,9))val longAcc2 = spark.sparkContext.longAccumulator(&quot;SumAccumulator2&quot;) rdd.foreach(f=&gt; &#123; longAcc2.add(f) &#125;)println(&quot;Accumulator value:&quot;+longAcc2.value) sample —— 随机抽样 功能：从 DataFrame 中抽取一些随机记录； 语法： 12345// withReplacement: 是否是有放回抽样; fraction: 抽样比例; seed: 抽样算法初始值sample(fraction: Double)sample(fraction: Double, seed: Long)sample(withReplacement: Boolean, fraction: Double)sample(withReplacement: Boolean, fraction: Double, seed: Long) 示例： 1234567891011121314151617181920212223df.sample(0.2).show()+--------------------+-----+------+------+| name| dob|gender|salary|+--------------------+-----+------+------+|[Maria , Anne, Jo...|39192| F| 4000|+--------------------+-----+------+------+df.sample(0.5, 1000L).show()+------------------+-----+------+------+| name| dob|gender|salary|+------------------+-----+------+------+| [James , , Smith]|36636| M| 3000||[Jen, Mary, Brown]| | F| -1|+------------------+-----+------+------+df.sample(true, 0.5, 1000L).show()+--------------------+-----+------+------+| name| dob|gender|salary|+--------------------+-----+------+------+|[Maria , Anne, Jo...|39192| F| 4000||[Maria , Anne, Jo...|39192| F| 4000|| [Jen, Mary, Brown]| | F| -1|+--------------------+-----+------+------+ split —— 随机分割 功能：将原始 DataFrame 随机拆分，这通常与机器学习算法一起使用以创建训练、验证和测试集； 语法：返回 Array(DataFrame)； 12randomSplit(weights: Array[Double])randomSplit(weights: Array[Double], seed: Long) 示例： 1234567891011121314151617val dfs = df.randomSplit(Array(0.8, 0.2))dfs(0).show()dfs(1).show()+--------------------+-----+------+------+| name| dob|gender|salary|+--------------------+-----+------+------+| [James , , Smith]|36636| M| 3000|| [Michael , Rose, ]|40288| M| 4000||[Maria , Anne, Jo...|39192| F| 4000|+--------------------+-----+------+------++--------------------+-----+------+------+| name| dob|gender|salary|+--------------------+-----+------+------+|[Robert , , Willi...|42114| M| 4000|| [Jen, Mary, Brown]| | F| -1|+--------------------+-----+------+------+ limit —— 限制 功能：限制从 DataFrame 中提取的内容，当你需要一个空的 DataFrame 但又想保留 Schema 信息时可以通过 df.limit(0) 来实现； 语法： 1df.limit(n) 示例： 1234567891011121314df.orderBy(&quot;dob&quot;).limit(3).show()+--------------------+-----+------+------+| name| dob|gender|salary|+--------------------+-----+------+------+| [Jen, Mary, Brown]| | F| -1|| [James , , Smith]|36636| M| 3000||[Maria , Anne, Jo...|39192| F| 4000|+--------------------+-----+------+------+df.limit(0).show()+----+---+------+------+|name|dob|gender|salary|+----+---+------+------++----+---+------+------+ first | last —— 首行或末行 功能：获取某列第一行/最后一行的值 语法： 12first(e: Column, ignoreNulls: Boolean)first(columnName: String, ignoreNulls: Boolean) 示例： 1234567df.select(first(&quot;name&quot;), first(&quot;dob&quot;), last(&quot;gender&quot;), last(&quot;salary&quot;)).show()+------------------+-----------------+-------------------+-------------------+|first(name, false)|first(dob, false)|last(gender, false)|last(salary, false)|+------------------+-----------------+-------------------+-------------------+| [James , , Smith]| 36636| F| -1|+------------------+-----------------+-------------------+-------------------+ 表操作union —— 合并 功能：在 Spark 中 union() 和 unionAll() 作用相同，用于合并两个 schema 相同（不会校验schema，只会校验字段数是否相同）的 DataFrame，但是都不会对结果进行去重，如果需要去重，可以通过去重算子对结果去重。 语法： 1df.union(df2) 示例： 12// 没有什么好展示的val df5 = df.union(df2).distinct() join —— 连接 功能：Spark SQL 支持传统 SQL 中可用的所有基本联接操作（这里不再赘述），尽管 Spark 核心联接在设计时不小心会产生巨大的性能问题，因为它涉及到跨网络的数据 shuffe，另一方面，Spark SQL 连接在默认情况下具有更多的优化（多亏了 DataFrames &amp; Dataset），但是在使用时仍然会有一些性能问题需要考虑； 语法: 三要素为连接表、连接谓词、连接类型； 123456789101112131) join(right: Dataset[_]): DataFrame// 使用 usingColumn：join 结果中只会保留左表的 usingColumn，以及左右表其他列2) join(right: Dataset[_], usingColumn: String): DataFrame3) join(right: Dataset[_], usingColumns: Seq[String]): DataFrame4) join(right: Dataset[_], usingColumns: Seq[String], joinType: String): DataFrame// 使用 joinExprs：joinExprs 返回一个布尔型 Column，join 结果会包含两个表的所有列5) join(right: Dataset[_], joinExprs: Column): DataFrame6) join(right: Dataset[_], joinExprs: Column, joinType: String): DataFrame// 笛卡尔积：将左表中的每一行与右表中的每一行进行连接7) crossJoin(right: Dataset[_]) join 类型: 对于上面语句 4 和语句 5，你可以使用 JoinType 或 Join String 中的一种，如果要使用 JoinType，应该先导入 import org.apache.spark.sql.catalyst.plans._，以下示例将采用上面语句 6 的形式 JoinType Join String Equivalent SQL Join Inner.sql inner INNER JOIN FullOuter.sql outer, full, fullouter, full_outer FULL OUTER JOIN LeftOuter.sql left, leftouter, left_outer LEFT JOIN RightOuter.sql right, rightouter, right_outer RIGHT JOIN Cross.sql cross - LeftAnti.sql anti, leftanti, left_anti - LeftSemi.sql semi, leftsemi, left_semi - 示例数据: 123456789101112131415161718192021222324252627282930313233343536373839404142val emp = Seq((1,&quot;Smith&quot;,-1,&quot;2018&quot;,&quot;10&quot;,&quot;M&quot;,3000), (2,&quot;Rose&quot;,1,&quot;2010&quot;,&quot;20&quot;,&quot;M&quot;,4000), (3,&quot;Williams&quot;,1,&quot;2010&quot;,&quot;10&quot;,&quot;M&quot;,1000), (4,&quot;Jones&quot;,2,&quot;2005&quot;,&quot;10&quot;,&quot;F&quot;,2000), (5,&quot;Brown&quot;,2,&quot;2010&quot;,&quot;40&quot;,&quot;&quot;,-1), (6,&quot;Brown&quot;,2,&quot;2010&quot;,&quot;50&quot;,&quot;&quot;,-1) )val empColumns = Seq(&quot;emp_id&quot;,&quot;name&quot;,&quot;superior_emp_id&quot;,&quot;year_joined&quot;, &quot;emp_dept_id&quot;,&quot;gender&quot;,&quot;salary&quot;)import spark.sqlContext.implicits._val empDF = emp.toDF(empColumns:_*)empDF.show(false)+------+--------+---------------+-----------+-----------+------+------+|emp_id|name |superior_emp_id|year_joined|emp_dept_id|gender|salary|+------+--------+---------------+-----------+-----------+------+------+|1 |Smith |-1 |2018 |10 |M |3000 ||2 |Rose |1 |2010 |20 |M |4000 ||3 |Williams|1 |2010 |10 |M |1000 ||4 |Jones |2 |2005 |10 |F |2000 ||5 |Brown |2 |2010 |40 | |-1 ||6 |Brown |2 |2010 |50 | |-1 |+------+--------+---------------+-----------+-----------+------+------+val dept = Seq((&quot;Finance&quot;,10),(&quot;Marketing&quot;,20),(&quot;Sales&quot;,30),(&quot;IT&quot;,40))val deptColumns = Seq(&quot;dept_name&quot;,&quot;dept_id&quot;)val deptDF = dept.toDF(deptColumns:_*)deptDF.show(false)+---------+-------+|dept_name|dept_id|+---------+-------+|Finance |10 ||Marketing|20 ||Sales |30 ||IT |40 |+---------+-------+ Inner JoinInner Join 内连接，只返回匹配成功的行。 1234567891011empDF.join(deptDF,empDF(&quot;emp_dept_id&quot;) === deptDF(&quot;dept_id&quot;),&quot;inner&quot;).show(false)+------+--------+---------------+-----------+-----------+------+------+---------+-------+|emp_id|name |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|+------+--------+---------------+-----------+-----------+------+------+---------+-------+|1 |Smith |-1 |2018 |10 |M |3000 |Finance |10 ||2 |Rose |1 |2010 |20 |M |4000 |Marketing|20 ||3 |Williams|1 |2010 |10 |M |1000 |Finance |10 ||4 |Jones |2 |2005 |10 |F |2000 |Finance |10 ||5 |Brown |2 |2010 |40 | |-1 |IT |40 |+------+--------+---------------+-----------+-----------+------+------+---------+-------+ Full JoinOuter/Full,/Fullouter Join 全外连接，匹配成功的 + 左表有右表没有 + 右表有左表没有 12345678910111213141516empDF.join(deptDF,empDF(&quot;emp_dept_id&quot;) === deptDF(&quot;dept_id&quot;),&quot;outer&quot;).show(false)empDF.join(deptDF,empDF(&quot;emp_dept_id&quot;) === deptDF(&quot;dept_id&quot;),&quot;full&quot;).show(false)empDF.join(deptDF,empDF(&quot;emp_dept_id&quot;) === deptDF(&quot;dept_id&quot;),&quot;fullouter&quot;).show(false)+------+--------+---------------+-----------+-----------+------+------+---------+-------+|emp_id|name |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|+------+--------+---------------+-----------+-----------+------+------+---------+-------+|2 |Rose |1 |2010 |20 |M |4000 |Marketing|20 ||5 |Brown |2 |2010 |40 | |-1 |IT |40 ||1 |Smith |-1 |2018 |10 |M |3000 |Finance |10 ||3 |Williams|1 |2010 |10 |M |1000 |Finance |10 ||4 |Jones |2 |2005 |10 |F |2000 |Finance |10 ||6 |Brown |2 |2010 |50 | |-1 |null |null ||null |null |null |null |null |null |null |Sales |30 |+------+--------+---------------+-----------+-----------+------+------+---------+-------+ Left JoinLeft/Leftouter Join 左连接，匹配成功的 + 左表有右表没有的 1234567891011121314empDF.join(deptDF,empDF(&quot;emp_dept_id&quot;) === deptDF(&quot;dept_id&quot;),&quot;left&quot;).show(false)empDF.join(deptDF,empDF(&quot;emp_dept_id&quot;) === deptDF(&quot;dept_id&quot;),&quot;leftouter&quot;).show(false)+------+--------+---------------+-----------+-----------+------+------+---------+-------+|emp_id|name |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|+------+--------+---------------+-----------+-----------+------+------+---------+-------+|1 |Smith |-1 |2018 |10 |M |3000 |Finance |10 ||2 |Rose |1 |2010 |20 |M |4000 |Marketing|20 ||3 |Williams|1 |2010 |10 |M |1000 |Finance |10 ||4 |Jones |2 |2005 |10 |F |2000 |Finance |10 ||5 |Brown |2 |2010 |40 | |-1 |IT |40 ||6 |Brown |2 |2010 |50 | |-1 |null |null |+------+--------+---------------+-----------+-----------+------+------+---------+-------+ Right JoinRight/Rightouter Join 右连接，匹配成功的 + 右表有左表没有的 12345678910111213empDF.join(deptDF,empDF(&quot;emp_dept_id&quot;) === deptDF(&quot;dept_id&quot;),&quot;right&quot;).show(false)empDF.join(deptDF,empDF(&quot;emp_dept_id&quot;) === deptDF(&quot;dept_id&quot;),&quot;rightouter&quot;).show(false)+------+--------+---------------+-----------+-----------+------+------+---------+-------+|emp_id|name |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|+------+--------+---------------+-----------+-----------+------+------+---------+-------+|4 |Jones |2 |2005 |10 |F |2000 |Finance |10 ||3 |Williams|1 |2010 |10 |M |1000 |Finance |10 ||1 |Smith |-1 |2018 |10 |M |3000 |Finance |10 ||2 |Rose |1 |2010 |20 |M |4000 |Marketing|20 ||null |null |null |null |null |null |null |Sales |30 ||5 |Brown |2 |2010 |40 | |-1 |IT |40 |+------+--------+---------------+-----------+-----------+------+------+---------+-------+ Left Semi JoinLeft Semi Join 左半连接，匹配成功的，只保留左表字段。 1234567891011empDF.join(deptDF,empDF(&quot;emp_dept_id&quot;) === deptDF(&quot;dept_id&quot;),&quot;leftsemi&quot;).show(false)+------+--------+---------------+-----------+-----------+------+------+|emp_id|name |superior_emp_id|year_joined|emp_dept_id|gender|salary|+------+--------+---------------+-----------+-----------+------+------+|1 |Smith |-1 |2018 |10 |M |3000 ||2 |Rose |1 |2010 |20 |M |4000 ||3 |Williams|1 |2010 |10 |M |1000 ||4 |Jones |2 |2005 |10 |F |2000 ||5 |Brown |2 |2010 |40 | |-1 |+------+--------+---------------+-----------+-----------+------+------+ Left Anti JoinLeft Anti Join 反左半连接，没有匹配成功的，只返回左表字段 12345678empDF.join(deptDF,empDF(&quot;emp_dept_id&quot;) === deptDF(&quot;dept_id&quot;),&quot;leftanti&quot;).show(false)+------+-----+---------------+-----------+-----------+------+------+|emp_id|name |superior_emp_id|year_joined|emp_dept_id|gender|salary|+------+-----+---------------+-----------+-----------+------+------+|6 |Brown|2 |2010 |50 | |-1 |+------+-----+---------------+-----------+-----------+------+------+ Self Join虽然没有自连接类型，但是可以使用以上任意一种 join 类型与自己关联，但是要通过别名的方式。为DataFrame 起别名 &quot;a&quot; 后，原有字段名 &quot;col&quot; 就变成 &quot;a.col&quot;，可以通过 &quot;a.*&quot; 把原有的列“释放”出来。 12345678910111213141516empDF.as(&quot;emp1&quot;).join(empDF.as(&quot;emp2&quot;), col(&quot;emp1.superior_emp_id&quot;) === col(&quot;emp2.emp_id&quot;),&quot;inner&quot;) .select(col(&quot;emp1.emp_id&quot;),col(&quot;emp1.name&quot;), col(&quot;emp2.emp_id&quot;).as(&quot;superior_emp_id&quot;), col(&quot;emp2.name&quot;).as(&quot;superior_emp_name&quot;) ) .show(false) +------+--------+---------------+-----------------+|emp_id|name |superior_emp_id|superior_emp_name|+------+--------+---------------+-----------------+|2 |Rose |1 |Smith ||3 |Williams|1 |Smith ||4 |Jones |2 |Rose ||5 |Brown |2 |Rose ||6 |Brown |2 |Rose |+------+--------+---------------+-----------------+ Cross JoinCross Join（笛卡尔连接、交叉连接）会将左侧 DataFrame 中的每一行与右侧 DataFrame 中的每一行进行连接，这将导致结果 DataFrame 中的行数发生绝对爆炸，仅在绝对必要时才应使用笛卡尔积，它们很危险！！！我们分几种场景来讨论和 Cross Join 相关的一些问题： join 算子中如果指定了连接谓词，那么，即使将参数 joinType 设置为 “cross”，实际执行的仍然是 inner join 12345678910empDF.join(deptDF, empDF(&quot;emp_dept_id&quot;) === deptDF(&quot;dept_id&quot;), &quot;cross&quot;).show()+------+--------+---------------+-----------+-----------+------+------+---------+-------+|emp_id| name|superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|+------+--------+---------------+-----------+-----------+------+------+---------+-------+| 1| Smith| -1| 2018| 10| M| 3000| Finance| 10|| 2| Rose| 1| 2010| 20| M| 4000|Marketing| 20|| 3|Williams| 1| 2010| 10| M| 1000| Finance| 10|| 4| Jones| 2| 2005| 10| F| 2000| Finance| 10|| 5| Brown| 2| 2010| 40| | -1| IT| 40|+------+--------+---------------+-----------+-----------+------+------+---------+-------+ join 算子中，如果将连接谓词设置为恒等式，可以实现笛卡尔积（joinType需同时设置为 “cross”） 12345678910111213141516171819202122232425empDF.join(deptDF, lit(1) === lit(1), &quot;cross&quot;).show()+------+--------+---------------+-----------+-----------+------+------+---------+-------+|emp_id| name|superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|+------+--------+---------------+-----------+-----------+------+------+---------+-------+| 1| Smith| -1| 2018| 10| M| 3000| Finance| 10|| 1| Smith| -1| 2018| 10| M| 3000|Marketing| 20|| 1| Smith| -1| 2018| 10| M| 3000| Sales| 30|| 1| Smith| -1| 2018| 10| M| 3000| IT| 40|| 2| Rose| 1| 2010| 20| M| 4000| Finance| 10|| 2| Rose| 1| 2010| 20| M| 4000|Marketing| 20|| 2| Rose| 1| 2010| 20| M| 4000| Sales| 30|| 2| Rose| 1| 2010| 20| M| 4000| IT| 40|| 3|Williams| 1| 2010| 10| M| 1000| Finance| 10|| 3|Williams| 1| 2010| 10| M| 1000|Marketing| 20|| 3|Williams| 1| 2010| 10| M| 1000| Sales| 30|| 3|Williams| 1| 2010| 10| M| 1000| IT| 40|| 4| Jones| 2| 2005| 10| F| 2000| Finance| 10|| 4| Jones| 2| 2005| 10| F| 2000|Marketing| 20|| 4| Jones| 2| 2005| 10| F| 2000| Sales| 30|| 4| Jones| 2| 2005| 10| F| 2000| IT| 40|| 5| Brown| 2| 2010| 40| | -1| Finance| 10|| 5| Brown| 2| 2010| 40| | -1|Marketing| 20|| 5| Brown| 2| 2010| 40| | -1| Sales| 30|| 5| Brown| 2| 2010| 40| | -1| IT| 40|+------+--------+---------------+-----------+-----------+------+------+---------+-------+ join 算子中，如果省略了连接谓词，则会报 AnalysisException 错误，一种解决办法是设置 spark.conf.set(&quot;spark.sql.crossJoin.enabled&quot;,true)，以允许笛卡尔积而不会发出警告或 Spark 不会尝试为您执行另一种连接 12345678910111213141516171819202122232425262728293031323334353637empDF.join(deptDF).show()org.apache.spark.sql.AnalysisException: Detected implicit cartesian product for INNER join between logical plansLocalRelation [emp_id#940, name#941, superior_emp_id#942, year_joined#943, emp_dept_id#944, gender#945, salary#946]andLocalRelation [dept_name#981, dept_id#982]Join condition is missing or trivial.Either: use the CROSS JOIN syntax to allow cartesian products between theserelations, or: enable implicit cartesian products by setting the configurationvariable spark.sql.crossJoin.enabled=true;spark.conf.set(&quot;spark.sql.crossJoin.enabled&quot;,true)empDF.join(deptDF).show()+------+--------+---------------+-----------+-----------+------+------+---------+-------+|emp_id| name|superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|+------+--------+---------------+-----------+-----------+------+------+---------+-------+| 1| Smith| -1| 2018| 10| M| 3000| Finance| 10|| 1| Smith| -1| 2018| 10| M| 3000|Marketing| 20|| 1| Smith| -1| 2018| 10| M| 3000| Sales| 30|| 1| Smith| -1| 2018| 10| M| 3000| IT| 40|| 2| Rose| 1| 2010| 20| M| 4000| Finance| 10|| 2| Rose| 1| 2010| 20| M| 4000|Marketing| 20|| 2| Rose| 1| 2010| 20| M| 4000| Sales| 30|| 2| Rose| 1| 2010| 20| M| 4000| IT| 40|| 3|Williams| 1| 2010| 10| M| 1000| Finance| 10|| 3|Williams| 1| 2010| 10| M| 1000|Marketing| 20|| 3|Williams| 1| 2010| 10| M| 1000| Sales| 30|| 3|Williams| 1| 2010| 10| M| 1000| IT| 40|| 4| Jones| 2| 2005| 10| F| 2000| Finance| 10|| 4| Jones| 2| 2005| 10| F| 2000|Marketing| 20|| 4| Jones| 2| 2005| 10| F| 2000| Sales| 30|| 4| Jones| 2| 2005| 10| F| 2000| IT| 40|| 5| Brown| 2| 2010| 40| | -1| Finance| 10|| 5| Brown| 2| 2010| 40| | -1|Marketing| 20|| 5| Brown| 2| 2010| 40| | -1| Sales| 30|| 5| Brown| 2| 2010| 40| | -1| IT| 40|+------+--------+---------------+-----------+-----------+------+------+---------+-------+only showing top 20 rows 以上方式虽然可以实现 cross Join，但并不推荐使用，从 spark-sql_2.11 2.1.0 之后的版本专门提供了 crossJoin 算子来实现笛卡尔积，使用 crossJoin 不用修改配置 12345678910111213141516171819202122232425empDF.crossJoin(deptDF).show()+------+--------+---------------+-----------+-----------+------+------+---------+-------+|emp_id| name|superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|+------+--------+---------------+-----------+-----------+------+------+---------+-------+| 1| Smith| -1| 2018| 10| M| 3000| Finance| 10|| 1| Smith| -1| 2018| 10| M| 3000|Marketing| 20|| 1| Smith| -1| 2018| 10| M| 3000| Sales| 30|| 1| Smith| -1| 2018| 10| M| 3000| IT| 40|| 2| Rose| 1| 2010| 20| M| 4000| Finance| 10|| 2| Rose| 1| 2010| 20| M| 4000|Marketing| 20|| 2| Rose| 1| 2010| 20| M| 4000| Sales| 30|| 2| Rose| 1| 2010| 20| M| 4000| IT| 40|| 3|Williams| 1| 2010| 10| M| 1000| Finance| 10|| 3|Williams| 1| 2010| 10| M| 1000|Marketing| 20|| 3|Williams| 1| 2010| 10| M| 1000| Sales| 30|| 3|Williams| 1| 2010| 10| M| 1000| IT| 40|| 4| Jones| 2| 2005| 10| F| 2000| Finance| 10|| 4| Jones| 2| 2005| 10| F| 2000|Marketing| 20|| 4| Jones| 2| 2005| 10| F| 2000| Sales| 30|| 4| Jones| 2| 2005| 10| F| 2000| IT| 40|| 5| Brown| 2| 2010| 40| | -1| Finance| 10|| 5| Brown| 2| 2010| 40| | -1|Marketing| 20|| 5| Brown| 2| 2010| 40| | -1| Sales| 30|| 5| Brown| 2| 2010| 40| | -1| IT| 40|+------+--------+---------------+-----------+-----------+------+------+---------+-------+ 同源 DataFrame JOIN 陷阱当同源 DataFrame（衍生于同一个 DataFrame ）之间进行 Join 时，可能会导致一些意想不到的错误。 12345678910111213141516var x = empDF.groupBy(&quot;superior_emp_id&quot;).agg(count(&quot;*&quot;).as(&quot;f_cnt&quot;))x.show()+---------------+-----+|superior_emp_id|f_cnt|+---------------+-----+| -1| 1|| 1| 2|| 2| 3|+---------------+-----+// join 后的结果不应该为空empDF.join(x, empDF(&quot;emp_id&quot;) === x(&quot;superior_emp_id&quot;)).show()+------+----+---------------+-----------+-----------+------+------+---------------+-----+|emp_id|name|superior_emp_id|year_joined|emp_dept_id|gender|salary|superior_emp_id|f_cnt|+------+----+---------------+-----------+-----------+------+------+---------------+-----++------+----+---------------+-----------+-----------+------+------+---------------+-----+ 有多种方式可以解决这个问题： 使用 SQL 表达式 123456789101112131415empDF.createOrReplaceTempView(&quot;empDF&quot;)x.createOrReplaceTempView(&quot;x&quot;)val sql = &quot;&quot;&quot;select * from empDF join x on empDF.emp_id = x.superior_emp_id&quot;&quot;&quot;spark.sql(sql).show()+------+---------+---------------+-----------+-------+------+------+---------------+-----+|emp_id|dept_name|superior_emp_id|year_joined|dept_id|gender|salary|superior_emp_id|f_cnt|+------+---------+---------------+-----------+-------+------+------+---------------+-----+| 1| Smith| -1| 2018| 10| M| 3000| 1| 2|| 2| Rose| 1| 2010| 20| M| 4000| 2| 3|+------+---------+---------------+-----------+-------+------+------+---------------+-----+ 为 DataFrame 起别名 1234567empDF.as(&quot;a&quot;).join(x.as(&quot;b&quot;), col(&quot;a.emp_id&quot;) === col(&quot;b.superior_emp_id&quot;)).show()+------+---------+---------------+-----------+-------+------+------+---------------+-----+|emp_id|dept_name|superior_emp_id|year_joined|dept_id|gender|salary|superior_emp_id|f_cnt|+------+---------+---------------+-----------+-------+------+------+---------------+-----+| 1| Smith| -1| 2018| 10| M| 3000| 1| 2|| 2| Rose| 1| 2010| 20| M| 4000| 2| 3|+------+---------+---------------+-----------+-------+------+------+---------------+-----+ withColumn 重命名列 12345678910111213141516171819val x = empDF.groupBy(&quot;superior_emp_id&quot;).agg(count(&quot;*&quot;).as(&quot;f_cnt&quot;)) .withColumnRenamed(&quot;superior_emp_id&quot;, &quot;superior_emp_id&quot;)empDF.join(x, empDF(&quot;emp_id&quot;) === x(&quot;superior_emp_id&quot;)).show()+------+---------+---------------+-----------+-------+------+------+---------------+-----+|emp_id|dept_name|superior_emp_id|year_joined|dept_id|gender|salary|superior_emp_id|f_cnt|+------+---------+---------------+-----------+-------+------+------+---------------+-----+| 1| Smith| -1| 2018| 10| M| 3000| 1| 2|| 2| Rose| 1| 2010| 20| M| 4000| 2| 3|+------+---------+---------------+-----------+-------+------+------+---------------+-----+val x = empDF.groupBy(&quot;superior_emp_id&quot;).agg(count(&quot;*&quot;).as(&quot;f_cnt&quot;)) .withColumn(&quot;superior_emp_id&quot;, col(&quot;superior_emp_id&quot;))empDF.join(x, empDF(&quot;emp_id&quot;) === x(&quot;superior_emp_id&quot;)).show()+------+-----+---------------+-----------+-----------+------+------+---------------+-----+|emp_id| name|superior_emp_id|year_joined|emp_dept_id|gender|salary|superior_emp_id|f_cnt|+------+-----+---------------+-----------+-----------+------+------+---------------+-----+| 1|Smith| -1| 2018| 10| M| 3000| 1| 2|| 2| Rose| 1| 2010| 20| M| 4000| 2| 3|+------+-----+---------------+-----------+-----------+------+------+---------------+-----+ toDF 重新定义其中一个 DataFrame 的 Schema： 1234567x = x.toDF(x.columns:_*)empDF.join(x, empDF(&quot;emp_id&quot;) === x(&quot;superior_emp_id&quot;)).show()+------+-----+---------------+-----------+-----------+------+------+---------------+-----+|emp_id| name|superior_emp_id|year_joined|emp_dept_id|gender|salary|superior_emp_id|f_cnt|+------+-----+---------------+-----------+-----------+------+------+---------------+-----+| 1|Smith| -1| 2018| 10| M| 3000| 1| 2|| 2| Rose| 1| 2010| 20| M| 4000| 2| 3| usingColumn 陷阱usingColumn 语法得到的结果 DataFrame 中会自动去除被 join DataFrame 的关联键，只保留主调 DataFrame 中的关联键，所以不能通过 select 或 expr 选择被调 DataFrame 中的关联键，但是却可以在 filter 中引用被调 DataFrame 中的关联键： 123456789101112131415161718192021222324252627282930313233343536373839val x = deptDF.limit(2).select(&quot;dept_id&quot;).toDF(&quot;dept_id&quot;)x.show()+-------+|dept_id|+-------+| 10|| 20|+-------+val res = deptDF.join(x, Seq(&quot;dept_id&quot;), &quot;left&quot;)res.show()res.printSchema+-------+---------+|dept_id|dept_name|+-------+---------+| 10| Finance|| 20|Marketing|| 30| Sales|| 40| IT|+-------+---------+res.filter(x(&quot;dept_id&quot;).isNull).show()+-------+---------+|dept_id|dept_name|+-------+---------+| 30| Sales|| 40| IT|+-------+---------+res.select(expr(&quot;x.dept_id&quot;)).show()org.apache.spark.sql.AnalysisException: cannot resolve &#x27;`x.dept_id`&#x27; given input columns: [dept_id, dept_name]; line 1 pos 0;&#x27;Project [&#x27;x.dept_id]+- Project [dept_id#456, dept_name#455] +- Join LeftOuter, (dept_id#456 = dept_id#497)res.select(x(&quot;dept_id&quot;)).show()org.apache.spark.sql.AnalysisException: Cannot resolve column name &quot;dept_id&quot; among (superior_emp_id, f_cnt); at org.apache.spark.sql.Dataset$$anonfun$resolve$1.apply(Dataset.scala:223) at org.apache.spark.sql.Dataset$$anonfun$resolve$1.apply(Dataset.scala:223) 处理 join 中的同名字段如果参与 join 的两个 DataFrame 之间存在相同名称的字段，很容易在后续的转换操作中出现 Reference is ambiguous 的错误，整体上有两种解决思路： 如果需要的字段少：那就 select 你所需要的字段就行了； 如果需要的字段多：那就 drop 不需要的字段； 在 join 前中后又可以有不同的处理方式： join 前：修改/删除其中一方 DataFrame 的同名字段名； join 中：如果同名字段是 join 的关联键，使用 usingColumn 语法，join 后只会保留左表关联字段； join 后： 要么通过 select(Expr) 明确指定需要的表字段； 要么通过 drop 删除不需要的表字段； 要么通过 withColumn 添加新的字段，此时 withColumn 如果用于修改已有同名字段的内容，将会同时修改所有同名字段，修改后的结果仍会保留同名字段； 示例： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128// 示例数据val emp = Seq((1,&quot;Smith&quot;,-1,&quot;2018&quot;,&quot;10&quot;,&quot;M&quot;,3000), (2,&quot;Rose&quot;,1,&quot;2010&quot;,&quot;20&quot;,&quot;M&quot;,4000), (3,&quot;Williams&quot;,1,&quot;2010&quot;,&quot;10&quot;,&quot;M&quot;,1000), (4,&quot;Jones&quot;,2,&quot;2005&quot;,&quot;10&quot;,&quot;F&quot;,2000), (5,&quot;Brown&quot;,2,&quot;2010&quot;,&quot;40&quot;,&quot;&quot;,-1), (6,&quot;Brown&quot;,2,&quot;2010&quot;,&quot;50&quot;,&quot;&quot;,-1) )val empColumns = Seq(&quot;emp_id&quot;,&quot;dept_name&quot;,&quot;superior_emp_id&quot;,&quot;year_joined&quot;, &quot;dept_id&quot;,&quot;gender&quot;,&quot;salary&quot;)import spark.sqlContext.implicits._val empDF = emp.toDF(empColumns:_*)empDF.show(false)+------+---------+---------------+-----------+-------+------+------+|emp_id|dept_name|superior_emp_id|year_joined|dept_id|gender|salary|+------+---------+---------------+-----------+-------+------+------+|1 |Smith |-1 |2018 |10 |M |3000 ||2 |Rose |1 |2010 |20 |M |4000 ||3 |Williams |1 |2010 |10 |M |1000 ||4 |Jones |2 |2005 |10 |F |2000 ||5 |Brown |2 |2010 |40 | |-1 ||6 |Brown |2 |2010 |50 | |-1 |+------+---------+---------------+-----------+-------+------+------+val dept = Seq((&quot;Finance&quot;,10),(&quot;Marketing&quot;,20),(&quot;Sales&quot;,30),(&quot;IT&quot;,40))val deptColumns = Seq(&quot;dept_name&quot;,&quot;dept_id&quot;)val deptDF = dept.toDF(deptColumns:_*)deptDF.show(false)+---------+-------+|dept_name|dept_id|+---------+-------+|Finance |10 ||Marketing|20 ||Sales |30 ||IT |40 |+---------+-------+// usingColumn 会去掉右侧 DataFrame 的关联键，这里使用 deptDF(&quot;*&quot;) 会报无法找到 dept_id 的错误res.select(deptDF(&quot;*&quot;)).show()org.apache.spark.sql.AnalysisException: Resolved attribute(s) dept_id#477 missing from emp_id#435,salary#441,year_joined#438,gender#440,dept_name#436,dept_id#439,dept_name#476,superior_emp_id#437 in operator !Project [dept_name#476, dept_id#477]. Attribute(s) with the same name appear in the operation: dept_id. Please check if the right attribute(s) are used.;;// 选择 empDF 中所有字段，以及 deptDF 中的 dept_name 字段res.select(empDF(&quot;*&quot;), deptDF(&quot;dept_name&quot;)).show()+------+---------+---------------+-----------+-------+------+------+---------+|emp_id|dept_name|superior_emp_id|year_joined|dept_id|gender|salary|dept_name|+------+---------+---------------+-----------+-------+------+------+---------+| 1| Smith| -1| 2018| 10| M| 3000| Finance|| 2| Rose| 1| 2010| 20| M| 4000|Marketing|| 3| Williams| 1| 2010| 10| M| 1000| Finance|| 4| Jones| 2| 2005| 10| F| 2000| Finance|| 5| Brown| 2| 2010| 40| | -1| IT|+------+---------+---------------+-----------+-------+------+------+---------+// 上面结果包含了同名字段 dept_name，如果直接引用字段名则会报 ambiguous 错误res.select(empDF(&quot;*&quot;), deptDF(&quot;dept_name&quot;)).select(&quot;dept_name&quot;).show()org.apache.spark.sql.AnalysisException: Reference &#x27;dept_name&#x27; is ambiguous, could be: dept_name, dept_name.;// 想通过先删除左表的 dept_name 再选择左表中所有字段，但 empDF(&quot;*&quot;) 仍然会包含已经删掉的字段res.drop(empDF(&quot;dept_name&quot;)).select(empDF(&quot;*&quot;), deptDF(&quot;dept_name&quot;)).show()org.apache.spark.sql.AnalysisException: Resolved attribute(s) dept_name#436 missing from// 其实只要先 select 再 drop 就可以了，但是这种方法有很大局限，一个是当用列对象参数时， drop(column) 只能删除一列，而且这一列还必须已存在，当用列名时，drop 又会把所有同名的列删除掉res.select(empDF(&quot;*&quot;), deptDF(&quot;dept_name&quot;)).drop(empDF(&quot;dept_name&quot;)).select(&quot;dept_name&quot;).show()+---------+|dept_name|+---------+| Finance||Marketing|| Finance|| Finance|| IT|+---------+// 值得说明的是 withColumn 并不会消除同名字段的分歧，只会同时改变同名字段的值res.withColumn(&quot;dept_name&quot;, lit(1)).show()+-------+------+---------+---------------+-----------+------+------+---------+|dept_id|emp_id|dept_name|superior_emp_id|year_joined|gender|salary|dept_name|+-------+------+---------+---------------+-----------+------+------+---------+| 10| 1| 1| -1| 2018| M| 3000| 1|| 20| 2| 1| 1| 2010| M| 4000| 1|| 10| 3| 1| 1| 2010| M| 1000| 1|| 10| 4| 1| 2| 2005| F| 2000| 1|| 40| 5| 1| 2| 2010| | -1| 1|+-------+------+---------+---------------+-----------+------+------+---------+// 综上，比较好的做法是在join前 drop 掉最后不需要的列（如果需要对其 select(&quot;*&quot;)的话）val res = empDF.drop(&quot;dept_name&quot;).as(&quot;a&quot;).join(deptDF.as(&quot;b&quot;), Seq(&quot;dept_id&quot;))res.show()+-------+------+---------------+-----------+------+------+---------+|dept_id|emp_id|superior_emp_id|year_joined|gender|salary|dept_name|+-------+------+---------------+-----------+------+------+---------+| 10| 1| -1| 2018| M| 3000| Finance|| 20| 2| 1| 2010| M| 4000|Marketing|| 10| 3| 1| 2010| M| 1000| Finance|| 10| 4| 2| 2005| F| 2000| Finance|| 40| 5| 2| 2010| | -1| IT|+-------+------+---------------+-----------+------+------+---------+res.select(&quot;a.*&quot;, &quot;b.dept_name&quot;).show()+-------+------+---------------+-----------+------+------+---------+|dept_id|emp_id|superior_emp_id|year_joined|gender|salary|dept_name|+-------+------+---------------+-----------+------+------+---------+| 10| 1| -1| 2018| M| 3000| Finance|| 20| 2| 1| 2010| M| 4000|Marketing|| 10| 3| 1| 2010| M| 1000| Finance|| 10| 4| 2| 2005| F| 2000| Finance|| 40| 5| 2| 2010| | -1| IT|+-------+------+---------------+-----------+------+------+---------+res.selectExpr(&quot;a.*&quot;, &quot;b.dept_name as f_new_name&quot;).show()+-------+------+---------------+-----------+------+------+----------+|dept_id|emp_id|superior_emp_id|year_joined|gender|salary|f_new_name|+-------+------+---------------+-----------+------+------+----------+| 10| 1| -1| 2018| M| 3000| Finance|| 20| 2| 1| 2010| M| 4000| Marketing|| 10| 3| 1| 2010| M| 1000| Finance|| 10| 4| 2| 2005| F| 2000| Finance|| 40| 5| 2| 2010| | -1| IT|+-------+------+---------------+-----------+------+------+----------+ join 最佳实践DataFrame API 的 JOIN 操作有诸多需要注意的地方，除了正确使用 JOIN 类型和 JOIN 语法外，经常引起困惑的地方在于如何从 JOIN 结果中选择我们需要的字段，对此，我们总结了一些最佳实践： 当 DataFrame 不方便通过一个变量来引用时，可以在 JOIN 语句中为 DataFrame 起别名： 可以通过 &quot;表别名.字段名&quot; 来引用对应字段； 如果不存在同名字段，也可以省略掉表别名，直接用 &quot;字段名&quot; 来应用对应字段； 当 JOIN 的两个 DataFrame 中包含同名字段时： 可以在 JOIN 前删除/重命名无用的同名字段； 如果同名字段作为关联字段，usingColumn 语法将只会保留左表关联字段； 可以在 JOIN 后 select(Expr) 需要的字段，drop 不需要的字段，withColumn 添加新的字段； 同源 DataFrame 之间 JOIN，在 JOIN 前通过 toDF() 转化其中一个 DataFrame； 看过上面的示例，你可能会觉得 DataFrame 的 JOIN 太不方便了，还不如直接写 SQL 表达式呢！事实上，DataFrame API 更加紧凑，更便于编写结构化代码，能够帮助我们完成大部分的语法检查，如果要在 DataFrame 中穿插 SQL 表达式，就使用 expr() 或 selectExpr() 函数吧！ repartition —— 重分区 功能：repartition 会导致数据的完全随机洗牌（shuffle），这意味着通常仅应在将来的分区数大于当前的分区数时或在按一组列进行分区时重新分区；如果经常要按照某个列进行过滤，则值得按该列重新分区； 语法： 12345// 指定所需的分区数repartition(numPartitions: Int)// 指定按照某列进行分区repartition(partitionExprs: Column*)repartition(numPartitions: Int, partitionExprs: Column*) 示例： 123df.repartition(3)df.repartition(col(&quot;dob&quot;))df.repartition(5, col(&quot;dob&quot;)) coalesce —— 分区合并 功能：coalesce 不会引起 full shuffle，并尝试合并分区（将来的分区数小于当前的分区数）； 语法： 1coalesce(numPartitions: Int) 示例： 1df.repartition(5, col(&quot;dob&quot;)).coalesce(2) cache | persist —— 缓存 功能：虽然 Spark 提供的计算速度是传统 Map Reduce 作业的 100 倍，但是如果您没有将作业设计为重用重复计算，那么当您处理数十亿或数万亿数据时，性能会下降。使用 cache() 和 persist() 方法，每个节点将其分区的数据存储在内存/磁盘中，并在该数据集的其他操作中重用它们，真正缓存是在第一次被相关 action 调用后才缓存。Spark 在节点上的持久数据是容错的，这意味着如果数据集的任何分区丢失，它将使用创建它的原始转换自动重新计算。Spark 会自动监视您进行的每个 persist（）和cache（）调用，并检查每个节点上的使用情况，如果不再使用或通过 least-recently-used (LRU) 算法，删除持久化数据，也可以使用 unpersist（）方法手动删除。unpersist（）将数据集标记为非持久性，并立即从内存和磁盘中删除它的所有块。 语法: 12345678910// StorageLevel 1) persist() : Dataset.this.type2) persist(newLevel : org.apache.spark.storage.StorageLevel) : Dataset.this.type// cache() 调用的也是 persist()，df.cache() 的默认存储级别为 MEMORY_AND_DISK，而RDD.chache() 的默认存储级别为 MEMORY_ONLYdef cache(): this.type = persist()// 手动取消持久化unpersist() : Dataset.this.typeunpersist(blocking : scala.Boolean) : Dataset.this.type 示例: 1234567891011121314// cacheval df = spark.read.options(Map(&quot;inferSchema&quot;-&gt;&quot;true&quot;,&quot;delimiter&quot;-&gt;&quot;,&quot;,&quot;header&quot;-&gt;&quot;true&quot;)).csv(&quot;src/main/resources/zipcodes.csv&quot;) val df2 = df.where(col(&quot;State&quot;) === &quot;PR&quot;).cache()df2.show(false)println(df2.count())val df3 = df2.where(col(&quot;Zipcode&quot;) === 704)println(df2.count())// persistval dfPersist = df.persist(StorageLevel.MEMORY_ONLY)dfPersist.show(false)// unpersistval dfPersist = dfPersist.unpersist() StorageLevel 有以下几个级别： 级别 使用空间 CPU时间 是否内存 是否磁盘 备注 MEMORY_ONLY 高 低 是 否 - MEMORY_ONLY_2 高 低 是 否 数据存2份 MEMORY_ONLY_SER_2 低 高 是 否 数据序列化，数据存2份 MEMORY_AND_DISK 高 中等 部分 部分 内存放不下，则溢写到磁盘 MEMORY_AND_DISK_2 高 中等 部分 部分 数据存2份 MEMORY_AND_DISK_SER 低 高 部分 部分 - MEMORY_AND_DISK_SER_2 低 高 部分 部分 数据存2份 DISK_ONLY 低 高 否 是 DISK_ONLY_2 低 高 否 是 数据存2份 NONE - - - - - OFF_HEAP - - - - - collect —— 收集到 driver 功能：collect() 和 collectAsList() 用于将 RDD/DataFrame/Dataset 中所有的数据拉取到 Driver 节点，然后你可以在 driver 节点使用 scala 进行进一步处理，通常用于较小的数据集，如果数据集过大可能会导致内存不足，很容易使 driver 节点崩溃并时区应用程序的状态，这也很昂贵，因为是逐条处理，而不是并行计算。 语法： 12collect() : scala.Array[T]collectAsList() : java.util.List[T] 示例： 123456789101112131415161718192021222324252627df.show()+---------------------+-----+------+------+|name |id |gender|salary|+---------------------+-----+------+------+|[James , , Smith] |36636|M |3000 ||[Michael , Rose, ] |40288|M |4000 ||[Robert , , Williams]|42114|M |4000 ||[Maria , Anne, Jones]|39192|F |4000 ||[Jen, Mary, Brown] | |F |-1 |+---------------------+-----+------+------+val colList = df.collectAsList()val colData = df.collect()colData.foreach(row =&gt; &#123; val salary = row.getInt(3) val fullName:Row = row.getStruct(0) val firstName = fullName.getString(0) val middleName = fullName.get(1).toString val lastName = fullName.getAs[String](&quot;lastname&quot;) println(firstName+&quot;,&quot;+middleName+&quot;,&quot;+lastName+&quot;,&quot;+salary) &#125;)James ,,Smith,3000Michael ,Rose,,4000Robert ,,Williams,4000Maria ,Anne,Jones,4000Jen,Mary,Brown,-1 其他操作when —— 条件判断 功能：when otherwise 类似于 SQL 中的 case when 语句； 语法：可以由多个 when 表达式（不满足前一个 when 条件则继续匹配下一个 when 条件），也可以不带 otherwise 表达式（不满足 when 条件则返回 null）； 12when(condition: Column, value: Any): Columnotherwise(value: Any): Column 示例： 12345678910111213141516171819202122232425262728293031323334df.withColumn(&quot;new_gender&quot;, when(col(&quot;gender&quot;) === &quot;M&quot;, &quot;Male&quot;)).show()+--------------------+-----+------+------+----------+| name| dob|gender|salary|new_gender|+--------------------+-----+------+------+----------+| [James , , Smith]|36636| M| 3000| Male|| [Michael , Rose, ]|40288| M| 4000| Male||[Robert , , Willi...|42114| M| 4000| Male||[Maria , Anne, Jo...|39192| F| 4000| null|| [Jen, Mary, Brown]| | F| -1| null|+--------------------+-----+------+------+----------+df.withColumn(&quot;new_gender&quot;, when(col(&quot;gender&quot;) === &quot;M&quot;, &quot;Male&quot;).otherwise(&quot;Unknown&quot;)).show()+--------------------+-----+------+------+----------+| name| dob|gender|salary|new_gender|+--------------------+-----+------+------+----------+| [James , , Smith]|36636| M| 3000| Male|| [Michael , Rose, ]|40288| M| 4000| Male||[Robert , , Willi...|42114| M| 4000| Male||[Maria , Anne, Jo...|39192| F| 4000| Unknown|| [Jen, Mary, Brown]| | F| -1| Unknown|+--------------------+-----+------+------+----------+df.withColumn(&quot;new_gender&quot;, when(col(&quot;gender&quot;) === &quot;M&quot;, &quot;Male&quot;) .when(col(&quot;gender&quot;) === &quot;F&quot;, &quot;Female&quot;) .otherwise(&quot;Unknown&quot;))+--------------------+-----+------+------+----------+| name| dob|gender|salary|new_gender|+--------------------+-----+------+------+----------+| [James , , Smith]|36636| M| 3000| Male|| [Michael , Rose, ]|40288| M| 4000| Male||[Robert , , Willi...|42114| M| 4000| Male||[Maria , Anne, Jo...|39192| F| 4000| Female|| [Jen, Mary, Brown]| | F| -1| Female|+--------------------+-----+------+------+----------+ flatten —— 列拆多列 功能：在 Spark SQL 中，扁平化 DataFrame 的嵌套结构列对于一级嵌套很简单，而对于多级嵌套和存在数百个列的情况下则很复杂。 扁平化嵌套 struct: 如果哦列数有限，可以通过引用列名似乎很容易解决，但是请想象一下，如果您有100多个列并在一个select中引用所有列，那么会很麻烦。可以通过创建一个递归函数 flattenStructSchema（）轻松地将数百个嵌套级别列展平。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101val structureData = Seq( Row(Row(&quot;James &quot;,&quot;&quot;,&quot;Smith&quot;),Row(Row(&quot;CA&quot;,&quot;Los Angles&quot;),Row(&quot;CA&quot;,&quot;Sandiago&quot;))), Row(Row(&quot;Michael &quot;,&quot;Rose&quot;,&quot;&quot;),Row(Row(&quot;NY&quot;,&quot;New York&quot;),Row(&quot;NJ&quot;,&quot;Newark&quot;))), Row(Row(&quot;Robert &quot;,&quot;&quot;,&quot;Williams&quot;),Row(Row(&quot;DE&quot;,&quot;Newark&quot;),Row(&quot;CA&quot;,&quot;Las Vegas&quot;))), Row(Row(&quot;Maria &quot;,&quot;Anne&quot;,&quot;Jones&quot;),Row(Row(&quot;PA&quot;,&quot;Harrisburg&quot;),Row(&quot;CA&quot;,&quot;Sandiago&quot;))), Row(Row(&quot;Jen&quot;,&quot;Mary&quot;,&quot;Brown&quot;),Row(Row(&quot;CA&quot;,&quot;Los Angles&quot;),Row(&quot;NJ&quot;,&quot;Newark&quot;))) )val structureSchema = new StructType() .add(&quot;name&quot;,new StructType() .add(&quot;firstname&quot;,StringType) .add(&quot;middlename&quot;,StringType) .add(&quot;lastname&quot;,StringType)) .add(&quot;address&quot;,new StructType() .add(&quot;current&quot;,new StructType() .add(&quot;state&quot;,StringType) .add(&quot;city&quot;,StringType)) .add(&quot;previous&quot;,new StructType() .add(&quot;state&quot;,StringType) .add(&quot;city&quot;,StringType)))val df = spark.createDataFrame( spark.sparkContext.parallelize(structureData),structureSchema)df.printSchema()root |-- name: struct (nullable = true) | |-- firstname: string (nullable = true) | |-- middlename: string (nullable = true) | |-- lastname: string (nullable = true) |-- address: struct (nullable = true) | |-- current: struct (nullable = true) | | |-- state: string (nullable = true) | | |-- city: string (nullable = true) | |-- previous: struct (nullable = true) | | |-- state: string (nullable = true) | | |-- city: string (nullable = true) df.show(false)+---------------------+----------------------------------+|name |address |+---------------------+----------------------------------+|[James , , Smith] |[[CA, Los Angles], [CA, Sandiago]]||[Michael , Rose, ] |[[NY, New York], [NJ, Newark]] ||[Robert , , Williams]|[[DE, Newark], [CA, Las Vegas]] ||[Maria , Anne, Jones]|[[PA, Harrisburg], [CA, Sandiago]]||[Jen, Mary, Brown] |[[CA, Los Angles], [NJ, Newark]] |+---------------------+----------------------------------+// 可以通过使用点符号（parentColumn.childColumn）来引用嵌套结构列，一种将嵌套结构打平的简单方法如下:val df2 = df.select(col(&quot;name.*&quot;), col(&quot;address.current.*&quot;), col(&quot;address.previous.*&quot;))val df2Flatten = df2.toDF(&quot;fname&quot;,&quot;mename&quot;,&quot;lname&quot;,&quot;currAddState&quot;, &quot;currAddCity&quot;,&quot;prevAddState&quot;,&quot;prevAddCity&quot;)df2Flatten.printSchema()df2Flatten.show(false)root |-- name_firstname: string (nullable = true) |-- name_middlename: string (nullable = true) |-- name_lastname: string (nullable = true) |-- address_current_state: string (nullable = true) |-- address_current_city: string (nullable = true) |-- address_previous_state: string (nullable = true) |-- address_previous_city: string (nullable = true)+--------+------+--------+------------+-----------+------------+-----------+|fname |mename|lname |currAddState|currAddCity|prevAddState|prevAddCity|+--------+------+--------+------------+-----------+------------+-----------+|James | |Smith |CA |Los Angles |CA |Sandiago ||Michael |Rose | |NY |New York |NJ |Newark ||Robert | |Williams|DE |Newark |CA |Las Vegas ||Maria |Anne |Jones |PA |Harrisburg |CA |Sandiago ||Jen |Mary |Brown |CA |Los Angles |NJ |Newark |+--------+------+--------+------------+-----------+------------+-----------+def flattenStructSchema(schema: StructType, prefix: String = null) : Array[Column] = &#123; schema.fields.flatMap(f =&gt; &#123; val columnName = if (prefix == null) f.name else (prefix + &quot;.&quot; + f.name) f.dataType match &#123; case st: StructType =&gt; flattenStructSchema(st, columnName) case _ =&gt; Array(col(columnName).as(columnName.replace(&quot;.&quot;,&quot;_&quot;))) &#125; &#125;) &#125; val df3 = df.select(flattenStructSchema(df.schema):_*)df3.printSchema()df3.show(false)+--------------+---------------+-------------+---------------------+--------------------+----------------------+---------------------+|name.firstname|name.middlename|name.lastname|address.current.state|address.current.city|address.previous.state|address.previous.city|+--------------+---------------+-------------+---------------------+--------------------+----------------------+---------------------+|James | |Smith |CA |Los Angles |CA |Sandiago ||Michael |Rose | |NY |New York |NJ |Newark ||Robert | |Williams |DE |Newark |CA |Las Vegas ||Maria |Anne |Jones |PA |Harrisburg |CA |Sandiago ||Jen |Mary |Brown |CA |Los Angles |NJ |Newark |+--------------+---------------+-------------+---------------------+--------------------+----------------------+---------------------+ 扁平化嵌套 Array: 上个示例展示了如何打平嵌套 Row，对于嵌套 Array 则可以通过 flatten() 方法除去嵌套数组第一层嵌套。 12345678910111213141516171819202122232425262728293031323334353637val arrayArrayData = Seq( Row(&quot;James&quot;,List(List(&quot;Java&quot;,&quot;Scala&quot;,&quot;C++&quot;),List(&quot;Spark&quot;,&quot;Java&quot;))), Row(&quot;Michael&quot;,List(List(&quot;Spark&quot;,&quot;Java&quot;,&quot;C++&quot;),List(&quot;Spark&quot;,&quot;Java&quot;))), Row(&quot;Robert&quot;,List(List(&quot;CSharp&quot;,&quot;VB&quot;),List(&quot;Spark&quot;,&quot;Python&quot;))) )val arrayArraySchema = new StructType().add(&quot;name&quot;,StringType) .add(&quot;subjects&quot;,ArrayType(ArrayType(StringType)))val df = spark.createDataFrame( spark.sparkContext.parallelize(arrayArrayData),arrayArraySchema)df.printSchema()df.show()root |-- name: string (nullable = true) |-- subjects: array (nullable = true) | |-- element: array (containsNull = true) | | |-- element: string (containsNull = true)+-------+-----------------------------------+|name |subjects |+-------+-----------------------------------+|James |[[Java, Scala, C++], [Spark, Java]]||Michael|[[Spark, Java, C++], [Spark, Java]]||Robert |[[CSharp, VB], [Spark, Python]] |+-------+-----------------------------------+df.select($&quot;name&quot;,flatten($&quot;subjects&quot;)).show(false)+-------+-------------------------------+|name |flatten(subjects) |+-------+-------------------------------+|James |[Java, Scala, C++, Spark, Java]||Michael|[Spark, Java, C++, Spark, Java]||Robert |[CSharp, VB, Spark, Python] |+-------+-------------------------------+ explode —— 行拆多行 功能：在处理 JSON，Parquet，Avro 和 XML 等结构化文件时，我们通常需要从数组、列表和字典等集合中获取数据。在这种情况下，explode 函数（explode，explorer_outer，posexplode，posexplode_outer）对于将集合列转换为行以便有效地在 Spark 中进行处理很有用。 语法： 示例： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150// 示例数据import spark.implicits._val arrayData = Seq( Row(&quot;James&quot;,List(&quot;Java&quot;,&quot;Scala&quot;),Map(&quot;hair&quot;-&gt;&quot;black&quot;,&quot;eye&quot;-&gt;&quot;brown&quot;)), Row(&quot;Michael&quot;,List(&quot;Spark&quot;,&quot;Java&quot;,null),Map(&quot;hair&quot;-&gt;&quot;brown&quot;,&quot;eye&quot;-&gt;null)), Row(&quot;Robert&quot;,List(&quot;CSharp&quot;,&quot;&quot;),Map(&quot;hair&quot;-&gt;&quot;red&quot;,&quot;eye&quot;-&gt;&quot;&quot;)), Row(&quot;Washington&quot;,null,null), Row(&quot;Jefferson&quot;,List(),Map()))val arraySchema = new StructType() .add(&quot;name&quot;,StringType) .add(&quot;knownLanguages&quot;, ArrayType(StringType)) .add(&quot;properties&quot;, MapType(StringType,StringType))val df = spark.createDataFrame(spark.sparkContext.parallelize(arrayData),arraySchema)df.printSchema()df.show(false)root |-- name: string (nullable = true) |-- knownLanguages: array (nullable = true) | |-- element: string (containsNull = true) |-- properties: map (nullable = true) | |-- key: string | |-- value: string (valueContainsNull = true)+----------+--------------+-----------------------------+|name |knownLanguages|properties |+----------+--------------+-----------------------------+|James |[Java, Scala] |[hair -&gt; black, eye -&gt; brown]||Michael |[Spark, Java,]|[hair -&gt; brown, eye -&gt;] ||Robert |[CSharp, ] |[hair -&gt; red, eye -&gt; ] ||Washington|null |null ||Jefferson |[] |[] |+----------+--------------+-----------------------------+// 将数组爆炸成行，爆炸后的列名默认为 &quot;col&quot;，如果数组为 null 或空则会被跳过，值为null则会返回 nulldf.select($&quot;name&quot;,explode($&quot;knownLanguages&quot;)).show(false)+-------+------+|name |col |+-------+------+|James |Java ||James |Scala ||Michael|Spark ||Michael|Java ||Michael|null ||Robert |CSharp||Robert | |+-------+------+// 将字典爆炸成行，爆炸后键列默认列名为 &quot;key&quot;，值列默认为 &quot;value&quot;df.select($&quot;name&quot;,explode($&quot;properties&quot;)).show(false)+-------+----+-----+|name |key |value|+-------+----+-----+|James |hair|black||James |eye |brown||Michael|hair|brown||Michael|eye |null ||Robert |hair|red ||Robert |eye | |+-------+----+-----+// explode_outer 遇到 null 或空的数组、字典将返回 nulldf.select($&quot;name&quot;,explode_outer($&quot;knownLanguages&quot;)).show(false)+----------+------+|name |col |+----------+------+|James |Java ||James |Scala ||Michael |Spark ||Michael |Java ||Michael |null ||Robert |CSharp||Robert | ||Washington|null ||Jeferson |null |+----------+------+df.select($&quot;name&quot;,explode_outer($&quot;properties&quot;)).show(false)+----------+----+-----+|name |key |value|+----------+----+-----+|James |hair|black||James |eye |brown||Michael |hair|brown||Michael |eye |null ||Robert |hair|red ||Robert |eye | ||Washington|null|null ||Jeferson |null|null |+----------+----+-----+// posexplode 会在 explode 基础上添加位置列 &quot;pos&quot;df.select($&quot;name&quot;,posexplode($&quot;knownLanguages&quot;)).show(false)+-------+---+------+|name |pos|col |+-------+---+------+|James |0 |Java ||James |1 |Scala ||Michael|0 |Spark ||Michael|1 |Java ||Michael|2 |null ||Robert |0 |CSharp||Robert |1 | |+-------+---+------+df.select($&quot;name&quot;,posexplode($&quot;properties&quot;)).show(false)+-------+---+----+-----+|name |pos|key |value|+-------+---+----+-----+|James |0 |hair|black||James |1 |eye |brown||Michael|0 |hair|brown||Michael|1 |eye |null ||Robert |0 |hair|red ||Robert |1 |eye | |+-------+---+----+-----+// posexplode_outer 会在 explode_outer 的基础上添加位置列 &quot;pos&quot;df.select($&quot;name&quot;,posexplode_outer($&quot;knownLanguages&quot;)).show(false)+----------+----+------+|name |pos |col |+----------+----+------+|James |0 |Java ||James |1 |Scala ||Michael |0 |Spark ||Michael |1 |Java ||Michael |2 |null ||Robert |0 |CSharp||Robert |1 | ||Washington|null|null ||Jeferson |null|null |+----------+----+------+df.select($&quot;name&quot;,posexplode_outer($&quot;properties&quot;)).show(false)+----------+----+----+-----+|name |pos |key |value|+----------+----+----+-----+|James |0 |hair|black||James |1 |eye |brown||Michael |0 |hair|brown||Michael |1 |eye |null ||Robert |0 |hair|red ||Robert |1 |eye | ||Washington|null|null|null ||Jeferson |null|null|null |+----------+----+----+-----+ pivot | stack —— 行转列 | 列转行 功能： pivot() 是一种聚合方法（类似于 Excel 中的数据透视表），用于将 DataFrame/Dataset 的行转列，该过程可以被分为三个步骤，① 按 x 列分组，x 的不同取值作为行向标签 ② 将 y 列的不同取值作为列向标签 ③ 将行列标签 (x,y) 对应 z 的聚合结果作为值，如果源表没有 (x,y) 对应的数据则补 null； stack() 方法可以将 DataFrame/Dataset 的列转行，注意 Spark 没有 unpivot 方法； 语法： 12groupBy(x).pivot(y).sum(z) // x 列不同值作为行标签，y 列不同值作为列标签，z 列的聚合作为值stack(n, expr1, ..., exprk) // 会将 expr1, ..., exprk 折叠为 n 行 示例： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152// 创建一个 DataFrameval data = Seq((&quot;Banana&quot;,1000,&quot;USA&quot;), (&quot;Carrots&quot;,1500,&quot;USA&quot;), (&quot;Beans&quot;,1600,&quot;USA&quot;), (&quot;Orange&quot;,2000,&quot;USA&quot;),(&quot;Orange&quot;,2000,&quot;USA&quot;),(&quot;Banana&quot;,400,&quot;China&quot;), (&quot;Carrots&quot;,1200,&quot;China&quot;),(&quot;Beans&quot;,1500,&quot;China&quot;),(&quot;Orange&quot;,4000,&quot;China&quot;), (&quot;Banana&quot;,2000,&quot;Canada&quot;),(&quot;Carrots&quot;,2000,&quot;Canada&quot;),(&quot;Beans&quot;,2000,&quot;Mexico&quot;))import spark.sqlContext.implicits._val df = data.toDF(&quot;Product&quot;,&quot;Amount&quot;,&quot;Country&quot;)df.show()+-------+------+-------+|Product|Amount|Country|+-------+------+-------+| Banana| 1000| USA||Carrots| 1500| USA|| Beans| 1600| USA|| Orange| 2000| USA|| Orange| 2000| USA|| Banana| 400| China||Carrots| 1200| China|| Beans| 1500| China|| Orange| 4000| China|| Banana| 2000| Canada||Carrots| 2000| Canada|| Beans| 2000| Mexico|+-------+-----+-------+// 行转列：不同 Product、不同 Country 下，Amount 的和val pivotDF = df.groupBy(&quot;Product&quot;).pivot(&quot;Country&quot;).sum(&quot;Amount&quot;)pivotDF.show()+-------+------+-----+------+----+|Product|Canada|China|Mexico| USA|+-------+------+-----+------+----+| Orange| null| 4000| null|4000|| Beans| null| 1500| 2000|1600|| Banana| 2000| 400| null|1000||Carrots| 2000| 1200| null|1500|+-------+------+-----+------+----+// pivot 是一个非常耗时的操作，Spark 2.0 以后的版本对 pivot 的性能进行了优化，如果使用的是更低的版本，可以通过传递一个列值参数来加速计算过程val pivotDF = df.groupBy(&quot;Product&quot;).pivot(&quot;Country&quot;, Seq(&quot;USA&quot;,&quot;China&quot;)).sum(&quot;Amount&quot;)pivotDF.show()+-------+----+-----+|Product| USA|China|+-------+----+-----+| Orange|4000| 4000|| Beans|1600| 1500|| Banana|1000| 400||Carrots|1500| 1200|+-------+----+-----+ 1234567891011121314151617// stack(n, 列1显示名, 列1, ..., 列n显示名, 列n)val unPivotDF = pivotDF.select($&quot;Product&quot;, expr(&quot;stack(2, &#x27;USA&#x27;, USA, &#x27;China&#x27;, China) as (Country,Total)&quot;)) .where(&quot;Total is not null&quot;)unPivotDF.show()+-------+-------+-----+|Product|Country|Total|+-------+-------+-----+| Orange| USA| 4000|| Orange| China| 4000|| Beans| USA| 1600|| Beans| China| 1500|| Banana| USA| 1000|| Banana| China| 400||Carrots| USA| 1500||Carrots| China| 1200|+-------+-------+-----+ 参考 《Spark 权威指南》_online/) Spark 2.2.x 中文文档 Spark By Examples org.apache.spark.sql.Dataset：Dataset 对象方法 org.apache.spark.sql.Dataset.Column：Column 对象方法]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark 指南：Spark SQL（一）—— 结构化对象]]></title>
    <url>%2FSpark%2FSpark%2FSpark%20%E6%8C%87%E5%8D%97%EF%BC%9ASpark%20SQL%EF%BC%88%E4%B8%80%EF%BC%89%E2%80%94%E2%80%94%20%E7%BB%93%E6%9E%84%E5%8C%96%E5%AF%B9%E8%B1%A1%2F</url>
    <content type="text"><![CDATA[SparkSession 是 Dataset 与 DataFrame API 的编程入口，从 Spark2.0 开始支持，用于统一原来的 HiveContext 和 SQLContext，统一入口提高了 Spark 的易用性，但为了兼容向后兼容，新版本仍然保留了这两个入口。下面的代码展示了如何创建一个 SparkSession： 1234567import org.apache.spark.sql.SparkSessionval spark = SparkSession .builder() .appName(&quot;Spark SQL basic example&quot;) .config(&quot;spark.some.config.option&quot;, &quot;some-value&quot;) .getOrCreate() DataFrame 仅仅只是 Dataset[Row] 的一个类型别名，创建 Dataset 的方式和创建 DataFrame 基本相同。 从内置方法创建spark.range 方法可以创建一个单列 DataFrame，其中列名为 id，列的类型为 LongType 类型，列中的值取 range 生成的值。 12345678910111213141516// 语法range(end: Long)range(start: Long, end: Long)range(start: Long, end: Long, step: Long)// 示例val ddf = spark.range(3) .withColumn(&quot;today&quot;, current_date()) .withColumn(&quot;now&quot;, current_timestamp())ddf.show(false)+---+----------+-----------------------+|id |today |now |+---+----------+-----------------------+|0 |2020-11-03|2020-11-03 21:05:26.657||1 |2020-11-03|2020-11-03 21:05:26.657||2 |2020-11-03|2020-11-03 21:05:26.657|+---+----------+-----------------------+ 从对象序列创建spark 提供了一系列隐式转换方法，可以将指定类型的对象序列 Seq[T] 或 RDD[T] 转化为 Dataset[T] 或 DataFrame，使用前需要先导入隐式转换： 12// spark 为入口 SparkSession 对象import spark.implicits._ toDF &amp; toDS如果 T 是 Int、Long、String 或 T &lt;: scala.Product(Tuple 或 case class) 类型中的一种，则可以通过 toDs() 或 toDf() 方法转化为 Dataset[T] 或 DataFrame。 toDF(): DataFrame 和 toDF(colNames: String*): DataFrame 方法提供了一种非常简洁的方式，将对象序列转化为一个 DataFrame； 列名：如果不提供 colNames，当结果只有一列时默认列名为 value，如果结果有多列 _1, _2,... 会作为默认列名； 类型：默认列类型将会通过输入数据的类型进行推断，如果要显式指定列的类型，可以通过 createDataFrame() 方法指定对应的 schema； 123456789101112131415161718192021222324252627282930313233343536373839404142434445// 序列元素为简单类型val seq = Seq(1,2,3)seq.toDF().show()+-----+|value|+-----+| 1|| 2|| 3|+-----+// 序列元素为元组val df = Seq( (&quot;Arya&quot;, &quot;Woman&quot;, 30), (&quot;Bob&quot;, &quot;Man&quot;, 28)).toDF(&quot;name&quot;, &quot;sex&quot;, &quot;age&quot;)df.show()+----+-----+---+|name| sex|age|+----+-----+---+|Arya|Woman| 30|| Bob| Man| 28|+----+-----+---+// 序列元素为样例类，通过反射读取样例类的参数名称，并映射成column的名称case class Person(name: String, age: Long)val df = Seq(Person(&quot;Andy&quot;, 32)).toDFdf.show()+----+---+|name|age|+----+---+|Andy| 32|+----+---+// 从 RDD 创建 DataFrame，parallelize 用于将序列转化为 RDDval rdd = spark.sparkContext.parallelize(List(1,2))val df = rdd.map(x=&gt;(x,x^2)).toDF(&quot;org&quot;,&quot;xor&quot;)df.show()+---+---+|org|xor|+---+---+| 1| 3|| 2| 0|+---+---+ toDS(): Dataset[T] 提供了一种将指定类型的对象序列转化为 DataSet 的简易方法 123456789101112131415161718192021222324252627282930313233343536373839404142// 序列元素为简单类型val ds = Seq(1,2,3).toDS()ds.show(false)+-----+|value|+-----+|1 ||2 ||3 |+-----+// 序列元素是元组val ds = Seq((&quot;Arya&quot;,20,&quot;woman&quot;), (&quot;Bob&quot;,28,&quot;man&quot;)).toDS()ds.show(false)+----+---+-----+|_1 |_2 |_3 |+----+---+-----+|Arya|20 |woman||Bob |28 |man |+----+---+-----+// 序列元素为样例类实例，样例类的字段会成为 DataSet 的字段// 注意，case class 的定义要在引用 case class函数的外面，否则即使 import spark.implicits._ 也还是会报错 value toDF is not a member of ***case class Person(name: String, age: Long, sex:String)val ds = Seq(Person(&quot;Arya&quot;, 20, &quot;woman&quot;), Person(&quot;Bob&quot;, 28, &quot;man&quot;)) .toDS().show()+----+---+-----+|name|age| sex|+----+---+-----+|Arya| 20|woman|| Bob| 28| man|+----+---+-----+ // 将 RDD 转化为 DataSetval rdd = spark.sparkContext.parallelize(Seq((&quot;Arya&quot;,20,&quot;woman&quot;), (&quot;Bob&quot;,28,&quot;man&quot;)))rdd.toDS().show()+----+---+-----+| _1| _2| _3|+----+---+-----+|Arya| 20|woman|| Bob| 28| man|+----+---+-----+ toDF 方法对 null 类型处理的不好，不建议在生产环境中使用。 createDataFrame &amp; createDataSet相比 toDF 和 toDS，createDataFrame 和 createDataSet 方法支持更多的数据类型，特别是 Seq[Row] 和 RDD[Row] 只能通过 create 方法来转化为 DataFrame。 createDataFrame 有多个重载方法：如果只传入数据，则数据只能是一个包含 Product 元素的序列或 RDD；如果传入 Schema，数据可以是 RDD[Row] 或 java.util.List[Row]；如果传入 beanClass，数据可以是 RDD[Java Bean] 或java.util.List[Java Bean] createDataFrame[A &lt;: Product : TypeTag](data: Seq[A]): DataFrame: 通过 Product 序列创建 DataFrame，如 tuple、case class createDataFrame[A &lt;: Product : TypeTag](rdd: RDD[A]): DataFrame: 通过 Product RDD 创建 DataFrame，如 tuple、case class createDataFrame(rows: List[Row], schema: StructType): DataFrame: 通过 java.util.List[Row] 并指定 Schema 创建 DataFrame createDataFrame(rowRDD: RDD[Row], schema: StructType): DataFrame: 通过 RDD[Row] 并指定 Schema 创建 DataFrame createDataFrame(rdd: RDD[_], beanClass: Class[_]): DataFrame: Applies a schema to an RDD of Java Beans createDataFrame(data: List[_], beanClass: Class[_]): DataFrame: Applies a schema to a List of Java Beans 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120// 只传入 Seq[Tuple]，列名为 &quot;_1&quot; &quot;_2&quot;val dfData = Seq((1,&quot;a&quot;), (2, &quot;b&quot;))val ds = spark.createDataFrame(dfData)ds.show()+---+---+| _1| _2|+---+---+| 1| a|| 2| b|+---+---+// 只传入 Seq[case class]，列名为样例类字段名case class Person(name:String, sex:String, age:Int)val dfData = Seq(Person(&quot;a&quot;, &quot;b&quot;, 1))val ds = spark.createDataFrame(dfData)ds.show()+----+---+---+|name|sex|age|+----+---+---+| a| b| 1|+----+---+---+// 只传入 RDD[Tuple]val dfData = spark.sparkContext.parallelize(Seq((1,&quot;a&quot;), (2, &quot;b&quot;)))val ds = spark.createDataFrame(dfData)ds.show()+---+---+| _1| _2|+---+---+| 1| a|| 2| b|+---+---+// 传入 schema，数据可以是 RDD[Row]import org.apache.spark.sql.Rowimport org.apache.spark.sql.types.&#123;StructType, StructField, StringType, IntegerType&#125;;val dfData = spark.sparkContext.parallelize( Seq( Row(&quot;Arya&quot;, &quot;Woman&quot;, 30), Row(&quot;Bob&quot;, &quot;Man&quot;, 28) ))val dfSchema = StructType( Seq( StructField(&quot;name&quot;, StringType, true), StructField(&quot;sex&quot;, StringType, true), StructField(&quot;age&quot;, IntegerType, true) ))val df = spark.createDataFrame(dfData, dfSchema)df.show()+----+-----+---+|name| sex|age|+----+-----+---+|Arya|Woman| 30|| Bob| Man| 28|+----+-----+---+// 传入 schema，数据可以是 java.util.List[Row]val dfData = new java.util.ArrayList[Row]()dfData.add(Row(&quot;Arya&quot;, &quot;Woman&quot;, 30))dfData.add(Row(&quot;Bob&quot;, &quot;Man&quot;, 28))val df = spark.createDataFrame(dfData, dfSchema)df.show()+----+-----+---+|name| sex|age|+----+-----+---+|Arya|Woman| 30|| Bob| Man| 28|+----+-----+---+// 构造复杂 Schema 时，使用实例化 StructType 对象的 add 方法更方便val data = Seq( Row(&quot;M&quot;, 3000, Row(&quot;James &quot;,&quot;&quot;,&quot;Smith&quot;), Seq(1,2), Map(&quot;1&quot;-&gt;&quot;a&quot;, &quot;11&quot;-&gt;&quot;aa&quot;)), Row(&quot;M&quot;, 4000, Row(&quot;Michael &quot;,&quot;Rose&quot;,&quot;&quot;), Seq(3,2), Map(&quot;2&quot;-&gt;&quot;b&quot;, &quot;22&quot;-&gt;&quot;bb&quot;)), Row(&quot;M&quot;, 4000, Row(&quot;Robert &quot;,&quot;&quot;,&quot;Williams&quot;), Seq(1,2), Map(&quot;3&quot;-&gt;&quot;c&quot;, &quot;33&quot;-&gt;&quot;cc&quot;)), Row(&quot;F&quot;, 4000, Row(&quot;Maria &quot;,&quot;Anne&quot;,&quot;Jones&quot;), Seq(3,3), Map(&quot;4&quot;-&gt;&quot;d&quot;, &quot;44&quot;-&gt;&quot;dd&quot;)), Row(&quot;F&quot;, -1, Row(&quot;Jen&quot;,&quot;Mary&quot;,&quot;Brown&quot;), Seq(5,2), Map(&quot;5&quot;-&gt;&quot;e&quot;)) )val schema = new StructType() .add(&quot;gender&quot;,StringType) .add(&quot;salary&quot;,IntegerType) .add(&quot;f_struct&quot;, new StructType() .add(&quot;firstname&quot;,StringType) .add(&quot;middlename&quot;,StringType) .add(&quot;lastname&quot;,StringType) ) .add(&quot;f_array&quot;, ArrayType(IntegerType)) .add(&quot;f_map&quot;, MapType(StringType, StringType))val df = spark.createDataFrame(spark.sparkContext.parallelize(data),schema)df.show()df.printSchema+------+------+--------------------+-------+------------------+|gender|salary| f_struct|f_array| f_map|+------+------+--------------------+-------+------------------+| M| 3000| [James , , Smith]| [1, 2]|[1 -&gt; a, 11 -&gt; aa]|| M| 4000| [Michael , Rose, ]| [3, 2]|[2 -&gt; b, 22 -&gt; bb]|| M| 4000|[Robert , , Willi...| [1, 2]|[3 -&gt; c, 33 -&gt; cc]|| F| 4000|[Maria , Anne, Jo...| [3, 3]|[4 -&gt; d, 44 -&gt; dd]|| F| -1| [Jen, Mary, Brown]| [5, 2]| [5 -&gt; e]|+------+------+--------------------+-------+------------------+root |-- gender: string (nullable = true) |-- salary: integer (nullable = true) |-- f_struct: struct (nullable = true) | |-- firstname: string (nullable = true) | |-- middlename: string (nullable = true) | |-- lastname: string (nullable = true) |-- f_array: array (nullable = true) | |-- element: integer (containsNull = true) |-- f_map: map (nullable = true) | |-- key: string | |-- value: string (valueContainsNull = true) createDataSet(x) 是 x.toDS() 的等价形式： 123456789101112131415161718192021222324252627282930313233343536373839// 序列元素为简单类型val ds = spark.createDataset(Seq(1,2,3))ds.show()+-----+|value|+-----+| 1|| 2|| 3|+-----+// 序列元素是元组val ds = spark.createDataset(Seq((&quot;Arya&quot;,20,&quot;woman&quot;), (&quot;Bob&quot;,28,&quot;man&quot;)))ds.show()+----+---+-----+| _1| _2| _3|+----+---+-----+|Arya| 20|woman|| Bob| 28| man|+----+---+-----+// 序列元素为样例类实例，样例类的字段会成为 DataSet 的字段case class Person(name: String, age: Long, sex:String)val ds = spark.createDataset(Seq(Person(&quot;Arya&quot;, 20, &quot;woman&quot;), Person(&quot;Bob&quot;, 28, &quot;man&quot;)))ds.show()+----+---+-----+|name|age| sex|+----+---+-----+|Arya| 20|woman|| Bob| 28| man|+----+---+-----+// 将 RDD 转化为 DataSetval ds = spark.createDataset(spark.sparkContext.parallelize(Seq((&quot;Arya&quot;,20,&quot;woman&quot;), (&quot;Bob&quot;,28,&quot;man&quot;))))ds.show()+----+---+-----+| _1| _2| _3|+----+---+-----+|Arya| 20|woman|| Bob| 28| man|+----+---+-----+ 从数据源加载Spark 有六个核心数据源和社区编写的数百个外部数据源（Cassandra、HBase、MongoDB、XML）： CSV JSON Parquet ORC JDBC/ODBC connections Plain-text files 纯文本文件 API 格式Read API读取数据源的通用 API 结构如下： 123456789DataFrameReader.format(...).option(&quot;key&quot;, &quot;value&quot;).schema(...).load(path)// 示例spark.read.format(&quot;csv&quot;) .option(&quot;mode&quot;, &quot;FAILFAST&quot;) .option(&quot;inferSchema&quot;, &quot;true&quot;) .option(&quot;path&quot;, &quot;path/to/file&quot;) .schema(someSchema) .load() 读取数据的基本要素： DataFrameReader 是 DataFrame 读取器，可以通过 SparkSession 的 read 属性来使用； format 是可选的，默认使用 Parquet 格式； option 允许设置键值配置，以参数化如何读取数据，也可以传入一个 Map； schema 如果数据源提供了 schema，或者你打算使用 schema 推断，则 schema 是可选的；每种格式都有一些必选项，我们将在讨论每种格式时进行详细讨论； Read modes 用于指定当 Spark 遇到格式错误的记录时如何处理： permissive ：默认值，遇到损坏的记录时，将所有损坏记录放在名为called_corrupt_record的字符串列中，将所有字段设置为 null； dropMalformed ：删除包含格式错误的行； failFast ：遇到格式错误的记录立即失败； Write API写入数据的通用 API 结构如下： 12345678DataFrameWriter.format(...).option(...).partitionBy(...).bucketBy(...).sortBy(...).save()// 示例df.write.format(&quot;csv&quot;) .option(&quot;mode&quot;, &quot;OVERWRITE&quot;) .option(&quot;dataFormat&quot;, &quot;yyyy-MM-dd&quot;) .option(&quot;path&quot;, &quot;path/to/file&quot;) .save(path) 数据写入的基本要素： DataFrameWriter 是 DataFrame 写入器，可以通过 DataFrame 的 write 属性来使用； format 是可选的，默认使用 Parquet 格式； option 允许设置键值配置，以参数化如何读取数据，也可以传入一个 Map；必须至少提供一个保存路径； Save modes 用于指定当 Spark 在指定位置找到数据将发生什么： apppend：将输出文件追加到该位置已存在的文件列表中； overwrite：将完全覆盖那里已经存在的任何数据； errorIfExists：默认值，如果指定位置已经存在数据或文件，则会引发错误并导致写入失败； ignore：如果该位置存在数据或文件，则不执行任何操作； CSVCSV 文件虽然看起来结构良好，但实际上是你将遇到的最棘手的文件格式之一，因为在生产方案中无法对其所包含的内容或结果进行很多假设，因此，CSV 读取器具有大量选项。 option 说明： 参数 解释 sep 默认是, 指定单个字符分割字段和值 encoding 默认是uft-8通过给定的编码类型进行解码 quote 默认是“，其中分隔符可以是值的一部分，设置用于转义带引号的值的单个字符。如果您想关闭引号，则需要设置一个空字符串，而不是null。 escape 默认(\)设置单个字符用于在引号里面转义引号 charToEscapeQuoteEscaping 默认是转义字符（上面的escape）或者\0，当转义字符和引号(quote)字符不同的时候，默认是转义字符(escape)，否则为\0 comment 默认是空值，设置用于跳过行的单个字符，以该字符开头。默认情况下，它是禁用的 header 默认是false，将第一行作为列名 enforceSchema 默认是true， 如果将其设置为true，则指定或推断的模式将强制应用于数据源文件，而CSV文件中的标头将被忽略。 如果选项设置为false，则在header选项设置为true的情况下，将针对CSV文件中的所有标题验证模式。模式中的字段名称和CSV标头中的列名称是根据它们的位置检查的，并考虑了*spark.sql.caseSensitive。虽然默认值为true，但是建议禁用 enforceSchema选项，以避免产生错误的结果 inferSchema inferSchema（默认为false`）：从数据自动推断输入模式。 *需要对数据进行一次额外的传递 samplingRatio 默认为1.0,定义用于模式推断的行的分数 ignoreLeadingWhiteSpace 默认为false,一个标志，指示是否应跳过正在读取的值中的前导空格 ignoreTrailingWhiteSpace 默认为false一个标志，指示是否应跳过正在读取的值的结尾空格 nullValue 默认是空的字符串,设置null值的字符串表示形式。从2.0.1开始，这适用于所有支持的类型，包括字符串类型 emptyValue 默认是空字符串,设置一个空值的字符串表示形式 nanValue 默认是Nan,设置非数字的字符串表示形式 positiveInf 默认是Inf negativeInf 默认是-Inf 设置负无穷值的字符串表示形式 dateFormat 默认是yyyy-MM-dd,设置指示日期格式的字符串。自定义日期格式遵循java.text.SimpleDateFormat中的格式。这适用于日期类型 timestampFormat 默认是yyyy-MM-dd’T’HH:mm:ss.SSSXXX，设置表示时间戳格式的字符串。自定义日期格式遵循java.text.SimpleDateFormat中的格式。这适用于时间戳记类型 maxColumns 默认是20480定义多少列数目的硬性设置 maxCharsPerColumn 默认是-1定义读取的任何给定值允许的最大字符数。默认情况下为-1，表示长度不受限制 mode 默认（允许）允许一种在解析过程中处理损坏记录的模式。它支持以下不区分大小写的模式。请注意，Spark尝试在列修剪下仅解析CSV中必需的列。因此，损坏的记录可以根据所需的字段集而有所不同。可以通过spark.sql.csv.parser.columnPruning.enabled（默认启用）来控制此行为。 columnNameOfCorruptRecord 默认值指定在spark.sql.columnNameOfCorruptRecord,允许重命名由PERMISSIVE模式创建的格式错误的新字段。这会覆盖spark.sql.columnNameOfCorruptRecord multiLine 默认是false,解析一条记录，该记录可能跨越多行 读取 CSV 示例： 12345678910111213141516171819202122232425262728val mySchema = new StructType( Array( new StructField(&quot;a&quot;, StringType, true), new StructField(&quot;b&quot;, IntegerType, true), new StructField(&quot;c&quot;, StringType, false) ))val df = spark.read.format(&quot;csv&quot;) .option(&quot;header&quot;, &quot;true&quot;) .option(&quot;mode&quot;, &quot;permissive&quot;) .schema(mySchema) .load(&quot;job.csv&quot;)df.show()df.printSchema+------+---+---+| a| b| c|+------+---+---+|caster| 0| 26|| like| 1| 30|| leo| 2| 30||rayray| 3| 27|+------+---+---+root |-- a: string (nullable = true) |-- b: integer (nullable = true) |-- c: string (nullable = true) 写入 CSV 示例：job2.csv 实际上是一个目录，其中包含很多文件，文件数对应分区数； 1234df.write.format(&quot;csv&quot;) .mode(&quot;overwrite&quot;) .option(&quot;seq&quot;, &quot;\t&quot;) .save(&quot;job2.csv&quot;) JSON在 Spark 中，当我们谈到 JSON 文件时，指的的是 line-delimited JSON 文件，这与每个文件具有较大 JSON 对象或数组的文件形成对比。line-delimited 和 multiline 由选项 multiLine 控制，当将此选项设置为 true 时，可以将整个文件作为一个 json 对象读取。line-delimited 的 JSON 实际上是一种更加稳定的格式，它允许你将具有新记录的文件追加到文件中，这也是建议你使用的格式。 option 说明： 属性名称 默认值 含义 primitivesAsString FALSE 将所有原始类型推断为字符串类型 prefersDecimal FALSE 将所有浮点类型推断为 decimal 类型，如果不适合，则推断为 double 类型 allowComments FALSE 忽略 JSON 记录中的 Java / C ++样式注释 allowUnquotedFieldNames FALSE 允许不带引号的 JSON 字段名称 allowSingleQuotes TRUE 除双引号外，还允许使用单引号 allowNumericLeadingZeros FALSE 允许数字前有零 allowBackslashEscapingAnyCharacter FALSE 允许反斜杠转义任何字符 allowUnquotedControlChars FALSE 允许JSON字符串包含不带引号的控制字符（值小于32的ASCII字符，包括制表符和换行符）或不包含。 mode PERMISSIVE PERMISSIVE：允许在解析过程中处理损坏记录； DROPMALFORMED：忽略整个损坏的记录；FAILFAST：遇到损坏的记录时抛出异常。 columnNameOfCorruptRecord columnNameOfCorruptRecord（默认值是spark.sql.columnNameOfCorruptRecord中指定的值）：允许重命名由PERMISSIVE 模式创建的新字段（存储格式错误的字符串）。这会覆盖spark.sql.columnNameOfCorruptRecord。 dateFormat dateFormat（默认yyyy-MM-dd）：设置表示日期格式的字符串。自定义日期格式遵循java.text.SimpleDateFormat中的格式。 timestampFormat timestampFormat（默认yyyy-MM-dd’T’HH：mm：ss.SSSXXX）：设置表示时间戳格式的字符串。 自定义日期格式遵循java.text.SimpleDateFormat中的格式。 multiLine FALSE 解析可能跨越多行的一条记录 读取 JSON 示例： 1234spark.read.format(&quot;json&quot;) .option(&quot;mode&quot;, &quot;FAILFAST&quot;) .schema(mySchema) .load(path) 写入 JSON 示例：同样每个分区将写入一个文件，而整个 DataFrame 将作为一个文件夹写入，每行将有一个 JSON 对象 1df.write.format(&quot;json&quot;).mode(&quot;overwrite&quot;).save(path) ParquetParquet 是 Spark 的默认文件格式（默认数据源可以通过 spark.sql.sources.default 进行设置），Parquet 是面向列的开源数据存储，可提供各种存储优化。它提供了列压缩，从而节省了存储空间，并允许读取单个列而不是整个文件。Parquet 支持复杂类型，如果你的列是 struct、array、map 类型，仍然可以正常读写该文件。 读取 Parquet 文件：Parquet 选项很少，因为它在存储数据时会强制执行自己的 Schema，你只需要设置格式就行了 1spark.read.format(&quot;parquet&quot;).load(path) 写入 Parquet 文件：只需要指定文件位置即可 123df.write.format(&quot;parquet&quot;) .mode(&quot;overwrite&quot;) .save(path) ORCORC 是一种专为 Hadoop workloads 设计的自我描述、有类型的列式文件格式。它针对大型数据流进行了优化，但是集成了对快速查找所需行的支持。ORC 实际上没有读取数据的选项，因为 Spark 非常了解这种文件格式，一个经常会被问到的问题是：ORC 和 Parquet 有什么区别？在大多数情况下，他们非常相似，根本的区别在于 Parquet 专门为 Spark 做了优化，而 ORC 专门为 Hive 做了优化。 读取 ORC 示例： 1spark.read.format(&quot;orc&quot;).load(path) 写入 ORC 示例： 1df.write.format(&quot;orc&quot;).mode(&quot;overwrite&quot;).save(path) Hive 数据源Spark SQL 还支持读取和写入存储在Apache Hive中的数据。但是，由于Hive具有大量依赖项，因此这些依赖项不包含在默认的Spark发布包中。如果可以在类路径上找到Hive依赖项，Spark将自动加载它们。请注意，这些Hive依赖项也必须存在于所有工作节点(worker nodes)上，因为它们需要访问Hive序列化和反序列化库（SerDes）才能访问存储在Hive中的数据。 在使用Hive时，必须实例化一个支持Hive的SparkSession，包括连接到持久性Hive Metastore，支持Hive 的序列化、反序列化（serdes）和Hive用户定义函数。没有部署Hive的用户仍可以启用Hive支持。如果未配置hive-site.xml，则上下文(context)会在当前目录中自动创建metastore_db，并且会创建一个由spark.sql.warehouse.dir配置的目录，其默认目录为spark-warehouse，位于启动Spark应用程序的当前目录中。请注意，自Spark 2.0.0以来，该在hive-site.xml中的hive.metastore.warehouse.dir属性已被标记过时(deprecated)。使用spark.sql.warehouse.dir用于指定warehouse中的默认位置。可能需要向启动Spark应用程序的用户授予写入的权限。 下面的案例为在本地运行(为了方便查看打印的结果)，运行结束之后会发现在项目的目录下 E:\IdeaProjects\myspark 创建了 spark-warehouse 和 metastore_db 的文件夹。可以看出没有部署Hive的用户仍可以启用Hive支持，同时也可以将代码打包，放在集群上运行。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123object SparkHiveExample &#123; case class Record(key: Int, value: String) def main(args: Array[String]) &#123; val spark = SparkSession .builder() .appName(&quot;Spark Hive Example&quot;) .config(&quot;spark.sql.warehouse.dir&quot;, &quot;e://warehouseLocation&quot;) .master(&quot;local&quot;)//设置为本地运行 .enableHiveSupport() .getOrCreate() Logger.getLogger(&quot;org.apache.spark&quot;).setLevel(Level.OFF) Logger.getLogger(&quot;org.apache.hadoop&quot;).setLevel(Level.OFF) import spark.implicits._ import spark.sql //使用Spark SQL 的语法创建Hive中的表 sql(&quot;CREATE TABLE IF NOT EXISTS src (key INT, value STRING) USING hive&quot;) sql(&quot;LOAD DATA LOCAL INPATH &#x27;file:///e:/kv1.txt&#x27; INTO TABLE src&quot;) // 使用HiveQL查询 sql(&quot;SELECT * FROM src&quot;).show() // +---+-------+ // |key| value| // +---+-------+ // |238|val_238| // | 86| val_86| // |311|val_311| // ... // 支持使用聚合函数 sql(&quot;SELECT COUNT(*) FROM src&quot;).show() // +--------+ // |count(1)| // +--------+ // | 500 | // +--------+ // SQL查询的结果是一个DataFrame，支持使用所有的常规的函数 val sqlDF = sql(&quot;SELECT key, value FROM src WHERE key &lt; 10 AND key &gt; 0 ORDER BY key&quot;) // DataFrames是Row类型的, 允许你按顺序访问列. val stringsDS = sqlDF.map &#123; case Row(key: Int, value: String) =&gt; s&quot;Key: $key, Value: $value&quot; &#125; stringsDS.show() // +--------------------+ // | value| // +--------------------+ // |Key: 0, Value: val_0| // |Key: 0, Value: val_0| // |Key: 0, Value: val_0| // ... //可以通过SparkSession使用DataFrame创建一个临时视图 val recordsDF = spark.createDataFrame((1 to 100).map(i =&gt; Record(i, s&quot;val_$i&quot;))) recordsDF.createOrReplaceTempView(&quot;records&quot;) //可以用DataFrame与Hive中的表进行join查询 sql(&quot;SELECT * FROM records r JOIN src s ON r.key = s.key&quot;).show() // +---+------+---+------+ // |key| value|key| value| // +---+------+---+------+ // | 2| val_2| 2| val_2| // | 4| val_4| 4| val_4| // | 5| val_5| 5| val_5| // ... //创建一个Parquet格式的hive托管表，使用的是HQL语法，没有使用Spark SQL的语法(&quot;USING hive&quot;) sql(&quot;CREATE TABLE IF NOT EXISTS hive_records(key int, value string) STORED AS PARQUET&quot;) //读取Hive中的表，转换成了DataFrame val df = spark.table(&quot;src&quot;) //将该DataFrame保存为Hive中的表，使用的模式(mode)为复写模式(Overwrite) //即如果保存的表已经存在，则会覆盖掉原来表中的内容 df.write.mode(SaveMode.Overwrite).saveAsTable(&quot;hive_records&quot;) // 查询表中的数据 sql(&quot;SELECT * FROM hive_records&quot;).show() // +---+-------+ // |key| value| // +---+-------+ // |238|val_238| // | 86| val_86| // |311|val_311| // ... // 设置Parquet数据文件路径 val dataDir = &quot;/tmp/parquet_data&quot; //spark.range(10)返回的是DataSet[Long] //将该DataSet直接写入parquet文件 spark.range(10).write.parquet(dataDir) // 在Hive中创建一个Parquet格式的外部表 sql(s&quot;CREATE EXTERNAL TABLE IF NOT EXISTS hive_ints(key int) STORED AS PARQUET LOCATION &#x27;$dataDir&#x27;&quot;) // 查询上面创建的表 sql(&quot;SELECT * FROM hive_ints&quot;).show() // +---+ // |key| // +---+ // | 0| // | 1| // | 2| // ... // 开启Hive动态分区 spark.sqlContext.setConf(&quot;hive.exec.dynamic.partition&quot;, &quot;true&quot;) spark.sqlContext.setConf(&quot;hive.exec.dynamic.partition.mode&quot;, &quot;nonstrict&quot;) // 使用DataFrame API创建Hive的分区表 df.write.partitionBy(&quot;key&quot;).format(&quot;hive&quot;).saveAsTable(&quot;hive_part_tbl&quot;) //分区键‘key’将会在最终的schema中被移除 sql(&quot;SELECT * FROM hive_part_tbl&quot;).show() // +-------+---+ // | value|key| // +-------+---+ // |val_238|238| // | val_86| 86| // |val_311|311| // ... spark.stop() &#125;&#125; JDBC 数据源Spark SQL 还包括一个可以使用 JDBC 从其他数据库读取数据的数据源。与使用 JdbcRDD 相比，应优先使用此功能。这是因为结果作为 DataFrame 返回，它们可以在 Spark SQL 中轻松处理或与其他数据源连接。JDBC 数据源也更易于使用 Java 或 Python，因为它不需要用户提供 ClassTag。 可以使用 Data Sources API 将远程数据库中的表加载为 DataFrame 或 Spark SQL 临时视图。用户可以在数据源选项中指定JDBC连接属性。user并且password通常作为用于登录数据源的连接属性提供。除连接属性外，Spark还支持以下不区分大小写的选项： 属性名称 含义 url 要连接的JDBC URL，可以再URL中指定特定于源的连接属性 dbtable 应该读取或写入的JDBC表 query 将数据读入Spark的查询语句 driver 用于连接到此URL的JDBC驱动程序的类名 numPartitions 表读取和写入中可用于并行的最大分区数，同时确定了最大并发的JDBC连接数 partitionColumn,lowerBound,upperBound 如果指定了任一选项，则必须指定全部选项。此外，还必须指定numPartitions。partitionColumn必须是表中的数字，日期或时间戳列。注意：lowerBound和upperBound（仅用于决定分区步幅，而不是用于过滤表中的行。因此，表中的所有行都将被分区并返回，这些选项仅用于读操作。） queryTimeout 超时时间（单位：秒），零意味着没有限制 fetchsize 用于确定每次往返要获取的行数（例如Oracle是10行），可以用于提升JDBC驱动程序的性能。此选项仅适用于读 batchsize JDBC批处理大小，默认 1000，用于确定每次往返要插入的行数。 这可以用于提升 JDBC 驱动程序的性能。此选项仅适用于写。 isolationLevel 事务隔离级别，适用于当前连接。它可以是 NONE，READ_COMMITTED，READ_UNCOMMITTED，REPEATABLE_READ 或 SERIALIZABLE 之一，对应于 JDBC的Connection 对象定义的标准事务隔离级别，默认值为 READ_UNCOMMITTED。此选项仅适用于写。 sessionInitStatement 在向远程数据库打开每个数据库会话之后，在开始读取数据之前，此选项将执行自定义SQL语句（或PL / SQL块）。 使用它来实现会话初始化，例如：option(“sessionInitStatement”, “”“BEGIN execute immediate ‘alter session set “_serial_direct_read”=true’; END;”””) truncate 当启用SaveMode.Overwrite时，此选项会导致 Spark 截断现有表，而不是删除并重新创建它。这样更高效，并且防止删除表元数据（例如，索引）。但是，在某些情况下，例如新数据具有不同的 schema 时，它将无法工作。此选项仅适用于写。 cascadeTruncate 如果JDBC数据库（目前为 PostgreSQL和Oracle）启用并支持，则此选项允许执行TRUNCATE TABLE t CASCADE（在PostgreSQL的情况下，仅执行TRUNCATE TABLE t CASCADE以防止无意中截断表）。这将影响其他表，因此应谨慎使用。此选项仅适用于写。 createTableOptions 此选项允许在创建表时设置特定于数据库的表和分区选项（例如，CREATE TABLE t (name string) ENGINE=InnoDB）。此选项仅适用于写。 createTableColumnTypes 创建表时要使用的数据库列数据类型而不是默认值。（例如：name CHAR（64），comments VARCHAR（1024））。指定的类型应该是有效的 spark sql 数据类型。 此选项仅适用于写。 customSchema 用于从JDBC连接器读取数据的自定义 schema。例如，id DECIMAL(38, 0), name STRING。您还可以指定部分字段，其他字段使用默认类型映射。 例如，id DECIMAL（38,0）。列名应与JDBC表的相应列名相同。用户可以指定Spark SQL的相应数据类型，而不是使用默认值。 此选项仅适用于读。 pushDownPredicate 用于 启用或禁用 谓词下推 到 JDBC数据源的选项。默认值为 true，在这种情况下，Spark会尽可能地将过滤器下推到JDBC数据源。否则，如果设置为 false，则不会将过滤器下推到JDBC数据源，此时所有过滤器都将由Spark处理。 读写 JDBC 示例： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354object JdbcDatasetExample &#123; def main(args: Array[String]): Unit = &#123; val spark = SparkSession .builder() .appName(&quot;JdbcDatasetExample&quot;) .master(&quot;local&quot;) //设置为本地运行 .getOrCreate() Logger.getLogger(&quot;org.apache.spark&quot;).setLevel(Level.OFF) Logger.getLogger(&quot;org.apache.hadoop&quot;).setLevel(Level.OFF) runJdbcDatasetExample(spark) &#125; private def runJdbcDatasetExample(spark: SparkSession): Unit = &#123; //注意：从JDBC源加载数据 val jdbcPersonDF = spark.read .format(&quot;jdbc&quot;) .option(&quot;url&quot;, &quot;jdbc:mysql://localhost/mydb&quot;) .option(&quot;dbtable&quot;, &quot;person&quot;) .option(&quot;user&quot;, &quot;root&quot;) .option(&quot;password&quot;, &quot;123qwe&quot;) .load() //打印jdbcDF的schema jdbcPersonDF.printSchema() //打印数据 jdbcPersonDF.show() val connectionProperties = new Properties() connectionProperties.put(&quot;user&quot;, &quot;root&quot;) connectionProperties.put(&quot;password&quot;, &quot;123qwe&quot;) //通过.jdbc的方式加载数据 val jdbcStudentDF = spark .read .jdbc(&quot;jdbc:mysql://localhost/mydb&quot;, &quot;student&quot;, connectionProperties) //打印jdbcDF的schema jdbcStudentDF.printSchema() //打印数据 jdbcStudentDF.show() // 保存数据到JDBC源 jdbcStudentDF.write .format(&quot;jdbc&quot;) .option(&quot;url&quot;, &quot;jdbc:mysql://localhost/mydb&quot;) .option(&quot;dbtable&quot;, &quot;student2&quot;) .option(&quot;user&quot;, &quot;root&quot;) .option(&quot;password&quot;, &quot;123qwe&quot;) .mode(SaveMode.Append) .save() jdbcStudentDF .write .mode(SaveMode.Append) .jdbc(&quot;jdbc:mysql://localhost/mydb&quot;, &quot;student2&quot;, connectionProperties) &#125;&#125; 参考 Spark DataSource Option 参数 《Spark 权威指南》_online/)]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark 指南：Spark SQL（〇）—— 结构化 API]]></title>
    <url>%2FSpark%2FSpark%2FSpark%20%E6%8C%87%E5%8D%97%EF%BC%9ASpark%20SQL%EF%BC%88%E3%80%87%EF%BC%89%E2%80%94%E2%80%94%20%E7%BB%93%E6%9E%84%E5%8C%96%20API%2F</url>
    <content type="text"><![CDATA[Spark SQL 是 Spark 用于处理结构化数据的一个模块，不同于 Spark RDD，Spark SQL 接口提供了更多关于数据的结构化信息，Spark SQL 会通过这些信息执行一些额外的优化操作。Spark SQL 提供了 SQL 和 DataSet 两种 API，二者底层使用的执行引擎相同，效率也一样，开发人员可以很容易地的在不同 API 之间切换，选择何种 API 要看哪种方式可以更自然地来表达给定的变换。 结构化 APISpark SQL API 可以在模块 org.apache.spark.sql下查看： 常用的 API 模块： Spark SQL 数据类型：org.apache.spark.sql.types Spark SQL 函数：org.apache.spark.sql.functions Spark SQL DataFrame:，Dataset 的大部分 API 同样适用于 DataFrame Spark SQL Column:org.apache.spark.sql.Column Spark SQL Row：org.apache.spark.sql.Row Spark SQL Window：org.apache.spark.sql.WindowSQLSpark SQL 的用法之一是执行 SQL 查询，它也可以从现有的 Hive 中读取数据，如果从其它编程语言内部运行 SQL，查询结果将作为一个 Dataset/DataFrame 返回。 表和视图与 DataFrame 基本相同，为我们只是针对它们执行 SQL 而不是 DataFrame 代码。 DataSetSpark 结构化 API 可以细分为两个 API：有类型的 Dataset 和无类型的 DataFrame。说 DataFrame 是无类型的并不准确，它们具有类型，但是 Spark 会完全维护它们，并且仅在运行时检查那些类型是否与模式中指定的类型一致。而 DataSet 在编译时检查类型是否符合规范，DataSet 仅适用于基于 Java 虚拟机（JVM）的语言（Scala 和 Java）。 Dataset 是一个分布式数据集，它是 Spark 1.6 版本中新增的一个接口, 它结合了 RDD（强类型，可以使用强大的 lambda 表达式函数） 和 Spark SQL 的优化执行引擎的好处。Dataset 可以从 JVM 对象构造得到，随后可以使用函数式的变换（map，flatMap，filter 等）进行操作。Dataset API 目前支持 Scala 和 Java 语言，还不支持 Python, 不过由于 Python 语言的动态性, Dataset API 的许多好处早就已经可用了，例如，你可以使用 row.columnName 来访问数据行的某个字段。 DataFrame 是按命名列方式组织的一个 Dataset。从概念上来讲，它等同于关系型数据库中的一张表或者 R 和 Python 中的一个 dataframe， 只不过在底层进行了更多的优化。DataFrame 可以从很多数据源构造得到，比如：结构化的数据文件，Hive 表，外部数据库或现有的 RDD。 DataFrame API 支持 Scala, Java, Python 以及 R 语言。在 Scala 和 Java 语言中, DataFrame 由 Row 的 Dataset 来 表示的。在 Scala API 中, DataFrame 仅仅只是 Dataset[Row] 的一个类型别名，而在 Java API 中, 开发人员需要使用 Dataset 来表示一个 DataFrame。 下图对比了 SQL、DataFrame 和 DataSet 三种 Spark SQL 编程方式错误检查机制： 对于 SQL 来说，编译的时候并不知道你写的对不对，只有到运行的时候才知道； 对于 DataFrame，语法错误可以在编译时发现（比如将 select 写错），但分析错误只有到运行时才能知道（比如将字段名写错）； 对于 DataSet，在编译阶段就可以发现语法和分析错误，即静态类型和运行时类型安全。 在大多数情况下，您可能会使用 DataFrame。对于 Scala-Spark，DataFrame 只是类型为 Row 的数据集，Row 类型是 Spark 内部优化表示的内部表示形式，这种格式可以进行高度专业化和高效的计算，而不是使用 JVM（可能导致高昂的垃圾处理和对象实例化成本）。对于 PySpark，一切都是 DataFrame。 DataFrame VS RDDDataFrame 和 RDD 都是可以并行处理的集合，但 DataFrame 更像是一个传统数据库里的表，除了数据之外还可以知道更多信息，比如列名、值、类型。从 API 角度来看 DataFrame 提供了更高级的 API，比 RDD 编程要方便很多，由于 R 语言和 Pandas 也有 DataFrame，这就降低了 Spark 的学习门槛，在编写 Spark 程序时根本不需要关心最后是运行在单机上还是分布式集群上，因为代码都是一样的。 假设 RDD 里面支持的是一个 Person 类型，那么每一条记录都相当于一个 Person，但是 Person 里面到底有什么我们并不知道。DataFrame 存储了各字段的列名、数据类型以及值，有了这些信息，Spark SQL 的查询优化器（Catalyst）在编译的时候就能够做更多的优化。 SQL、DataFrame 和 RDD 运行时性能对比：在大多数情况下 SQL 和 DataFrame 性能要好于 RDD 优化器 CatalystSpark SQL 的核心是 Catalyst 优化器，一种函数式的可扩展的查询优化器： 优化：Catalyst 使查询以更少的资源获取更快的效率； 函数式：Catalyst 基于 Scala 的模式匹配和 quasiquotes 机制； 可扩展：Catalyst 允许用户扩展优化器； Catalyst 优化策略Catalyst 支持两种优化策略： 基于规则的优化(Rule-Based Optimization, RBO)：使用一组规则来确定如何执行查询；RBO 是一种经验式、启发式优化思路，对于核心优化算子 join 有点力不从心，如两张表执行join 到底使用 BroadcaseHashJoin 还是 sortMergeJoin，目前 Spark SQL 是通过手工设定参数来确定的，如果一个表的数据量小于某个阈值（默认10M）就使用BroadcastHashJoin； 基于代价的优化(Cost-Based Optimization, CBO)：使用规则生成多个计划，然后选取代价最小的计划执行查询；不同 Physical Plans 输入到代价模型，调整 Join 顺序，减少中间Shuffle 数据集大小，达到最优输出； Catalyst 工作流程 无论是直接使用 SQL 语句还是使用 DataFrame，都会经过如下环节转换成 DAG 对 RDD 的操作： Parser：通过 ANTLR 将 Spark SQL 字符串解析为抽象语法树(Abstract Syntax Tree，AST)，即未解析的逻辑计划(Unresolved Logical Plan, ULP)； Analyzer：通过元数据信息 Catalog 将 ULP 解析为携带 Schema 信息的逻辑计划(Logical Plan, LP)； RBO：通过 RBO 对 Logical Plan 进行谓词下推、列值裁剪、常量累加等操作，得到优化后的逻辑计划(Optimized logical plan, OLP)； Planner：将 OLP 转换成多个物理计划(Physical Plan)； CBO：根据 Cost Model 算出每个 Physical Plan 的代价并选取代价最小的 Physical Plan 作为最终的 Physical Plan； WholeStageCodegen：生成 Java bytecode 然后在每一台机器上执行，形成 RDD graph/DAG； Parser 阶段Spark2.x SQL 语句的解析采用的是 ANTLR4，ANTLR4 根据语法文件 SqlBase.g4 自动解析生成两个Java类：词法解析器 SqlBaseLexer 和语法解析器 SqlBaseParser。使用这两个解析器将SQL字符串语句解析成了ANTLR4 的 ParseTree 语法树结构。然后在 parsePlan 过程中，使用 AstBuilder.scala 将 ParseTree 转换成catalyst 表达式逻辑计划 Unresolved Logical Plan，ULP。 Analyzer 阶段ULP 还只是一个语法树，系统需要通过元数据信息 Calalog 来获取表的 schema 信息（表名、列名、数据类型）和函数信息（类信息）。Analyzer 会再次遍历整个 AST，对树上的每个节点进行数据类型绑定以及函数绑定，比如people 词素会根据元数据表信息解析为包含 age、id 以及 name 三列的表，people.age会被解析为数据类型为 int 的变量，sum 会被解析为特定的聚合函数，解析后得到 Logical Plan，LP。 RBO 阶段RBO 的优化策略就是对语法树进行一次遍历，模式匹配能够满足特定规则的节点，再进行相应的等价转换，即将一棵树等价地转换为另一棵树，最终得到优化后的逻辑计划 Optimized logical plan, OLP。 SQL 中经典的常见优化规则有： 谓词下推（predicate pushdown）：将 Filter 算子尽可能下推，尽可能早地对数据源进行过滤，以减少参与计算的数据量（语法树是从下往上看的） 列值裁剪（column pruning）：剪裁不需要的字段，特别是嵌套里面的不需要字段。如只需people.age，不需要 people.address，那么可以将 address 字段丢弃 常量合并（constant folding）：从100+80优化为180，避免每一条 record 都需要执行一次100+80的操作 Planner 阶段OLP 只是逻辑上可行，实际上 spark 并不知道如何去执行这个OLP。一个逻辑计划（Logical Plan）经过一系列的策略（Strategy）处理之后，得到多个物理计划（Physical Plans），物理计划在 Spark 是由 SparkPlan 实现的。 CBO 阶段RBO 属于 LogicalPlan 的优化，所有优化均基于 LogicalPlan 本身的特点，未考虑数据本身的特点，也未考虑算子本身的代价。CBO 充分考虑了数据本身的特点（如大小、分布）以及操作算子的特点（中间结果集的分布及大小）及代价，从而更好的选择执行代价最小的物理执行计划，即 SparkPlan。 比如 join 算子，Spark 根据不同场景为该算子制定了不同的算法策略，有 broadcastHashJoin、shuffleHashJoin 以及 sortMergeJoin。CBO 中常见的优化是 join 换位，以便尽量减少中间shuffle 数据集大小，达到最优输出。 Code Generation 阶段选出的物理计划还是不能直接交给 Spark 执行，Spark 最后仍然会用一些 Rule 对 SparkPlan 进行处理： 全阶段代码生成（Whole-stage Code Generation）：用来将多个处理逻辑整合到单个代码模块中。通过引入全阶段代码生成，大大减少了虚函数的调用，减少了 CPU 的调用，使得 SQL 的执行速度有很大提升。 代码编译：生成代码之后需要解决的另一个问题是如何将生成的代码进行编译然后加载到同一个 JVM 中去，Spark 引入了 Janino 项目，参见 SPARK-7956。Janino 是一个超级小但又超级快的 Java™ 编译器. 它不仅能像 javac 工具那样将一组源文件编译成字节码文件，还可以对一些 Java 表达式，代码块，类中的文本(class body)或者内存中源文件进行编译，并把编译后的字节码直接加载到同一个 JVM 中运行。Janino 不是一个开发工具, 而是作为运行时的嵌入式编译器，比如作为表达式求值的翻译器或类似于 JSP 的服务端页面引擎，关于 Janino 的更多知识请参见这里。通过引入了 Janino 来编译生成的代码，结果显示 SQL 表达式的编译时间减少到 5ms。需要注意的是，代码生成是在 Driver 端进行的，而代码编译是在 Executor 端进行的。 参考 《Spark 权威指南》_online/)：正如书名所言，对Spark 各个方面做了权威的介绍，中文版现已出版，网上也有牛人博客的翻译 Spark 2.2.x 中文文档：官方文档中文版翻译，每一块内容都蜻蜓点水 Spark By Examples：通过实际例子学习 Spark 的绝佳去处 org.apache.spark.sql.Dataset：Dataset 对象方法 org.apache.spark.sql.Dataset.Column：Column 对象方法 Spark SQL Catalyst优化器 一条 SQL 在 Apache Spark 之旅（上） 一条 SQL 在 Apache Spark 之旅（中） 一条 SQL 在 Apache Spark 之旅（下） SparkSql的优化器-Catalyst Spark SQL / Catalyst 内部原理 与 RBO Spark SQL 性能优化再进一步 CBO 基于代价的优化 Spark SQL Optimization – Understanding the Catalyst Optimizer]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据科学：工具篇（一）—— Jupyter Lab 配置环境]]></title>
    <url>%2F%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%2F%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%2F%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%EF%BC%9A%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8%EF%BC%88%E4%B8%80%EF%BC%89%E2%80%94%E2%80%94%20Jupyter%20Lab%20%E9%85%8D%E7%BD%AE%E7%8E%AF%E5%A2%83%2F</url>
    <content type="text"><![CDATA[JupyterLab 是 Jupyter 团队为 Jupyter 项目开发的下一代基于 Web 的交互式开发环境。相对于 Jupyter Notebook，它的集成性更强、更灵活并且更易扩展。它支持 100 种多种语言，支持多种文档相互集成，实现了交互式计算的新工作流程。如果说 Jupyter Notebook 像是一个交互式的笔记本，那么 Jupyter Lab 更像是一个交互式的 VSCode。另外，JupyterLab 非常强大的一点是，你可以将它部署在云服务器，不管是电脑、平板还是手机，都只需一个浏览器，即可远程访问使用。使用 JupyterLab，你可以进行数据分析相关的工作，可以进行交互式编程，可以学习社区中丰富的 Notebook 资料。 本文只是提供一个 Jupyter lab 的基本配置思路和索引，Jupyter lab 还在快速发展，文中提到的很多内容可能已经不再适用了，大家在配置时不要拘泥于文中细节，还是要去官网上查看具体安装细节，否则可能导致版本兼容的各种问题 安装 Jupyter建议先安装 Anaconda，Anaconda 自带 Jupyter 和常用的科学计算包，且方便通过 conda 进行环境管理。为了不污染本地 Python 环境，建议单独为 Jupyter lab 创建一个虚拟环境（在 base 环境下可能遇到各种奇怪的错误）: 123456789101112131415161718192021222324252627282930313233# 创建虚拟环境，同时安装完整anaconda集合包（假设已经成功安装了 Anaconda）$ conda create -n mylab python=3.7 anaconda# 激活虚拟环境$ conda activate mylab# 查看 Jupyter 版本$ jupyter --versionjupyter core : 4.6.3jupyter-notebook : 6.0.3qtconsole : 4.7.5ipython : 7.16.1ipykernel : 5.3.2jupyter client : 6.1.6jupyter lab : 2.1.5nbconvert : 5.6.1ipywidgets : 7.5.1nbformat : 5.0.7traitlets : 4.3.3# 查看相关路径$ jupyter lab pathsApplication directory: /Users/likewang/opt/anaconda3/share/jupyter/labUser Settings directory: /Users/likewang/.jupyter/lab/user-settingsWorkspaces directory: /Users/likewang/.jupyter/lab/workspaces# 查看配置文件路径$ jupyter notebook --generate-configOverwrite /Users/likewang/.jupyter/jupyter_notebook_config.py with default config? [y/N]n# 修改配置文件，设置 jupyter 默认打开的目录$ vim .jupyter/jupyter_notebook_config.pyc.NotebookApp.notebook_dir = &#x27;/Users/likewang/ilab&#x27; 插件管理jupyter-lab 提供了两种方式来管理 Jupyter-lab 的插件： 命令行： 1234567891011121314151617# jupyter-lab 运行插件需要先安装 nodejs$ conda install nodejs# 查询安装的插件$ jupyter labextension list# 安装插件$ jupyter labextension install xxx# 删除插件$ jupyter labextension uninstall xxx# 更新所有插件（当插件版本过低或与当前jupyter版本不兼容的时候很好用）$ jupyter labextension update --all# 构建插件$ jupyter lab build 通过 juputer-lab 插件图形化管理：进入jupyter界面，点击插件图标，在搜索栏中搜索对应插件名，如jupytext，可直接管理对应的插件 安装插件时，通常需要先通过 pip/conda 安装相关依赖，再通过 jupyter labextension 来安装对应插件，部分插件在成功安装之后需要重启 jupyter-lab 才能生效。建议只安装必要的插件，插件过多会拖慢 jupyter-lab 的打开速度。 kite —— 代码补全kite 是一个功能非常强大的代码补全工具，目前可用于 Python 与 javascript，为许多知名的编辑器譬如 Vs Code、Pycharm 提供对应的插件，详细的安装过程可以参考Jupyter lab 最强代码补全插件。 安装安装 kite 的一般步骤： 下载安装 kite 客户端：安装后登陆 kite 客户端，并保持 kite 客户端开启； 配置 jupyter-lab：需要注意的是 kite 只支持 2.2.0 以上版本的jupyter lab，但是目前jupyter lab的最新正式版本为2.1.5，因此我们需要使用pip来安装其提前发行版本，这里我选择2.2.0a1； 12345678# 升级 jupyterlab 到 2.2.0$ pip install --pre jupyterlab==2.2.0a1# 安装 jupyter-kite 依赖$ pip install jupyter-kite# 安装 @kiteco/jupyterlab-kite 插件$ jupyter labextension install @kiteco/jupyterlab-kite 使用成功安装 kite 后，会自动跳转到 kite 使用说明文档 kite_tutorial.ipynb，这里简单介绍 kite 的几项核心功能： 自动补全：写代码的时候不需要按 健，也会弹出代码补全提示，可以在命令面板中通过 Kite: Toggle Docs Panel 来关闭或打开完整说明文档 手动补全：仍然可以继续使用 jupyter-lab 本身的 补全功能 实时文档：如果在 Kite 中打开了 Copilot，Copilot 会自动地根据光标在 Jupyter-lab 中的位置更新说明文档 jupyterlab_code_formatter —— 代码格式化jupyterlab_code_formatter 用于代码一键格式化。 安装1234567# 安装依赖$ conda install -c conda-forge jupyterlab_code_formatter$ jupyter labextension install @ryantam626/jupyterlab_code_formatter# 安装插件$ jupyter serverextension enable --py jupyterlab_code_formatter# 安装支持的代码格式$ conda install black isort 使用 jupyterlab-go-to-definition —— 代码跳转jupyterlab-go-to-definition 用于Lab笔记本和文件编辑器中跳转到变量或函数的定义 安装1234# JuupyterLab 2.x$ jupyter labextension install @krassowski/jupyterlab_go_to_definition # JupyterLab 1.x$ jupyter labextension install @krassowski/jupyterlab_go_to_definition@0.7.1 使用默认快捷键 alt+click： jupyterlab-git —— 版本管理jupyterlab-git 是 jupyter-lab 的 git 插件，可以方便地进行版本管理。 安装12$ conda install -c conda-forge jupyterlab jupyterlab-gitjupyter lab build 使用 qgrid —— DataFrame 交互qgrid 是一个可以用交互的方式操作 Pandas DataFrame 的插件，主要优点有： 直接用点选的方式进行选择、排序甚至是修改单元格中的值； 做 EDA 时可以看到整个 DataFrame 的全貌，而不是用 … 的方式来显示，而且读取速度很快； 安装123$ conda install qgrid$ jupyter labextension install @jupyter-widgets/jupyterlab-manager$ jupyter labextension install qgrid2 使用 以交互的方式显示 Pandas DataFrame：可以显示完整数据 1234567891011121314151617181920# 載入所需套件import qgridimport pandas as pdimport numpy as np# 為了讓結果相同，設定種子以及資料數量np.random.seed(1)nrow = 1000000# 建立 Dataframedf = pd.DataFrame(&#123;&#x27;Index&#x27;: range(nrow), &#x27;Sex&#x27;: np.random.choice([&#x27;M&#x27;, &#x27;F&#x27;], nrow), &#x27;Age&#x27;: np.random.randint(12, 56, nrow), &#x27;Height&#x27;: np.round(np.random.random(nrow),3)*30+160, &#x27;Weight&#x27;: np.round(np.random.random(nrow),3)*30+55, &#x27;Tag&#x27;: np.random.choice([True, False], nrow)&#125;)qgrid_widget = qgrid.show_grid(df, show_toolbar=True)qgrid_widget 在 DataFrame 上排序、筛选数据： 甚至可以直接更改 Dataframe 的值： 还可以获取改动过的数据：qgrid_widget.get_changed_df() 可以获取经过筛选、排序、修改后的 DataFrame 数据： 1qgrid_widget.get_changed_df() jupyter_bokeh —— 可视化效果jupyter_bokeh 该插件可以在 Lab 中展示bokeh 可视化效果。 安装123conda install -c bokeh jupyter_bokehjupyter labextension install @jupyter-widgets/jupyterlab-managerjupyter labextension install @bokeh/jupyter_bokeh 使用 jupyterlab-dash —— 单独面板jupyterlab-dash 该插件可以在Lab中展示 plotly dash 交互式面板。 安装12$ conda install -c plotly -c defaults -c conda-forge &quot;jupyterlab&gt;=1.0&quot; jupyterlab-dash=0.1.0a3$ jupyter labextension install jupyterlab-dash@0.1.0-alpha.3 使用 jupyterlab_variableinspector —— 变量显示jupyterlab_variableinspector 可以在 Lab 中展示代码中的变量及其属性，类似RStudio中的变量检查器。你可以一边撸代码，一边看有哪些变量。对 Spark 和 Tensorflow 的支持需要解决依赖。 安装1$ jupyter labextension install @lckr/jupyterlab_variableinspector 使用 jupyterlab-system-monitor —— 资源监控jupyterlab-system-monitor 用于监控 jupyter-lab 的资源使用情况。 安装12$ conda install -c conda-forge nbresuse$ jupyter labextension install jupyterlab-topbar-extension jupyterlab-system-monitor 使用默认只显示内存使用情况： 编辑配置文件 ~/.jupyter/jupyter_notebook_config.py：添加一下内容，重启 jupyter-lab 就可以显示 CPU 利用率以及内存使用情况了。 12345678c = get_config()# memoryc.NotebookApp.ResourceUseDisplay.mem_limit = &lt;size_in_GB&gt; *1024*1024*1024# cpuc.NotebookApp.ResourceUseDisplay.track_cpu_percent = Truec.NotebookApp.ResourceUseDisplay.cpu_limit = &lt;number_of_cpus&gt; 示例： 1234# 示例：限制最大内存 4G，2 个 CPU，显示 CPU 利用率c.NotebookApp.ResourceUseDisplay.mem_limit = 4294967296c.NotebookApp.ResourceUseDisplay.track_cpu_percent = Truec.NotebookApp.ResourceUseDisplay.cpu_limit = 2 jupyterlab-toc —— 显示目录jupyterlab-toc 用于在 jupyter-lab 中显示文档的目录。 安装1$ jupyter labextension install @jupyterlab/toc 使用 Collapsible_Headings —— 折叠标题Collapsible_Headings 可实现标题的折叠。 安装1$ jupyter labextension install @aquirdturtle/collapsible_headings 使用 jupyterlab_html —— 显示 HTML该插件允许你在Jupyter Lab内部呈现HTML文件，这在打开例如d3可视化效果时非常有用 安装1$ jupyter labextension install @mflevine/jupyterlab_html 使用 jupyterlab-drawio —— 绘制流程图jupyterlab-drawio 可以在Lab中启用 drawio 绘图工具，drawio是一款非常棒的流程图工具。 安装1$ jupyter labextension install jupyterlab-drawio 使用 jupyterlab-tabular-data-editor —— CSV 编辑jupyterlab-tabular-data-editor 插件赋予我们高度的交互式操纵 csv 文件的自由，无需excel，就可以实现对csv表格数据的增删改查。 安装1$ jupyter labextension install jupyterlab-tabular-data-editor 使用 jupyter-themes —— 切换主题jupyterlab-themes 用于切换 jupyter 的主题。 安装12# 目前还只能一个一个安装$ jupyter labextension install @arbennett/base16-&#123;$themename&#125; 使用 更多插件更多插件可以参考以下网站： Awesome JupyterLab 15个好用到爆炸的Jupyter Lab插件 kernel 管理Jupyter kernel 可以用任何语言实现，只要它们遵循基于 ZeroMQ 的 Jupyter 通信协议。IPython 是最流行的内核，默认情况下包括在内。这并不奇怪，因为 Jupyter（Jupyter，Jupyter，Python，R）来自IPython项目。它是将独立于语言的部分从IPython内核中分离出来，使其能够与其他语言一起工作的结果，现在有超过100种编程语言的内核可用。 除了内核和前端之外，Jupyter 还包括与语言无关的后端部分，它管理内核、笔记本和与前端的通信。这个组件称为Jupyter服务器。笔记本存储在.ipynb文件中，在服务器上以Json格式编码。基于Json的格式允许以结构化的方式存储单元输入、输出和元数据。二进制输出数据采用base64编码。缺点是，与基于行的文本格式相比，json使diff和merge更困难。您可以将笔记本导出为其他格式，如Markdown、Scala（仅包含代码输入单元格）或类似本文的HTML。 1234# 查看 kernel 列表jupyter kernelspec list# 卸载指定 kernel jupyter kernelspec remove kernel_name 安装 Scala kernel在Scala中对Jupyter的支持是怎样的？实际上有很多不同的内核。但是，如果仔细观察，它们中的许多在功能上有一定的局限性，存在可伸缩性问题，甚至已经被放弃。其他人只关注Spark而不是Scala和其他框架。 其中一个原因是，几乎所有现有内核都构建在默认REPL之上。由于其局限性，他们对其进行定制和扩展，以添加自己的特性，如依赖关系管理或框架支持。一些内核还使用sparkshell，它基本上是scalarepl的一个分支，专门为Spark支持而定制。这一切都会导致碎片化、重用困难和重复工作，使得创建一个与其他语言相当的内核变得更加困难。 关于一些原因的更详细讨论，请查看 Alexandre Archambault 在2017年 JupyterCon 上的演讲 Scala: Why hasn’t an Official Scala Kernel for Jupyter emerged yet?。 almond（推荐） almond（之前叫jupyter-scala） 使得 jupyter 强大的功能向 Scala 开放，包括 Ammonite 的所有细节，尽管它还需要一些更多的集成和文档，但是它已经非常有用，并且非常有趣。——Interactive Computing in Scala with Jupyter and almond 安装 almond 需要特别注意 almond 版本、Scala 版本以及 Spark版本之间的兼容性（almond 0.10.0 支持 scala 2.12.11 and 2.13.2 支持 park 2.4.x），almond 详细安装过程及版本对应关系请参考 almond 官方文档。 123456789101112131415161718192021222324252627# 查看可用的 Scala 版本$ brew search scala==&gt; Formulaescala scala@2.11 scala@2.12 scalaenv scalapack scalariform scalastyle# 安装 scala 2.12.x$ brew install scala@2.12# 查看实际安装的 scala 版本$ scala -versionScala code runner version 2.12.11 -- Copyright 2002-2020, LAMP/EPFL and Lightbend, Inc.# 安装 coursier，scala 的依赖解析器$ brew install coursier/formulas/coursier# 通过 coursier 安装 almond，指定 almond 版本=0.10.0，scala版本=2.12.11，重复安装需要加--force$ coursier launch --fork almond:0.10.0 --scala 2.12.11 -- --install --force# 成功安装后，可以看到 jupyter kernelspec 多了一个 Scala 的核$ jupyter kernelspec listAvailable kernels: scala /Users/likewang/Library/Jupyter/kernels/scala python3 /Users/likewang/opt/anaconda3/envs/mylab/share/jupyter/kernels/python3 python2 /usr/local/share/jupyter/kernels/python2 spylon-kernel /usr/local/share/jupyter/kernels/spylon-kernel# 安装 Spark 依赖 配置 Spark： 123456789101112131415161718# Or use any other 2.x version hereimport $ivy.`org.apache.spark::spark-sql:2.4.0`# Not required since almond 0.7.0 (will be automatically added when importing spark)import $ivy.`sh.almond::almond-spark:0.10.9` # 通常，为了避免污染单元输出，您需要禁用日志记录import org.apache.log4j.&#123;Level, Logger&#125;Logger.getLogger(&quot;org&quot;).setLevel(Level.OFF)# 引入 NotebookSparkSessionimport org.apache.spark.sql._val spark = &#123; NotebookSparkSession.builder() .master(&quot;local[*]&quot;) .getOrCreate()&#125;# 引入隐式转换import spark.implicits._ 其他 Scala kernel spylon-kernel 是一个 Scala Jupyter Kernel。 jupyter-scala 依赖于 scala 2.11.x，还不支持 2.12；jupyter-scala 只能用于 jupyter notebook 无法用于 jupyter lab： 安装 python kernel由于我们是在 python 3 虚拟环境下安装了 jupyter lab，自带的是 python 3 kernel，现在需要添加 python 2 的 kernel： 1234# 假设已经安装了名为 python2 的虚拟环境，切换到 python 2 环境$ conda activate python2# 安装 python 2 kernel $ python2 -m ipykernel install --name python2 安装成功后，在 jupyter lab 新建文件页面会出现 python 2 的图标： 各种奇怪的问题插件同时出现在已安装和未安装列表中 问题描述：在uninstall 插件后，插件同时出现在 Known labextensions 和 Uninstalled core extensions。 123456789101112131415(base) ➜ ~ jupyter labextension listJupyterLab v1.2.6Known labextensions: app dir: /Users/likewang/opt/anaconda3/share/jupyter/lab @bokeh/jupyter_bokeh v1.2.0 enabled OK @jupyterlab/github v1.0.1 enabled OK @jupyterlab/toc v2.0.0 enabled OK @mflevine/jupyterlab_html v0.1.4 enabled OK @pyviz/jupyterlab_pyviz v0.8.0 enabled OK @ryantam626/jupyterlab_code_formatter v1.1.0 enabled OK jupyterlab-dash v0.1.0-alpha.3 enabled OK jupyterlab-drawio v0.6.0 enabled OKUninstalled core extensions: @ryantam626/jupyterlab_code_formatter 解决办法：删除 jupyter\lab\settings\build_config.json，https://github.com/jupyterlab/jupyterlab/issues/8122 12345678910111213141516(base) ➜ ~ jupyter lab build[LabBuildApp] JupyterLab 1.2.6[LabBuildApp] Building in /Users/likewang/opt/anaconda3/share/jupyter/lab[LabBuildApp] Building jupyterlab assets (build:prod:minimize)(base) ➜ ~ jupyter labextension listJupyterLab v1.2.6Known labextensions: app dir: /Users/likewang/opt/anaconda3/share/jupyter/lab @bokeh/jupyter_bokeh v1.2.0 enabled OK @jupyterlab/github v1.0.1 enabled OK @jupyterlab/toc v2.0.0 enabled OK @mflevine/jupyterlab_html v0.1.4 enabled OK @pyviz/jupyterlab_pyviz v0.8.0 enabled OK @ryantam626/jupyterlab_code_formatter v1.1.0 enabled OK jupyterlab-dash v0.1.0-alpha.3 enabled OK jupyterlab-drawio v0.6.0 enabled OK No module named ‘jupyter_nbextensions_configurator’ 问题描述：启动 jupyter-lab 时报错 ModuleNotFoundError: No module named &#39;jupyter_nbextensions_configurator&#39; 1234567891011(mylab) ➜ ilab jupyter-lab[W 19:37:20.086 LabApp] Error loading server extension jupyter_nbextensions_configurator Traceback (most recent call last): File &quot;/Users/likewang/opt/anaconda3/envs/mylab/lib/python3.7/site-packages/notebook/notebookapp.py&quot;, line 1670, in init_server_extensions mod = importlib.import_module(modulename) File &quot;/Users/likewang/opt/anaconda3/envs/mylab/lib/python3.7/importlib/__init__.py&quot;, line 127, in import_module return _bootstrap._gcd_import(name[level:], package, level) File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1006, in _gcd_import File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 983, in _find_and_load File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 965, in _find_and_load_unlocked ModuleNotFoundError: No module named &#x27;jupyter_nbextensions_configurator&#x27; 解决办法：以上问题出现在虚拟环境中启动 jupyter-lab，jupyter-nbextensions_configurator 和 python pip 不在同一个环境，解决办法是在对应的虚拟环境中安装 jupyter_nbextensions_configurator 123456789101112(mylab) ➜ ~ which jupyter-nbextensions_configurator/Users/likewang/opt/anaconda3/bin/jupyter-nbextensions_configurator(mylab) ➜ ~ which python/Users/likewang/opt/anaconda3/envs/mylab/bin/python(mylab) ➜ ~ which jupyter/Users/likewang/opt/anaconda3/envs/mylab/bin/jupyter(mylab) ➜ ~ which jupyter-notebook/Users/likewang/opt/anaconda3/envs/mylab/bin/jupyter-notebook# 重新在虚拟环境安装 jupyter_nbextensions_configuratorconda install -c conda-forge jupyter_nbextensions_configurator(mylab) ➜ .jupyter which jupyter-nbextensions_configurator/Users/likewang/opt/anaconda3/envs/mylab/bin/jupyter-nbextensions_configurator 参考 Getting the most out of Jupyter Lab]]></content>
      <categories>
        <category>数据科学</category>
      </categories>
      <tags>
        <tag>数据科学</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scala 教程：Basics（二）—— 核心类型]]></title>
    <url>%2FScala%2FScala%2FScala%20%E6%95%99%E7%A8%8B%EF%BC%9ABasics%EF%BC%88%E4%BA%8C%EF%BC%89%E2%80%94%E2%80%94%20%E6%A0%B8%E5%BF%83%E7%B1%BB%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[Scala的核心类型，包括String，以及数值类型 Byte、Short、Int、Long、Float、Double、Char 和 Boolean。 数值类型Byte、Short、Int、Long和Char类型统称整数类型，加上Float和Double称作数值类型。 以上列出的基本类型除了Java.lang.String外都是scala包的成员，Int的完整名称是scala.Int，不过scala包的所有成员在scala源文件中都已经自动引入，可以在任何地方使用简单名称。 以上列出的所有基础类型都可以使用字面值(literal)来书写，下图是指定字面值类型的记法： 示例： 12345678910111213141516scala&gt; val f = 1.234f: Double = 1.234scala&gt; val ff = 1.234fff: Float = 1.234scala&gt; val fff: Float = 1.234&lt;console&gt;:11: error: type mismatch; found : Double(1.234) required: Float val fff: Float = 1.234 ^scala&gt; val fff: Float = 1.234ffff: Float = 1.234 整数类型一些常见的整数字面值： 1234567891011121314151617181920212223242526// 如果整数以非0开头，默认被视为十进制数scala&gt; val a = 10a: Int = 10// 十六进制数以0x开头，shell默认打印其十进制整数值scala&gt; val b = 0xFb: Int = 15// 将Int类型整数赋值给Long型，发生隐式类型转化scala&gt; val long: Long = 10long: Long = 10// 也可以在字面值末尾加上`l`或`L`，指明整数位Long型scala&gt; val c = 35lc: Long = 35// 将 Int 型赋值给Short或Byte，如果在范围内就会自动转化，否则报错scala&gt; val d: Short = 3d: Short = 3scala&gt; val e: Byte = 129&lt;console&gt;:11: error: type mismatch; found : Int(129) required: Byte val e: Byte = 129 ^scala&gt; val e: Byte = 127e: Byte = 127 浮点类型浮点数以十进制数字+可选的小数点+可选的E或e打头的指数组成： 1234567891011121314151617181920// 浮点数字面值默认为Double型scala&gt; val double = 3.14double: Double = 3.14scala&gt; val float = 3.14float: Double = 3.14// 如需使用Float类型字面值，必须在数字后面加上f或Fscala&gt; val float = 3.14ffloat: Float = 3.14scala&gt; val float: Float = 3.14&lt;console&gt;:11: error: type mismatch; found : Double(3.14) required: Float val float: Float = 3.14 ^// e前面部分 ✖️ 10的后面部分次幂scala&gt; val e = 3.14e2e: Double = 314.0 字符类型 原字符表示法：使用一对单引号和中间的任意单个Unicode字符组成 1234567scala&gt; val c = &#x27;we&#x27;&lt;console&gt;:1: error: unclosed character literal (or use &quot; for string literal &quot;we&quot;) val c = &#x27;we&#x27; ^scala&gt; val c = &#x27;w&#x27;c: Char = w Unicode字符表示法：\u加上字符对应的四位十六进制数字，Unicode字符可以出现在Scala程序的任何位置 123456// 出现在字面值字符中scala&gt; val d = &#x27;\u0041&#x27;d: Char = A// 出现在变量中scala&gt; val d\u0041 = &#x27;x&#x27;dA: Char = x 转义字符： String类型Scala 本身没有 String 类，字符串的类型实际上是 java.lang.String，String 是一个不可变对象，对字符串的修改会生成一个新的字符串对象。 String字面值 普通字符串字面值：普通字符串字面值由用双引号括起来的字符组成，普通字符串中的\会被解析为转义符： 12345scala&gt; val c1 = &quot;hello world&quot;c1: String = hello worldscala&gt; val c2 = &quot;\\\&quot;\&#x27;&quot;c2: String = \&quot;&#x27; 原生字符串字面值：原生字符串由三重引号括起来的字符组成，原生字符串中每个字符都会被当做该字符本身进行原样输出： 12345678910111213141516171819202122// 转义符会被当做普通字符scala&gt; val c4 = &quot;&quot;&quot;\\\&quot;\&#x27;&quot;&quot;&quot;c4: String = \\\&quot;\&#x27;// 空格 换行都会被原样输出scala&gt; val mutiLine = &quot;&quot;&quot;hello \t world | \n nihao | china&quot;&quot;&quot;mutiLine: String =hello \t world\n nihaochina// 管道符 `|` 的作用是标识每一行字符串字面值的开始位置：scala&gt; println(&quot;&quot;&quot;welcome to china | you are great&quot;&quot;&quot;)welcome to china you are greatscala&gt; println(&quot;&quot;&quot;welcome to china | |you are great&quot;&quot;&quot;)welcome to chinayou are great 字符串插值Scala默认提供了三种插值器来实现在字符串字面值中嵌入表达式，你也可以定义自己的插值器来满足不同的需求。 s插值器： 语法： s&quot;$&#123;expression&#125;&quot; 解析：定位表达式 -&gt; 表达式求值 -&gt; 对值调用toString方法 raw插值器： 语法：raw&quot;$&#123;expression&#125;&quot; 解析：和s插值器相似，但是会把其他字符作为原义字符对待 f插值器： 语法：f&quot;$&#123;expression&#125;%.2f&quot; 解析：和s插值器相似，多个格式化输出 1234567891011121314151617scala&gt; val x = 314x: Int = 314scala&gt; &quot;hell &quot; + xres22: String = hell 314scala&gt; s&quot;hell $x world $&#123;math.Pi&#125;&quot;res23: String = hell 314 world 3.141592653589793scala&gt; s&quot;hello\t$x world $&#123;math.Pi&#125;&quot;res24: String = hello 314 world 3.141592653589793scala&gt; raw&quot;hello\t$x world $&#123;math.Pi&#125;&quot;res26: String = hello\t314 world 3.141592653589793scala&gt; f&quot;hello\t$x world $&#123;math.Pi&#125;%.2f&quot;res27: String = hello 314 world 3.14 字符串的常用方法下表列出了 java.lang.String 中常用的方法，你可以在 Scala 中使用： 序号 方法 描述 1 char charAt(int index) 返回指定位置的字符 2 int compareTo(Object o) 比较字符串与对象 3 int compareTo(String anotherString) 按字典顺序比较两个字符串 4 int compareToIgnoreCase(String str) 按字典顺序比较两个字符串，不考虑大小写 5 String concat(String str) 将指定字符串连接到此字符串的结尾，等价于 + 6 boolean contentEquals(StringBuffer sb) 将此字符串与指定的 StringBuffer 比较。 7 static String copyValueOf(char[] data) 返回指定数组中表示该字符序列的 String 8 static String copyValueOf(char[] data, int offset, int count) 返回指定数组中表示该字符序列的 String 9 boolean endsWith(String suffix) 测试此字符串是否以指定的后缀结束 10 boolean equals(Object anObject) 将此字符串与指定的对象比较 11 boolean equalsIgnoreCase(String anotherString) 将此 String 与另一个 String 比较，不考虑大小写 12 byte getBytes() 使用平台的默认字符集将此 String 编码为 byte 序列，并将结果存储到一个新的 byte 数组中 13 byte[] getBytes(String charsetName 使用指定的字符集将此 String 编码为 byte 序列，并将结果存储到一个新的 byte 数组中 14 void getChars(int srcBegin, int srcEnd, char[] dst, int dstBegin) 将字符从此字符串复制到目标字符数组 15 int hashCode() 返回此字符串的哈希码 16 int indexOf(int ch) 返回指定字符在此字符串中第一次出现处的索引 17 int indexOf(int ch, int fromIndex) 返回在此字符串中第一次出现指定字符处的索引，从指定的索引开始搜索 18 int indexOf(String str) 返回指定子字符串在此字符串中第一次出现处的索引 19 int indexOf(String str, int fromIndex) 返回指定子字符串在此字符串中第一次出现处的索引，从指定的索引开始 20 String intern() 返回字符串对象的规范化表示形式 21 int lastIndexOf(int ch) 返回指定字符在此字符串中最后一次出现处的索引 22 int lastIndexOf(int ch, int fromIndex) 返回指定字符在此字符串中最后一次出现处的索引，从指定的索引处开始进行反向搜索 23 int lastIndexOf(String str) 返回指定子字符串在此字符串中最右边出现处的索引 24 int lastIndexOf(String str, int fromIndex) 返回指定子字符串在此字符串中最后一次出现处的索引，从指定的索引开始反向搜索 25 int length() 返回此字符串的长度 26 boolean matches(String regex) 告知此字符串是否匹配给定的正则表达式 27 boolean regionMatches(boolean ignoreCase, int toffset, String other, int ooffset, int len) 测试两个字符串区域是否相等 28 boolean regionMatches(int toffset, String other, int ooffset, int len) 测试两个字符串区域是否相等 29 String replace(char oldChar, char newChar) 返回一个新的字符串，它是通过用 newChar 替换此字符串中出现的所有 oldChar 得到的 30 String replaceAll(String regex, String replacement 使用给定的 replacement 替换此字符串所有匹配给定的正则表达式的子字符串 31 String replaceFirst(String regex, String replacement) 使用给定的 replacement 替换此字符串匹配给定的正则表达式的第一个子字符串 32 String[] split(String regex) 根据给定正则表达式的匹配拆分此字符串 33 String[] split(String regex, int limit) 根据匹配给定的正则表达式来拆分此字符串 34 boolean startsWith(String prefix) 测试此字符串是否以指定的前缀开始 35 boolean startsWith(String prefix, int toffset) 测试此字符串从指定索引开始的子字符串是否以指定前缀开始。 36 CharSequence subSequence(int beginIndex, int endIndex) 返回一个新的字符序列，它是此序列的一个子序列 37 String substring(int beginIndex) 返回一个新的字符串，它是此字符串的一个子字符串 38 String substring(int beginIndex, int endIndex) 返回一个新字符串，它是此字符串的一个子字符串 39 char[] toCharArray() 将此字符串转换为一个新的字符数组 40 String toLowerCase() 使用默认语言环境的规则将此 String 中的所有字符都转换为小写 41 String toLowerCase(Locale locale) 使用给定 Locale 的规则将此 String 中的所有字符都转换为小写 42 String toString() 返回此对象本身（它已经是一个字符串！） 43 String toUpperCase() 使用默认语言环境的规则将此 String 中的所有字符都转换为大写 44 String toUpperCase(Locale locale) 使用给定 Locale 的规则将此 String 中的所有字符都转换为大写 45 String trim() 删除指定字符串的首尾空白符 46 static String valueOf(primitive data type x) 返回指定类型参数的字符串表示形式 Boolean类型Boolean类型有两个字面量，true和false： 12345scala&gt; val t = truet: Boolean = truescala&gt; val f = falsef: Boolean = false 和很多动态语言不同， Scala不支持其他类型到Boolean类型的隐式转换： 12345678scala&gt; if(4&gt;3) print(&quot;4&gt;3&quot;)4&gt;3scala&gt; if(1) print(&quot;1&quot;)&lt;console&gt;:12: error: type mismatch; found : Int(1) required: Boolean if(1) print(&quot;1&quot;) ^ 核心类型间的转换隐式转换 数值类型的隐式准换：当Scala在进行赋值或者运算时，精度小的数值类型会自动转换为精度高的数值类型： 举例： 1234567891011121314scala&gt; val a = &#x27;a&#x27;a: Char = ascala&gt; a + 1res36: Int = 98scala&gt; val x = 1x: Int = 1scala&gt; val y: Short = 2y: Short = 2scala&gt; x + yres37: Int = 3 String的隐式转换：s + 会自动调用的toString方法进行字符串拼接 12scala&gt; &quot;hello&quot; + 2019res38: String = hello2019 显式转换有几种方式： to.类型名 1234567891011121314151617181920212223242526scala&gt; val a = 97a: Int = 97scala&gt; a.toByteres48: Byte = 97scala&gt; a.toShortres49: Short = 97scala&gt; a.toCharres50: Char = ascala&gt; a.toLongres51: Long = 97scala&gt; a.toFloatres52: Float = 97.0scala&gt; a.toDoubleres53: Double = 97.0scala&gt; a.toStringres54: String = 97scala&gt; &#x27;a&#x27;.toIntres55: Int = 97 asInstanceOf[type]：测定某个对象是否属于给定的类，用isInstanceOf方法，如果测试成功，可以用asInstanceOf方法转换 123456789101112scala&gt; a.asInstanceOf[Int]res65: Int = 97scala&gt; a.asInstanceOf[Long]res66: Long = 97scala&gt; a.asInstanceOf[Short]res67: Short = 97scala&gt; a.asInstanceOf[String]java.lang.ClassCastException: java.lang.Integer cannot be cast to java.lang.String ... 28 elided]]></content>
      <categories>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>教程</tag>
        <tag>Scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scala 教程：Basics（三）—— 操作符&表达式]]></title>
    <url>%2FScala%2FScala%2FScala%20%E6%95%99%E7%A8%8B%EF%BC%9ABasics%EF%BC%88%E4%B8%89%EF%BC%89%E2%80%94%E2%80%94%20%E6%93%8D%E4%BD%9C%E7%AC%A6%26%E8%A1%A8%E8%BE%BE%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[操作符即方法：操作符和方法只不过是操作的两种语法形式 一切操作符都只不过是方法调用的漂亮语法一切方法都可以写作操作符表示法 操作符Scala中的操作符 算术操作符: A 为 10，B 为 20 关系操作符: A 为 10，B 为 20，==的实现很用心，大部分场合都能返回给你需要的相等性比较的结果，其背后的规则是：首先检查左侧是否为null，如果不为Null，调用equals方法 逻辑操作符：A 为 true，B 为 false；&amp;&amp; 和 || 遵循短路原则，对应的非短路版本为 &amp; 和 |； 位操作符：A = 60; 及 B = 13; 赋值运算符：注意，在Java中赋值语句的返回值是被赋上的值，而在Scala中赋值语句的返回值永远是 Unit类型的单元值() 操作符的优先级和结合性由于Scala并不是真的有操作符，操作符仅仅是用操作符表示法使用方法的一种方式，Scala通过操作符的首字符来决定操作符的优先级，通过操作符的尾字符决定操作符的结合性。 尽管你能够记住这些操作符的优先级，为了使得代码更加易于理解，你只应该在算术操作符合赋值操作符上利用操作符的优先级，其他情形还是老老实实加上括号吧。 操作符的优先级Scala中操作符的优先级由操作符的首字符决定：举例来说，以*开始的操作符优先级比以+开始的操作符优先级更高，下图列出了Scala中不同首字母的操作度的优先级（自上而下，依次递减；同一行具有相同优先级）： 上面红框分类不是很严谨，只是为了方便记忆，比较两个操作符的优先级的时候这样做： 得到两个操作符的首字符A和B; 看看首字母是属于算术-&gt;关系-&gt;逻辑-&gt;字母-&gt;赋值中的哪一类； 在以上顺序中，位于前面的优先级高； 记住两个特例：:在算术和关系之间，^在 &amp; 和 | 之间； 12345// + 的优先级在 &lt; 之上，因而 + 先执行scala&gt; 2 &lt;&lt; 2 + 2res107: Int = 32// + 的优先级在赋值操作之上，因而 + 先执行x *= y + 1 等价于 x *= (y+1) 操作符的结合性当多个同等优先级的操作符并排在一起的时候，操作符的结合性由操作符的尾字符决定：任何以 : 结尾的操作符都是在它右侧的操作元调用，传入左侧操作元，以任何其他字符结尾的方法则相反。 1a ::: b ::: c 等价于 a ::: (b:::c) 任何操作符都是方法调用Scala中的操作符只是方法调用的漂亮语法，换句话说Scala中所有操作符都可以写作方法调用的形式。 中缀操作符：&lt;operator1&gt; &lt;operate&gt; &lt;operator2&gt; ，如果是左结合性可以写作 &lt;operator1&gt;.&lt;operate&gt;(&lt;operator2)，如果是右结合性可以写作&lt;operator2&gt;.&lt;operate&gt;(&lt;operator1)； 123456// 1 + 2scala&gt; 1.+(2)res108: Int = 3// 2 &lt;&lt; 1scala&gt; 2.&lt;&lt;(1)res113: Int = 4 前缀操作符：只有一元操作符(unary)+、-、!、~可以被用作前缀操作符，&lt;operate&gt; &lt;operator1&gt; 可以写作 &lt;operator1&gt;.unary_&lt;operate&gt; 123456// -2.0scala&gt; (2.0).unary_-res111: Double = -2.0// ! truescala&gt; true.unary_!res115: Boolean = false 任何方法都可以是操作符Scala中操作符并不是特殊的语法，任何方法都可以是操作符。 中缀操作符表示法：&lt;operator1&gt;.&lt;operate&gt;(&lt;operator2&gt;) 可以写作 &lt;operator1&gt; &lt;operate&gt; &lt;operator2&gt; 1234567891011scala&gt; &quot;hello world&quot;.indexOf(&quot;w&quot;)res122: Int = 6scala&gt; &quot;hello world&quot; indexOf &quot;w&quot;res123: Int = 6scala&gt; &quot;hello world&quot; indexOf (&quot;o&quot;,5)res125: Int = 7scala&gt; &quot;hello world&quot;.indexOf(&quot;o&quot;,5)res126: Int = 7 后缀操作符表示法：后缀操作符是那些不接受任何参数的方法，在Scala中可以在方法调用时省略空的圆括号，除非方法有副作用，比如println() 12345678scala&gt; import scala.language.postfixOpsimport scala.language.postfixOpsscala&gt; &quot;Hello WOrld&quot; toLowerCaseres130: String = hello worldscala&gt; &quot;Hello WOrld&quot;.toLowerCaseres131: String = hello world 表达式 表达式：表达式是执行后会返回一个值的代码单元 123// 一个最简单的表达式scala&gt; 1res132: Int = 1 表达式块：可以用大括号结合多个表达式创建一个表达式块，块中最后一个表达式将作为整个表达式块的返回值，表达式块可以进行嵌套，每个表达式块拥有自己的变量和作用域 123456// 表达式块scala&gt; val amount = &#123; | val x = 5 * 20 | x + 10 | &#125;amount: Int = 110 语句：语句就是不返回值的表达式，语句的返回类型为Unit；由于不返回值，语句通常用来修改现有的数据或者完成应用之外的修改；Scala中常见的语句包括 println()调用、变量声明语句、while控制语句 12345scala&gt; println(&quot;hello world&quot;)hello worldscala&gt; val a = 1a: Int = 1 表达式为函数式编程提供了基础：表达式可以返回数据而不修改现有数据，这就允许使用不可变数据，函数也可以用来返回新的数据，在某种意义上这种函数是另一种类型的表达式。]]></content>
      <categories>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>教程</tag>
        <tag>Scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scala 教程：Basics（四）—— 控制结构]]></title>
    <url>%2FScala%2FScala%2FScala%20%E6%95%99%E7%A8%8B%EF%BC%9ABasics%EF%BC%88%E5%9B%9B%EF%BC%89%E2%80%94%E2%80%94%20%E6%8E%A7%E5%88%B6%E7%BB%93%E6%9E%84%2F</url>
    <content type="text"><![CDATA[Scala中大多数控制结构都是表达式，有返回值 Scala 只有为数不多的几个内建的控制结构：if、match、for、while、try和函数调用，由于它们有返回值，可以很好地支持函数式编程。 条件控制结构if表达式语法： if (&lt;Boolean expression&gt;) &lt;expression&gt;：返回值是 Any 类型； if (&lt;Boolean expression&gt;) &lt;expression&gt; else &lt;expression&gt;：返回值的类型是两种结果类型的最近公共父类型； if (&lt;Boolean expression&gt;) &lt;expression&gt; else if (&lt;Boolean expression&gt;) ... else &lt;expression&gt;：本质上是 if ... else 表达式的嵌套，返回值的类型是所有可能返回结果类型的最近公共父类型； 执行：如果布尔表达式成立则执行第一个表达式，否则执行另外一个表达式 示例： 123456789101112131415161718192021222324252627// if ... 返回值类型必为 Anyscala&gt; if (3&lt;4) 4res0: AnyVal = 4// if ... else ... ，返回值的类型是所有可能返回结果类型的最近公共父类型scala&gt; val x = 1x: Int = 1scala&gt; val y = 2y: Int = 2scala&gt; if (x &gt; y) xres1: AnyVal = ()scala&gt; if (x &gt; y) x else yres2: Int = 2// if ... else if ... else ...，本质上是嵌套的 if ... else 表达式scala&gt; if (2 == 3)&#123; | 0 | &#125; else | if (2 &gt; 3)&#123; | 1 | &#125; else &#123; | -1 | &#125;res23: Int = -1 match表达式模式匹配是检查某个值（value）是否匹配某一个模式的机制，它是Java中的switch语句的升级版，同样可以用于替代一系列的 if/else 语句。 语法： 123&lt;expression&gt; match &#123; case &lt;pattern&gt; =&gt; &lt;expression&gt; [case ...] 执行：获取输入表达式的值，逐一匹配备选模式，匹配成功则执行并返回对应模式后的表达式，匹配不成功则触发MatchError，返回值类型是各个备选结果表达式类型的最近公共父类型。 示例： 12345678910// 对 if (x &gt; y) x else y 的改写scala&gt; val x = 1; val y = 2x: Int = 1y: Int = 2scala&gt; val max = x &gt; y match &#123; | case true =&gt; x | case false =&gt; y | &#125;max: Int = 2 变形：match 表达式的变形主要发生在 复合模式：使用 &lt;pattern1&gt; | &lt;pattern2&gt; ... 可以对多个模式重用 case 块 123456scala&gt; &quot;MON&quot; match &#123; | case &quot;SAT&quot; | &quot;SUN&quot; =&gt; &quot;weekend&quot; | case &quot;MON&quot; | &quot;TUE&quot; | &quot;WED&quot; | &quot;THU&quot; | &quot;FRI&quot; =&gt; | &quot;weekday&quot; | &#125;res9: String = weekday 通配模式：使用通配符 _ 可以匹配任意模式，但是不能在 =&gt; 右侧访问通配符 12345scala&gt; &quot;MON&quot; match &#123; | case &quot;SAT&quot; | &quot;SUN&quot; =&gt; &quot;weekend&quot; | case _ =&gt; &quot;weekday&quot; | &#125;res8: String = weekday 变量模式：使用一个模式变量可以将输入表达式的值绑定到该变量，变量可以在 =&gt; 右侧访问 12345scala&gt; &quot;MON&quot; match &#123; | case &quot;SAT&quot; | &quot;SUN&quot; =&gt; &quot;weekend&quot; | case x =&gt; &quot;weekday&quot; + x | &#125;res10: String = weekdayMON 类型模式：使用 模式变量: 类型 可以匹配输入表达式返回值的具体类型，需要注意的是备选模式的类型必须是输入表达式返回值类型的子类，否则会触发异常：error: scrutinee is incompatible with pattern type 12345678910111213141516scala&gt; val x: Int = 1x: Int = 1scala&gt; val y: Any = xy: Any = 1scala&gt; yres25: Any = 1scala&gt; y match &#123; | case t: Float =&gt; &quot;Float&quot; | case t: Long =&gt; &quot;Long&quot; | case t: Int =&gt; &quot;Int&quot; | case _ =&gt; &quot;_&quot; | &#125;res26: String = Int 哨兵模式：在模式变量后面加上 if &lt;boolean expression&gt;，可以为匹配表达式添加匹配条件，只有条件满足时才算匹配成功 12345678910111213141516171819202122def showImportantNotification(notification: Notification, importantPeopleInfo: Seq[String]): String = &#123; notification match &#123; case Email(sender, _, _) if importantPeopleInfo.contains(sender) =&gt; &quot;You got an email from special someone!&quot; case SMS(number, _) if importantPeopleInfo.contains(number) =&gt; &quot;You got an SMS from special someone!&quot; case other =&gt; showNotification(other) // nothing special, delegate to our original showNotification function &#125;&#125;val importantPeopleInfo = Seq(&quot;867-5309&quot;, &quot;jenny@gmail.com&quot;)val someSms = SMS(&quot;867-5309&quot;, &quot;Are you there?&quot;)val someVoiceRecording = VoiceRecording(&quot;Tom&quot;, &quot;voicerecording.org/id/123&quot;)val importantEmail = Email(&quot;jenny@gmail.com&quot;, &quot;Drinks tonight?&quot;, &quot;I&#x27;m free after 5!&quot;)val importantSms = SMS(&quot;867-5309&quot;, &quot;I&#x27;m here! Where are you?&quot;)println(showImportantNotification(someSms, importantPeopleInfo))println(showImportantNotification(someVoiceRecording, importantPeopleInfo))println(showImportantNotification(importantEmail, importantPeopleInfo))println(showImportantNotification(importantSms, importantPeopleInfo)) 循环表达式/语句for表达式Scala 的for表达式是用于迭代的瑞士军刀，每次迭代会执行一个表达式，并返回所有表达式返回值的一个集合（可选）。 语法：enumerators 是一个枚举器，可以包含多个生成器（items &lt;- items）和过滤器（if ）； 1for (enumerators) [yield] &lt;expression&gt; 执行：每次从枚举器中取出一个元素，执行表达式，返回所有返回值构成的一个集合（如果加了 yield 的话）。 示例： 12345// 不带 yield，没有返回值scala&gt; for (i &lt;- 1 to 10) &#123;2 * i&#125;// 带了yeild，返回所有返回值构成的一个集合scala&gt; for (i &lt;- 1 to 10) yield &#123;2 * i&#125;res30: scala.collection.immutable.IndexedSeq[Int] = Vector(2, 4, 6, 8, 10, 12, 14, 16, 18, 20) 变形： 迭代器哨兵：枚举器中可以包含多个过滤器 12345scala&gt; for (i &lt;- 1 to 10 if i % 2 == 0) yield &#123;2 * i&#125;res32: scala.collection.immutable.IndexedSeq[Int] = Vector(4, 8, 12, 16, 20)scala&gt; for (i &lt;- 1 to 10 if i % 2 == 0 if i &gt; 5) yield &#123;2 * i&#125;res33: scala.collection.immutable.IndexedSeq[Int] = Vector(12, 16, 20) 迭代器嵌套：枚举器中可以包含多个迭代器 12345678scala&gt; for &#123;i &lt;- 1 to 10 | j &lt;- 1 until 3 | if i &gt; j | if i % 2 == 0 | &#125; yield &#123; | i + j | &#125;res37: scala.collection.immutable.IndexedSeq[Int] = Vector(3, 5, 6, 7, 8, 9, 10, 11, 12) 值绑定：在for循环中使用值绑定，可以把循环的大部分逻辑都集中在定义中，可以得到一个更为简洁的 yield 表达式 1234567scala&gt; for &#123; | i &lt;- 1 to 8 | pow = 1 &lt;&lt; i | &#125; yield &#123; | pow | &#125;res38: scala.collection.immutable.IndexedSeq[Int] = Vector(2, 4, 8, 16, 32, 64, 128, 256) while语句Scala 同样支持 while 和 do/while 循环语句，不过没有 for 表达式那么常用，因为它不是表达式，不能用来返回值。事实上，while 循环和 var通常是一起使用的，要想对程序产生任何效果，while循环通常要么更新一个var要么执行I/O。Scala 没有内建的 break 和 continue 语句，但可以通过 if 表达式来改写。 语法： 1234// whilewhile (Boolean expression) statement// do ... whiledo statement while (Boolean expression) 执行： while 只要条件为true，循环体就会一遍接着一遍执行； do/while：一遍接着一遍执行循环体，直至条件为false while 和 do/while语句也有自己的用途，比如需要不断读取外部输入知道没有可读的内容为止，不过Scala提供了很多更有表述性且功能更强的方法来处理循环。 try 表达式异常传播机制：方法除了正常返回某个值外，也可以通过抛出异常终止执行，方法调用方要么捕获并处理这个异常，要么自我终止，让异常传播到更上层的方法调用方，异常通过这种方式传播，逐个展开调用栈，直至某个方法处理该异常或再没有更多方法为止。 抛出异常语法： 1throw new classException(&quot;something&quot;) 执行：抛出对应类型的异常，返回值类型为Nothing 123scala&gt; throw new IllegalArgumentException(&quot;ddfs&quot;)java.lang.IllegalArgumentException: ddfs ... 28 elided 捕获异常语法： 12345678try &#123; &lt;expression1&gt;&#125; catch &#123; case &lt;pattern1&gt; =&gt; &lt;expression2&gt; case &lt;pattern2&gt; =&gt; &lt;expression3&gt;&#125; finally &#123; &lt;expression4&gt;&#125; 执行： try子句：首先执行代码体 ，如果出现异常则先执行 catch 子句后再执行finally 子句，如果没有异常，则直接执行finally子句 catch子句：根据try子句抛出的异常，依次尝试匹配每个模式，匹配成功则执行模式后面对应的表达式（使用方式和match表达式一致） finally子句：将那些无论是否抛出异常都想执行的代码以表达式的形式包在finally子句里，finally子句一般都是执行清理工作，这是正确关闭非内存资源的惯用做法，比如关闭文件、套接字、数据库连接 返回值： 如果没有抛出异常，返回try表达式子句的结果； 如果抛出异常且被捕获，则返回对应catch子句的结果； 如果抛出异常但没有被捕获，则整个表达式没有结果； 如果finally子句包含一个显式地返回语句，则整个表达式会返回finally子句的结果，否则按前三个规则 示例: 123456789101112131415import java.io.FileReadertry &#123; val file = new FileReader(&quot;input.txt&quot;) // 使用文件&#125; catch &#123; // 捕获并处理异常 case e: FileNotFoundException =&gt; &quot;未找到对应文件&quot; case e: IOException =&gt; &quot;处理其他I/O错误&quot; &#125; finally &#123; // 关闭文件 file.close() // 显式返回一个值 return 1&#125;]]></content>
      <categories>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>教程</tag>
        <tag>Scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scala 教程：Basics（五）—— 函数]]></title>
    <url>%2FScala%2FScala%2FScala%20%E6%95%99%E7%A8%8B%EF%BC%9ABasics%EF%BC%88%E4%BA%94%EF%BC%89%E2%80%94%E2%80%94%20%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[在Scala中，函数是命名的参数化表达式，而匿名函数实际上就是参数化表达式，函数可以出现在任何表达式可以出现的地方在Scala中，函数是首类的，不仅可以得到声明和调用，还具有类型和值，函数类型和函数值可以出现在任何类型和值可以出现的地方 对于 Scala 和其他函数式编程语言来说，函数尤其重要。标准函数式编程方法论建议我们尽可能地构建纯（pure）函数，纯函数相对于非纯函数更加稳定，他们没有状态，且与外部数据正交，事实上它们是不可破坏的纯逻辑表达式： 有一个或多个输入参数，只使用输入参数完成计算，返回一个值； 对相同输入总是返回相同的值； 不使用或影响函数之外的任何数据，也不受函数之外的任何数据的影响； 作为传统函数Scala 函数可以像传统函数那样进行声明和调用，还可以进行嵌套和递归。 函数声明函数声明的一般格式： 1def &lt;function_name&gt;[[type_param]](&lt;param1&gt;: &lt;param1_type&gt; [,...]): &lt;function_type&gt; = &lt;expression&gt; def：函数声明的关键字 function_name：函数名 type_param：类型参数，如果传入了类型参数，类型参数在函数定义的后续代码中就可以像普通类型一样使用 param1：值参数 :：每个参数后面都必须加上以冒号开始的类型标注，因为Scala并不会推断函数参数的类型 param1_type：值参数类型 function_type：函数的返回值类型是可选的，Scalade的类型推断会根据函数的实际返回值来推断函数的返回值类型，但在无法推断出函数返回值类型时必须显式提供函数返回值类型，比如递归函数必须显式给出函数的结果类型 =：等号也有特别的含义，表示在函数式的世界观里，函数定义的是一个可以获取到结果值的表达式 expression：函数体，由表达式或表达式块组成，最后一行将成为函数的返回值，如果需要在函数的表达式块结束前退出并返回一个值，可以使用return关键字显式指定函数的返回值，然后退出函数；如果函数只有一条语句，也可以选择不使用花括号 没有参数的函数只是表达式的一个命名包装器：适用于通过一个函数来格式化当前数据或者返回一个固定的值 123456789101112131415scala&gt; def hi() = &quot;hi&quot;hi: ()Stringscala&gt; hires11: String = hiscala&gt; hi()res12: String = hiscala&gt; def hi = &quot;hi&quot;hi: Stringscala&gt; hires13: String = hi 没有返回值的函数被称作过程：以一个语句结尾的函数，如果函数没有显式的返回类型，且最后是一个语句，则Scala会推导出这个函数的返回类型为Unit 12scala&gt; def log(d: Double) = println(f&quot;Got Value $d%.2f&quot;)log: (d: Double)Unit 函数调用函数调用的通用语法： 1&lt;function identifier&gt;(&lt;params&gt;) 调用无参函数时，空括号是可选的：如果在定义时加了空括号，在调用时可加可不加，但如果在定义时没有加，在调用时也不能加，这可以避免混淆调用无括号函数与调用函数返回值。 1234567891011121314151617181920scala&gt; def hi() = &quot;hi&quot;hi: ()Stringscala&gt; hires11: String = hiscala&gt; hi()res12: String = hiscala&gt; def hi = &quot;hi&quot;hi: Stringscala&gt; hires13: String = hiscala&gt; hi()&lt;console&gt;:13: error: not enough arguments for method apply: (index: Int)Char in class StringOps.Unspecified value parameter index. hi() ^ 当函数只有一个参数时，可以使用表达式块来发送参数：不必先计算一个量，然后把它保存在局部值中再传递给函数，完全可以在表达式块中完成计算，表达式块会在调用函数之前计算，将表达式块的返回值用作函数的参数 1234567891011scala&gt; def len(s: String) = &#123; | s.length() | &#125;len: (s: String)Intscala&gt; len&#123; | val x = &quot;Hello&quot; | val y = &quot;World&quot; | x + &quot; &quot; + y | &#125;res6: Int = 11 参数传递按顺序传参&amp;按关键字传参Scala 中的参数默认按照参数顺序传递，也可以按照关键字传递： 12345678scala&gt; def greet(prefix: String, name: String) = s&quot;$prefix $name&quot;greet: (prefix: String, name: String)Stringscala&gt; greet(&quot;Mr&quot;, &quot;Bob&quot;)res0: String = Mr Bobscala&gt; greet(name = &quot;Bob&quot;, prefix = &quot;Mr&quot;)res1: String = Mr Bob 默认参数Scala 可以为函数的任意参数指定默认值，使得调用者可以忽略这个参数： 1def &lt;identifier&gt;(&lt;identifier&gt;: &lt;type&gt; = &lt;value&gt; [,...]): &lt;type&gt; = &lt;expression&gt; 如果默认参数后面还有非默认参数，那只能按照关键字传参，因为无法利用参数的顺序了；如果默认参数后面没有非默认参数，则可以按照顺序来传递前面的参数。 变长参数Scala 支持vararg参数，可以定义输入参数个数可变的函数，可变参数后面不可以有非可变参数，因为无法加以区分。 语法：在参数类型后面加上 * 来标识这是一个可变参数 123456789scala&gt; def sum(items: Int*): Int = &#123; | var total = 0 | for (i &lt;- items) total += i | total | &#125;sum: (items: Int*)Intscala&gt; sum(1,2,3)res8: Int = 6 类型参数Scala 函数不仅可以传入“值”参数，还可以传入“类型”参数，这可以提高函数的灵活性和可重用性，这样函数参数或返回值的类型不再是固定的，而是可以由函数调用者控制。 语法：在函数名后的[]传入类型参数R之后，R就可以像一个具体的类型一样在后面使用了 1def &lt;identifier&gt;[type-param](&lt;value-param&gt;: &lt;type-param&gt;): &lt;type&gt; = &lt;expression&gt; 示例： 123456789101112131415scala&gt; def identity[R](r: R): R = ridentity: [R](r: R)Rscala&gt; identity[String](&quot;sds&quot;)res9: String = sdsscala&gt; identity[Int](23)res10: Int = 23scala&gt; identity[Int](&quot;fs&quot;)&lt;console&gt;:13: error: type mismatch; found : String(&quot;fs&quot;) required: Int identity[Int](&quot;fs&quot;) ^ 函数调用时，类型参数的类型推断：在调用包含类型参数的函数时，如果未明确指定类型参数的具体类型，scala会根据第一个参数列表的类型来推断类型参数的类型，如果第一个参数列表的类型也未知则会抛出异常。因此在设计柯里化函数时，往往将非函数参数放在第一个参数列表，将函数参数放在最后一个参数列表，这样函数的类型参数的具体类型可以通过第一个非函数入参的类型推断出来，而这个类型又能被继续用于对函数参数列表类型进行检查，使用者需要给出的类型信息更少，在编写函数字面量时可以更精简； 1234567891011121314// 类型参数未指定，且第一个参数函数字面值类型未指定，抛出异常scala&gt; def curry[T](f: T =&gt; T)(x:T) = f(x)curry: [T](f: T =&gt; T)(x: T)Tscala&gt; curry(_ * 2)(3)&lt;console&gt;:13: error: missing parameter type for expanded function ((x$1: &lt;error&gt;) =&gt; x$1.$times(2)) curry(_ * 2)(3) ^// 类型参数的具体类型可以通过第一个参数列表的类型推断出来，继而推断出第二个参数列表的类型scala&gt; def curry[T](x: T)(f: T =&gt; T) = f(x)curry: [T](x: T)(f: T =&gt; T)Tscala&gt; curry(3)(_ * 2)res97: Int = 6 递归函数递归函数在函数式编程中很普遍，因为他们为迭代处理数据结构或计算提供了一种很好的方法，而且不必使用可变的数据，因为每个函数调用自己的栈来存储参数。 示例： 123456789101112// 计算正数次幂scala&gt; greet(name = &quot;Bob&quot;, prefix = &quot;Mr&quot;)res1: String = Mr Bobscala&gt; def power(x:Int,n:Int):Long = &#123; | if (n &gt; 1) x * power(x, n-1) | else 1 | &#125;power: (x: Int, n: Int)Longscala&gt; power(2,8)res2: Long = 128 使用递归函数可能会遇到”栈溢出“错误，为了避免这种情况，Scala编译器可以使用尾递归（tail-recursion）优化一些递归函数，使得递归调用不使用额外的栈空间，而只使用当前函数的栈空间。但是只有最后一个语句是递归调用的函数时（调用函数本身的结果作为直接返回值），Scala编译器才能完成尾递归优化。 示例： 123456789// 用尾递归的方式重写powerscala&gt; def power(x: Int, n: Int, t: Int = 1): Int = &#123; | if (n &lt; 1) t | else power(x, n - 1, x * t) | &#125;power: (x: Int, n: Int, t: Int)Intscala&gt; power(2, 8)res4: Int = 256 嵌套函数函数是命名的参数化表达式，而表达式是可以嵌套的，所以函数本身也是可以嵌套的。当需要在一个方法中重复某个逻辑，但是把它作为外部方法有没有太大意义时，可以在函数中定义一个内部函数，这个内部函数只能在该函数内部使用。 示例： 12345678scala&gt; def max(a: Int, b: Int, c: Int) = &#123; | def max(x: Int, y: Int) = if (x &gt; y) x else y | max(a,max(b,c)) | &#125;max: (a: Int, b: Int, c: Int)Intscala&gt; max(1,2,3)res7: Int = 3 作为首类函数 函数式编程的一个关键是函数应当是首类的（first-class）：函数不仅能得到声明和调用，还具有类型和值，函数类型和函数值可以出现在任何类型和值可以出现的地方 函数类型与函数返回值类型不同，函数类型是函数本身的类型，函数类型可以用 参数类型 =&gt; 返回值类型 来表示： 1([&lt;type&gt;, ...]) =&gt; &lt;type&gt; 函数类型可以出现在任何类型可以出现的地方： 123456789101112131415161718192021scala&gt; def func(a: Int, b: Int): Int = if (a &gt; b) a else bfunc: (a: Int, b: Int)Int// 1. 出现在变量/值声明语句中scala&gt; val x: (Int, Int) =&gt; Int = funcx: (Int, Int) =&gt; Int = $$Lambda$1096/23426726@70b037acscala&gt; x(1,2)res9: Int = 2// 2. 出现在函数参数类型中scala&gt; def max(a: Int, b: (Int, Int) =&gt; Int): Int = b(a, 3)max: (a: Int, b: (Int, Int) =&gt; Int)Intscala&gt; max(1, func)res10: Int = 3// 3. 出现在函数返回值类型中scala&gt; def dummy(): (Int, Int) =&gt; Int = funcdummy: ()(Int, Int) =&gt; Intscala&gt; dummy()(1,2)res11: Int = 2 函数值与函数返回值不同，函数值是函数本身的值，每个函数值都是某个扩展自scala包的FunctionN系列当中的一个特质的类的实例，比如Function0表示不带参数的函数，Function1表示带一个参数的函数，等等。每一个FunctionN特质都有一个apply方法用来调用该函数。 函数值可以出现在任何值可以出现的地方： 可以用字面值形式创建，而不必指定标识符； 可以存储在某个容器，比如值、变量或数据结构； 作为另一个函数的参数或返回值； Scala 中有一些特殊的方法来创建或返回函数值，包括： 创建函数字面值/匿名函数； 当通过函数名为一个显式声明为函数类型的变量赋值时，函数名会被推断为一个函数值，而不是函数调用； 使用通配符替换部分参数来部分调用函数，将返回一个能够接收剩余参数的函数值； 函数柯里化提供了一种更加简洁的方式来实现部分调用函数； 匿名函数（Anonymous function）匿名函数是一个没有名字的函数值，匿名函数可以用 输入参数 =&gt; 返回值 来表示： 1([&lt;param1&gt;: &lt;type&gt;...]) =&gt; &lt;expression&gt; 示例： 123456// 一个没有输入的函数字面值scala&gt; val func = () =&gt; &quot;hi&quot;func: () =&gt; String = $$Lambda$1182/355159860@42e71f6cscala&gt; val doubler = (x: Int) =&gt; x * 2doubler: Int =&gt; Int = $$Lambda$1181/1140744858@40fd8aa1 匿名函数有很多名字： 函数字面量(function literal)：由于匿名函数的创建不必指定标识符，且可以出现在一切函数值可以出现的地方，和一般类型中的字面值作用类似；Lambda表达式：C#和Java8都采用这种说法，这是从原先数学中的lambda演算语法得来的；functionN：Scala编译器对函数字面量的叫法，根据输入参数的个数而定； 当函数字面值满足以下两个条件时，甚至可以使用通配符语法把参数和箭头也给匿了： 函数的显式类型在字面量之外指定，Scala可以通过类型推断推断出参数类型； 参数最多被使用一次； 123456789101112131415161718192021// 一个参数的情形scala&gt; def x(s: String, f: String =&gt; String) = &#123; | if (s != null) f(s) else s | &#125;x: (s: String, f: String =&gt; String)Stringscala&gt; x(&quot;Ready&quot;, _.reverse)res14: String = ydaeR// 多个参数的情形：通配符会按照顺序替换输入参数，通配符必须与输入参数个数一致scala&gt; def com(x: Int, y:Int, f:(Int, Int)=&gt; Int) = f(x, y)com: (x: Int, y: Int, f: (Int, Int) =&gt; Int)Intscala&gt; com(23, 12, _ * _)res15: Int = 276// 使用类型参数的情形scala&gt; def x[A, B](a: A, b: A, c: A, f:(A, A, A)=&gt;B) = f(a,b,c)x: [A, B](a: A, b: A, c: A, f: (A, A, A) =&gt; B)Bscala&gt; x[Int, Int](1,2,3,_*_+_)res18: Int = 5 通配符语法在处理数据结构和集合事尤其有帮助，很多核心的排序、过滤和其他数据结构方法都会使用首类函数和占位符语法来减少调用这些方法所需的额外代码。 偏函数（partial function）偏函数是只对满足某些特定模式的输入进行输出的函数字面值，如果输入匹配不到任何给定模式则会导致一个Scala错误（如果要避免这样的错误可以在末尾使用一个通配符）： 123456789101112131415scala&gt; val statusHandler: Int =&gt; String = &#123; | case 200 =&gt; &quot;Okay&quot; | case 400 =&gt; &quot;Your Error&quot; | case 500 =&gt; &quot;Our Error&quot; | &#125;statusHandler: Int =&gt; String = $$Lambda$1196/551773385@24efdd16scala&gt; statusHandler(200)res22: String = Okayscala&gt; statusHandler(20)scala.MatchError: 20 (of class java.lang.Integer) at .$anonfun$statusHandler$1(&lt;console&gt;:11) at .$anonfun$statusHandler$1$adapted(&lt;console&gt;:11) ... 28 elided 偏函数无法单独存在，必须要赋值给变量/参数。偏函数有点像 Sql 中的 case when 语句，在处理集合和模式匹配时更为有用。 函数名用作函数值函数名出现的时候会被默认视作一次函数调用，但是当将函数名赋值/传递给一个显式声明的变量/参数时，Scala会将其推断为一个函数值： 1234567891011121314151617181920scala&gt; def double(x: Int): Int = x * 2double: (x: Int)Intscala&gt; val myDouble: (Int) =&gt; Int = doublemyDouble: Int =&gt; Int = $$Lambda$1135/1858051117@753c7411scala&gt; myDouble(5)res6: Int = 10// 没有参数的函数不建议这样使用scala&gt; def func() = &quot;hi&quot;func: ()Stringscala&gt; val x = funcx: String = hiscala&gt; val x: () =&gt; String = func&lt;console&gt;:12: warning: Eta-expansion of zero-argument methods is deprecated. To avoid this warning, write (() =&gt; func()). val x: () =&gt; String = func ^x: () =&gt; String = $$Lambda$1177/572488693@2986db02 部分调用函数（partially apply）对于多参数函数，如果固定其中某些参数，剩余参数用通配符替换，将返回一个只接收剩余参数的函数值： 123456789101112131415161718192021222324def sum(a: Int, b: Int, c: Int) = a + b + c// 返回保留一个参数的函数值scala&gt; val left1 = sum(1, _, 3)left1: Int =&gt; Int = $$Lambda$1090/466959452@38affd02scala&gt; left1(2)res8: Int = 6// 返回保留两个参数的函数值scala&gt; val left2 = sum(1, _, _)left2: (Int, Int) =&gt; Int = $$Lambda$1088/1147105139@27f31d91scala&gt; left2(2,3)res7: Int = 6// 返回保留三个参数，等价于 sum _scala&gt; val left3 = sum(_, _, _)left3: (Int, Int, Int) =&gt; Int = $$Lambda$1087/417004859@2954c429scala&gt; left3(1, 2, 3)res6: Int = 6// 返回保留所有参数的函数值scala&gt; val leftAll = sum _leftAll: (Int, Int, Int) =&gt; Int = $$Lambda$1103/118175968@79414283scala&gt; leftAll(1,2,3)res12: Int = 6 函数柯里化（function Currying） 柯里化（Currying）是以逻辑学家 Haskell Curry 命名的一种将多参数函数转化为单参数函数链的技术。某些分析技术只能应用于具有单个参数的函数，在处理多参数函数时，柯里化通过逐一固定参数来得到关于剩余参数的新的函数，这样每次只需要处理单参数函数。 函数柯里化可以看做是部分调用函数的一种简洁语法：使用有多个参数表的函数，而不是将一个参数表分解为调用参数和非调用参数，每次调用一个函数表将返回一个函数而非函数值： 12345678910111213141516171819// 定义一个多参数表的函数scala&gt; def sum(x: Int)(y: Int)(z: Int): Int = x + y + zsum: (x: Int)(y: Int)(z: Int)Int// 调用一个参数表将返回一个函数，这个函数默认被视为函数调用，因此报错scala&gt; sum(1)&lt;console&gt;:13: error: missing argument list for method sumUnapplied methods are only converted to functions when a function type is expected.You can make this conversion explicit by writing `sum _` or `sum(_)(_)(_)` instead of `sum`. sum(1) ^// 使用部分调用函数语法返回一个函数值scala&gt; sum(1) _res17: Int =&gt; (Int =&gt; Int) = $$Lambda$1143/106305065@26156929scala&gt; sum(1)(2) _res18: Int =&gt; Int = $$Lambda$1144/141828288@1cdb0d7b// 函数完全调用后得到函数最终的返回值scala&gt; sum(1)(2)(3)res19: Int = 6 高阶函数（high-order function）如果一个函数不接收任何函数作为入参，就被称为初阶（first-order）函数，高阶（high-order）函数则是包含了函数类型的参数或返回值的函数。 12345678910scala&gt; def safeStringOp(s: String, f: String =&gt; String) = &#123; | if (s != null) f(s) else s | &#125;safeStringOp: (s: String, f: String =&gt; String)Stringscala&gt; def reverser(s: String) = s.reversereverser: (s: String)Stringscala&gt; safeStringOp(&quot;Hello&quot;, reverser)res20: String = olleH 传名参数（by-name）对于普通的传值参数（by-value）来说，如果向其传递一个函数调用，那么只会在参数传递的时候调用这个函数并将其返回值传递给传值参数，后面在使用这个参数的时候使用的都是它的值。而传名参数（by-name）不同，可以获取一个值，也可以获取最终返回一个值的函数，如果向这个函数传入一个值，和传值参数效果相同，但如果向它传入一个函数调用，那么每次使用这个参数时都会调用这个函数，整体上起到了“延迟调用”的效果。 传名参数的声明语法：仅仅是在参数和参数类型中间加了一个 =&gt;： 1&lt;identifier&gt;: =&gt; &lt;type&gt; 示例： 1234567891011121314151617181920212223242526272829scala&gt; def f(i: Int) = &#123; | println(s&quot;Hello from f($i)&quot;) | i | &#125;f: (i: Int)Int// 传值参数scala&gt; def doubles(x: Int) = &#123; | println(&quot;Now doubling&quot; + x) | x * 2 | &#125;scala&gt; doubles(f(3))Hello from f(3)Now doubling3res5: Int = 6// 传名参数scala&gt; def doubles(x: =&gt; Int) = &#123; | println(&quot;Now doubling&quot; + x) | x * 2 | &#125; scala&gt; doubles(f(3))Hello from f(3)Now doubling3Hello from f(3)res4: Int = 6scala&gt; doubles(3)Now doubling3res0: Int = 6]]></content>
      <categories>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>教程</tag>
        <tag>Scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scala 教程：Collections（〇）—— 集合框架]]></title>
    <url>%2FScala%2FScala%2FScala%20%E6%95%99%E7%A8%8B%EF%BC%9ACollections%EF%BC%88%E3%80%87%EF%BC%89%E2%80%94%E2%80%94%20%E9%9B%86%E5%90%88%E6%A1%86%E6%9E%B6%2F</url>
    <content type="text"><![CDATA[Scala 2.8 的集合框架有以下特点： 易用：使用 20~50 个方法的词汇量就足以解决大部分的集合问题； 简洁：可以通过单独的一个词来执行一个或多个循环； 安全：Scala 集合的静态类型和函数性质意味着在编译时就可以捕获绝大多数错误； 快速：集合操作已经在类库中优化过； 通用：集合类提供了在一些类型上的相同操作； Seq、Map、Set 是 Scala 最重要的三种集合类（容器），此外还有 Tuple、Option 等，这些会在后面小节逐一讲解，本节将按照自顶向下的层级结构来学习不同集合类的通用特性。 可变/不可变类型（Mutable/Immutable）Scala 集合框架系统地区分了可变的(mutable)和不可变的(immutable)集合，并且可以很方便地在两者之间进行转换。你可以对可变集合中的元素进行增、删、改操作，你也可以对不可变类型模拟这些操作，但每个操作都会返回一个新的集合，原来的集合不会发生改变。 集合类的继承树Scala 所有集合类都可以在以下包中找到： scala.collection：包中的集合既可以是可变的也可以是不可变的，下图展示了这个包中所有的集合类，这些都是高级抽象类或特质，它们通常有可变和不可变两种实现方式 scala.collection.immutable ：包中的集合类是不可变的，Scala会默认导入这个包，这意味着Scala默认使用不可变集合类，当你写下 Set 而没有加任何前缀，你会得到一个不可变的 Set，下图展示了这个包中所有的集合类 scala.collection.mutable：包中的集合类是可变的，如果你想要使用可变的集合类，通用的做法是导入scala.collection.mutable包即可，当你使用没有前缀的 Set 时仍然指的是一个不可变集合，当你使用 mutable.Set时指的是可变的集合类，下图展示了这个包中所有的集合类 scala.collection.generic：包含了集合的构建块，集合类延迟了collection.generic 类中的部分操作实现 集合类的通用方法Scala 中的集合类有以下通用方法： 集合创建：每一种集合都可以通过在集合类名后紧跟元素的方式进行创建 12345678910Traversable(1, 2, 3)Iterable(&quot;x&quot;, &quot;y&quot;, &quot;z&quot;)Map(&quot;x&quot; -&gt; 24, &quot;y&quot; -&gt; 25, &quot;z&quot; -&gt; 26)Set(Color.red, Color.green, Color.blue)SortedSet(&quot;hello&quot;, &quot;world&quot;)Buffer(x, y, z)IndexedSeq(1.0, 2.0)LinearSeq(a, b, c)List(1, 2, 3)HashMap(&quot;x&quot; -&gt; 24, &quot;y&quot; -&gt; 25, &quot;z&quot; -&gt; 26) toString：所有集合类都可以用toString的方式进行打印； 返回类型一致原则：所有集合类的map方法都会返回相同类型的集合类，例如，在一个List上调用map会又生成一个List，在Set上调用会再生成一个Set，以此类推； 大多数类在集合树种都存在三种变体：root、mutable、immutable； 可遍历特质（Trait Traversable）可遍历（Traversable）是容器（collection）类的最高级别特质，它唯一的抽象操作是foreach。foreach 是 Traversable 所有操作的基础，用于遍历容器中所有元素，并对每个元素进行指定的操作： 12// Elem 是容器中元素的类型，U是一个任意的返回值类型，对f的调用仅仅是容器遍历的副作用，实际上所有计算结果都被foreach抛弃（没有返回值）def foreach[U](f: Elem =&gt; U) 要实现 Traversable 的容器类仅需要定义与之相关的方法，其他所有方法都可以从 Traversable 中继承，Traversable 定义了许多方法： 相加操作++（addition）表示把两个traversable对象附加在一起或者把一个迭代器的所有元素添加到traversable对象的尾部 Map操作有map，flatMap和collect，它们可以通过对容器中的元素进行某些运算来生成一个新的容器 转换器（Conversion）操作包括toArray，toList，toIterable，toSeq，toIndexedSeq，toStream，toSet，和toMap，它们可以按照某种特定的方法对一个Traversable 容器进行转换 拷贝（Copying）操作有copyToBuffer和copyToArray。从字面意思就可以知道，它们分别用于把容器中的元素元素拷贝到一个缓冲区或者数组里 Size info操作包括有isEmpty，nonEmpty，size和hasDefiniteSize 元素检索（Element Retrieval）操作有head，last，headOption，lastOption和find。这些操作可以查找容器的第一个元素或者最后一个元素，或者第一个符合某种条件的元素。注意，尽管如此，但也不是所有的容器都明确定义了什么是“第一个”或”最后一个“。例如，通过哈希值储存元素的哈希集合（hashSet），每次运行哈希值都会发生改变。在这种情况下，程序每次运行都可能会导致哈希集合的”第一个“元素发生变化。如果一个容器总是以相同的规则排列元素，那这个容器是有序的。大多数容器都是有序的，但有些不是（例如哈希集合）– 排序会造成一些额外消耗。排序对于重复性测试和辅助调试是不可或缺的。这就是为什么Scala容器中的所有容器类型都把有序作为可选项。例如，带有序性的HashSet就是LinkedHashSet 子容器检索（sub-collection Retrieval）操作有tail，init，slice，take，drop，takeWhilte，dropWhile，filter，filteNot和withFilter。它们都可以通过范围索引或一些论断的判断返回某些子容器 拆分（Subdivision）操作有splitAt，span，partition和groupBy，它们用于把一个容器（collection）里的元素分割成多个子容器 元素测试（Element test）包括有exists，forall和count，它们可以用一个给定论断来对容器中的元素进行判断 折叠（Folds）操作有foldLeft，foldRight，/:，:\，reduceLeft和reduceRight，用于对连续性元素的二进制操作 特殊折叠（Specific folds）包括sum, product, min, max。它们主要用于特定类型的容器（数值或比较） 字符串（String）操作有mkString，addString和stringPrefix，可以将一个容器通过可选的方式转换为字符串 视图（View）操作包含两个view方法的重载体。一个view对象可以当作是一个容器客观地展示 Traversable对象的操作： 可以选择使用操作符记法，也可以选择点记法，这取决于个人喜好，但是没有参数的方法除外，这时必须使用点记法，为了一致性推荐使用点记法。 可迭代特质（Trait Iterable）可迭代是容器类的另一个特质，这个特质里所有方法的定义都基于一个抽象方法iterator，从Traversable Trait中继承来的foreach方法在这里也是利用 iterator 来实现的： 1234def foreach[U](f: Elem =&gt; U): Unit = &#123; val it = iterator while (it.hasNext) f(it.next())&#125; Iterator 有两个方法返回迭代器：grouped和sliding，这些迭代器返回的不是单个元素，而是原容器元素的全部子序列，grouped方法返回元素的增量分块，sliding方法生成一个滑动元素的窗口： 12345678910111213141516scala&gt; val xs = List(1, 2, 3, 4, 5)xs: List[Int] = List(1, 2, 3, 4, 5)scala&gt; val git = xs grouped 3git: Iterator[List[Int]] = non-empty iteratorscala&gt; git.next()res3: List[Int] = List(1, 2, 3)scala&gt; git.next()res4: List[Int] = List(4, 5)scala&gt; val sit = xs sliding 3sit: Iterator[List[Int]] = non-empty iteratorscala&gt; sit.next()res5: List[Int] = List(1, 2, 3)scala&gt; sit.next()res6: List[Int] = List(2, 3, 4)scala&gt; sit.next()res7: List[Int] = List(3, 4, 5) Iterator 在 Traversable 的基础上添加了一些其他方法： 参考Mutable和Immutable集合 类型 Option]]></content>
      <categories>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>教程</tag>
        <tag>Scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scala 教程：Collections（二）—— Set]]></title>
    <url>%2FScala%2FScala%2FScala%20%E6%95%99%E7%A8%8B%EF%BC%9ACollections%EF%BC%88%E4%BA%8C%EF%BC%89%E2%80%94%E2%80%94%20Set%2F</url>
    <content type="text"><![CDATA[Set 是不包含重复元素的可迭代对象，Scala 默认使用的是不可变集合，对集合的任何修改都会生成一个新的集合，如果你想使用可变集合，需要引用 scala.collection.mutable.Set 。 Set 创建集合的一般创建方式： 123456789101112131415// 创建空 Setscala&gt; val setImmut = Set()// 默认创建不可变集合scala&gt; val setImmut = Set(1,2,3)scala&gt; println(setImmut.getClass.getName)// 创建可变集合scala&gt; import scala.collection.mutablescala&gt; val setMut = mutable.Set(1,2,3)scala&gt; println(setMut.getClass.getName)// 将可变集合转化为不可变集合scala&gt; val set = setMut.toSetscala&gt; println(set.getClass.getName) Set 操作基本操作集合的任何操作都可以使用以下三个基本操作来表达： head：返回集合第一个元素 tail：返回一个集合，包含除了第一元素之外的其他元素 isEmpty：在集合为空时返回 true 123456789101112scala&gt; val site = Set(&quot;Runoob&quot;, &quot;Google&quot;, &quot;Baidu&quot;)scala&gt; val nums: Set[Int] = Set()scala&gt; println( &quot;第一网站是 : &quot; + site.head)scala&gt; println( &quot;最后一个网站是 : &quot; + site.tail)scala&gt; println( &quot;查看列表 site 是否为空 : &quot; + site.isEmpty)scala&gt; println( &quot;查看 nums 是否为空 : &quot; + nums.isEmpty)第一网站是 : Runoob最后一个网站是 : Set(Google, Baidu)查看列表 site 是否为空 : false查看 nums 是否为空 : true 不可变 Set不可变 Set 的测、增、删、集合操作： 1234567891011121314151617181920212223242526272829303132333435363738394041424344scala&gt; val setA = Set(1,2,3)scala&gt; val setB = Set(3,4,5)scala&gt; val setC = Set(1,2,3,4,5)// 测：判断是否包含某个元素scala&gt; println(setA.contains(3))scala&gt; println(setA(3))// 测：判断是否是另一个集合的子集scala&gt; println(setA.subsetOf(setC))truetruetrue// 增：追加单个元素、多个元素、集合scala&gt; println(setA + 4)scala&gt; println(setA + (3,4,5))scala&gt; println(setA ++ Set(3,4) )Set(1, 2, 3, 4)Set(5, 1, 2, 3, 4)Set(1, 2, 3, 4)// 删：删除单个元素、多个元素、集合scala&gt; println(setA - 3)scala&gt; println(setA - (1,2))scala&gt; println(setA -- setB)Set(1, 2)Set(3)Set(1, 2)// 删：清空集合scala&gt; println(setA.empty)Set()// 查：返回集合中最小元素scala&gt; println(setA.min)1// 二元操作scala&gt; println(setA &amp; setB)scala&gt; println(setA | setB)scala&gt; println(setA &amp;~ setB)Set(3)Set(5, 1, 2, 3, 4)Set(1, 2) 可变 Set可变 Set 支持不可变集合的所有操作，同时还支持对集合的原地修改操作： 1234567891011121314151617181920212223242526scala&gt; import scala.collection.mutablescala&gt; val setA = mutable.Set(1,2,3)scala&gt; val setB = mutable.Set(3,4,5)scala&gt; val setC = mutable.Set(1,2,3,4,5)scala&gt; setA += 4scala&gt; println(setA)Set(1, 2, 3, 4)scala&gt; setA -= 5scala&gt; println(setA)Set(1, 2, 3, 4)scala&gt; setA ++= setBscala&gt; println(setA)Set(1, 5, 2, 3, 4)scala&gt; setA --= setBscala&gt; println(setA)Set(1, 2)scala&gt; setC.retain(x=&gt;x % 2==0)scala&gt; println(setC)Set(2, 4)scala&gt; setB(999) = truescala&gt; println(setB)Set(999, 5, 3, 4) 对比 Set 和 mutable.Set： 可变集合同样提供了 +、++ 和 -、-- 来添加或删除元素，但很少使用，因为这些操作都需要通过集合拷贝来实现，可变集合提供了更有效的更新方法 +=、++= 和 -=、--=，这些方法在集合中添加或删除元素，返回变化后的集合； 不可变集合同样提供了 += 和 -= 操作，虽然效果相同，但它们在实现上是不同的，可变集合的+=是在可变集合上调用+=方法，它会改变s的内容，但不可变类型的+=却是赋值操作的简写，它是在集合上应用方法+，并把结果赋值给集合变量；这体现了一个重要的原则：我们通常能用一个非不可变集合的变量(var)来替换可变集合的常量(val)； 可变集合默认使用哈希表来存储集合元素，不可变集合则根据元素个数不同使用不同的方式来实现元素个数不超过4的集合可以使用单例对象来表达（较小的不可变集合往往会比可变集合更加高效），超过4个元素的不可变集合则使用trie树来实现； 可变 Set 和不可变 Set 相互转化可变 Set 和不可变 Set 可以通过 Seq 作为中间桥梁进行相互转化： 1234567891011scala&gt; val set_im = Set(1,2,3)set_im: scala.collection.immutable.Set[Int] = Set(1, 2, 3)scala&gt; val set_mm = mutable.Set(1,2,3)set_mm: scala.collection.mutable.Set[Int] = Set(1, 2, 3)scala&gt; mutable.Set(set_im.toSeq:_*)res26: scala.collection.mutable.Set[Int] = Set(1, 2, 3)scala&gt; Set(set_mm.toSeq:_*)res27: scala.collection.immutable.Set[Int] = Set(1, 2, 3) Set 选择选择一个 Set 比选择一个 Seq 要简单得多，可以直接使用可变与不可变的 Set。SoredSet 是按内容排序存储；LinkedHashSet 是按插入顺序存储；ListSet 可以像使用 List 一样使用，按插入顺序反序存储。 Immutable Mutable Description BitSet ✓ ✓ A set of “non-negative integers represented as variable-size arrays of bits packed into 64-bit words.” Used to save memory when you have a set of integers. HashSet ✓ ✓ The immutable version “implements sets using a hash trie”; the mutable version “implements sets using a hashtable.” LinkedHashSet ✓ A mutable set implemented using a hashtable. Returns elements in the order in which they were inserted. ListSet ✓ A set implemented using a list structure. TreeSet ✓ ✓ The immutable version “implements immutable sets using a tree.” The mutable version is a mutable SortedSet with “an immutable AVL Tree as underlying data structure.” Set ✓ ✓ Generic base traits, with both mutable and immutable implementations. SortedSet ✓ ✓ A base trait. (Creating a variable as a SortedSet returns a TreeSet.) 集合和映射类型常用操作的性能特点： 是否可变类型 具体类型 lookup add remove min immutable HashSet/HashMap eC eC eC L immutable TreeSet/TreeMap Log Log Log Log immutable BitSet C L L eC1 immutable ListMap L L L L mutable HashSet/HashMap eC eC eC L mutable WeakHashMap eC eC eC L mutable BitSet C aC C eC1 mutable TreeSet Log Log Log Log 操作说明： 操作 说明 lookup 测试一个元素是否被包含在集合中，或者找出一个键对应的值 add 添加一个新的元素到一个集合中或者添加一个键值对到一个映射中。 remove 移除一个集合中的一个元素或者移除一个映射中一个键。 min 集合中的最小元素，或者映射中的最小键。 参考 Scala Set(集合) Scala 集合]]></content>
      <categories>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>教程</tag>
        <tag>Scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scala 教程：Collections（三）—— Map]]></title>
    <url>%2FScala%2FScala%2FScala%20%E6%95%99%E7%A8%8B%EF%BC%9ACollections%EF%BC%88%E4%B8%89%EF%BC%89%E2%80%94%E2%80%94%20Map%2F</url>
    <content type="text"><![CDATA[Map 是一种由键值对构成的可迭代对象，Scala 默认使用的是不可变 map，对 map 的任何修改都会生成一个新的 map，如果你想使用可变 map，需要引用 scala.collection.mutable.Map。 Map 创建Map 的一般创建方式： 12345678910111213// 创建空 Mapscala&gt; var A:Map[Char,Int] = Map()// 通过（k, v）二元组创建 Mapscala&gt; val colors1 = Map((&quot;red&quot;, &quot;#FF0000&quot;), (&quot;azure&quot;, &quot;#F0FFFF&quot;), (&quot;peru&quot;, &quot;#CD853F&quot;))// Scala 的 Predef 类提供了隐式转换，允许使用`key -&gt; vale`来代替`(key, value)`scala&gt; val colors2 = Map(&quot;blue&quot; -&gt; &quot;#0033FF&quot;, &quot;yellow&quot; -&gt; &quot;#FFFF00&quot;, &quot;red&quot; -&gt; &quot;#FF0000&quot;)// 如果需要使用可变 Map，需要引用 scala.collection.mutable.Mapimport scala.collection.mutableval colors = mutable.Map(&quot;red&quot; -&gt; &quot;#FF0000&quot;, &quot;azure&quot; -&gt; &quot;#F0FFFF&quot;) Map 操作基本操作Scala Map 有三个基本操作： keys：返回 Map 所有的键(key) values：返回 Map 所有的值(value) isEmpty：在 Map 为空时返回 true 1234567scala&gt; val colors = Map(&quot;red&quot; -&gt; &quot;#FF0000&quot;, &quot;azure&quot; -&gt; &quot;#F0FFFF&quot;, &quot;peru&quot; -&gt; &quot;#CD853F&quot;)scala&gt; println( &quot;colors 中的键为 : &quot; + colors.keys )scala&gt; println( &quot;colors 中的值为 : &quot; + colors.values )scala&gt; println( &quot;检测 colors 是否为空 : &quot; + colors.isEmpty ) 不可变 Map不可变 Map 的增、删、查、集操作： 查询： 123456789101112131415161718192021// 查询键对应的值：返回一个 Option，如果 key 存在，返回 Some(value)，否则返回 Nonescala&gt; println(colors1.get(&quot;red&quot;))scala&gt; println(colors1.get(&quot;black&quot;))Some(#FF0000)None// 查询键对应的值：如果 key 存在，返回对应的 value，否则报错scala&gt; println(colors1(&quot;red&quot;))scala&gt; println(colors1(&quot;black&quot;))#FF0000error// 查询键对应的值：如果 key 存在，返回对应的 value，否则返回默认值 dscala&gt; println(colors1.getOrElse(&quot;red&quot;, &quot;hello&quot;))#FF0000scala&gt; println(colors1.getOrElse(&quot;black&quot;, &quot;hello&quot;))hello// 查询键是否在 Map 中scala&gt; println(colors1.contains(&quot;black&quot;))false 添加： 12345678910111213scala&gt; colors1 + (&quot;blue&quot; -&gt; &quot;#0033FF&quot;, &quot;yellow&quot; -&gt; &quot;#FFFF00&quot;)Map(blue -&gt; #0033FF, azure -&gt; #F0FFFF, peru -&gt; #CD853F, yellow -&gt; #FFFF00, red -&gt; #FF0000) scala&gt; colors1 ++ colors2Map(blue -&gt; #0033FF, azure -&gt; #F0FFFF, peru -&gt; #CD853F, yellow -&gt; #FFFF00, red -&gt; #FF0000) 移除： 12345678scala&gt; colors1 - (&quot;red&quot;, &quot;azure&quot;)Map(peru -&gt; #CD853F)scala&gt; colors1 -- colors2.keysMap(azure -&gt; #F0FFFF, peru -&gt; #CD853F)scala&gt; colors1.filterKeys(x=&gt;Set(&quot;red&quot;, &quot;peru&quot;).contains(x))Map(red -&gt; #FF0000, peru -&gt; #CD853F) 可变 Map可变 Map 支持不可变 Map 的所有操作，同时还支持原地修改操作： 1234567891011121314151617181920212223242526scala&gt; val colors1 = mutable.Map(&quot;red&quot; -&gt; &quot;#FF0000&quot;, &quot;azure&quot; -&gt; &quot;#F0FFFF&quot;, &quot;peru&quot; -&gt; &quot;#CD853F&quot;)colors1: scala.collection.mutable.Map[String,String] = Map(azure -&gt; #F0FFFF, red -&gt; #FF0000, peru -&gt; #CD853F)scala&gt; val colors2 = mutable.Map(&quot;blue&quot; -&gt; &quot;#0033FF&quot;, | &quot;yellow&quot; -&gt; &quot;#FFFF00&quot;, | &quot;red&quot; -&gt; &quot;#FF0000&quot;)colors2: scala.collection.mutable.Map[String,String] = Map(yellow -&gt; #FFFF00, red -&gt; #FF0000, blue -&gt; #0033FF)scala&gt; colors1(&quot;red&quot;) = &quot;changed&quot;res29: scala.collection.mutable.Map[String,String] = Map(azure -&gt; #F0FFFF, red -&gt; changed, peru -&gt; #CD853F)scala&gt; colors1 += (&quot;blue&quot; -&gt; &quot;#0033FF&quot;, &quot;yellow&quot; -&gt; &quot;#FFFF00&quot;)res30: colors1.type = Map(yellow -&gt; #FFFF00, azure -&gt; #F0FFFF, red -&gt; changed, peru -&gt; #CD853F, blue -&gt; #0033FF)scala&gt; colors1 -= (&quot;blue&quot;, &quot;yellow&quot;)res32: colors1.type = Map(azure -&gt; #F0FFFF, red -&gt; changed, peru -&gt; #CD853F)scala&gt; colors1 ++= colors2res34: colors1.type = Map(yellow -&gt; #FFFF00, azure -&gt; #F0FFFF, red -&gt; #FF0000, peru -&gt; #CD853F, blue -&gt; #0033FF)scala&gt; colors1 --= colors2.keysres36: colors1.type = Map(azure -&gt; #F0FFFF, peru -&gt; #CD853F)scala&gt; colors1.getOrElseUpdate(&quot;black&quot;, &quot;new color&quot;) 可变 Map 和不可变 Map 相互转化可变 Map 和不可变 Map 可以通过 Seq 作为中间桥梁进行相互转化： 1234567891011scala&gt; val map_im = Map(&quot;a&quot;-&gt;&quot;1&quot;, &quot;b&quot;-&gt;&quot;2&quot;)map_im: scala.collection.immutable.Map[String,String] = Map(a -&gt; 1, b -&gt; 2)scala&gt; val map_mm = mutable.Map(&quot;a&quot;-&gt;&quot;1&quot;, &quot;b&quot;-&gt;&quot;2&quot;)map_mm: scala.collection.mutable.Map[String,String] = Map(b -&gt; 2, a -&gt; 1)scala&gt; mutable.Map(map_im.toSeq:_*)res24: scala.collection.mutable.Map[String,String] = Map(b -&gt; 2, a -&gt; 1)scala&gt; Map(map_mm.toSeq:_*)res25: scala.collection.immutable.Map[String,String] = Map(b -&gt; 2, a -&gt; 1) Map 遍历有三种常见的 Map 遍历方法： 123456789101112131415// 遍历关键字scala&gt; for (k &lt;- colors1.keys) &#123; println(s&quot;$&#123;k&#125;:$&#123;colors1(k)&#125;&quot;)&#125;// 遍历(key,value)二元组scala&gt; for (kv &lt;- colors1) &#123; println(s&quot;$&#123;kv._1&#125;:$&#123;kv._2&#125;&quot;)&#125;// 遍历(key,value)二元组，并复制给二元组scala&gt; for ((k, v) &lt;- colors1) &#123; println(s&quot;$k:$v&quot;)&#125;red:#FF0000azure:#F0FFFFperu:#CD853F Map 选择Map 像 Set 一样，可以直接使用可变的和不可变的Map。SortedMap不可变但是其内容是按key值排序的；LinkedHashMap是可变的，其内容按插入的顺序存储；ListMap则是按插入顺序反序存储；TreeMap是使用红黑树存储。 Immutable Mutable Description HashMap ✓ ✓ The immutable version “implements maps using a hash trie”; the mutable version “implements maps using a hashtable.” LinkedHashMap ✓ “Implements mutable maps using a hashtable.” Returns elements by the order in which they were inserted. ListMap ✓ ✓ A map implemented using a list data structure. Returns elements in the opposite order by which they were inserted, as though each element is inserted at the head of the map. Map ✓ ✓ The base map, with both mutable and immutable implementations. SortedMap ✓ A base trait that stores its keys in sorted order. (Creating a variable as a SortedMap currently returns a TreeMap.) TreeMap ✓ An immutable, sorted map, implemented as a red-black tree. WeakHashMap ✓ A hash map with weak references, it’s a wrapper around java.util.WeakHashMap. 参考 Scala Map(集合) Scala 集合]]></content>
      <categories>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>教程</tag>
        <tag>Scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scala 教程：Collections（四）—— Tuple]]></title>
    <url>%2FScala%2FScala%2FScala%20%E6%95%99%E7%A8%8B%EF%BC%9ACollections%EF%BC%88%E5%9B%9B%EF%BC%89%E2%80%94%E2%80%94%20Tuple%2F</url>
    <content type="text"><![CDATA[元组（Tuple）是一个异构、不可变的有序容器。元组存在的意义只是作为一个容纳多个值的容器，这可以免去创建那些简单的主要用于承载数据的类的麻烦.用户有时可能在元组和 case 类之间难以选择，通常，如果元素具有更多含义，则首选 case 类。 元组创建可以通过三种方式创建一个元组： 通过用逗号分隔的值写入值，并用一对括号括起来 12scala&gt; val t = (1, 1.2, &#x27;h&#x27;, &quot;hello&quot;)t: (Int, Double, Char, String) = (1,1.2,h,hello) 通过 Tuplen 类创建元组对象： 12scala&gt; val t3 = Tuple3(1, 1.2, &quot;hello&quot;)t3: (Int, Double, String) = (1,1.2,hello) 通过使用关系运算符 -&gt;创建二元组： 12scala&gt; val t = &#x27;a&#x27; -&gt; 1t: (Char, Int) = (a,1) Scala 中的元组包含一系列类：Tuple2、Tuple3 ... Tuple22，目前 Scala 支持的元组最大长度为 22。当我们创建一个包含 n 个元素（n 位于 2 和 22 之间）的元组时，Scala 基本上就是从上述的一组类中实例化一个相对应的类，使用组成元素的类型进行参数化；对于更大长度你可以使用集合，或者扩展元组。 元组操作访问元素使用下划线语法 tuple._n 可以取出第 n 个元素（假设有足够多元素）： 12scala&gt; t3._2res0: Double = 1.2 解构元组Scala 元组也支持解构： 1234scala&gt; val (a, b, c) = t3a: Int = 1b: Double = 1.2c: String = hello 元组解构也可用于模式匹配： 12345678910111213141516val planetDistanceFromSun = List((&quot;Mercury&quot;, 57.9), (&quot;Venus&quot;, 108.2), (&quot;Earth&quot;, 149.6 ), (&quot;Mars&quot;, 227.9), (&quot;Jupiter&quot;, 778.3))planetDistanceFromSun.foreach&#123; tuple =&gt; &#123; tuple match &#123; case (&quot;Mercury&quot;, distance) =&gt; println(s&quot;Mercury is $distance millions km far from Sun&quot;) case p if(p._1 == &quot;Venus&quot;) =&gt; println(s&quot;Venus is $&#123;p._2&#125; millions km far from Sun&quot;) case p if(p._1 == &quot;Earth&quot;) =&gt; println(s&quot;Blue planet is $&#123;p._2&#125; millions km far from Sun&quot;) case _ =&gt; println(&quot;Too far....&quot;) &#125; &#125;&#125;Mercury is 57.9 millions km far from SunVenus is 108.2 millions km far from SunBlue planet is 149.6 millions km far from SunToo far....Too far.... 迭代元组元组无法直接进行迭代，但可以通过Tuple.productIterator() 方法来迭代： 1234scala&gt; t3.productIterator.foreach&#123; i =&gt; println(&quot;Value = &quot; + i )&#125;Value = 1Value = 1.2Value = hello]]></content>
      <categories>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>教程</tag>
        <tag>Scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scala 教程：Collections（五）—— Option]]></title>
    <url>%2FScala%2FScala%2FScala%20%E6%95%99%E7%A8%8B%EF%BC%9ACollections%EF%BC%88%E4%BA%94%EF%BC%89%E2%80%94%E2%80%94%20Option%2F</url>
    <content type="text"><![CDATA[值 null 通常被滥用于表征一个可能会缺失的值，Java 开发者一般都知道 NullPointerException，通常这是由于某个方法返回了 null ，但这并不是开发者所希望发生的，代码也不好去处理这种异常。 在Scala中，Option 是 null 值的安全替代：Option[A] 是一个类型为 A 的可选值的容器，如果值存在，Option[A] 返回一个 Some[A] ，如果不存在， Option[A] 返回对象 None。在类型层面上指出一个值是否存在，使用你的代码的开发者（也包括你自己）就会被编译器强制去处理这种可能性，而不能依赖值存在的偶然性。 Option 创建 可以通过直接实例化 Some 样例类来创建一个 Option，或者在知道值缺失时，直接使用 None 对象： 12345scala&gt; val a: Option[Int] = Nonea: Option[Int] = Nonescala&gt; val b: Option[Int] = Some(1)b: Option[Int] = Some(1) 在实际工作中，不可避免地要去操作一些 Java 库，或者是其他将 null 作为缺失值的JVM 语言的代码，为此， Option 伴生对象提供了一个工厂方法，可以根据给定的参数创建相应的 Option： 12345scala&gt; val c: Option[String] = Option(null)c: Option[String] = Nonescala&gt; val d: Option[String] = Option(&quot;hello&quot;)d: Option[String] = Some(hello) Option 操作从 Option 抽取值有多种方法从 Option 中抽取值： get 属性：如果值存在，则返回值，如果不存在，则报错 12345678scala&gt; println(&quot;a.get: &quot; + a.get)java.util.NoSuchElementException: None.get at scala.None$.get(Option.scala:366) at scala.None$.get(Option.scala:364) ... 28 elidedscala&gt; println(&quot;b.get: &quot; + b.get)b.get: 1 getOrElse() 方法：如果值存在，则返回值，如果不存在，则返回默认值 12345scala&gt; println(&quot;a.getOrElse(0): &quot; + a.getOrElse(0) )a.getOrElse(0): 0scala&gt; println(&quot;b.getOrElse(10): &quot; + b.getOrElse(10) )b.getOrElse(10): 1 模式匹配：用模式匹配处理 Option 实例是非常啰嗦的，这也是它非惯用法的原因。所以，即使你很喜欢模式匹配，也尽量用其他方法吧 1234567891011scala&gt; a match &#123; | case Some(xx) =&gt; xx | case None =&gt; &quot;Nothing&quot; | &#125;res8: Any = Nothingscala&gt; b match &#123; | case Some(xx) =&gt; xx | case None =&gt; &quot;Nothing&quot; | &#125;res9: Any = 1 作为集合的 Option 你可以把 Option 看作是某种集合，这个特殊的集合要么只包含一个元素，要么就什么元素都没有。虽然在类型层次上，Option 并不是 Scala 的集合类型，但凡是你觉得 Scala 集合好用的方法， Option 也有，你甚至可以将其转换成一个集合，比如说 List。 1234567891011121314151617181920212223242526272829303132333435363738394041424344// 使用 .foreach scala&gt; a.foreach(x =&gt; println(x))scala&gt; b.foreach(x =&gt; println(x))1// 使用 mapscala&gt; a.map(x =&gt; x+100)res13: Option[Int] = Nonescala&gt; b.map(x =&gt; x+100)res11: Option[Int] = Some(101)// 使用 flatMap，如果用 map 将 x 映射为 Option 类型的值，映射结果类型就编程了嵌套 Option 类型，可以使用 flatMap 将结果打平，最终结果仍是 Option 类型scala&gt; a.map(x=&gt;Option(1))res2: Option[Option[Int]] = Nonescala&gt; b.map(x=&gt;Option(1))res3: Option[Option[Int]] = Some(Some(1))scala&gt; a.flatMap(x=&gt;Option(1))res6: Option[Int] = None scala&gt; b.flatMap(x=&gt;Option(1))res5: Option[Int] = Some(1)// 使用 filterscala&gt; a.filter(_ &gt; 30)res16: Option[Int] = Nonescala&gt; b.filter(_ &gt; 30)res17: Option[Int] = Nonescala&gt; b.filter(_ &lt; 30)res18: Option[Int] = Some(1)// 使用 isEmpty() 方法来检测元组中的元素是否为 Nonescala&gt; println(&quot;a.isEmpty: &quot; + a.isEmpty )scala&gt; println(&quot;b.isEmpty: &quot; + b.isEmpty )a.isEmpty: falseb.isEmpty: true// 使用 forscala&gt; for (t &lt;- a) println(t)scala&gt; for (t &lt;- b) println(t)1 参看Scala 初学者指南]]></content>
      <categories>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>教程</tag>
        <tag>Scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL：索引优化]]></title>
    <url>%2FMySQL%2FMysql%2FMySQL%EF%BC%9A%E7%B4%A2%E5%BC%95%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[What索引是存储引擎用于快速查找记录的一种数据结构。索引优化是优化查询性能最有效的手段，好的索引能够轻易将查询性能提高几个数量级。但索引并不总是最好的工具，只有当索引帮助存储引擎快速查找记录带来的好处大于其带来的额外工作时，索引才是有效的，一般来讲： 对于小表：全表扫描更高效； 对于中表：索引非常有效； 对于大表：建议采用分区； Lahdenmaki 和 Leach 在 Relational Database Index Design and the Optimizers 一书中介绍了如何评价一个索引是否适合某个查询的“三星系统”： 聚集：索引将相关的记录放到一起则获得“一星”； 有序：索引中的数据顺序和查找中的排列顺序一致则获得“二星”； 覆盖：索引中的列包含了查询中需要的全部列则获得“三星”； 索引类型索引是在存储引擎层而不是服务器层实现的，所以没有统一的索引标准：不同存储引擎所支持的索引类型不同，即使同一种类型的索引，其底层实现也可能不同。MySQL 中涉及的索引类型有： B-Tree 索引：当人们谈论索引的时候，如果没有特别指明索引类型，一般指的是 B-Tree 索引。大多数 Mysql 引擎都支持这种索引（Archive 例外），NDB 集群存储引擎实际上使用的是 T-Tree，但其名字是 BTREE，InnoDB 使用的是 B+Tree。不同存储引擎以不停方式使用 B-Tree，性能也有所不同，MyISAM 使用前缀压缩技术使得索引更小，InnoDB 则按照原始数据格式进行存储，MyISAM 通过数据的物理位置引用被索引的行，InnoDB 则根据主键引用被索引的行。 哈希索引：存储引擎会对每一行数据的所有索引列计算一个哈希值，哈希索引将所有的哈希值存储在索引中，并保存指向每个数据行的指针；哈希索引的访问速度非常快，但哈希索引只包含哈希值和行指针，无法避免读取行，也无法用于排序，此外哈希索引只支持等值和比较查询，不支持部分索引列匹配。 空间数据索引：MyISAM 表支持空间索引，可以用作地理数据存储。和 B-Tree 索引不同，这类索引无须前缀查询。空间索引会从所有维度来索引数据。查询时，可以有效地使用任意维度来组合查询。必须使用 MySQL 的 GIS 相关函数如 MBRCONTAINS() 等来维护数据。MySQL 的 GIS 支持并不完善，所以大部分人都不会使用这个特性。 全文索引：全文索引是一种特殊类型的索引，它查找的是文本中的关键词，而不是直接比较索引中的值。全文搜索和其他几类索引的匹配方式完全不一样。它有许多需要注意的细节，如停用词、词干和复数、布尔搜索等。全文索引更类似于搜索引擎做的事情，而不是简单的WHERE条件匹配。 本文接下来要讨论的是 InnoDB 所采用的 B+Tree 索引，这也是最常用的索引类型和实现方式。 B+TreeM 阶 B+Tree，满足以下特征： 根节点至少有两个子树，最多有 M 个子树，节点内关键字个数与子树个数相同，每个关键字作为子树的索引，等于对应子树中最大/小关键字； 中间节点至少有 M/2 个子树，最多有 M 个子树，节点内关键字个数与子树个数相同，每个关键字作为子树的索引，等于对应子树中最大/小关键字； 叶子节点包含了全部关键字，及指向对应记录的指针，叶子节点按关键字排序组成双向链表； B+Tree 可以进行两种查找运算： 从最小关键字开始，在叶子节点中顺序查找； 从根节点开始，向下进行二分查找； 相比 B-Tree，B+Tree 的优势： 每次可以读取更多的索引，减少 IO 操作； 查询性能稳定，只有达到子节点才能命中； 范围查询简便，对于范围查询，先从根节点出发找到最小索引，再在叶子节点中顺序查找至最大索引； 聚簇索引和二级索引InnoDB 存储引擎同时维护了两种索引存储方式，两种方式均通过 B+Tree 实现： 聚簇索引(Clustered Index)：也叫主键索引，以主键作为节点页，数据行作为叶子页；聚簇索引不能人为指定，只能自动生成；如果没有定义主键，InnodDB 会选择一个唯一的非空索引代替，如果没有这样的索引，InnoDB 会隐式定义一个主键来作为聚簇索引；一个表只能有一个聚簇索引，在聚簇索引树中，可以通过主键快读查询到对应的数据行。 二级索引(Secondary index)：也叫非聚簇索引，以索引作为节点页，主键作为叶子页；一个表可以有多个二级索引，使用二级索引查找数据时，如果查询列包含了二级索引没有覆盖的列，则需要先在二级索引树中查询到对应的主键，再在聚簇索引树中查询主键对应的数据行，称为”回表查询“，性能较扫一遍索引树差。 对于索引列值为 NULL 的二级索引记录来说，它们会被放在 B+ 树的最左边： How定义索引创建索引MySQL创建索引的语法如下： 123CREATE [UNIQUE|FULLTEXT|SPATIAL] INDEX index_name[USING index_type]ON table_name (index_col_name,...) 其中： [UNIQUE|FULLTEXT|SPATIAL]：分别代表唯一索引、全文索引、空间索引，如果不指定任何关键字，默认为普通索引； index_name：索引名称，用户自行定义； index_type：索引类型，存储引擎为 MyISAM 和 InnoDB 的表中只能使用 BTREE；存储引擎为 MEMORY 或者 HEAP 的表中可以使用 HASH 和 BTREE 两种类型的索引，默认值为 HASH； index_col_name：要创建索引的字段名称，可以针对多个字段创建联合索引，只需要在多个字段名称之间以英文逗号隔开即可；对于 CHAR 或 VARCHAR 类型的字段，我们还可以只使用字段内容前面的一部分来创建索引，只需要在对应的字段名称后面加上形如(length)的指令即可，表示只需要使用字段内容前面的length个字符来创建索引； 创建索引的语法有以下变体： 12ALTER TABLE table_nameADD [UNIQUE|FULLTEXT|SPATIAL] INDEX index_name (index_col_name,...) [USING index_type] 示例： 12345678910111213141516171819202122232425262728mysql&gt; show create table index_test;CREATE TABLE `index_test` ( `f_id` mediumint(9) NOT NULL AUTO_INCREMENT, `f_date` varchar(8) DEFAULT NULL COMMENT &#x27;日期&#x27;, `f_minute` varchar(12) NOT NULL DEFAULT &#x27;0&#x27; COMMENT &#x27;分钟&#x27;, `f_bizname` varchar(32) NOT NULL DEFAULT &#x27;&#x27; COMMENT &#x27;业务大类&#x27;, `f_pv` int(11) NOT NULL DEFAULT &#x27;0&#x27; COMMENT &#x27;pv&#x27;, `f_uv` int(11) NOT NULL DEFAULT &#x27;0&#x27; COMMENT &#x27;uv&#x27;, PRIMARY KEY (`f_id`)) ENGINE=InnoDB AUTO_INCREMENT=196606 DEFAULT CHARSET=utf8 COMMENT=&#x27;大类实时数据迁移至tube&#x27; -- 创建部分索引mysql&gt; alter table index_test add index idx_biz (f_bizname(6));-- 创建联合索引mysql&gt; alter table index_test add index idx_mix (f_date, f_minute, f_bizname);mysql&gt; show create table index_test;CREATE TABLE `index_test` ( `f_id` mediumint(9) NOT NULL AUTO_INCREMENT, `f_date` varchar(8) DEFAULT NULL COMMENT &#x27;日期&#x27;, `f_minute` varchar(12) NOT NULL DEFAULT &#x27;0&#x27; COMMENT &#x27;分钟&#x27;, `f_bizname` varchar(32) NOT NULL DEFAULT &#x27;&#x27; COMMENT &#x27;业务大类&#x27;, `f_pv` int(11) NOT NULL DEFAULT &#x27;0&#x27; COMMENT &#x27;pv&#x27;, `f_uv` int(11) NOT NULL DEFAULT &#x27;0&#x27; COMMENT &#x27;uv&#x27;, PRIMARY KEY (`f_id`), KEY `idx_biz` (`f_bizname`(6)), KEY `idx_mix` (`f_date`,`f_minute`,`f_bizname`)) ENGINE=InnoDB AUTO_INCREMENT=196606 DEFAULT CHARSET=utf8 COMMENT=&#x27;大类实时数据迁移至tube&#x27; 删除索引在MySQL中删除索引的方法非常简单，其完整语法如下： 123--删除指定表中指定名称的索引ALTER TABLE table_name DROP INDEX index_name; 修改索引在MySQL中并没有提供修改索引的直接指令，一般情况下，我们需要先删除掉原索引，再根据需要创建一个同名的索引，从而变相地实现修改索引操作。 12345--先删除ALTER TABLE userDROP INDEX idx_user_username;--再以修改后的内容创建同名索引CREATE INDEX idx_user_username ON user (username(8)); 查看索引在MySQL中，要查看某个数据库表中的索引也非常简单： 1234567891011121314-- 语法SHOW INDEX FROM table_name-- 示例mysql&gt; show index from index_test;+------------+------------+----------+--------------+-------------+-----------+-------------+----------+--------+------+------------+---------+---------------+| Table | Non_unique | Key_name | Seq_in_index | Column_name | Collation | Cardinality | Sub_part | Packed | Null | Index_type | Comment | Index_comment |+------------+------------+----------+--------------+-------------+-----------+-------------+----------+--------+------+------------+---------+---------------+| index_test | 0 | PRIMARY | 1 | f_id | A | 146475 | NULL | NULL | | BTREE | | || index_test | 1 | idx_biz | 1 | f_bizname | A | 130 | 6 | NULL | | BTREE | | || index_test | 1 | idx_mix | 1 | f_date | A | 4 | NULL | NULL | YES | BTREE | | || index_test | 1 | idx_mix | 2 | f_minute | A | 7709 | NULL | NULL | | BTREE | | || index_test | 1 | idx_mix | 3 | f_bizname | A | 146475 | NULL | NULL | | BTREE | | |+------------+------------+----------+--------------+-------------+-----------+-------------+----------+--------+------+------------+---------+---------------+ 其中： Collation：列以什么方式存储在索引中。在MySQL中，有值‘A’（升序）或 NULL（无分类）； Cardinality：索引基数，表示索引中不同值个数的预估值，值越大，索引的“区分度“（或者说”选择性“）越好； Sub_part：列如果被部分编入索引，则为编入索引的字符数，否则为 NULL； Packed：指示关键字如何被压缩，如果没有被压缩，则为NULL； Null：如果列含有NULL，则含有YES，如果没有，则该列含有NO； Index_type：索引类型； 使用索引下面的内容可能会因为 MySQL 版本的不同而有所差异，另外，查询优化器对索引的选择依赖于对索引统计信息的估计，这种估计可能是不准确的、随数据量变化的，前后两次执行同一个查询可能会出现使用不同索引的情况。 能否命中索引只有在表达式中正确使用了索引列，才能命中索引（possible_keys）： 索引列单独出现：索引列既不能是表达式的一部分，也不能是函数的参数；特别地，如果索引列发生了隐式类型转换，可能会使得索引不可用； 精确匹配最左前缀 + 范围匹配后一列：对于联合索引，索引前几列采用精确匹配，后一列采用范围匹配，而范围匹配索引列后面（按索引定义中的顺序）的索引都将失效，有以下几种特殊形态 精确匹配最左前缀：使用的索引列构成了联合索引的某种前缀，索引列的使用顺序无关紧要，因为查询优化器会将其优化为何索引定义的顺序； 全值匹配：精确匹配所有索引列，精确匹配指的是 =； 范围匹配：使用 &gt;、&lt;、&gt;=、&lt;=，!= 、&lt;&gt;，between，或 LIKE 不以通配符(%)开头的索引列与常量值比较作为范围条件，如果在联合索引中使用了范围索引，那么在范围索引后面的索引会失效（在10.1.9-MariaDB实测，范围条件后面的索引也会生效，可能是版本带来的差异）； 最左前缀原则： 示例： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123-- 索引结构mysql&gt; show create table index_test;CREATE TABLE `index_test` ( `f_id` mediumint(9) NOT NULL AUTO_INCREMENT, `f_date` int(11) NOT NULL DEFAULT &#x27;0&#x27; COMMENT &#x27;日期&#x27;, `f_minute` varchar(12) NOT NULL DEFAULT &#x27;0&#x27; COMMENT &#x27;分钟&#x27;, `f_bizname` varchar(32) NOT NULL DEFAULT &#x27;&#x27; COMMENT &#x27;业务大类&#x27;, `f_pv` int(11) NOT NULL DEFAULT &#x27;0&#x27; COMMENT &#x27;pv&#x27;, `f_uv` int(11) NOT NULL DEFAULT &#x27;0&#x27; COMMENT &#x27;uv&#x27;, `f_const` int(11) NOT NULL DEFAULT &#x27;0&#x27;, PRIMARY KEY (`f_id`), KEY `idx_mix` (`f_date`,`f_minute`,`f_bizname`), KEY `idx_const` (`f_const`)) ENGINE=InnoDB AUTO_INCREMENT=2032124 DEFAULT CHARSET=utf8 COMMENT=&#x27;大类实时数据迁移至tube&#x27;-- 索引明细mysql&gt; show index from index_test;+------------+------------+-----------+--------------+-------------+-----------+-------------+----------+--------+------+------------+---------+---------------+| Table | Non_unique | Key_name | Seq_in_index | Column_name | Collation | Cardinality | Sub_part | Packed | Null | Index_type | Comment | Index_comment |+------------+------------+-----------+--------------+-------------+-----------+-------------+----------+--------+------+------------+---------+---------------+| index_test | 0 | PRIMARY | 1 | f_id | A | 1191444 | NULL | NULL | | BTREE | | || index_test | 1 | idx_mix | 1 | f_date | A | 44 | NULL | NULL | | BTREE | | || index_test | 1 | idx_mix | 2 | f_minute | A | 70084 | NULL | NULL | | BTREE | | || index_test | 1 | idx_mix | 3 | f_bizname | A | 1191444 | NULL | NULL | | BTREE | | || index_test | 1 | idx_const | 1 | f_const | A | 2 | NULL | NULL | | BTREE | | |+------------+------------+-----------+--------------+-------------+-----------+-------------+----------+--------+------+------------+---------+---------------+-- 不带 WHERE 条件，且查询了非索引字段，全表扫描MariaDB [db_data_anlysis]&gt; explain -&gt; select * -&gt; from index_test;+------+-------------+------------+------+---------------+------+---------+------+---------+-------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+------+-------------+------------+------+---------------+------+---------+------+---------+-------+| 1 | SIMPLE | index_test | ALL | NULL | NULL | NULL | NULL | 1191444 | |+------+-------------+------------+------+---------------+------+---------+------+---------+-------+-- 不带 WHERE 条件，count(*) 直接在二级索引树种就可以计算出来MariaDB [db_data_anlysis]&gt; explain -&gt; select count(*) -&gt; from index_test;+------+-------------+------------+-------+---------------+-----------+---------+------+---------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+------+-------------+------------+-------+---------------+-----------+---------+------+---------+-------------+| 1 | SIMPLE | index_test | index | NULL | idx_const | 4 | NULL | 1191444 | Using index |+------+-------------+------------+-------+---------------+-----------+---------+------+---------+-------------+-- 按主键/索引排序，索引扫描MariaDB [db_data_anlysis]&gt; explain -&gt; select * -&gt; from index_test -&gt; order by f_id;+------+-------------+------------+-------+---------------+---------+---------+------+---------+-------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+------+-------------+------------+-------+---------------+---------+---------+------+---------+-------+| 1 | SIMPLE | index_test | index | NULL | PRIMARY | 3 | NULL | 1191444 | |+------+-------------+------------+-------+---------------+---------+---------+------+---------+-------+-- 索引列不能作为表达式的一部分MariaDB [db_data_anlysis]&gt; explain -&gt; select * -&gt; from index_test -&gt; where f_date - 1= 20200622;+------+-------------+------------+------+---------------+------+---------+------+---------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+------+-------------+------------+------+---------------+------+---------+------+---------+-------------+| 1 | SIMPLE | index_test | ALL | NULL | NULL | NULL | NULL | 1191444 | Using where |+------+-------------+------------+------+---------------+------+---------+------+---------+-------------+-- 索引列不能作为函数的参数MariaDB [db_data_anlysis]&gt; explain -&gt; select * -&gt; from index_test -&gt; where cast(f_date as unsigned) = 20200622;+------+-------------+------------+------+---------------+------+---------+------+---------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+------+-------------+------------+------+---------------+------+---------+------+---------+-------------+| 1 | SIMPLE | index_test | ALL | NULL | NULL | NULL | NULL | 1191444 | Using where |+------+-------------+------------+------+---------------+------+---------+------+---------+-------------+-- 索引查找，只使用了联合索引idx_mix中第一个字段MariaDB [db_data_anlysis]&gt; explain -&gt; select * -&gt; from index_test -&gt; where f_date = &#x27;&#x27;;+------+-------------+------------+------+---------------+---------+---------+-------+------+-------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+------+-------------+------------+------+---------------+---------+---------+-------+------+-------+| 1 | SIMPLE | index_test | ref | idx_mix | idx_mix | 4 | const | 1 | |+------+-------------+------------+------+---------------+---------+---------+-------+------+-------+-- 索引查找，只使用了联合索引idx_mix中前两个字段，索引使用顺序不影响MariaDB [db_data_anlysis]&gt; explain -&gt; select * -&gt; from index_test -&gt; where f_minute=&#x27;202006220001&#x27; and f_date = &#x27;20200622&#x27;;+------+-------------+------------+------+---------------+---------+---------+-------------+------+-----------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+------+-------------+------------+------+---------------+---------+---------+-------------+------+-----------------------+| 1 | SIMPLE | index_test | ref | idx_mix | idx_mix | 42 | const,const | 39 | Using index condition |+------+-------------+------------+------+---------------+---------+---------+-------------+------+-----------------------+-- 全值匹配MariaDB [db_data_anlysis]&gt; explain -&gt; select * -&gt; from index_test -&gt; where f_minute=&#x27;202006220001&#x27; and f_date = &#x27;20200622&#x27; and f_bizname=&#x27;首页&#x27;;+------+-------------+------------+------+---------------+---------+---------+-------------------+------+-----------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+------+-------------+------------+------+---------------+---------+---------+-------------------+------+-----------------------+| 1 | SIMPLE | index_test | ref | idx_mix | idx_mix | 140 | const,const,const | 1 | Using index condition |+------+-------------+------------+------+---------------+---------+---------+-------------------+------+-----------------------+-- 最左前缀 + 范围查询，范围后的索引列仍会有效（在我测试的10.1.9-MariaDB版本是这样）MariaDB [db_data_anlysis]&gt; explain -&gt; select * -&gt; from index_test -&gt; where f_minute=&#x27;202006220001&#x27; and f_bizname=&#x27;首页&#x27; and f_date between 20200622 and 20200623;+------+-------------+------------+-------+---------------+---------+---------+------+--------+-----------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+------+-------------+------------+-------+---------------+---------+---------+------+--------+-----------------------+| 1 | SIMPLE | index_test | range | idx_mix | idx_mix | 140 | NULL | 101998 | Using index condition |+------+-------------+------------+-------+---------------+---------+---------+------+--------+-----------------------+ 是否使用索引查询优化器即使正确定义并命中了索引，最终的执行计划是否会使用索引仍要取决于查询优化器的判断，优化器会对比所有可能的方案，找出成本最低的方案作为最终的执行计划，大致过程如下： 根据查找条件找出所有可能使用的索引，计算使用不同索引执行查询的代价； 计算全表扫描的代价； 选择代价最低的方案作为执行计划； Mysql 的代价模型比较复杂，感兴趣的话可以参考 MySQL 5.7 代价模型浅析，这里不对细节进行深究。 范围索引范围访问方法使用单个索引来检索包含在一个或多个索引值区间内的表行的子集，这里的单个索引可以是单列索引或联合索引。 对于单列索引，可以通过 WHERE 子句中的条件表达式来表示范围条件，当使用 &gt;、&lt;、&gt;=、&lt;=，between，!= 、&lt;&gt; 或 LIKE 不以通配符(%)开头的索引列与常量值比较时，是个范围条件，多个范围条件使用 OR 或 AND 组合而成。 对于联合索引，区间可能适用于 AND 的条件组合，其中每个条件使用 =、&lt;=&gt;、IS NULL、&gt;、&gt;=、&lt;、&lt;=、!= 、&lt;&gt;、BETWEEN 或 LIEK 模糊匹配(不能通配符开头) 来比较索引和常量值。 对于范围索引，查询优化器会通过“索引统计信息”来估计每个范围的行数，如果行数占表总行比例较大，且不会触发覆盖索引，查询优化器会放弃使用范围索引，直接使用全表扫描： 12345678910111213141516171819202122-- 只是修改了数据范围，就改变了索引选择；实验表明，如果减少表中整体的数据量，以下两个都不会走索引MariaDB [db_data_anlysis]&gt; explain -&gt; select * -&gt; from index_test -&gt; where f_date between 20200620 and 20200621 -&gt; ;+------+-------------+------------+-------+---------------+---------+---------+------+--------+-----------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+------+-------------+------------+-------+---------------+---------+---------+------+--------+-----------------------+| 1 | SIMPLE | index_test | range | idx_mix | idx_mix | 4 | NULL | 115016 | Using index condition |+------+-------------+------------+-------+---------------+---------+---------+------+--------+-----------------------+1 row in set (0.03 sec)MariaDB [db_data_anlysis]&gt; explain -&gt; select * -&gt; from index_test -&gt; where f_date between 20200618 and 20200619;+------+-------------+------------+------+---------------+------+---------+------+---------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+------+-------------+------------+------+---------------+------+---------+------+---------+-------------+| 1 | SIMPLE | index_test | ALL | idx_mix | NULL | NULL | NULL | 1191444 | Using where |+------+-------------+------------+------+---------------+------+---------+------+---------+-------------+ 覆盖索引如果一个索引包含（覆盖）所有需要查询的字段（主键以外的字段），我们就称之为“覆盖索引”。覆盖索引可以直接在二级索引树种查询到所有字段，无需进行回表查询，可以极大提高查询性能。 12345678910111213-- (f_minute, f_bizname) 是主键，(f_date, f_minute, f_bizname) 是一个联合索引explain select f_date, f_minute, f_biznamefrom index_test where f_date=&#x27;20200622&#x27; and f_minute like &#x27;2020062200%&#x27;-- Extra 列中的 Using index 表示使用的是覆盖索引+------+-------------+------------+-------+---------------+---------+---------+------+------+--------------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+------+-------------+------------+-------+---------------+---------+---------+------+------+--------------------------+| 1 | SIMPLE | index_test | range | idx_mix | idx_mix | 42 | NULL | 2160 | Using where; Using index |+------+-------------+------------+-------+---------------+---------+---------+------+------+--------------------------+ 但如果索引不能覆盖查询所需的全部列，那就不得不每扫描一条索引记录就回表查询一次对应的行。这基本上都是随机I/O，因此按索引顺序读取数据的速度通常要比顺序地全表扫描慢，尤其是在I/O密集型的工作负载时。 1234567891011121314151617181920212223242526272829303132MariaDB [db_data_anlysis]&gt; pager cat &gt; /dev/null;MariaDB [db_data_anlysis]&gt; select f_date, f_minute, f_bizname -&gt; from index_test -&gt; order by 1,2,3;1195949 rows in set (1.63 sec)MariaDB [db_data_anlysis]&gt; select f_date, f_minute, f_bizname, f_uv -&gt; from index_test -&gt; order by 1,2,3;1195949 rows in set (4.00 sec)-- 排序没有走索引，而是进行了全表扫描MariaDB [db_data_anlysis]&gt; explain -&gt; select f_date, f_minute, f_bizname, f_uv -&gt; from index_test -&gt; order by 1,2,3;+------+-------------+------------+------+---------------+------+---------+------+---------+----------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+------+-------------+------------+------+---------------+------+---------+------+---------+----------------+| 1 | SIMPLE | index_test | ALL | NULL | NULL | NULL | NULL | 1191444 | Using filesort |+------+-------------+------------+------+---------------+------+---------+------+---------+----------------+1 row in set (0.02 sec)MariaDB [db_data_anlysis]&gt; explain -&gt; select f_date, f_minute, f_bizname -&gt; from index_test -&gt; order by 1,2,3;+------+-------------+------------+-------+---------------+---------+---------+------+---------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+------+-------------+------------+-------+---------------+---------+---------+------+---------+-------------+| 1 | SIMPLE | index_test | index | NULL | idx_mix | 140 | NULL | 1191444 | Using index |+------+-------------+------------+-------+---------------+---------+---------+------+---------+-------------+ 延迟关联如果查询字段中包含了该索引（以及主键）以外的字段，便无法使用覆盖索引，此时可以通过一种“延迟关联”的技术先在子查询中通过覆盖索引查询对应主键集合，再通过主键关联其余字段。这样就不用每次都进行回表查询，减少了磁盘 I/O 读取次数。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647-- f_uv 不在索引内，无法使用覆盖索引MariaDB [db_data_anlysis]&gt; explain -&gt; select f_date, f_minute, f_bizname, f_uv -&gt; from index_test -&gt; where f_date between &#x27;20200620&#x27; and &#x27;20200622&#x27; -&gt; ;+------+-------------+------------+------+---------------+------+---------+------+---------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+------+-------------+------------+------+---------------+------+---------+------+---------+-------------+| 1 | SIMPLE | index_test | ALL | idx_mix | NULL | NULL | NULL | 1191444 | Using where |+------+-------------+------------+------+---------------+------+---------+------+---------+-------------+1 row in set (0.03 sec)-- f_date, f_minute, f_bizname 在索引内，可以使用覆盖索引MariaDB [db_data_anlysis]&gt; explain -&gt; select f_date, f_minute, f_bizname -&gt; from index_test -&gt; where f_date between &#x27;20200620&#x27; and &#x27;20200622&#x27; -&gt; ;+------+-------------+------------+-------+---------------+---------+---------+------+--------+--------------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+------+-------------+------------+-------+---------------+---------+---------+------+--------+--------------------------+| 1 | SIMPLE | index_test | range | idx_mix | idx_mix | 4 | NULL | 217974 | Using where; Using index |+------+-------------+------------+-------+---------------+---------+---------+------+--------+--------------------------+1 row in set (0.03 sec)-- 延迟关联，子查询使用覆盖索引，a 使用索引查找MariaDB [db_data_anlysis]&gt; explain -&gt; select -&gt; a.* -&gt; from -&gt; index_test a -&gt; join -&gt; (select f_date, f_minute, f_bizname -&gt; from index_test -&gt; where f_date between &#x27;20200620&#x27; and &#x27;20200622&#x27; -&gt; ) b -&gt; on a.f_bizname = b.f_bizname and a.f_minute = b.f_minute -&gt; ;+------+-------------+------------+-------+-----------------+---------+---------+--------------------------------------+--------+--------------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+------+-------------+------------+-------+-----------------+---------+---------+--------------------------------------+--------+--------------------------+| 1 | SIMPLE | index_test | range | idx_biz,idx_mix | idx_mix | 4 | NULL | 217974 | Using where; Using index || 1 | SIMPLE | a | ref | idx_biz | idx_biz | 20 | db_data_anlysis.index_test.f_bizname | 10451 | Using where |+------+-------------+------------+-------+-----------------+---------+---------+--------------------------------------+--------+--------------------------+2 rows in set (0.03 sec) 索引提示MySQL 支持通过“索引提示”显式地告诉优化器使用哪个索引，这在以下两种情况下可能会用到： 查询优化器错误地选择了某个索引：这种情况在最新的 MySQL 版本中已经非常少见了； 查询可选索引非常多，导致选择执行计划的事件开销很大：这是可以通过索引提示强行指定索引来完成查询； 常用的 Index Hint 语法如下：如果是对主键的 Hint，可以用 PRI 指代主键 1234567891011121314151617-- USE INDEX 只是告诉优化器可以选择该索引，但实际上优化器还是会再根据自己的判断进行选择SELECT *FROM tUSE INDEX(index_a)WHERE a=1 and b=2-- FORCE INDEX 可以强制优化器使用该索引SELECT *FROM tFORCE INDEX(index_a)WHERE a=1 and b=2-- IGNORE INDEX 可以强制优化器不使用某个索引SELECT *FROM tIGNORE INDEX(index_a)WHERE a=1 and b=2 辅助信息查看查询耗时在执行完查询语句之后可以通过 show profiles 查看查询时间： 1234567891011121314151617181920212223-- 查看profiling 是否是on状态mysql&gt; show variables like &#x27;%profiling%&#x27;;-- 如果是off，则 mysql&gt; set profiling = 1;-- 执行自己的sql语句mysql&gt; select f_date, f_minute, f_bizname from index_test where f_minute = 202006200001 and f_bizname = &#x27;首页&#x27; +----------+--------------+-----------+| f_date | f_minute | f_bizname |+----------+--------------+-----------+| 20200620 | 202006200001 | 首页 |+----------+--------------+-----------+1 row in set (0.20 sec)-- show profiles 查看语句执行时间mysql&gt; show profiles;+----------+------------+------------------------------------------------------------------------------------------------------------+| Query_ID | Duration | Query |+----------+------------+------------------------------------------------------------------------------------------------------------+| 1 | 0.16595042 | select f_date, f_minute, f_biznamefrom index_test where f_minute = 202006200001 and f_bizname = &#x27;首页&#x27; |+----------+------------+------------------------------------------------------------------------------------------------------------+ 查看索引统计信息MySQL 执行 SQL 会经过 SQL 解析和查询优化的过程，解析器将 SQL 分解成数据结构并传递到后续步骤，查询优化器发现执行 SQL 查询的最佳方案、生成执行计划。查询优化器决定 SQL 如何执行，依赖于数据库的统计信息，下面我们介绍MySQL 5.7中innodb统计信息的相关内容。 MySQL统计信息的存储分为两种，非持久化和持久化统计信息。 非持久化统计信息非持久化统计信息存储在内存里，如果数据库重启，统计信息将丢失，有两种方式可以设置为非持久化统计信息： 全局变量，INNODB_ STATS_PERSISTENT=OFF CREATE/ALTER 表的参数，STATS_PERSISTENT=0 非持久化统计信息在以下情况会被自动更新： 执行 ANALYZE TABLE 语句 innodb_stats_on_metadata=ON 情况下，执 SHOW TABLE STATUS, SHOW INDEX, 查询 INFORMATION_SCHEMA下的TABLES, STATISTICS 启用 —auto-rehash 功能情况下，使用 mysql client 登录 表第一次被打开 距上一次更新统计信息，表 1/16 的数据被修改 非持久化统计信息的缺点显而易见，数据库重启后如果大量表开始更新统计信息，会对实例造成很大影响，所以目前都会使用持久化统计信息。 持久化统计信息5.6.6开始，MySQL默认使用了持久化统计信息，即 INNODB_STATS_PERSISTENT=ON ，持久化统计信息保存在表 mysql.innodb_table_stats 和 mysql.innodb_index_stats 。 持久化统计信息在以下情况会被自动更新： INNODB_ STATS _AUTO_RECALC=ON 情况下，表中 10% 的数据被修改 增加新的索引 innodb_table_stats 是表的统计信息： 字段 含义 database_name 数据库名 table_name 表名 last_update 统计信息最后一次更新时间 n_rows 表的行数 clustered_index_size 聚集索引的页的数量 sum_of_other_index_sizes 其他索引的页的数量 innodb_index_stats 是索引的统计信息： 字段 含义 database_name 数据库名 table_name 表名 index_name 索引名 last_update 统计信息最后一次更新时间 stat_name 统计信息名 stat_value 统计信息的值 sample_size 采样大小 stat_description 类型说明 示例： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748CREATE TABLE t1 ( a INT, b INT, c INT, d INT, e INT, f INT, PRIMARY KEY (a, b), KEY i1 (c, d), UNIQUE KEY i2uniq (e, f)) ENGINE=INNODB;mysql&gt; select * from t1;+---+---+------+------+------+------+| a | b | c | d | e | f |+---+---+------+------+------+------+| 1 | 1 | 10 | 11 | 100 | 101 || 1 | 2 | 10 | 11 | 200 | 102 || 1 | 3 | 10 | 12 | 100 | 103 || 1 | 4 | 10 | 12 | 200 | 104 |+---+---+------+------+------+------+mysql&gt; select last_update, n_rows, clustered_index_size, sum_of_other_index_sizesfrom mysql.innodb_table_stats where table_name=&#x27;t1&#x27;+---------------------+--------+----------------------+--------------------------+| last_update | n_rows | clustered_index_size | sum_of_other_index_sizes |+---------------------+--------+----------------------+--------------------------+| 2020-07-02 15:42:11 | 4 | 1 | 2 |+---------------------+--------+----------------------+--------------------------+mysql&gt; select index_name, stat_name, stat_value, stat_descriptionfrom mysql.innodb_index_stats where table_name=&#x27;t1&#x27;+------------+--------------+------------+-----------------------------------+| index_name | stat_name | stat_value | stat_description |+------------+--------------+------------+-----------------------------------+| PRIMARY | n_diff_pfx01 | 1 | a || PRIMARY | n_diff_pfx02 | 4 | a,b || PRIMARY | n_leaf_pages | 1 | Number of leaf pages in the index || PRIMARY | size | 1 | Number of pages in the index || i1 | n_diff_pfx01 | 1 | c || i1 | n_diff_pfx02 | 2 | c,d || i1 | n_diff_pfx03 | 2 | c,d,a || i1 | n_diff_pfx04 | 4 | c,d,a,b || i1 | n_leaf_pages | 1 | Number of leaf pages in the index || i1 | size | 1 | Number of pages in the index || i2uniq | n_diff_pfx01 | 2 | e || i2uniq | n_diff_pfx02 | 4 | e,f || i2uniq | n_leaf_pages | 1 | Number of leaf pages in the index || i2uniq | size | 1 | Number of pages in the index |+------------+--------------+------------+-----------------------------------+ stat_name=size 时：stat_value 表示索引页的数量 stat_name=n_leaf_pages 时： stat_value 表示叶子节点的数量 stat_name=n_diff_pfxNN 时： stat_value 表示索引字段上唯一值的数量，此处做一下具体说明： n_diff_pfx01 表示索引第一列 distinct 之后的数量，如 PRIMARY 的a列，只有一个值1，所以 index_name=’PRIMARY’ and stat_name=’n_diff_pfx01’ 时， stat_value=1； n_diff_pfx02 表示索引前两列 distinct 之后的数量，如 i2uniq 的 e,f 列，有4个值，所以 index_name=’i2uniq’ and stat_name=’n_diff_pfx02’ 时， stat_value=4； 对于非唯一索引，会在原有列之后加上主键索引，如 index_name=’i1’ and stat_name=’n_diff_pfx03’ ，在原索引列c,d后加了主键列 a，(c,d,a) 的 distinct 结果为2； 了解了 stat_name 和 stat_value 的具体含义，就可以协助我们排查SQL执行时为什么没有使用合适的索引，例如某个索引 n_diff_pfxNN 的 stat_value 远小于实际值，查询优化器认为该索引选择度较差，就有可能导致使用错误的索引。 统计信息不准确的处理我们查看执行计划，发现未使用正确的索引，如果是 innodb_index_stats 中统计信息差别较大引起，可通过以下方式处理： 手动更新统计信息 ANALYZE TABLE TABLE_NAME，注意执行过程中会加读锁； 如果更新后统计信息仍不准确，可考虑增加表采样的数据页，两种方式可以修改： 全局变量 INNODB_STATS_PERSISTENT_SAMPLE_PAGES，默认为20； 单个表可以指定该表的采样：ALTER TABLE TABLE_NAME STATS_SAMPLE_PAGES=40； 经测试，此处 STATS_SAMPLE_PAGES 的最大值是65535，超出会报错。 目前MySQL并没有提供直方图的功能，某些情况下（如数据分布不均）仅仅更新统计信息不一定能得到准确的执行计划，只能通过 index hint 的方式指定索引，新版本8.0会增加直方图功能。 索引设计原则实践中的一些经验法则： 在使用频繁的条件字段上建立索引：根据表上最频繁的查询语句，在 WHERE 条件字段或 JOIN 关联字段上建立索引，关联字段尽量保持和关联表字段相同的数据类型； 在选择性好的字段上建索引：选择性可以用索引基数来衡量，尽量选择不同值较多的字段建立索引，如果索引基数很小，就没有建索引的必要了，建了优化器可能也不会选择，除非是为了满足最左前缀的需求； 过长的字段可以在列前缀上建立索引：在类似 url 这样长度特别大的字段上建立索引，会增加系统开销，如果前几个字段就有不错的选择性，只需要在其前缀上建立索引就好了； 定义联合索引以实现覆盖索引：覆盖索引能够带来性能的极大提升，可以考虑把需要经常查询的字段也加到索引中去； 联合索引选择合适的索引列顺序：需要综合考虑满足频繁查询的最左前缀，以及高选择性字段在前的原则； 减少冗余索引：优化器在选择索引的时候会计算各种索引方案的代价，索引太多也会影响查询性能 如果创建了索引(A, B) 再创建 (A) 就是冗余索引，因为这只是一个索引前缀 如果创建了索引(A, B) 再创建 (A, PRIMARY) 也是冗余索引，因为主键已将包含在二级索引树中了 删除重复索引，重复索引是指在相同列上按相同顺序创建的相同类型的索引，如已经将ID作为主键了，再添加先的索引 INDEX(ID) 删除未使用的索引 理解索引是如何工作的非常重要，应该根据这些理解来创建最合适的索引，而不是根据一些诸如“在多列索引中将选择性最高的列放在第一列”或“应该为WHERE子句中出现的所有列创建索引”之类的经验法则及其推论。 参考 漫画：什么是 B+树 B树和B+树 MySQL统计信息系列 我以为我对Mysql索引很了解，直到我遇到了阿里的面试官 MySQL · 特性分析 · 5.7 代价模型浅析 MySQL系列(二十)：优化 之 范围查询优化]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux：定时任务]]></title>
    <url>%2FLinux%2FTools%2FLinux%EF%BC%9A%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[在 Linux 里配置定时任务主要是通过 cron 和 crontab 两个程序来控制: cron：是定期执行指定命令的守护程序，负责配置的解析和处理； crontab：是用户维护定时任务配置文件的命令； Cron1234567891011$ man cronNAME cron - daemon to execute scheduled commandsSYNOPSIS cron [-n | -p | -s | -m&lt;mailcommand&gt;] cron -x [ext,sch,proc,pars,load,misc,test,bit]DESCRIPTION Cron should be started from /etc/rc.d/init.d or /etc/init.d... ... 启动 CronCron 可以通过 /etc/rc.d/init.d 或 /etc/init.d 路径下的 crond 脚本随系统开机自动启动，也可以通过以下方式手动启动： 12345678910# 如果系统有 service 命令/sbin/service crond start # 启动服务/sbin/service crond stop # 关闭服务/sbin/service crond restart # 重启服务/sbin/service crond reload # 重新载入配置# 如果系统没有 service 命令$sudo /etc/init.d/cron start$sudo /etc/init.d/cron stop$sudo /etc/init.d/cron restart 查看服务是否已经在运行： 123$ ps -ax | grep cron 13651 ? Ss 5:50 /usr/sbin/crond -n48218 pts/17 S+ 0:00 grep --color=auto cron 配置文件Cron 平时处于休眠状态，每分钟醒来一次，检查以下路径中配置文件的最新修改时间，并重新加载已改变的内容，所以即使修改了 crontab 文件也没有必要重新启动 cron 守护程序： /var/spool/cron/user：用户调度的配置文件，不同用户可以拥有独立的 crontab 配置文件，每个文件以对应用户名命名，事实上，当用户通过 crontab 命令修改定时任务配置时，修改的就是这里的文件； 12345678910111213141516171819202122[root@100-117-147-160 /etc/rc.d/init.d]# cd /var/spool/cron[root@100-117-147-160 /var/spool/cron]# ll总用量 12-rw------- 1 root root 0 7月 4 2017 adm-rw------- 1 root root 0 7月 4 2017 arpwatch-rw------- 1 root root 0 7月 4 2017 bin-rw------- 1 root root 0 7月 4 2017 daemon-rw------- 1 root root 0 7月 4 2017 dbus-rw------- 1 root root 0 7月 4 2017 ftp-rw------- 1 root root 0 7月 4 2017 games-rw------- 1 root root 0 7月 4 2017 gopher-rw------- 1 root root 0 7月 4 2017 haldaemon-rw------- 1 root root 0 7月 4 2017 halt-rw------- 1 root root 0 7月 4 2017 lp-rw------- 1 root root 0 7月 4 2017 mail-rw------- 1 mqq mqq 6711 6月 16 13:19 mqq[root@100-117-147-160 /var/spool/cron]# cat mqq#wsd script manage add by hansli# add by tafServer, mon_taf_node.sh * * * * * (cd /usr/local/app/taf/tafnode/util/ &amp;&amp; ./mon_taf_node.sh &gt;&gt; /usr/local/app/taf/app_log/taf/tafnode/mon_taf_node.log) &gt;/dev/null 2&gt;&amp;1... ... /etc/crontab：系统调度的配置文件， 123456789101112131415# 配置crond任务运行的环境变量## SHELL变量指定了系统要使用哪个shellSHELL=/bin/bash## PATH变量指定了系统执行命令的路径PATH=/sbin:/bin:/usr/sbin:/usr/bin## MAILTO变量指定了crond的任务执行信息将通过电子邮件发送给root用户，如果MAILTO变量的值为空，则表示不发送任务执行信息给用户MAILTO=root## HOME变量指定了在执行命令或者脚本时使用的主目录HOME=/# run-parts 遍历文件夹内的所有配置文件51 * * * * root run-parts /etc/cron.hourly24 7 * * * root run-parts /etc/cron.daily22 4 * * 0 root run-parts /etc/cron.weekly42 4 1 * * root run-parts /etc/cron.monthly /etc/cron.daily：作为系统管理员，定时任务模式大都是每小时触发、每日触发、每周触发等，这时可以不用配置 cron 项，只需要把需要定时执行的脚本放在对应的 /etc/cron.daily、/etc/cron.hourly 之类的文件下就行了，事实上很多系统自身需要的定时任务就是这么做的； /etc/cron.d/：有时，某些处理特定任务的进程也希望能够创建定时任务，比如编写或安装的第三方任务，这些任务不希望依附于某一个用户，而希望拥有独立的配置文件，方便修改和卸载。这时就可以新建一个 cron 配置文件，放置于/etc/cron.d/ 目录下，进行统一管理，像 csf，lfd 这类进程就是这样做的。 CrontabCrontab 命令格式命令格式： 1234567891011121314151617181920212223242526272829303132333435363738$ man crontabNAME：为不同用户维护 crontab 文件 crontab - maintain crontab files for individual usersSYNOPSIS crontab [-u user] file 表示将 file 做为crontab 的任务列表文件并载入 crontab。如果在命令行中没有指定这个文件，crontab命令将接受标准输入（键盘）上键入的命令，并将它们载入crontab。会用 file 中的内容覆盖当前配置文件，不推荐 crontab [-u user] [-l | -r | -e | -s] [-i] default operation is replace -u, --user 用来设定某个用户的crontab服务 edit some other user&#x27;s crontab -l, --list 显示某个用户的crontab文件内容，如果不指定用户，则表示显示当前用户的crontab文件内容 list user&#x27;s crontab -r, --remove 从/var/spool/cron目录中删除某个用户的crontab文件，如果不指定用户，则默认删除当前用户的crontab文件，千万别乱运行crontab -r。它从Crontab目录（/var/spool/cron）中删除用户的Crontab文件。删除了该用户的所有crontab都没了。 delete user&#x27;s crontab -e, --edit 编辑某个用户的crontab文件内容。如果不指定用户，则表示编辑当前用户的crontab文件 edit user&#x27;s crontab -s, --show 显示有 crontab 任务的所有用户 show all user who have a crontab -i, --ask 在删除用户的crontab文件时给确认提示 prompt before deleting user&#x27;s crontabDESCRIPTION Crontab 是一个程序，用于让用户以传统 cron 格式安装、卸载或列出定时作业 Crontab is the program used to let users install, deinstall or list recurrent jobs in the legacy cron format. 每个用户都可以有自己的 crontab文件，尽管这些是 /var/spool/cron/crontab` 中的文件，但通常不会直接编辑它们 Each user can have their own crontab, and though these are files in /var/spool/cron/crontabs, they are not intended to be edited directly. These jobs are then automatically translated in systemd Timers &amp; Units by systemd-crontab-generator. FILES：Crontab 通过 /etc/cron.allow 和 /etc/cron.deny 来控制用户权限，只有在白名单里的用户才能使用 crontab 命令，在黑名单里的用户无法使用 crontab 命令，原则上两个文件不能同时存在，如果同时存在，只有白名单会生效，如果两个文件都不存在，根据 linux 版本不同，有的默认所有用户都有权限，有的默认只有 root 用户才有权限 /var/spool/cron/crontabs Directory for users crontabs. /etc/cron.allow list of users that can use crontab /etc/cron.deny list of users that aren&#x27;t allowed to use crontab (by default, only root can use crontab) 示例： 123456789101112[running]mqq@100.117.147.160:/etc/cron.d$ crontab -l#wsd script manage add by hansli# add by tafServer, mon_taf_node.sh * * * * * (cd /usr/local/app/taf/tafnode/util/ &amp;&amp; ./mon_taf_node.sh &gt;&gt; /usr/local/app/taf/app_log/taf/tafnode/mon_taf_node.log) &gt;/dev/null 2&gt;&amp;1# added by hangruan for wenzhen_union_test# 10 03 * * * (python /usr/local/app/dataanalysis/daf_proj/DafFramework.py --module=IhInqueryMessageChart --jobs=union --dest=test --source=test &gt;/dev/null 2&gt;&amp;1)* * * * * sleep 10; sh /data/jaysonding/address/port_monitor.sh &gt;&gt; /tmp/port_monitor.log 2&gt;&amp;1[running]mqq@100.117.147.160:/etc/cron.d$ crontab -e打开 vim 编辑器 crontab 文件格式用户所建立的crontab文件中，每一行都代表一项任务，每行的每个字段代表一项设置，它的格式共分为六个字段，前五段是时间设定段，第六段是要执行的命令段，格式如下： 在以上各个字段中，还可以使用以下特殊字符： ““代表所有的取值范围内的数字，如月份字段为，则表示1到12个月； “/“代表每一定时间间隔的意思，如分钟字段为*/10，表示每10分钟执行1次。 “-“代表从某个区间范围，是闭区间。如“2-5”表示“2,3,4,5”，小时字段中0-23/2表示在0~23点范围内每2个小时执行一次。 “,”分散的数字（不一定连续），如1,2,3,4,7,9。 示例： 1234567891011# 每分钟执行一次 myCommand* * * * * myCommand# 每隔两天的上午8点到11点的第3和第15分钟执行3,15 8-11 */2 * * myCommand# 每周六、周日的1 : 10重启smb10 1 * * 6,0 /etc/init.d/smb restart# 晚上11点到早上7点之间，每隔一小时重启smb0 23-7 * * * /etc/init.d/smb restart 说明：可以通过Crontab表达式执行时间验证来验证写好的Crontab表达式的实际执行效果 其他问题执行结果重定向每条任务调度执行完毕，系统都会将任务输出信息通过电子邮件的形式发送给当前系统用户，这样日积月累，日志信息会非常大，可能会影响系统的正常运行，因此，将每条任务进行重定向处理非常重要。 例如，可以在crontab文件中设置如下形式，忽略日志输出: 1234# 忽略日志输出 2&gt;&amp;1 代表执行结果及错误信息0 */3 * * * /usr/local/apache2/apachectl restart &gt;/dev/null 2&gt;&amp;1# 也可重定向到指定文件0 */3 * * * /usr/local/apache2/apachectl restart &gt;/dev/my.log 2&gt;&amp;1 环境变量问题有时我们创建了一个crontab，但是这个任务却无法自动执行，而手动执行这个任务却没有问题，这种情况一般是由于在crontab文件中没有配置环境变量引起的。 在crontab文件中定义多个调度任务时，需要特别注环境变量的设置，因为我们手动执行某个任务时，是在当前shell环境下进行的，程序当然能找到环境变量，而系统自动执行任务调度时，是不会加载任何环境变量的，因此，就需要在crontab文件中指定任务运行所需的所有环境变量，这样，系统执行任务调度时就没有问题了。 不要假定cron知道所需要的特殊环境，它其实并不知道。所以你要保证在shelll脚本中提供所有必要的路径和环境变量，除了一些自动设置的全局变量。所以注意如下3点： （1）脚本中涉及文件路径时写全局路径； （2）脚本执行要用到java或其他环境变量时，通过source命令引入环境变量，如: 12345cat start_cbp.sh!/bin/shsource /etc/profileexport RUN_CONF=/home/d139/conf/platform/cbp/cbp_jboss.conf/usr/local/jboss-4.0.5/bin/run.sh -c mev &amp; （3）当手动执行脚本OK，但是crontab死活不执行时,很可能是环境变量惹的祸，可尝试在crontab中直接引入环境变量解决问题。如: 10 * * * * . /etc/profile;/bin/sh /var/www/java/audit_no_count/bin/restart_audit.sh 系统级任务调度与用户级任务调度系统级任务调度主要完成系统的一些维护操作，用户级任务调度主要完成用户自定义的一些任务，可以将用户级任务调度放到系统级任务调度来完成（不建议这么做），但是反过来却不行，root用户的任务调度操作可以通过”crontab –uroot –e”来设置，也可以将调度任务直接写入/etc/crontab文件，需要注意的是，如果要定义一个定时重启系统的任务，就必须将任务放到/etc/crontab文件，即使在root用户下创建一个定时重启系统的任务也是无效的。 杂项 在crontab中%是有特殊含义的，表示换行的意思。如果要用的话必须进行转义%，如经常用的date ‘+%Y%m%d’在crontab里是不会执行的，应该换成date ‘+%Y%m%d’； 新创建的cron job，不会马上执行，至少要过2分钟才执行。如果重启cron则马上执行； 当crontab失效时，可以尝试/etc/init.d/crond restart解决问题。或者查看日志看某个job有没有执行/报错tail -f /var/log/cron； 千万别乱运行crontab -r。它从Crontab目录（/var/spool/cron）中删除用户的Crontab文件。删除了该用户的所有crontab都没了； 更新系统时间时区后需要重启cron； 参考 Linux 下定时任务配置深入理解 Linux 命令搜索 Crontab表达式执行时间验证 Linux Tools Quick Tutorial]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据科学：时间序列（一）—— ARIMA]]></title>
    <url>%2F%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%2F%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%2F%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%EF%BC%9A%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%EF%BC%88%E4%B8%80%EF%BC%89%E2%80%94%E2%80%94%20ARIMA%2F</url>
    <content type="text"><![CDATA[自回归差分移动平均(Autoregressive Integrated Moving Average, ARIMA)模型。 原理平稳过程平稳过程是一种特殊的随机过程，可分为强平稳过程和弱平稳过程： 强平稳过程（Strict(ly) stationary process）：时间序列中给定长度的任意两段子序列联合概率分布相同。可以推知强平稳过程 $x_t$ 的期望和方差不随t变化； 弱平稳过程：时间序列满足均值平稳性（stationary in mean）和二阶平稳性（secondary order stationary） 均值平稳性：均值不变，与t无关 二阶平稳性：$X(t)$ 与 $X(t-k)$ 的协方差只与滞后阶数k有关，与t无关，即相关系数只取决于时间间隔而与时间起点无关 到目前为止，时间序列分析基本上是以平稳时间序列为基础的，对于非平稳时间序列的统计分析，其方法和理论都很有局限性。强平稳过程在实际中几乎不可能被满足，通常情况下时间序列分析讨论的是弱平稳过程。但是非平稳序列一般都可以通过对数和差分方法转化为平稳序列，消除（或减小）时间序列的趋势和季节性。直观上看，平稳序列的时序图应该始终在一个常数值附近随机波动，而且波动的范围有明显的相似性特点。 自相关性定义： $rt$ 和 $r{t-k}$ 的相关系数称为 $r_t$ 的间隔为 $k$ 的自相关系数。 在弱平稳假设下，这个间隔为 k 的自相关系数与时间 t 无关，而仅仅与间隔 k 有关，由 $\rho_k $ 表示。由相关系数的定义可知： ACF 自相关图：X(t) 时间序列与 X(t-1), X(t-2),.., X(t-k)各阶之后时间序列之间的相关系数与滞后阶数 k 的关系。 如果 k 阶滞后序列 X(t-k)与原始序列 X(t)的相关系数在 [-0.2, 0.2] 之间，则称 X(t-k)与 X(t)不具有相关性。如果一个时间序列的任何一阶滞后序列与原始序列都不具有相关性，则该时间序列不具有自相关性 白噪声序列：均值为 0，且不具有自相关性的平稳序列 ARp 阶自回归模型 $AR(p)$可以表示如下： y_{t} = c + \phi_{1}y_{t-1} + \phi_{2}y_{t-2} + \dots + \phi_{p}y_{t-p} + \varepsilon_{t}其中： $c$：常数项； $y_{t-k}$：前k期历史值； ${\varepsilon _{t}}$：白噪声； 对于一个 $AR(1)$ 模型而言： 当 $ϕ_1=0$ 时, $y_t$ 相当于白噪声； 当 $ϕ_1=1$ 并且 $c=0$ 时, $y_t$ 相当于随机游走模型； 当 $ϕ_1=1$ 并且 $c≠0$ 时, $y_t$ 相当于带漂移的随机游走模型； 当 $ϕ_1&lt;0$ 时, $y_t$ 倾向于在正负值之间上下浮动; 我们通常将自回归模型的应用限制在平稳数据上，并且对回归系数也施加一些约束条件： 对于 $AR(1)$ 模型：$−1&lt;ϕ1&lt;1$； 对于 $AR(2)$ 模型：$−1&lt;ϕ2&lt;1$，$ϕ_1+ϕ_2&lt;1$，$ϕ_2−ϕ_1&lt;1$ 。 当 $p≥3$ 时，约束条件会更为复杂一些。R在估计自回归模型时可以解决这个问题 优点与限制： 必须具有自相关，自相关系数（$\varphi _{i}$）是关键。如果自相关系数(R)小于0.5，则不宜采用，否则预测结果极不准确； 自回归只能适用于预测与自身前期相关的经济现象，即受自身历史因素影响较大的经济现象，如矿的开采量，各种自然资源产量等；对于受社会因素影响较大的经济现象，不宜采用自回归，而应改采可纳入其他变数的向量自回归模型； MA不同于使用预测变量的历史值来进行回归，移动平均模型（moving average model）使用历史预测误差来建立一个类似回归的模型。q 阶移动平均模模型 $MA(q)$ 可以表示如下： y_{t} = c + \varepsilon_t + \theta_{1}\varepsilon_{t-1} + \theta_{2}\varepsilon_{t-2} + \dots + \theta_{q}\varepsilon_{t-q},其中： $c$：常量； $\varepsilon_t$ ：白噪声; 任何一个 $AR(p)$ 模型其实都是可以用一个 $MA(∞)$模型来表示： \begin{align*} y_t &= \phi_1y_{t-1} + \varepsilon_t\\ &= \phi_1(\phi_1y_{t-2} + \varepsilon_{t-1}) + \varepsilon_t\\ &= \phi_1^2y_{t-2} + \phi_1 \varepsilon_{t-1} + \varepsilon_t\\ &= \phi_1^3y_{t-3} + \phi_1^2\varepsilon_{t-2} + \phi_1 \varepsilon_{t-1} + \varepsilon_t\\ &\text{etc.} \end{align*}假定中 $−1&lt;ϕ_1&lt;1$ ， $ϕ^k_1$ 的值会随着 k 的增大而减小，最终可以得到下式： y_t = \varepsilon_t + \phi_1 \varepsilon_{t-1} + \phi_1^2 \varepsilon_{t-2} + \phi_1^3 \varepsilon_{t-3} + \cdots,如果我们对MA模型的系数加以限制，那么我们也可以反过来用AR模型来表示MA模型。这样的MA模型被称为可逆的(invertible)。也就是说，我们可以将任何一个可逆的 $MA(q)$ 模型表示为一个 $AR(∞)$ 模型。可逆模型不仅仅是让我们足以将任何MA模型转化为AR模型，它们还拥有很多非常棒的数学性质。 其他模型关于可逆性的限制和平稳性的限制类似： 对于MA(1)模型: $−1&lt;θ_1&lt;1$ 对于MA(2)模型: $−1&lt;θ_2−1$，$θ_1−θ_2&lt;1$ 对于 $q≥3$的其他更复杂的情况，R会在在估计模型时解决这个问题 差分延迟算子 B 是一个重要的标记，它被用于表示时间序列的延迟，当 B 被用于 $y_t$ 时，意味着将时间反向回溯一个单位时段： B^k y_{t} = y_{t - k}$y_t$ 的 d 阶差分可以表示为： (1 - B)^{d} y_{t}ARIMA当我们将差分和自回归模型以及移动平均模型结合起来的时候，我们可以得到一个非季节性 ARIMA 模型。ARIMA 是 AutoRegressive Integrated Moving Average 的简称。（在这里Integrated指的是差分的逆过程) ARIMA模型的表示如下： \begin{equation} y'_{t} = c + \phi_{1}y'_{t-1} + \cdots + \phi_{p}y'_{t-p} + \theta_{1}\varepsilon_{t-1} + \cdots + \theta_{q}\varepsilon_{t-q} + \varepsilon_{t} \end{equation}上式中 $y′_t$ 是差分序列（它可能经过多次差分）。右侧的“预测变量”包括 $y_t$ 的延迟值和延迟的误差。我们将这个模型称为ARIMA( p,d,q ) 模型，参数含义如下： 123p = 自回归模型阶数d = 差分阶数q = 移动平均模型阶数 我们之前讨论的很多模型其实都是ARIMA模型的特殊情况，如下表所示： 类型 参数 白噪声 ARIMA(0,0,0) 随机游走模型 ARIMA(0,1,0) with no constant 带漂移的随机游走模型 ARIMA(0,1,0) with a constant 自回归模型 ARIMA(p,0,0) 移动平均模型 ARIMA(0,0,q) 一旦我们开始组合不同模型来形成复杂的模型，延迟算子就会显得格外简便。ARIMA( p,d,q ) 可以被表示为: \begin{equation} \begin{array}{c c c c} (1-\phi_1B - \cdots - \phi_p B^p) & (1-B)^d y_{t} &= &c + (1 + \theta_1 B + \cdots + \theta_q B^q)\varepsilon_t\\ {\uparrow} & {\uparrow} & &{\uparrow}\\ \text{AR(p)} & \text{d differences} & & \text{MA(q)}\\ \end{array} \end{equation}实践ARIMA 模型使用的一般流程： 根据时间序列的散点图、自相关函数和偏自相关函数图识别其平稳性。 对非平稳的时间序列数据进行平稳化处理。直到处理后的自相关函数和偏自相关函数的数值非显著非零。 根据所识别出来的特征建立相应的时间序列模型。平稳化处理后，若偏自相关函数是截尾的，而自相关函数是拖尾的，则建立AR模型；若偏自相关函数是拖尾的，而自相关函数是截尾的，则建立MA模型；若偏自相关函数和自相关函数均是拖尾的，则序列适合ARMA模型。 参数估计，检验是否具有统计意义。 假设检验，判断（诊断）残差序列是否为白噪声序列。 利用已通过检验的模型进行预测。 时间序列预处理 平稳性检验 时序图检验：根据平稳时间序列的均值和方差都为常数的性质，平稳序列的时序图显示该序列值始终在一个常数附近随机波动，而且波动的范围有界；如果有明显的趋势性或者周期性，那它通常不是平稳序列 自相关图检验：平稳序列具有短期相关性，这个性质表明对平稳序列而言通常只有近期的序列值对现时值得影响比较明显，间隔越远的过去值对现时值得影响越小。随着延迟期数k的增加，平稳序列的自相关系数会比较快的衰减趋向于零，并在零附近随机波动，而非平稳序列的自相关系数衰减的速度比较慢。 单位根检验：最常用，原理待补充 白噪声检验：一般是构造检验统计量来检验序列的纯随机性，常用的检验统计量有Q统计量、LB统计量，由样本各延迟期数的自相关系数可以计算得到检验统计量，然后计算出对应的p值，如果p值显著大于显著性水平a，则表示该序列不能拒绝纯随机的原假设，可以停止对该序列的分析 平稳化处理 取对数 取差分 参数选择自相关图和偏自相关图如果数据来自于 ARIMA(p, d, 0)或者ARIMA(0, d, q)模型，则自相关图和偏自相关图 在判定 p 或者 q 的取值时非常有用。如果 p 和 q 都为正，则这些图在寻找最合适的 p 和 q 值时不再有用。 截尾：自相关函数（ACF）或偏自相关函数（PACF）在某阶后在0附近随机波动 拖尾：始终有非零取值，不会在k大于某个常数后就恒等于零 可以通过自相关和偏自相关图的截尾还是拖尾来选择模型和阶数： AR模型：自相关系数拖尾，偏自相关系数截尾； MA模型：自相关系数截尾，偏自相关函数拖尾； ARMA模型：自相关函数和偏自相关函数均拖尾。 如果你的图片成这样，估计十有八九是一个ARMA模型了。自相关7阶拖尾（n从7开始缩至置信区间），偏自相关2阶拖尾。 极大似然估计AIC信息准则（Akaike Information Criterion）在选择用于回归模型的变量时非常有用， 同样在确定ARIMA模型阶数时也可以发挥很大作用。它可以写作: \text{AIC} = -2 \log(L) + 2(p+q+k+1),这里的 L 是数据的似然函数,当 $c≠0$ 时 $k=1$ 且当 $c=0$ 时 $k=0$。需要注意到是式子中的最后一项是模型中参数的个数（包括残差方差 $σ2$ ） 对于ARIMA模型而言，修正过的AIC值可以被表示为： \text{AICc} = \text{AIC} + \frac{2(p+q+k+1)(p+q+k+2)}{T-p-q-k-2},BIC信息准则（Bayesian Information Criterion）的表示方式如下: \text{BIC} = \text{AIC} + [\log(T)-2](p+q+k+1).通过最小化AIC，AICc或者BIC我们都可以得到最优模型。其中我们倾向于使用AICc。有一点需要格外注意：这些信息准则在选择模型的合适的差分阶数（ d ）时效果并不好，只能被用于选择 p 和 q 的取值。这是因为差分改变了似然函数计算所使用数据，这会使得不同差分阶数的模型的AIC值无法比较。因此我们需要用其他方法先选择出合适的 d ，然后再通过 AICc 来选择 p 和 q 的取值。 参数选择如果您有一个SARIMAX（p，d，q）x（p，d，q，s）模型，快速的封套计算表明您至少需要： 12345678910111213141516d+d*s+max（3*q+1，3*q*s+1，p，p*s）+1个观测值（0，0，0）x（1，0，0，52）=&gt;0+0+52+1=53（0，0，0）x（1，1，0，52）=&gt;0+52+52+1=105（0，0，0）x（2，1，0，52）=&gt;0+52+104+1=157以及：（0，0，0）x（0，0，1，52）=&gt;0+0+3*52+1+1=158（0，0，0）x（0，1，1，52）=&gt;0+52+3*52+1+1=210（0，0，0）x（1，1，1，52）=&gt;0+52+3*51+1+1=210 参考 Python时间序列&amp;ARIMA股票预测 时间序列分析2 ARIMA（python） ARIMA时间序列模型 什么是时间序列模型，什么是弱平稳和严格平稳？ 预测：方法与实践 12 滑动平均模型]]></content>
      <categories>
        <category>数据科学</category>
      </categories>
      <tags>
        <tag>数据科学</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 科学计算：Numpy]]></title>
    <url>%2FPython%2FPython%2FPython%20%E7%A7%91%E5%AD%A6%E8%AE%A1%E7%AE%97%EF%BC%9ANumpy%2F</url>
    <content type="text"><![CDATA[NumPy(Numerical Python) 是 Python 的一个扩展库，主要用于多维数组 (ndarray) 的快速计算，通常与 SciPy 和 Matplotlib 一起使用来代替 MatLab 做科学计算，也有助于我们通过 Python 学习数据科学或者机器学习。 任何笔记都难以穷尽所有细节，笔记的真正意义在于提纲挈领，在需要时能够快速唤醒大部分核心知识，更多的细节请参考官方文档或Google： NumPy 官网 http://www.numpy.org/ Numpy 中文官网 https://www.numpy.org.cn/ 1import numpy as np 数据类型内置数据类型numpy 支持的数据类型比 Python 内置的类型要多很多，基本上可以和 C 语言的数据类型对应上，其中部分类型对应为 Python 内置的类型。下表列举了常用 NumPy 基本类型： 名称 描述 bool_ 布尔型数据类型（True 或者 False） int_ 默认的整数类型（类似于 C 语言中的 long，int32 或 int64） intc 与 C 的 int 类型一样，一般是 int32 或 int 64 intp 用于索引的整数类型（类似于 C 的 ssize_t，一般情况下仍然是 int32 或 int64） int8 字节（-128 to 127） int16 整数（-32768 to 32767） int32 整数（-2147483648 to 2147483647） int64 整数（-9223372036854775808 to 9223372036854775807） uint8 无符号整数（0 to 255） uint16 无符号整数（0 to 65535） uint32 无符号整数（0 to 4294967295） uint64 无符号整数（0 to 18446744073709551615） float_ float64 类型的简写 float16 半精度浮点数，包括：1 个符号位，5 个指数位，10 个尾数位 float32 单精度浮点数，包括：1 个符号位，8 个指数位，23 个尾数位 float64 双精度浮点数，包括：1 个符号位，11 个指数位，52 个尾数位 complex_ complex128 类型的简写，即 128 位复数 complex64 复数，表示双 32 位浮点数（实数部分和虚数部分） complex128 复数，表示双 64 位浮点数（实数部分和虚数部分） 数据类型对象（dtype）数据类型对象描述了数组对应的内存区域如何使用，包括 数据的类型：整数，浮点数或者 Python 对象 数据的大小：例如， 整数使用多少个字节存储 数据的字节顺序：”&lt;”意味着小端法(最小值存储在最小的地址，即低位组放在最前面)。”&gt;”意味着大端法(最重要的字节存储在最小的地址，即高位组放在最前面) 结构化类型的字段名和字段类型：结构化类型指的是每个元素的类型，如果结构化类型为 [(filed_name1, filed_type1), (filed_name2, filed_type2)]，那么数组对应的元素 (a, b)，a 的字段名为filed_name1，字段类型为 filed_type1，b 的字段名为filed_name2，字段类型为 filed_type2； numpy.dtype(object, align, copy) object - 要转换为的数据类型对象 align - 如果为 true，填充字段使其类似 C 的结构体。 copy - 复制 dtype 对象 ，如果为 false，则是对内置数据类型对象的引用 每个内建类型都有一个唯一定义它的字符代码，如下： 字符 对应类型 b 布尔型 i (有符号) 整型 u 无符号整型 integer f 浮点型 c 复数浮点型 m timedelta（时间间隔） M datetime（日期时间） O (Python) 对象 S, a (byte-)字符串 U Unicode V 原始数据 (void) 1234print np.dtype(np.int32)# int8, int16, int32, int64 四种数据类型可以使用字符串 &#x27;i1&#x27;, &#x27;i2&#x27;,&#x27;i4&#x27;,&#x27;i8&#x27; 代替print np.dtype(&#x27;i4&#x27;)print np.dtype(&#x27;&lt;i4&#x27;) int32 int32 int32 1234567# 结构化类型：元素的类型为结构化类型，元素的每个域可以用(字段名, 字段类型)组成student = np.dtype([(&#x27;name&#x27;,&#x27;S20&#x27;), (&#x27;age&#x27;, &#x27;i1&#x27;), (&#x27;marks&#x27;, &#x27;f4&#x27;)]) # a 是一个一维数组，每个元素是一个三元组，元素类型是结构化类型a = np.array([(&#x27;abc&#x27;, 21, 50),(&#x27;xyz&#x27;, 18, 75)], dtype = student) # 可以通过结构类型的字段名来引用该列print a[&#x27;name&#x27;]a [&#39;abc&#39; &#39;xyz&#39;] array([(&#39;abc&#39;, 21, 50.), (&#39;xyz&#39;, 18, 75.)], dtype=[(&#39;name&#39;, &#39;S20&#39;), (&#39;age&#39;, &#39;i1&#39;), (&#39;marks&#39;, &#39;&lt;f4&#39;)]) 数组创建NumPy的主要对象是同构多维数组，可以使用数组创建API中详述的各种方法来创建数组。 通过构造函数创建numpy.array(object, dtype = None, copy = True, order = None, subok = False, ndmin = 0) 名称 描述 object 数组或嵌套的数列 dtype 数组元素的数据类型，可选 copy 对象是否需要复制，可选 order 创建数组的样式，C为行方向，F为列方向，A为任意方向（默认） subok 默认返回一个与基类类型一致的数组 ndmin 指定生成数组的最小维度 1234567li = [[3 * i + j for j in range(3)] for i in range(2)]a = np.array(li)ac = np.array(li, order=&#x27;C&#x27;)af = np.array(li, order=&#x27;F&#x27;)print &#x27;默认方向：\n&#123;&#125;&#x27;.format(a)print &#x27;C行方向：\n&#123;&#125;&#x27;.format(ac)print &#x27;F列方向：\n&#123;&#125;&#x27;.format(af) 默认方向： [[0 1 2] [3 4 5]] C行方向： [[0 1 2] [3 4 5]] F列方向： [[0 1 2] [3 4 5]] 通过 numpy 方法创建 numpy.empty(shape, dtype = float, order = &#39;C&#39;) ：创建指定形状和类型的数组，数组元素以随机值来填充； numpy.zeros(shape, dtype = float, order = &#39;C&#39;)：创建指定形状和类型的数组，数组元素以 0 来填充； numpy.ones(shape, dtype = None, order = &#39;C&#39;)：创建指定形状和类型的数组，数组元素以 1 来填充； numpy.eye(N, M=None, k=0, dtype=&lt;class &#39;float&#39;&gt;, order=&#39;C&#39;)：返回一个二维数组，对角线为1，其他位置为0； 1np.empty([3,2], dtype = int) array([[140719574482952, 140719574514864], [ 4531377328, 4517086152], [ 6, 0]]) 1np.zeros([3,2], dtype = float, order = &#x27;C&#x27;) array([[0., 0.], [0., 0.], [0., 0.]]) 1np.ones([3,2], dtype = None, order = &#x27;C&#x27;) array([[1., 1.], [1., 1.], [1., 1.]]) 1np.eye(3,4,1) array([[0., 1., 0., 0.], [0., 0., 1., 0.], [0., 0., 0., 1.]]) 通过数值范围创建 numpy.arange(start, stop, step, dtype): 根据 start 与 stop 指定的范围以及 step 设定的步长，生成一个 ndarray np.linspace(start, stop, num=50, endpoint=True, retstep=False, dtype=None): 根据 start 与 stop 指定的数量生成一个等差数列，endpoint 表示是否包含stop的值，retstep代表是否显示步长 np.logspace(start, stop, num=50, endpoint=True, base=10.0, dtype=None): 根据 start 与 stop 指定的数量生成一个等比数列，base是对数的时候log的下标 1np.arange(5, dtype = float) array([0., 1., 2., 3., 4.]) 1np.linspace(1, 10, 10, endpoint=False, retstep=True) (array([1. , 1.9, 2.8, 3.7, 4.6, 5.5, 6.4, 7.3, 8.2, 9.1]), 0.9) 1np.logspace(1, 6, 6, base=2) array([ 2., 4., 8., 16., 32., 64.]) 通过坐标函数创建numpy.fromfunction(function, shape, **kwargs) 通过作用于坐标上的函数构建数组 function：function 是一个python函数，参数个数等于shape的维度，例如shape=(2,2)，传给 function 的参数将是 array([[0, 0], [1, 1]]) 和 array([[0, 1], [0, 1]])，分别是 shape=(2,2) 的行标数组和列标数组 shape：输出数组的shape 1np.fromfunction(lambda i, j: i == j, (3, 3), dtype=int) array([[ True, False, False], [False, True, False], [False, False, True]]) 数组属性 属性 说明 ndarray.ndim 秩，即轴的数量或维度的数量 ndarray.shape 数组的维度，对于矩阵，n 行 m 列 ndarray.size 数组元素的总个数，相当于 .shape 中 n*m 的值 ndarray.dtype ndarray 对象的元素类型 ndarray.itemsize ndarray 对象中每个元素的大小，以字节为单位 ndarray.flags ndarray 对象的内存信息 ndarray.real ndarray元素的实部 ndarray.imag ndarray 元素的虚部 ndarray.data 包含实际数组元素的缓冲区，由于一般通过数组的索引获取元素，所以通常不需要使用这个属性。 12345678a = np.array([[1,2,3],[4,5,6]]) print &#x27;a 的秩：&#123;&#125;&#x27;.format(a.ndim)print &#x27;a 的形状：&#123;&#125;&#x27;.format(a.shape)print &#x27;a 的元素个数：&#123;&#125;&#x27;.format(a.size)print &#x27;a 的元素类型：&#123;&#125;&#x27;.format(a.dtype)print &#x27;a 的元素大小：&#123;&#125;&#x27;.format(a.itemsize)print &#x27;a 的内存信息：\n&#123;&#125;&#x27;.format(a.flags)a a 的秩：2 a 的形状：(2, 3) a 的元素个数：6 a 的元素类型：int64 a 的元素大小：8 a 的内存信息： C_CONTIGUOUS : True F_CONTIGUOUS : False OWNDATA : True WRITEABLE : True ALIGNED : True WRITEBACKIFCOPY : False UPDATEIFCOPY : False array([[1, 2, 3], [4, 5, 6]]) 1234# 关于一维数组的形状print &#x27;一维数组形状：\n&#123;&#125;&#x27;.format(np.arange(3).shape)print &#x27;二维数组形状：\n&#123;&#125;&#x27;.format(np.arange(3).reshape(1,3))print &#x27;二维数组形状：\n&#123;&#125;&#x27;.format(np.arange(3).reshape(3,1)) 一维数组形状： (3,) 二维数组形状： [[0 1 2]] 二维数组形状： [[0] [1] [2]] 数组方法这里只是对众多数组方法的简单索引，每种方法的详细使用细则参见数组方法。 数组转换 方法 描述 ndarray.item(*args) 将数组元素复制到标准Python标量并返回它。 ndarray.tolist() 将数组作为a.ndim-levels深层嵌套的Python标量列表返回。 ndarray.itemset(*args) 将标量插入数组（如果可能，将标量转换为数组的dtype） ndarray.tostring([order]) 构造包含数组中原始数据字节的Python字节。 ndarray.tobytes([order]) 构造包含数组中原始数据字节的Python字节。 ndarray.tofile(fid[, sep, format]) 将数组作为文本或二进制写入文件（默认）。 ndarray.dump(file) 将数组的pickle转储到指定的文件。 ndarray.dumps() 以字符串形式返回数组的pickle。 ndarray.astype(dtype[, order, casting, …]) 数组的副本，强制转换为指定的类型。 ndarray.byteswap([inplace]) 交换数组元素的字节 ndarray.copy([order]) 返回数组的副本。 ndarray.view([dtype, type]) 具有相同数据的数组的新视图。 ndarray.getfield(dtype[, offset]) 返回给定数组的字段作为特定类型。 ndarray.setflags([write, align, uic]) 分别设置数组标志WRITEABLE，ALIGNED，（WRITEBACKIFCOPY和UPDATEIFCOPY）。 ndarray.fill(value) 使用标量值填充数组。 形状操作 方法 描述 ndarray.reshape(shape[, order]) 返回包含具有新形状的相同数据的数组。 ndarray.resize(new_shape[, refcheck]) 就地更改数组的形状和大小。 ndarray.transpose(*axes) 返回轴转置的数组视图。 ndarray.swapaxes(axis1, axis2) 返回数组的视图，其中axis1和axis2互换。 ndarray.flatten([order]) 将折叠的数组的副本返回到一个维度。 ndarray.ravel([order]) 返回一个扁平的数组。 ndarray.squeeze([axis]) 从形状除去单维输入一个。 对 axis 的理解：轴是为超过一维的数组定义的属性，二维数据拥有两个轴，第 0 轴沿着行的垂直往下，第 1 轴沿着列的方向水平延伸。 项目选择和操作 方法 描述 ndarray.take(indices[, axis, out, mode]) 返回由给定索引处的a元素组成的数组。 ndarray.put(indices, values[, mode]) 为索引中的所有n设置。a.flat[n] = values[n] ndarray.repeat(repeats[, axis]) 重复数组的元素。 ndarray.choose(choices[, out, mode]) 使用索引数组从一组选项中构造新数组。 ndarray.sort([axis, kind, order]) 对数组进行就地排序。 ndarray.argsort([axis, kind, order]) 返回将对此数组进行排序的索引。 ndarray.partition(kth[, axis, kind, order]) 重新排列数组中的元素，使得第k个位置的元素值位于排序数组中它应该在的位置。 比它小的都在前面，大的都在后面 ndarray.argpartition(kth[, axis, kind, order]) 返回将对此数组进行分区的索引。 ndarray.searchsorted(v[, side, sorter]) 查找应在其中插入v的元素以维护顺序的索引。 ndarray.nonzero() 返回非零元素的索引。 ndarray.compress(condition[, axis, out]) 沿给定轴返回此数组的选定切片。 ndarray.diagonal([offset, axis1, axis2]) 返回指定的对角线。 计算 方法 描述 ndarray.max([axis，out，keepdims，initial，...]） 沿给定轴返回最大值 ndarray.argmax([axis, out]) 返回给定轴上的最大值的索引 ndarray.min([axis，out，keepdims，initial，...]) 沿给定轴返回最小值 ndarray.argmin([axis, out]) 返回最小值的索引沿给定轴线一个 ndarray.ptp([axis, out, keepdims]) 沿给定轴的峰峰值（最大值 - 最小值） ndarray.clip([min，max，out]) 返回值限制为的数组。[min, max] ndarray.conj() 复合共轭所有元素 ndarray.round([decimals, out]) 返回a，每个元素四舍五入到给定的小数位数 ndarray.trace([offset, axis1, axis2, dtype, out]) 返回数组对角线的总和 ndarray.sum([axis, dtype, out, keepdims, …]) 返回给定轴上的数组元素的总和 ndarray.cumsum([axis, dtype, out]) 返回给定轴上元素的累积和 ndarray.mean([axis, dtype, out, keepdims]) 返回给定轴上数组元素的平均值 ndarray.var([axis, dtype, out, ddof, keepdims]) 返回给定轴的数组元素的方差 ndarray.std([axis, dtype, out, ddof, keepdims]) 返回沿给定轴的数组元素的标准偏差 ndarray.prod([axis, dtype, out, keepdims, …]) 返回给定轴上的数组元素的乘积 ndarray.cumprod([axis, dtype, out]) 返回沿给定轴的元素的累积乘积 ndarray.all([axis, out, keepdims]) 如果所有元素都计算为True，则返回True ndarray.any([axis, out, keepdims]) 如果任何元素为True，则返回true 其中许多方法都采用名为 axis 的参数： 如果 axis 为 None （默认值），则将数组视为1-D数组，并对整个数组执行操作。 如果self是0维数组或数组标量，则此行为也是默认行为。 （数组标量是类型/类float32，float64等的实例，而0维数组是包含恰好一个数组标量的ndarray实例。） 如果 axis 是整数，则操作在给定轴上完成（对于可沿给定轴创建的每个1-D子数组） 1234567891011x = np.array([[[ 0, 1, 2], [ 3, 4, 5], [ 6, 7, 8]], [[ 9, 10, 11], [12, 13, 14], [15, 16, 17]], [[18, 19, 20], [21, 22, 23], [24, 25, 26]]])print x.sum()print x.sum(axis=0) 351 [[27 30 33] [36 39 42] [45 48 51]] 算术运算和比较运算几个关键点： 算术和比较操作 ndarrays 被定义为逐元素操作，并且通常返回 ndarray 对象； 参与运算的数组必须具有相同规模，或者满足广播条件； 每个算术运算（的+，-，*，/，//，%，divmod()，**或pow()，&lt;&lt;，&gt;&gt;，&amp;， ^，|，~）和比较（==，&lt;，&gt;，&lt;=，&gt;=，!=）等效于numpy中对应的通用函数。 12345x = np.arange(12).reshape(3,4)y = np.arange(3).reshape(3,1)print &#x27;x:\n%s&#x27; %xprint &#x27;y:\n%s&#x27; %yprint &#x27;x+y:\n%s&#x27; %(x + y) x: [[ 0 1 2 3] [ 4 5 6 7] [ 8 9 10 11]] y: [[0] [1] [2]] x+y: [[ 0 1 2 3] [ 5 6 7 8] [10 11 12 13]] wherewhere(condition, [x, y])，有两种用法： where(condition)：等价于 np.asarray(condition).nonzero()，返回 True 对应的各轴索引元组； where(condition, x, y)：如果 condition 为 True，则从 x 中选取对应元素，否则从 y 中选取元素，x, y 和 condition 必须是可广播的。 12x = np.arange(12).reshape(3,4)x array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]]) 1np.where(x &gt; 5) (array([1, 1, 2, 2, 2, 2]), array([2, 3, 0, 1, 2, 3])) 1np.asarray(x &gt; 5).nonzero() (array([1, 1, 2, 2, 2, 2]), array([2, 3, 0, 1, 2, 3])) 123x, y = np.ogrid[:3, :4]np.where(x &lt; y, x, 10 + y) # both x and 10+y are broadcast array([[10, 0, 0, 0], [10, 11, 1, 1], [10, 11, 12, 2]]) 数组广播Numpy 操作通常在数组对之间逐个元素进行，在最简单的情况下，两个数组具有相同形状，对于不同形状的数组，广播（BroadCasting）提供了一种将较小数组扩展至和较大数组具有相同形状的机制。这只是概念上的，Numpy实际上并不会制作副本，以节省内存、提高计算效率。 广播的条件：数组对按照 shape 尾对齐后，比较对应维度的长度，要么相等要么其中一个是1 广播的过程：从内到外/从后向前比较对应维度的长度 如果两个数组在该维度下长度相同，就跳过 如果长度不同但其中一个数组在该维度下长度为1，则将该数组沿着该维度”复制“为和另一数组具有相同长度 如果一个数组维度结束了，就在该数组外面加一个[]，相当于该数组在该维度下长度变为1 如果两个数组在该维度下长度不同且长度都不为1，则产生 ValueError 广播的结果：广播后的数组每一维度的长度都是两个数组中对应维度长度的最大值 123A (4d array): 8 x 1 x 6 x 1B (3d array): 7 x 1 x 5Result (4d array): 8 x 7 x 6 x 5 1234from numpy import array, newaxisa = array([0.0, 10.0, 20.0, 30.0])b = array([1.0, 2.0, 3.0])a[:,newaxis] + b array([[ 1., 2., 3.], [11., 12., 13.], [21., 22., 23.], [31., 32., 33.]]) 索引切片Numpy 将 Python 的切片概念扩展到了n维，x[(exp1，exp2，.，EXPN)] 等同于 x[exp1，exp2，.，EXPN]，后者只是前者的语法糖。exp 可以有多种形式： 基本索引 整数索引：c， 切片：start:stop:step 高级索引 整数数组索引：由索引组成的数组 布尔数组索引：由Boolean值组成的数组 基本索引生成的所有数组始终是原数组的是视图，而不是复制，视图是对原始数组中一小部分的引用，对视图中元素的修改同样会反映到原始数组中，如果需要明确使用复制，可以调用数组的copy方法。与基本索引不同，高级索引始终返回数据的副本。 整数索引 形式：x[c1, c2,.,cN]，第一个元素的索引从0开始，假设某个维度元素个数为n，那么索引的有效取值范围为 -n ~ n-1，其中负数-k等价于 n-k 含义：返回由索引 (c1, c2, . , cN) 定位到的元素标量 1234x = np.arange(10).reshape(2,5)print xprint x[1,1]print x[-1,1] [[0 1 2 3 4] [5 6 7 8 9]] 6 6 切片 形式：x[i1:j1:k1, ..., i2:j2:k2, :, i3:j3:k3] 含义：序列切片的标准规则适用于基于每维（假设某维长度为n）的基本切片： 基本切片语法是 $i:j:k$，其中 i 是起始索引，j 是停止索引，k 是步长（$k\neq0$），这将选择具有索引值（在相应的维度中）$i, i+k, …, i+(m-1) k$ 的 m 个元素，其中 $m = q + (r\neq0)$，q 和 r 是 j-i 除以 k 得到的商和余数：$j - i = q k + r$，使得 $i + ( m - 1 ) k &lt; j$。 -i 和 -j 被解释为 n - i 和 n - j ，其中 n 是相应维度中的元素数量； 如果没有给出 i，对于 k &gt; 0，它默认为 0，对于 k &lt; 0，它默认为 n - 1，默认取到头； 如果没有给出 j，对于 k &gt; 0，它默认为 n，对于 k &lt; 0，则默认为 - n - 1； 如果没有给出 k，则默认为 1； : 代表此轴所有索引； 对于 N 维数组，如果索引表达式个数 K 小于 N，后面 N-K 维默认以 : 填充； numpy.Ellipsis ... 等价于多个连续的 :,:,:，表示中间维度包含所有索引，... 最多只能出现一次； numpy.newaxis：numpy.newaxis 是 None 的别名，作用是为前面一个轴内的每个元素添加 []，从而在其所处位置添加一个轴； 1x[:, 1:-1:2] array([[1, 3], [6, 8]]) 1x[..., ::-2] array([[4, 2, 0], [9, 7, 5]]) 1np.newaxis == None True 1x[np.newaxis,:, 1:-1:2] array([[[1, 3], [6, 8]]]) 1x[:, np.newaxis, 1:-1:2] array([[[1, 3]], [[6, 8]]]) 1x[:, 1:-1:2, np.newaxis] array([[[1], [3]], [[6], [8]]]) 整数数组索引整数数组索引也叫花式索引： 形式：x[index_array1, index_array2, ., index_arrayN] 含义：不同维度的索引数组之间必须满足广播条件，否则就会报错 广播：广播索引数组 index_arrayk, k=1,...,N，得到形状相同的索引数组 index_arraykb, k=1,...,N 组合：对广播索引数组，进行逐元素配对（类似于 zip 的过程） 查找：按照广播组合后的索引定位对应位置的元素 返回：将查找到的元素替换到广播后的索引数组对应位置，作为结果返回 12x = np.arange(30).reshape(2,3,5)x array([[[ 0, 1, 2, 3, 4], [ 5, 6, 7, 8, 9], [10, 11, 12, 13, 14]], [[15, 16, 17, 18, 19], [20, 21, 22, 23, 24], [25, 26, 27, 28, 29]]]) 12345678910111213141516171819202122&quot;&quot;&quot;三个维度的整数数组索引 [[0],[0]],[1,2],[[1,1], [2,2]]，满足广播条件2 * 11 * 22 * 2广播后三个维度的整数数组索引为：[[0, 0], [[1,2], [[1,1], [0, 0]] [1,2]] [2,2]] 进行逐元素组合：[[(0,1,1), (0, 2, 1)], [(0,1,2), (0, 2, 2)]]定位对应元素：x[(0,1,1)] x[(0, 2, 1)] x[(0,1,2)] x[(0, 2, 2)]替换索引数组对应位置的值[[x[(0,1,1)], x[(0,2,1)]], [x[(0,1,2)], x[(0,2,2)]]]&quot;&quot;&quot;x[[[0],[0]], [1,2], [[1,1], [2,2]]] array([[ 6, 11], [ 7, 12]]) 12array([[x[(0,1,1)], x[(0, 2, 1)]], [x[(0,1,2)], x[(0, 2, 2)]]]) array([[ 6, 11], [ 7, 12]]) 123456# np.ix_(arr_1,...,arr_N) 返回N个数组，每个数组arr_k的形状为 1 * ... * k * ... * 1，即除了其所在维长度等于对应数组本身长度，其余维度元素个数均为1rows = np.array([0, 3], dtype=np.intp)columns = np.array([0, 2], dtype=np.intp)construct = np.ix_(rows, columns)print constructx[np.ix_(rows, columns)] (array([[0], [3]]), array([[0, 2]])) array([[ 0, 2], [ 9, 11]]) 布尔数组索引 形式：x[obj]，我中 obj 可以是 与所在维度长度相等的数组； 与x整体形状相同的布尔数组； 含义：x[ind_1，boolean_array，ind_2] 等价于 x[(ind_1，)+boolean_array.nonzero()+(ind_2，)]，如果索引包含布尔数组，则结果与将 obj.nonzero() 插入到相同位置并使用上述整数组索引机制相同。 12x = np.arange(30).reshape(2,3,5)x array([[[ 0, 1, 2, 3, 4], [ 5, 6, 7, 8, 9], [10, 11, 12, 13, 14]], [[15, 16, 17, 18, 19], [20, 21, 22, 23, 24], [25, 26, 27, 28, 29]]]) 1234# 布尔数组长度与所在维度长度相等的数组print x[[True, False], [True, True, False], [True, True, False, False, False]]b_index = np.nonzero([True, False]), np.nonzero([True, True, False]), np.nonzero([True, True, False, False, False])x[b_index] [0 6] array([[0, 6]]) 12# 布尔索引数组和被索引的数组形状相同x[x % 2 == 0] array([ 0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28]) 混合索引对于索引切片 x[(exp1，exp2，.，EXPN)]，expk 可以是以下四种形式： 索引名称 形式 含义 广播维度 视图/复制 整数索引 c 在k轴中取出索引为 c 元素 索引维长度为 0 视图 切片索引 i:j:k 在k轴中取出 i:j:k 的切片 索引维长度为 divmod(j-I, k) 的商加余数 视图 整数数组索引 array(m*n) 对所有整数数组索引广播、组合、查找、替换 索引维形状为整数数组索引形状 复制 布尔数组索引 bool_array(s*t) 等价于np.nonzero( bool_array)对应的整数数组索引 首先转化为整数数组索引，按照整数数组索引的规则确定索引维形状 复制 以上四种索引形式可以在不同维混合使用，但需满足以下两个条件： 各维度索引满足对应索引形式的使用规范； 不同维度之间满足广播条件：包括基本索引之间、高级索引之间、基本索引和高级索引之间； 确定混合索引结果数组的步骤： 对基本索引和高级索引分别进行广播： 基本索引广播：假设广播后得到的基本索引为 basic_index，形状为 (m * n) 整数索引：1 * ... * 0 * ... * 1，整数索引对应维度元素长度为0，其他基本索引维度取默认值1； 切片索引：1 * ... * ijk * ... * 1，切片索引对应维度元素长度为切片长度，其他基本索引维度取默认值1；特殊地，: 元素长度即为原数组对应维的长度； 高级索引广播：假设广播后得到的高级索引为 advanced_index，形状为(s * t) 整数数组索引：array_index.shape，整数数组索引对应的形状就是整数数组索引本身的形状； 布尔数组索引：nonzero_array_index.shape，布尔数组索引本质上是整数数组索引，规则与整数数组索引相同； 对广播后的基本索引和高级索引进行拼接广播：最终的结果数组形状为 s t m n，即将广播后的高级索引中的每个元素替换为 m n 的形状，其中替换后的索引数组中，每个元素都是一个索引元组，元组前半部分取自广播后的高级索引，后半部分取自广播后的基本索引； 使用原数组中对应的元素替换组合后的索引数组中的元素（索引元组），得到最终的结果数组； 12print x.shapex (2, 3, 5) array([[[ 0, 1, 2, 3, 4], [ 5, 6, 7, 8, 9], [10, 11, 12, 13, 14]], [[15, 16, 17, 18, 19], [20, 21, 22, 23, 24], [25, 26, 27, 28, 29]]]) 1234567891011121314151617181920212223242526&quot;&quot;&quot;示例1：普通索引和切片 1. 确定各维度索引形状 : 对应的索引数组形状 2 * 1 * 1，对应的完整索引数组为 [[[0]], [[1]]] :2 对应的索引数组形状 1 * 2 * 1，对应的完整索引数组为 [[[0], [1]]] 4 对应的索引数组形状 1 * 1 * 0，对应的完整索引数组为 [[4]] 2. 广播 2 * 1 * 1 1 * 2 * 1 =&gt; 2 * 2 1 * 0 * 1 广播后各维度索引形状分别为 [[[0], [0]], [[[0], [1]], [[4]] [[1], [1]]] [[0], [1]]] =&gt; [[0, 0], [[0, 1], [[4, 4] [1, 1]] [0, 1]] [4, 4]] 3. 组合 [[(0,0,4), (0,1,4)], [(1,0,4), (1,1,4)]] 4. 替换 [[x[(0,0,4)], x[(0,1,4)]], [x[(1,0,4)], x[(1,1,4)]]]&quot;&quot;&quot;print [[x[(0,0,4)], x[(0,1,4)]], [x[(1,0,4)], x[(1,1,4)]]]x[:, :2, 4] [[4, 9], [19, 24]] array([[ 4, 9], [19, 24]]) 12345678910111213141516&quot;&quot;&quot;示例2：高级索引（高级索引和基本索引要分开来算） 1. 确定广播后的高级索引 1. 确定各维度索引形状，三个维度均为高级索引，统一确定整数数组索引的维度 [0,1] 对应的索引数组形状 1 * 2 [True, True, False] np.nonzero([True, True, False]) == [0,1] 对应的索引数组形状 1 * 2 [4] 对应的索引数组形状 1 * 1 2. 广播后各维度索引形状分别为 1 * 2，对应的整数数组索引为 [0,1] [0,1] [4,4] 3. 组合 [(0,0,4), (1,1,4)] 2. 确定基本索引，没有基本索引 3. 拼接高级索引和基本索引 [(0,0,4), (1,1,4)] 4. 替换 [x[(0,0,4)], x[(1,1,4)]]&quot;&quot;&quot;print [x[(0,0,4)], x[(1,1,4)]]x[[0,1], [True, True, False], [4]] [4, 24] array([ 4, 24]) 123456789101112131415161718&quot;&quot;&quot;示例3：高级索引 + 基本索引（高级索引和基本索引要分开来算） 1. 高级索引内部广播 1. 确定各维度索引形状 [0,1] 对应的索引数组形状 (2,) [True, False, True] np.nonzero([True, True, False]) == [0,2] 对应的索引数组形状 (2,) 2. 广播后高级索引形状分别为 (2,)，对应的整数数组索引为 [0,1] [0,2] 3. 组合 [(0,0), (1,2)] 2. 确定基本索引 1. 4:5 基本索引形状为 (1,)，索引为 [4] 3. 整体广播，高级索引和基本索引拼接，对应整数数组索引形状为 (2,1)，索引分别为 [[(0,0)], [(1,2)]] [[4], [4]] 3. 拼接高级索引和基本索引 [[0,0,4], [1,2,4]] 4. 替换 [[x[(0,0,4)]], [x[(1,2,4)]]]&quot;&quot;&quot;print [[x[(0,0,4)]], [x[(1,2,4)]]]x[[0,1], [True, False, True], 4:5] [[4], [29]] array([[ 4], [29]]) 迭代数组迭代数组中每个元素12345678# 默认按照 order =&#x27;K&#x27; 以行优先迭代数组中的元素a = np.arange(6).reshape(2,3)print aprint a.T.copy(order=&#x27;C&#x27;)for x in np.nditer(a): print x, [[0 1 2] [3 4 5]] [[0 3] [1 4] [2 5]] 0 1 2 3 4 5 123456# 迭代时访问每个元素的索引it = np.nditer(a, flags=[&#x27;multi_index&#x27;])while not it.finished: print(&quot;%d %s&quot; % (it[0], it.multi_index)) it.iternext() 0 (0, 0) 1 (0, 1) 2 (0, 2) 3 (1, 0) 4 (1, 1) 5 (1, 2) 迭代广播数组向 nditer 传递一个数组列表，多个数组先进行广播组合，然后再迭代。 1234a = np.arange(3)b = np.arange(6).reshape(2,3)for x, y in np.nditer([a,b]): print(&quot;%d:%d&quot; % (x,y)) 0:0 1:1 2:2 0:3 1:4 2:5 通用函数通用函数（universal function, ufunc）是以逐元素方式作用于 ndarray 上的函数，支持数组广播、类型转换以及其他几种特性。ufunc 是函数的矢量化包装器，接收固定数量的输入，并产生固定数量的输出。 在NumPy中，通用函数是numpy.ufunc类的实例，许多内置函数都是在编译的C代码中实现的，用户可以通过 frompyfunc 工厂函数生成自定义的通用函数示例。 ufunc 属性ufunc 有一些信息属性，这些属性不可以设置： 方法 描述 ufunc.nin 输入数量。 ufunc.nout 输出数量。 ufunc.nargs 参数的数量。 ufunc.ntypes 类型数量。 ufunc.types 返回包含input-&gt; output类型的列表。 ufunc.identity 身份价值。 ufunc.signature 广义ufunc操作的核心元素的定义。 12345678sin_func = np.sinprint sin_func.ninprint sin_func.noutprint sin_func.nargsprint sin_func.ntypesprint sin_func.typesprint sin_func.identityprint sin_func.signature 1 1 2 11 [&#39;e-&gt;e&#39;, &#39;f-&gt;f&#39;, &#39;d-&gt;d&#39;, &#39;e-&gt;e&#39;, &#39;f-&gt;f&#39;, &#39;d-&gt;d&#39;, &#39;g-&gt;g&#39;, &#39;F-&gt;F&#39;, &#39;D-&gt;D&#39;, &#39;G-&gt;G&#39;, &#39;O-&gt;O&#39;] None None ufunc 方法所有ufunc都有四种方法。但是，这些方法只对采用两个输入参数并返回一个输出参数的标量ufunc有意义。尝试在其他ufunc上调用这些方法将导致ValueError。reduce-like方法都采用axis关键字、dtype关键字和out关键字，并且数组的维数都必须大于等于1。 axis 关键字指定将在其上进行缩减的数组的轴（负值向后计数）。一般来说，它是一个整数，但是对于 ufunc.reduce，它也可以是int的元组，一次减少多个轴、或者不减少、或者减少所有轴 dtype 关键字允许您更改进行 reduce 的数据类型（因此更改输出的类型） out 关键字允许你提供一个输出数组，目前只支持单一输出的 ufunc，如果提供了out参数，dtype 参数将被忽略 ufunc还有第五种方法，允许使用特殊的索引执行就地操作。在使用花式索引的维度上不使用缓冲，因此花式索引可以多次列出一个项，并且将对该项的上一个操作的结果执行该操作。 方法 描述 ufunc.reduce(a[, axis, dtype, out, …]) 减少一个接一个的尺寸，由沿一个轴施加ufunc。 ufunc.accumulate(array[, axis, dtype, out]) 累积将运算符应用于所有元素的结果。 ufunc.reduceat(a, indices[, axis, dtype, out]) 在单个轴上使用指定切片执行（局部）缩减。 ufunc.outer(A, B, **kwargs) 将ufunc op应用于所有对（a，b），其中a中的a和b中的b。 ufunc.at(a, indices[, b]) 对’index’指定的元素在操作数’a’上执行无缓冲的就地操作。 12x = np.arange(10).reshape(2,5)x array([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]) 1np.add.reduce(x, axis=0, dtype=&#x27;i8&#x27;) array([ 5, 7, 9, 11, 13]) 1np.add.accumulate(x, axis=1) array([[ 0, 1, 3, 6, 10], [ 5, 11, 18, 26, 35]]) 内置 ufunc 方法目前在numpy中定义的一个或多个类型的通用函数超过60个，涵盖了各种各样的操作。当使用相关的中缀符号时，这些ufunc中的一些在数组上被自动调用（例如，当a+b被写入并且a或b是ndarray时，add（a，b）在内部被调用）。不过，您可能仍然希望使用ufunc调用，以便使用可选的输出参数将输出放置在您选择的一个或多个对象中。 回想一下，每个ufunc都逐个操作元素。因此，每个标量 ufunc 将被描述为作用于一组标量输入以返回一组标量输出。 数学运算： 方法 描述 add(x1, x2, [, out, where, cast, order, ...]) 按元素添加参数。 subtract(x1, x2, [, out, where, cast, ...]) 从元素方面减去参数。 multiply(x1, x2, [, out, where, cast, ...]) 在元素方面乘以论证。 divide(x1, x2, [, out, where, cast, ...]) 以元素方式返回输入的真正除法。 logaddexp(x1, x2, [, out, where, cast, ...]) 输入的取幂之和的对数。 logaddexp2(x1, x2, [, out, where, cast, ...]) base-2中输入的取幂之和的对数。 true_divide(x1, x2, [, out, where, ...]) 以元素方式返回输入的真正除法。 floor_divide(x1, x2, [, out, where, ...]) 返回小于或等于输入除法的最大整数。 negative(x, [, out, where, cast, order, ...]) 数字否定, 元素方面。 positive(x, [, out, where, cast, order, ...]) 数字正面, 元素方面。 power(x1, x2, [, out, where, cast, ...]) 第一个数组元素从第二个数组提升到幂, 逐个元素。 remainder(x1, x2, [, out, where, cast, ...]) 返回除法元素的余数。 mod(x1, x2, [, out, where, cast, order, ...]) 返回除法元素的余数。 fmod(x1, x2, [, out, where, cast, ...]) 返回除法的元素余数。 divmod(x1, x2 [, out1, out2], [[, out, ...]) 同时返回逐元素的商和余数。 absolute(x, [, out, where, cast, order, ...]) 逐个元素地计算绝对值。 fabs(x, [, out, where, cast, order, ...]) 以元素方式计算绝对值。 rint(x, [, out, where, cast, order, ...]) 将数组的元素舍入为最接近的整数。 sign(x, [, out, where, cast, order, ...]) 返回数字符号的元素指示。 heaviside(x1, x2, [, out, where, cast, ...]) 计算Heaviside阶跃函数。 conj(x, [, out, where, cast, order, ...]) 以元素方式返回复共轭。 conjugate(x, [, out, where, cast, ...]) 以元素方式返回复共轭。 exp(x, [, out, where, cast, order, ...]) 计算输入数组中所有元素的指数。 exp2(x, [, out, where, cast, order, ...]) 计算输入数组中所有 p 的 2**p。 log(x, [, out, where, cast, order, ...]) 自然对数, 元素方面。 log2(x, [, out, where, cast, order, ...]) x的基数为2的对数。 log10(x, [, out, where, cast, order, ...]) 以元素方式返回输入数组的基数10对数。 expm1(x, [, out, where, cast, order, ...]) 计算数组中的所有元素。exp(x) - 1 log1p(x, [, out, where, cast, order, ...]) 返回一个加上输入数组的自然对数, 逐个元素。 sqrt(x, [, out, where, cast, order, ...]) 以元素方式返回数组的非负平方根。 square(x, [, out, where, cast, order, ...]) 返回输入的元素方块。 cbrt(x, [, out, where, cast, order, ...]) 以元素方式返回数组的立方根。 reciprocal(x, [, out, where, cast, ...]) 以元素方式返回参数的倒数。 gcd(x1, x2, [, out, where, cast, order, ...]) 返回 \ x1 \ 和的最大公约数 \ x2 \ 。 lcm(x1, x2, [, out, where, cast, order, ...]) 返回 \ x1 \ 和的最小公倍数 \ x2 \ 。 三角函数： 方法 描述 sin(x, [, out, where, cast, order, ...]) 三角正弦, 元素方式。 cos(x, [, out, where, cast, order, ...]) 余弦元素。 tan(x, [, out, where, cast, order, ...]) 计算切线元素。 arcsin(x, [, out, where, cast, order, ...]) 反向正弦, 元素方式。 arccos(x, [, out, where, cast, order, ...]) 三角反余弦, 元素方式。 arctan(x, [, out, where, cast, order, ...]) 三角反正切, 逐元素。 arctan2(x1, x2, [, out, where, cast, ...]) x1/x2正确选择象限的逐元素反正切。 hypot(x1, x2, [, out, where, cast, ...]) 给定直角三角形的“腿”, 返回其斜边。 sinh(x, [, out, where, cast, order, ...]) 双曲正弦, 元素。 cosh(x, [, out, where, cast, order, ...]) 双曲余弦, 元素。 tanh(x, [, out, where, cast, order, ...]) 计算双曲正切元素。 arcsinh(x, [, out, where, cast, order, ...]) 逆双曲正弦元素。 arccosh(x, [, out, where, cast, order, ...]) 反双曲余弦, 元素。 arctanh(x, [, out, where, cast, order, ...]) 逆双曲正切元素。 deg2rad(x, [, out, where, cast, order, ...]) 将角度从度数转换为弧度。 rad2deg(x, [, out, where, cast, order, ...]) 将角度从弧度转换为度数。 位运算函数： 方法 描述 bitwise_and(x1, x2, [, out, where, ...]) 逐个元素地计算两个数组的逐位AND。 bitwise_or(x1, x2, [, out, where, cast, ...]) 逐个元素地计算两个数组的逐位OR。 bitwise_xor(x1, x2, [, out, where, ...]) 逐个元素地计算两个数组的逐位XOR。 invert(x, [, out, where, cast, order, ...]) 计算逐位反转, 或逐位NOT, 逐元素计算。 left_shift(x1, x2, [, out, where, cast, ...]) 将整数位移到左侧。 right_shift(x1, x2, [, out, where, ...]) 将整数位移到右侧。 比较函数： 方法 描述 greater(x1, x2, [, out, where, cast, ...]) 以元素方式返回(x1 &gt; x2)的真值。 greater_equal(x1, x2, [, out, where, ...]) 以元素方式返回(x1 &gt;= x2)的真值。 less(x1, x2, [, out, where, cast, ...]) 返回(x1 &lt; x2)元素的真值。 less_equal(x1, x2, [, out, where, cast, ...]) 以元素方式返回(x1 =&lt; x2)的真值。 not_equal(x1, x2, [, out, where, cast, ...]) 以元素方式返回(x1 != x2)。 equal(x1, x2, [, out, where, cast, ...]) 以元素方式返回(x1 == x2)。 逻辑函数：不要使用Python关键字and并or组合逻辑数组表达式。这些关键字将测试整个数组的真值(不是你想象的逐个元素)。使用按位运算符＆和| 代替 方法 描述 logical_and(x1, x2, [, out, where, ...]) 计算x1和x2元素的真值。 logical_or(x1, x2, [, out, where, cast, ...]) 计算x1 OR x2元素的真值。 logical_xor(x1, x2, [, out, where, ...]) 以元素方式计算x1 XOR x2的真值。 logical_not(x, [, out, where, cast, ...]) 计算NOT x元素的真值。 浮点函数： 方法 描述 isfinite(x, [, out, where, cast, order, ...]) 测试元素的有限性(不是无穷大或不是数字)。 isinf(x, [, out, where, cast, order, ...]) 正面或负面无穷大的元素测试。 isnan(x, [, out, where, cast, order, ...]) 测试NaN的元素, 并将结果作为布尔数组返回。 isnat(x, [, out, where, cast, order, ...]) 为NaT(不是时间)测试元素, 并将结果作为布尔数组返回。 fabs(x, [, out, where, cast, order, ...]) 以元素方式计算绝对值。 signbit(x, [, out, where, cast, order, ...]) 返回元素为True设置signbit(小于零)。 copysign(x1, x2, [, out, where, cast, ...]) 将元素x1的符号更改为x2的符号。 nextafter(x1, x2, [, out, where, cast, ...]) 将x1之后的下一个浮点值返回x2(元素方向)。 spacing(x, [, out, where, cast, order, ...]) 返回x与最近的相邻数字之间的距离。 modf(x [, out1, out2], [[, out, where, ...]) 以元素方式返回数组的小数和整数部分。 ldexp(x1, x2, [, out, where, cast, ...]) 以元素方式返回x1 * 2 ** x2。 frexp(x [, out1, out2], [[, out, where, ...]) 将x的元素分解为尾数和二进制指数。 fmod(x1, x2, [, out, where, cast, ...]) 返回除法的元素余数。 floor(x, [, out, where, cast, order, ...]) 以元素方式返回输入的底限。 ceil(x, [, out, where, cast, order, ...]) 以元素方式返回输入的上限。 trunc(x, [, out, where, cast, order, ...]) 以元素方式返回输入的截断值。 其他函数： 方法 描述 maximum(x1, x2, [, out, where, cast, ...]) 元素最大的数组元素。 minimum(x1, x2, [, out, where, cast, ...]) 元素最小的数组元素。 123x = np.arange(10).reshape(2,5)y = 3np.maximum(x,y) array([[3, 3, 3, 3, 4], [5, 6, 7, 8, 9]]) 自定义 ufuncnumpy.frompyfunc(func, nin, nout) 接收一个python函数，返回一个Numpy函数 func：python函数 nin：python函数输入参数的个数 nout：python函数输出参数的个数 自定义 ufunc 的步骤： 定义一个python函数 将python函数转化为numpy函数 使用numpy函数: numpy函数总是返回 PyObject 类型的数组，可以通过astype()做类型转换 123456def func(x,y): return x + yufunc = np.frompyfunc(func, 2, 1)result = ufunc(np.array([1,2,3]), np.array([4,5,6]))result, result.astype(int) (array([5, 7, 9], dtype=object), array([5, 7, 9])) 沿轴应用函数numpy.apply_along_axis(func, axis, arr, *args, **kwargs) 沿轴对数组里的每一个元素进行变换，得到目标的结果： func：接收数组 arr 元素的函数 axis：轴向 arr：数组 args, *kwargs都是func()函数额外的参数。 12345678def my_func(a): return (a[0] + a[-1]) * 0.5b=np.array([[1,2,3,4],[5,6,7,8],[9,10,11,12]])print np.apply_along_axis(my_func, 0, b)print np.apply_along_axis(my_func, 1, b) [5. 6. 7. 8.] [ 2.5 6.5 10.5] 常量123456print &#x27;正无穷:&#123;&#125;&#x27;.format(np.inf)print &#x27;负无穷:&#123;&#125;&#x27;.format(-np.inf)print &#x27;自然对数:&#123;&#125;&#x27;.format(np.e)print &#x27;圆周率:&#123;&#125;&#x27;.format(np.pi) 正无穷:inf 负无穷:-inf 自然对数:2.71828182846 圆周率:3.14159265359 日期numpy 中的日期数据类型称为 “datetime64”。 12345print np.datetime64(&#x27;2005-02-25&#x27;)print np.datetime64(&#x27;2005-02&#x27;)print np.datetime64(&#x27;2005-02&#x27;, &#x27;D&#x27;)# 不是时间print np.datetime64(&#x27;nat&#x27;) 2005-02-25 2005-02 2005-02-01 NaT 12# 生成一个日期范围数组np.arange(&#x27;2005-02&#x27;, &#x27;2005-03&#x27;, dtype=&#x27;datetime64[D]&#x27;) array([&#39;2005-02-01&#39;, &#39;2005-02-02&#39;, &#39;2005-02-03&#39;, &#39;2005-02-04&#39;, &#39;2005-02-05&#39;, &#39;2005-02-06&#39;, &#39;2005-02-07&#39;, &#39;2005-02-08&#39;, &#39;2005-02-09&#39;, &#39;2005-02-10&#39;, &#39;2005-02-11&#39;, &#39;2005-02-12&#39;, &#39;2005-02-13&#39;, &#39;2005-02-14&#39;, &#39;2005-02-15&#39;, &#39;2005-02-16&#39;, &#39;2005-02-17&#39;, &#39;2005-02-18&#39;, &#39;2005-02-19&#39;, &#39;2005-02-20&#39;, &#39;2005-02-21&#39;, &#39;2005-02-22&#39;, &#39;2005-02-23&#39;, &#39;2005-02-24&#39;, &#39;2005-02-25&#39;, &#39;2005-02-26&#39;, &#39;2005-02-27&#39;, &#39;2005-02-28&#39;], dtype=&#39;datetime64[D]&#39;) 1234# 日期间隔d = np.timedelta64(1, &#x27;D&#x27;)x = np.datetime64(&#x27;2009-01-01&#x27;) - dx, d (numpy.datetime64(&#39;2008-12-31&#39;), numpy.timedelta64(1,&#39;D&#39;)) 杂项numpy 提供了丰富的科学计算 API，用到时可以查看 API 文档，不再赘述，以上以上涵盖了 numpy 的核心框架。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据科学：时间序列（一）—— ETS 预估 DAU]]></title>
    <url>%2F%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%2F%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%2F%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%EF%BC%9A%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%EF%BC%88%E4%BA%8C%EF%BC%89%E2%80%94%E2%80%94%20ETS%20%E9%A2%84%E4%BC%B0%20DAU%2F</url>
    <content type="text"><![CDATA[12345678910111213141516171819202122232425262728293031323334353637383940414243444546import itertoolsimport warningsimport pandas as pdimport numpy as npimport seaborn as snsfrom matplotlib import cmimport matplotlib as mplimport matplotlib.pyplot as pltimport datetimefrom matplotlib.ticker import FuncFormatterfrom dateutil.parser import parsefrom statsmodels.tsa.arima_model import ARIMAfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacffrom statsmodels.tsa.stattools import adfullerfrom statsmodels.tsa.seasonal import seasonal_decomposefrom statsmodels.stats.diagnostic import acorr_ljungboximport statsmodels.tsa.stattools as tsimport statsmodels.api as smfrom sklearn.metrics import mean_squared_errorfrom statsmodels.tsa.api import ExponentialSmoothingfrom mpl_toolkits.mplot3d import Axes3Dfrom datetime import datetime, timedeltawarnings.filterwarnings(&quot;ignore&quot;) # register the convertersfrom pandas.plotting import register_matplotlib_convertersregister_matplotlib_converters()# 打印图像%matplotlib inline # 定义种子np.random.seed(sum(map(ord,&quot;aesthetics&quot;))) # 取消科学计数pd.set_option(&#x27;display.float_format&#x27;,lambda x : &#x27;%.2f&#x27; % x)# 显示中文mpl.rcParams[&#x27;font.sans-serif&#x27;] = [&#x27;SimHei&#x27;]mpl.rcParams[&#x27;font.serif&#x27;] = [&#x27;SimHei&#x27;]mpl.rcParams[&#x27;axes.formatter.useoffset&#x27;] = False# 解决保存图像是负号&#x27;-&#x27;显示为方块的问题plt.rcParams[&#x27;axes.unicode_minus&#x27;] = False# 解决Seaborn中文显示问题并调整字体大小sns.set_style(&quot;darkgrid&quot;,&#123;&quot;font.sans-serif&quot;:[&#x27;SimHei&#x27;, &#x27;Arial&#x27;]&#125;)# 设置图片分辨率plt.rcParams[&#x27;savefig.dpi&#x27;] = 150 plt.rcParams[&#x27;figure.dpi&#x27;] = 150 # plt.gcf().autofmt_xdate() 1234567891011121314151617181920212223242526272829303132def plot_2d(data, title, figsize=(18,5)): plt.figure(figsize=figsize) plt.title(title) data.plot() def plot_3d_surface(X, Y, Z, figsize=(18,10)): fig = plt.figure(figsize=figsize) ax = Axes3D(fig) # 选色带 https://matplotlib.org/3.2.1/tutorials/colors/colormaps.html # cmap: rainbow coolwarm jet PiYG surf = ax.plot_surface(X, Y, Z * 100, rstride=1, cstride=1, cmap=&#x27;coolwarm&#x27;, linewidth=0.1, shade=True, alpha=0.8, norm=mpl.colors.Normalize(vmin=-1., vmax=40) ) # ax.contourf(X, Y, Z*100, zdir=&#x27;y&#x27;, offset=121, cmap=cm.coolwarm) # ax.set_xlim(-10, 120) ax.set_xlabel(&#x27;N days&#x27;) ax.set_ylabel(&#x27;date&#x27;) ax.set_zlabel(&#x27;ratio(%)&#x27;) # ax.set_title(&#x27;Retention rate of new users&#x27;) fig.colorbar(surf, shrink=0.6, aspect=6) plt.show()def df_show(df, m=5, n=5): print(&#x27;shape=&#123;&#125;&#x27;.format(df.shape)) return pd.concat([df.head(m),df.tail(n)]) 原理 \begin{align*} DAU_0 &= \sum_{k=0}^{\infty }(DAU_{k}^{new}\times r_k)\\ & = DAU_0^{new} \times r_0 + \sum_{k=1}^{K-1} DAU_k^{new} \times r_K +DAU_K^{new} \times r_K\\ & = DAU_0^{new} + \sum_{k=1}^{K-1} DAU_k^{new} \times r_k +DAU_K \times r_K\\ \end{align*} $k$：k 天前 $r_k$：k 天前新用户第 k 日留存 $s_k$：k 天前新用户占比 $K$：历史新老用户分解线，比如将2019年前所有用户当做老用户，之后的新用户看做是新用户 数据123456789101112131415161718192021222324252627282930313233343536def get_dau(path=&#x27;./data/0610_dau.xlsx&#x27;, usecols=[&#x27;f_date&#x27;, &#x27;dau&#x27;], index_col=0, parse_dates=[&#x27;f_date&#x27;]): data = pd.read_excel(io=path, usecols=usecols, index_col=index_col, parse_dates=parse_dates).dau * 10000 data.index = pd.DatetimeIndex(data.index, freq=&#x27;D&#x27;) return data.astype(int)def get_dau_arima(path=&#x27;./data/0610_dau_arima.xlsx&#x27;, usecols=[&#x27;f_date&#x27;, &#x27;DAU&#x27;], index_col=0, parse_dates=[&#x27;f_date&#x27;]): data = pd.read_excel(io=path, usecols=usecols, index_col=index_col, parse_dates=parse_dates).DAU data.index = pd.DatetimeIndex(data.index, freq=&#x27;D&#x27;) return data.astype(int)def get_dau_new(path=&#x27;./data/0610_dau_new.xlsx&#x27;, usecols=[&#x27;f_date&#x27;, &#x27;f_dau_new&#x27;], index_col=0, parse_dates=[&#x27;f_date&#x27;]): data = pd.read_excel(io=path, usecols=usecols, index_col=index_col, parse_dates=parse_dates).f_dau_new data.index = pd.DatetimeIndex(data.index, freq=&#x27;D&#x27;) return datadef get_retention_new(path=&#x27;./data/0610_retention_new.xlsx&#x27;, usecols=[&#x27;f_visit_day&#x27;, &#x27;f_remain_days&#x27;, &#x27;f_ratio&#x27;], parse_dates=[&#x27;f_visit_day&#x27;]): data = pd.read_excel(io=path, usecols=usecols, parse_dates=parse_dates) data = data.drop_duplicates([&#x27;f_visit_day&#x27;, &#x27;f_remain_days&#x27;], keep=&#x27;last&#x27;) \ .set_index([&#x27;f_visit_day&#x27;, &#x27;f_remain_days&#x27;]) \ .unstack() \ .sort_index() \ .f_ratio data.index = pd.DatetimeIndex(data.index, freq=&#x27;D&#x27;) # pd.to_datetime(datetime.date.today() - datetime.timedelta(121)) data = data.loc[&#x27;2020-01-01&#x27;:] return datadef get_retention_old(path=&#x27;./data/0610_retention_old.xlsx&#x27;, usecols=[&#x27;f_date&#x27;, &#x27;f_ratio&#x27;], index_col=0, parse_dates=[&#x27;f_date&#x27;]): data = pd.read_excel(io=path, usecols=usecols, index_col=index_col, parse_dates=parse_dates).f_ratio data.index = pd.DatetimeIndex(data.index, freq=&#x27;D&#x27;) return data 小程序 DAU12dau = get_dau()plot_2d(dau.loc[&#x27;2020-03-01&#x27;:], u&#x27;DAU&#x27;) 1df_show(dau, 10, 10) shape=(524,) f_date 2019-01-04 2960 2019-01-05 2318 2019-01-06 2274 2019-01-07 2601 2019-01-08 2520 2019-01-09 2514 2019-01-10 2315 2019-01-11 2179 2019-01-12 1947 2019-01-13 1813 2020-06-01 3994272 2020-06-02 3546456 2020-06-03 3248834 2020-06-04 3129509 2020-06-05 3216982 2020-06-06 2895258 2020-06-07 2926685 2020-06-08 3023940 2020-06-09 2911481 2020-06-10 2737987 Name: dau, dtype: int64 新增用户12s_dau_new = get_dau_new()df_show(s_dau_new) shape=(161,) f_date 2020-01-01 183431 2020-01-02 175389 2020-01-03 172991 2020-01-04 157629 2020-01-05 158287 2020-06-05 440420 2020-06-06 419283 2020-06-07 409955 2020-06-08 422092 2020-06-09 412235 Name: f_dau_new, dtype: int64 12# &#x27;2020-03-01&#x27;:plot_2d(s_dau_new.loc[:], u&#x27;新增用户&#x27;) 新用户留存12df_ret_new = get_retention_new()df_show(df_ret_new) shape=(160, 120) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } f_remain_days 1 2 3 4 5 6 7 8 9 10 ... 111 112 113 114 115 116 117 118 119 120 f_visit_day 2020-01-01 0.02 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.00 0.01 ... 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 2020-01-02 0.02 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 ... 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 2020-01-03 0.02 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 ... 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 2020-01-04 0.02 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 ... 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 2020-01-05 0.02 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 ... 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 2020-06-04 0.04 0.02 0.01 0.01 0.01 nan nan nan nan nan ... nan nan nan nan nan nan nan nan nan nan 2020-06-05 0.04 0.02 0.01 0.01 nan nan nan nan nan nan ... nan nan nan nan nan nan nan nan nan nan 2020-06-06 0.03 0.02 0.01 nan nan nan nan nan nan nan ... nan nan nan nan nan nan nan nan nan nan 2020-06-07 0.03 0.02 nan nan nan nan nan nan nan nan ... nan nan nan nan nan nan nan nan nan nan 2020-06-08 0.04 nan nan nan nan nan nan nan nan nan ... nan nan nan nan nan nan nan nan nan nan 10 rows × 120 columns 1234567df_ret_new_plot = df_ret_new.loc[&#x27;2020-02-10&#x27;:]X = df_ret_new_plot.columnsY = (df_ret_new_plot.index - df_ret_new_plot.index.min()).map(lambda x: x.days)X, Y = np.meshgrid(X,Y)Z = df_ret_new_plot.values.astype(np.double)plot_3d_surface(X, Y, Z, figsize=(18, 8)) 老用户留存12s_ret_old = get_retention_old()df_show(s_ret_old) shape=(101,) f_date 2020-03-01 0.05 2020-03-02 0.05 2020-03-03 0.04 2020-03-04 0.04 2020-03-05 0.04 2020-06-05 0.01 2020-06-06 0.01 2020-06-07 0.01 2020-06-08 0.01 2020-06-09 0.01 Name: f_ratio, dtype: float64 1plot_2d(s_ret_old,u&#x27;老用户留存&#x27;) 模型123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240class MyEts: def __init__(self, s_dau_new, df_ret_new, s_ret_old, base_date, pred_start_date, n_forcast=7, dau_old=273385370, n_train=90): &quot;&quot;&quot; Args: s_dau_new: pd.Series 新增用户DAU df_ret_new: pd.DataFrame 新用户留存矩阵 s_ret_old: pd.Series 老用户留存率 base_date: String 新老用户边界日期，新用户最早日期 pred_start_date: String 开始预测日期 n_forcast: int 预测天数 dau_old: 老用户总数 n_train: 训练集容量 &quot;&quot;&quot; self.base_date = pd.to_datetime(base_date) self.pred_start_date_origin = pd.to_datetime(pred_start_date) self.pred_end_date_origin = self.pred_start_date_origin + timedelta(n_forcast - 1) # 数据末端对齐, 重新定义开始预估日期和预估长度 max_dau_new_date = s_dau_new.index.max() max_ret_new_date = df_ret_new.index.max() + timedelta(1) max_ret_old_date = s_ret_old.index.max() end_date = min(max_dau_new_date, max_ret_new_date, max_ret_old_date, self.pred_start_date_origin - timedelta(1)) self.s_dau_new = s_dau_new.loc[: end_date] self.df_ret_new = df_ret_new.loc[: end_date - timedelta(1)] self.s_ret_old = s_ret_old.loc[: end_date] self.pred_start_date = end_date + timedelta(1) self.pred_end_date = self.pred_end_date_origin self.n_forcast = (self.pred_end_date - self.pred_start_date).days + 1 self.n_train = n_train self.n_offset = (self.pred_start_date - self.base_date).days self.n_forcast = n_forcast self.dau_old = dau_old self.ratio_matrix = None self.dau_matrix = None self.df_result = pd.DataFrame(index=pd.date_range(self.pred_start_date, self.pred_end_date)) def ets(self, s_train, trend=&#x27;add&#x27;, seasonal=&#x27;add&#x27;, seasonal_periods=7, damped=False, smoothing_level=None, smoothing_slope=None, smoothing_seasonal=None, damping_slope=None, use_boxcox=False, remove_bias=False): &quot;&quot;&quot; 指数平滑，返回未来 n_forcast 天的预估值构成的Series :s_train 训练集 Series :n_forcast 预测天数 :trend 趋势类型 :seasonal 季节类型 :seasonal_periods 季节周期 &quot;&quot;&quot; model = ExponentialSmoothing(s_train, trend=trend, seasonal=seasonal, seasonal_periods=seasonal_periods, damped=damped).fit(smoothing_level=smoothing_level, smoothing_slope=smoothing_slope, smoothing_seasonal=smoothing_seasonal, damping_slope=damping_slope, use_boxcox=use_boxcox, remove_bias=remove_bias) pred_result = model.forecast(self.n_forcast) return pred_result def arima(self, train, pdq, pdqs, trend): &quot;&quot;&quot; arima 模型: 输入训练集和预测未来天数，返回预测结果Series &quot;&quot;&quot; train_log = np.log(train.loc[self.base_date:]) model = sm.tsa.statespace.SARIMAX(train_log, order=pdq, seasonal_order=pdqs, trend=trend, measurement_error=False, time_varying_regression=False, mle_regression=True, simple_differencing=False, enforce_stationarity=True, enforce_invertibility=True, hamilton_representation=False, concentrate_scale=False ).fit() pred_result = model.predict(self.pred_start_date, self.pred_end_date, dynamic=True, typ=&#x27;levels&#x27;) return np.exp(pred_result) def pred_dau_new(self, trend=None, seasonal=&#x27;add&#x27;, seasonal_periods=7, damped=False, use_boxcox=False, model=&#x27;ets&#x27;, smoothing_level=None, smoothing_slope=None, smoothing_seasonal=None, damping_slope=None): &quot;&quot;&quot; 预测新用户 DAU，返回Series； 将 base_date 之前的老用户 DAU 算作第零天 DAU &quot;&quot;&quot; s_train_data = self.s_dau_new if model == &#x27;ets&#x27;: s_pred_data = self.ets(s_train_data, trend=trend, seasonal=seasonal, seasonal_periods=seasonal_periods, damped=damped, use_boxcox=use_boxcox, smoothing_level=smoothing_level, smoothing_slope=smoothing_slope, smoothing_seasonal=smoothing_seasonal, damping_slope=damping_slope, remove_bias=True) elif model == &#x27;arima&#x27;: s_pred_data = self.arima(s_train_data, pdq=[1, 2, 0], pdqs=[3, 0, 2, 7], trend=&#x27;c&#x27;) else: raise Exception(&#x27;请输入正确的model: ets 或者 arima&#x27;) s_pred_data = pd.concat([s_train_data, s_pred_data]).loc[self.base_date: self.pred_end_date] s_pred_data[pd.to_datetime(self.base_date) - timedelta(1)] = self.dau_old s_pred_data = s_pred_data.sort_index() return s_pred_data def pred_ret_new(self, trend=&#x27;add&#x27;, seasonal=&#x27;add&#x27;, seasonal_periods=7, damped=False, use_boxcox=False, smoothing_level=None, smoothing_slope=None, smoothing_seasonal=None, damping_slope=None): &quot;&quot;&quot; 预测新用户第 N 日留存率，返回 DataFrame shape=(self.n_offset + self.n_forcast, self.n_offset + self.n_forcast) 加上第 0 日留存的一列 &quot;&quot;&quot; pred_matrix = pd.DataFrame(index=pd.date_range(self.base_date, self.pred_end_date)) # todo 数据源数据不全检查 for n in range(98,100): for n in range(self.n_offset + self.n_forcast): if not n: pred_matrix[n] = 1 else: train_start_date = min(self.pred_start_date - timedelta(n + self.n_train), self.base_date) train_end_date = self.pred_start_date - timedelta(n + 1) s_train_data = self.df_ret_new.loc[train_start_date:train_end_date, n] s_pred_data = self.ets(s_train_data, trend=trend, seasonal=seasonal, seasonal_periods=seasonal_periods, damped=damped, use_boxcox=use_boxcox, smoothing_level=smoothing_level, smoothing_slope=smoothing_slope, smoothing_seasonal=smoothing_seasonal, damping_slope=damping_slope, remove_bias=True) pred_matrix[n] = pd.concat([s_train_data, s_pred_data]).sort_index() pred_matrix.sort_index(axis=1) return pred_matrix def pred_ret_old(self, trend=&#x27;add&#x27;, seasonal=&#x27;add&#x27;, seasonal_periods=7, damped=False, use_boxcox=False, smoothing_level=0.5, smoothing_slope=None, smoothing_seasonal=None, damping_slope=None): &quot;&quot;&quot; 预测老用户第 N 日留存率，返回时间序列 DataFrame 转化为和新用户留存率矩阵行的形式，并加上第0日留存率=1 &quot;&quot;&quot; s_train_data = self.s_ret_old.copy() s_pred_data = self.ets(s_train_data, trend=trend, seasonal=seasonal, seasonal_periods=seasonal_periods, damped=damped, use_boxcox=use_boxcox, smoothing_level=smoothing_level, smoothing_slope=smoothing_slope, smoothing_seasonal=smoothing_seasonal, damping_slope=damping_slope, remove_bias=True) min_index = pd.to_datetime(self.base_date) - timedelta(1) s_train_data[min_index] = 1 result = pd.concat([s_train_data, s_pred_data]).sort_index() result = result.loc[min_index: self.pred_end_date].to_frame(&#x27;ratio&#x27;) result[&#x27;date&#x27;] = min_index result[&#x27;offset&#x27;] = (result.index - min_index).map(lambda x: x.days) result = result.pivot(index=&#x27;date&#x27;, columns=&#x27;offset&#x27;, values=&#x27;ratio&#x27;).rename_axis(index=None, columns=None) result = result.sort_index().sort_index(axis=1) return result def predict(self): &quot;&quot;&quot; 综合预测未来DAU &quot;&quot;&quot; df_dau_new = self.pred_dau_new(trend=&#x27;add&#x27;, seasonal=&#x27;add&#x27;, seasonal_periods=7, damped=True, use_boxcox=False, model=&#x27;ets&#x27;, smoothing_level=0.8, smoothing_slope=0.9, smoothing_seasonal=None, damping_slope=0.85) df_ratio_new = self.pred_ret_new(trend=&#x27;add&#x27;, seasonal=&#x27;add&#x27;, seasonal_periods=7, damped=False, use_boxcox=False, smoothing_level=0.9, smoothing_slope=0.7, smoothing_seasonal=None, damping_slope=0.75) df_ratio_old = self.pred_ret_old() self.ratio_matrix = pd.concat([df_ratio_old, df_ratio_new]).sort_index().sort_index(axis=1) self.dau_matrix = self.ratio_matrix.mul(df_dau_new, axis=&#x27;index&#x27;) self.df_result[&#x27;total_dau&#x27;] = [np.sum(np.diag(np.fliplr(np.array(self.dau_matrix)), d)) for d in range(self.n_forcast - 1, -1, -1)] return self.df_result @classmethod def rmse(cls, predictions, targets): &quot;&quot;&quot; 计算序列 predictions 和 targets 的均方根误差率 Args: predictions: targets: Returns: &quot;&quot;&quot; return np.sqrt((((predictions - targets) / targets) ** 2).mean()) @classmethod def predict_compare(cls, predictions, targets, title): &quot;&quot;&quot; 计算误差率，绘制图形 &quot;&quot;&quot; pred_index = targets.index compare = pd.concat([targets, predictions], axis=1) colname_origin, colname_predict = &#x27;&#123;&#125;_origin&#x27;.format(targets.name), &#x27;&#123;&#125;_predict&#x27;.format(targets.name) compare.columns = [colname_origin, colname_predict] compare[&#x27;&#123;&#125;_diff&#x27;.format(targets.name)] = compare[colname_predict] - compare[colname_origin] compare[&#x27;&#123;&#125;_rate&#x27;.format(targets.name)] = compare[&#x27;&#123;&#125;_diff&#x27;.format(targets.name)] / compare[colname_origin] fig = plt.figure(figsize=(18, 8)) plt.plot(compare.index, compare[colname_origin], label=colname_origin) plt.plot(compare.index, compare[colname_predict], label=colname_predict) plt.legend() plt.title(u&#x27;预估效果对比:&#123;&#125;&#x27;.format(title)) plt.xticks(rotation=30) rmse_rate = cls.rmse(predictions, targets[predictions.index]) print(&#x27;RMSE=&#123;&#125;&#x27;.format(rmse_rate)) return compare 123456base_date, pred_start_date = pd.to_datetime(&#x27;2020-03-01&#x27;), pd.to_datetime(&#x27;2020-06-01&#x27;)n_forcast = 10pred_end_date = pred_start_date + timedelta(n_forcast-1)myets = MyEts(s_dau_new, df_ret_new, s_ret_old, base_date=base_date, pred_start_date=pred_start_date, n_forcast=n_forcast)pred_start_date, pred_end_date (Timestamp(&#39;2020-06-01 00:00:00&#39;), Timestamp(&#39;2020-06-10 00:00:00&#39;)) 新增用户ARIMA平稳性检验1234567891011121314151617181920212223242526272829303132333435def judge_stationarity(data_sanya_one): &quot;&quot;&quot; 平稳性检验 &quot;&quot;&quot; dftest = ts.adfuller(data_sanya_one) print(dftest) dfoutput = pd.Series(dftest[0:4], index=[&#x27;Test Statistic&#x27;,&#x27;p-value&#x27;,&#x27;#Lags Used&#x27;,&#x27;Number of Observations Used&#x27;]) stationarity = 1 for key, value in dftest[4].items(): dfoutput[&#x27;Critical Value (%s)&#x27;%key] = value if dftest[0] &gt; value: stationarity = 0 print(dfoutput) print(&quot;数据是否平稳(1/0): %d&quot; %(stationarity)) return stationaritydef season_resolve(data): &quot;&quot;&quot; 季节性分解：observed = trend + seasonal + residual &quot;&quot;&quot; decomposition = seasonal_decompose(data) trend = decomposition.trend seasonal = decomposition.seasonal residual = decomposition.resid fig = decomposition.plot() fig.set_size_inches(6, 4) print(&quot;test: p=&#123;&#125;&quot;.format(ts.adfuller(seasonal)[1])) # 残差是否平稳 stationarity = judge_stationarity(residual.dropna())s_dau_new_log = np.log(myets.s_dau_new.loc[&#x27;2020-03-01&#x27;:])s_dau_new_log_diff = s_dau_new_log.diff().dropna()s_dau_new_log_diff2 = s_dau_new_log_diff.diff().dropna()season_resolve(s_dau_new_log) test: p=0.0 (-3.387682117502709, 0.01138602363842863, 11, 74, &#123;&#39;5%&#39;: -2.9014701097664504, &#39;1%&#39;: -3.5219803175527606, &#39;10%&#39;: -2.58807215485756&#125;, -100.45009341551648) Test Statistic -3.39 p-value 0.01 #Lags Used 11.00 Number of Observations Used 74.00 Critical Value (5%) -2.90 Critical Value (1%) -3.52 Critical Value (10%) -2.59 dtype: float64 数据是否平稳(1/0): 0 1season_resolve(s_dau_new_log_diff) test: p=0.0 (-5.650643581278351, 9.878845977099614e-07, 9, 75, &#123;&#39;5%&#39;: -2.9009249540740742, &#39;1%&#39;: -3.520713130074074, &#39;10%&#39;: -2.5877813777777776&#125;, -88.7065598940126) Test Statistic -5.65 p-value 0.00 #Lags Used 9.00 Number of Observations Used 75.00 Critical Value (5%) -2.90 Critical Value (1%) -3.52 Critical Value (10%) -2.59 dtype: float64 数据是否平稳(1/0): 1 1season_resolve(s_dau_new_log_diff2) test: p=0.0 (-5.898783625957804, 2.8100827285187457e-07, 11, 72, &#123;&#39;5%&#39;: -2.9026070739026064, &#39;1%&#39;: -3.524624466842421, &#39;10%&#39;: -2.5886785262345677&#125;, -68.75760108315257) Test Statistic -5.90 p-value 0.00 #Lags Used 11.00 Number of Observations Used 72.00 Critical Value (5%) -2.90 Critical Value (1%) -3.52 Critical Value (10%) -2.59 dtype: float64 数据是否平稳(1/0): 1 自相关图12345678910111213def plot_acf_pacf(df_list): &quot;&quot;&quot; 绘制自相关图，偏自相关图 &quot;&quot;&quot; n = len(df_list) base_num = 100 * n + 20 plt.figure(figsize=(16, 3 * n)) plt.subplots_adjust(wspace =0.1, hspace =0.3) for i in range(n): tmp = base_num + 2 * i + 1 one_1 = plot_acf(df_list[i], lags=40, title= u&#x27;ACF&#x27;, ax=plt.subplot(tmp)) one_2 = plot_pacf(df_list[i], lags=40, title= u&#x27;PACF&#x27;, ax=plt.subplot(tmp+1)) 1plot_acf_pacf([s_dau_new_log, s_dau_new_log_diff]) 网格搜索1234567891011121314151617181920212223def parameter_selection(df, ps=[0,1,2], ds=[2], qs=[0,1,2]): &quot;&quot;&quot; 通过网格搜索对模型p,d,q进行定阶，取损失最小 &quot;&quot;&quot; pdqs = [(x[0], x[1], x[2]) for x in itertools.product(ps, ds, qs)] best_bic = 1000000.0 best_pdqs = None for param in pdqs: p, d, q = param try: model = ARIMA(df, (p, d, q)) results = model.fit() bic = results.aic print(&#x27;&#123;&#125; - BIC:&#123;&#125;&#x27;.format(param, bic)) if bic &lt; best_bic: best_bic = bic best_pdqs = param except Exception as e: pass# print &#x27;error:&#123;&#125;&#x27;.format(e) print &#x27;最优参数：x&#123;&#125; - AIC:&#123;&#125;&#x27;.format(best_pdqs, best_bic) return best_pdqs, best_bic 1matrix = parameter_selection(s_dau_new_log, ps=range(4), qs=range(4)) (0, 2, 0) - BIC:15.5504937045 (0, 2, 1) - BIC:-3.90439313959 (0, 2, 2) - BIC:-16.7008738132 (1, 2, 0) - BIC:13.7741898435 (1, 2, 1) - BIC:-13.1288961798 (1, 2, 2) - BIC:-14.7089870806 (1, 2, 3) - BIC:-13.2074940281 (2, 2, 0) - BIC:0.12566262446 (2, 2, 1) - BIC:-14.3782928385 (2, 2, 2) - BIC:-13.0656053424 (2, 2, 3) - BIC:-13.1196434498 (3, 2, 0) - BIC:-1.77302826664 (3, 2, 1) - BIC:-13.5275309286 (3, 2, 2) - BIC:-10.9182600295 (3, 2, 3) - BIC:-10.6767970671 最优参数：x(0, 2, 2) - AIC:-16.7008738132 12345678910111213141516171819202122232425262728293031def max_node(p, d, q, P, D, Q, s): return d + D * s + max(3 * q + 1, 3 * Q * s + 1, p, P * s) + 1 def get_arima_params(data, pdq, ps=[0,1,2], ds=[0,1,2], qs=[0,1,2], m=7): &quot;&quot;&quot; 通过网格搜索确定季节ARIMA的最优参数 @data: 用于拟合的数据源 @pdq: 最优非季节性参数元组(p, d, q) @季节性周期 &quot;&quot;&quot; p, d, q = pdq seasonal_pdq = [(x[0], x[1], x[2], m) for x in list(itertools.product(ps, ds, qs))] score_aic = 1000000.0 warnings.filterwarnings(&quot;ignore&quot;) for param_seasonal in seasonal_pdq: P, D, Q, s = param_seasonal try: mod = sm.tsa.statespace.SARIMAX(data, order=pdq, seasonal_order=param_seasonal, enforce_stationarity=False, enforce_invertibility=False) results = mod.fit() print(&#x27;x&#123;&#125; - AIC:&#123;&#125; need &#123;&#125; observations&#x27;.format(param_seasonal, results.aic, max_node(p, d, q, P, D, Q, s))) if results.aic &lt; score_aic: score_aic = results.aic params = param_seasonal, results.aic except Exception as e: print &#x27;error:&#123;&#125;&#x27;.format(e) pass param_seasonal, results.aic = params print(&#x27;最优参数：x&#123;&#125; - AIC:&#123;&#125;&#x27;.format(param_seasonal, results.aic)) 12pdq = [0, 2, 2]get_arima_params(s_dau_new_log, pdq, range(4),range(3), range(3), 7) x(0, 0, 0, 7) - AIC:-15.9235194034 need 10 observations x(0, 0, 1, 7) - AIC:-14.7113328488 need 25 observations x(0, 0, 2, 7) - AIC:-29.4064147183 need 46 observations x(0, 1, 0, 7) - AIC:63.48723362 need 17 observations x(0, 1, 1, 7) - AIC:-2.14367734565 need 32 observations x(0, 1, 2, 7) - AIC:-11.8848525187 need 53 observations x(0, 2, 0, 7) - AIC:141.898368919 need 24 observations x(0, 2, 1, 7) - AIC:39.8814708492 need 39 observations x(0, 2, 2, 7) - AIC:2.67172770386 need 60 observations x(1, 0, 0, 7) - AIC:-13.7461303101 need 10 observations x(1, 0, 1, 7) - AIC:-13.4284053199 need 25 observations x(1, 0, 2, 7) - AIC:-30.6083506043 need 46 observations x(1, 1, 0, 7) - AIC:26.5931599614 need 17 observations x(1, 1, 1, 7) - AIC:11.54952856 need 32 observations x(1, 1, 2, 7) - AIC:-11.5907261101 need 53 observations x(1, 2, 0, 7) - AIC:79.078108805 need 24 observations x(1, 2, 1, 7) - AIC:47.7740298619 need 39 observations x(1, 2, 2, 7) - AIC:6.90057268677 need 60 observations x(2, 0, 0, 7) - AIC:-41.7769269787 need 17 observations x(2, 0, 1, 7) - AIC:-40.1960951619 need 25 observations x(2, 0, 2, 7) - AIC:-37.2401608236 need 46 observations x(2, 1, 0, 7) - AIC:-9.33829251277 need 24 observations x(2, 1, 1, 7) - AIC:-19.2066580221 need 32 observations x(2, 1, 2, 7) - AIC:-22.4909020767 need 53 observations x(2, 2, 0, 7) - AIC:25.240615073 need 31 observations x(2, 2, 1, 7) - AIC:18.7858056712 need 39 observations x(2, 2, 2, 7) - AIC:11.9538869508 need 60 observations x(3, 0, 0, 7) - AIC:-38.0001167775 need 24 observations x(3, 0, 1, 7) - AIC:-45.2099169457 need 25 observations x(3, 0, 2, 7) - AIC:-49.2523157208 need 46 observations x(3, 1, 0, 7) - AIC:-32.9888239169 need 31 observations x(3, 1, 1, 7) - AIC:-31.4343836764 need 32 observations x(3, 1, 2, 7) - AIC:-30.8665326957 need 53 observations x(3, 2, 0, 7) - AIC:8.55482266627 need 38 observations x(3, 2, 1, 7) - AIC:-4.72304771546 need 39 observations x(3, 2, 2, 7) - AIC:-3.35845958847 need 60 observations 最优参数：x(3, 0, 2, 7) - AIC:-49.2523157208 1234567pred_dau_arima = myets.arima(train=myets.s_dau_new, pdq=[0,2,2], pdqs=[3, 0, 2, 7], trend=&#x27;t&#x27; )pred_dau_arima_diff = myets.predict_compare(pred_dau_arima.loc[pred_start_date: pred_end_date], s_dau_new.loc[&#x27;2020-03-01&#x27;:], u&#x27;新用户第N日留存率-ARIMA&#x27;) RMSE=0.078732475591 1df_show(pred_dau_arima_diff.loc[pred_start_date: pred_end_date], 0, 14).dropna() shape=(10, 4) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } f_dau_new_origin f_dau_new_predict f_dau_new_diff f_dau_new_rate f_date 2020-06-01 588936.00 566249.56 -22686.44 -0.04 2020-06-02 507322.00 529802.47 22480.47 0.04 2020-06-03 459898.00 516113.69 56215.69 0.12 2020-06-04 434520.00 486794.93 52274.93 0.12 2020-06-05 440420.00 451129.71 10709.71 0.02 2020-06-06 419283.00 455463.38 36180.38 0.09 2020-06-07 409955.00 433574.34 23619.34 0.06 2020-06-08 422092.00 398749.40 -23342.60 -0.06 2020-06-09 412235.00 374202.33 -38032.67 -0.09 ETS1234567891011121314pred_dau_new = myets.pred_dau_new(trend=&#x27;add&#x27;, seasonal=&#x27;add&#x27;, seasonal_periods=7, damped=True, use_boxcox=False, model=&#x27;ets&#x27;, smoothing_level=0.8, smoothing_slope=0.9, smoothing_seasonal=None, damping_slope=0.85)pred_dau_new_diff = myets.predict_compare(pred_dau_new.loc[pred_start_date: pred_end_date], s_dau_new.loc[&#x27;2020-03-01&#x27;:], u&#x27;新用户第N日留存率-ETS&#x27;) RMSE=0.0654776482142 1df_show(pred_dau_new_diff.loc[pred_start_date: pred_end_date],0,14).dropna() shape=(10, 4) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } f_dau_new_origin f_dau_new_predict f_dau_new_diff f_dau_new_rate 2020-06-01 588936.00 563557.69 -25378.31 -0.04 2020-06-02 507322.00 518941.51 11619.51 0.02 2020-06-03 459898.00 517735.65 57837.65 0.13 2020-06-04 434520.00 486268.62 51748.62 0.12 2020-06-05 440420.00 463959.35 23539.35 0.05 2020-06-06 419283.00 431672.77 12389.77 0.03 2020-06-07 409955.00 417944.87 7989.87 0.02 2020-06-08 422092.00 416701.86 -5390.14 -0.01 2020-06-09 412235.00 394114.05 -18120.95 -0.04 新用户留存1df_show(df_ret_new,3,10) shape=(160, 120) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } f_remain_days 1 2 3 4 5 6 7 8 9 10 ... 111 112 113 114 115 116 117 118 119 120 f_visit_day 2020-01-01 0.02 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.00 0.01 ... 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 2020-01-02 0.02 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 ... 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 2020-01-03 0.02 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 ... 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 2020-05-30 0.11 0.05 0.03 0.02 0.01 0.01 0.01 0.01 0.01 0.01 ... nan nan nan nan nan nan nan nan nan nan 2020-05-31 0.07 0.03 0.02 0.01 0.01 0.01 0.01 0.01 0.01 nan ... nan nan nan nan nan nan nan nan nan nan 2020-06-01 0.06 0.03 0.02 0.01 0.01 0.01 0.01 0.01 nan nan ... nan nan nan nan nan nan nan nan nan nan 2020-06-02 0.05 0.02 0.02 0.01 0.01 0.01 0.01 nan nan nan ... nan nan nan nan nan nan nan nan nan nan 2020-06-03 0.05 0.02 0.01 0.01 0.01 0.01 nan nan nan nan ... nan nan nan nan nan nan nan nan nan nan 2020-06-04 0.04 0.02 0.01 0.01 0.01 nan nan nan nan nan ... nan nan nan nan nan nan nan nan nan nan 2020-06-05 0.04 0.02 0.01 0.01 nan nan nan nan nan nan ... nan nan nan nan nan nan nan nan nan nan 2020-06-06 0.03 0.02 0.01 nan nan nan nan nan nan nan ... nan nan nan nan nan nan nan nan nan nan 2020-06-07 0.03 0.02 nan nan nan nan nan nan nan nan ... nan nan nan nan nan nan nan nan nan nan 2020-06-08 0.04 nan nan nan nan nan nan nan nan nan ... nan nan nan nan nan nan nan nan nan nan 13 rows × 120 columns 新用户第 N 日留存预估123456789101112131415161718192021222324def predict_ret_new_n(n): train_start_date = myets.pred_start_date - timedelta(n + myets.n_train) train_end_date = myets.pred_start_date - timedelta(n + 1) s_train_data = myets.df_ret_new.loc[train_start_date:train_end_date, n] s_pred_data = myets.ets(s_train_data, trend=&#x27;add&#x27;, seasonal=&#x27;add&#x27;, seasonal_periods=7, damped=True, use_boxcox=False, smoothing_level=0.8, smoothing_slope=0.75, smoothing_seasonal=None, damping_slope=0.76) return s_pred_data# n = 18for n in range(1,3): s_ret_new_n = df_ret_new.loc[:, n].dropna() pred_ret_new_n = predict_ret_new_n(n) pred_dau_new_diff = myets.predict_compare(pred_ret_new_n, s_ret_new_n.loc[&#x27;2020-03-01&#x27;:],n) RMSE=0.227393637754 RMSE=1.19606960639 新用户第 N 日留存预估矩阵123456789pred_ret_new_ratio = myets.pred_ret_new(trend=None, seasonal=&#x27;add&#x27;, seasonal_periods=7, damped=False, use_boxcox=False, smoothing_level=0.9, smoothing_slope=0.7, smoothing_seasonal=None, damping_slope=0.75) 1df_show(pred_ret_new_ratio,3,15) shape=(102, 102) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 3 4 5 6 7 8 9 ... 92 93 94 95 96 97 98 99 100 101 2020-03-01 1 0.14 0.06 0.04 0.03 0.02 0.02 0.02 0.02 0.01 ... 0.00 0.00 0.00 0.00 0.01 0.00 0.01 0.01 0.00 0.00 2020-03-02 1 0.10 0.05 0.03 0.03 0.02 0.02 0.02 0.01 0.01 ... 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 nan 2020-03-03 1 0.08 0.04 0.03 0.02 0.02 0.02 0.02 0.02 0.01 ... 0.01 0.01 0.00 0.00 0.00 0.00 0.01 0.01 nan nan 2020-05-27 1 0.18 0.08 0.04 0.02 0.02 0.01 0.01 0.01 0.01 ... nan nan nan nan nan nan nan nan nan nan 2020-05-28 1 0.17 0.06 0.03 0.03 0.02 0.02 0.01 0.01 0.01 ... nan nan nan nan nan nan nan nan nan nan 2020-05-29 1 0.14 0.06 0.04 0.03 0.02 0.02 0.01 0.01 0.01 ... nan nan nan nan nan nan nan nan nan nan 2020-05-30 1 0.11 0.07 0.04 0.03 0.02 0.02 0.01 0.01 0.01 ... nan nan nan nan nan nan nan nan nan nan 2020-05-31 1 0.11 0.06 0.04 0.03 0.02 0.01 0.01 0.01 0.01 ... nan nan nan nan nan nan nan nan nan nan 2020-06-01 1 0.11 0.06 0.04 0.03 0.02 0.01 0.01 0.01 0.01 ... nan nan nan nan nan nan nan nan nan nan 2020-06-02 1 0.11 0.06 0.04 0.02 0.02 0.01 0.01 0.01 nan ... nan nan nan nan nan nan nan nan nan nan 2020-06-03 1 0.11 0.06 0.03 0.02 0.02 0.01 0.01 nan nan ... nan nan nan nan nan nan nan nan nan nan 2020-06-04 1 0.11 0.06 0.03 0.03 0.02 0.02 nan nan nan ... nan nan nan nan nan nan nan nan nan nan 2020-06-05 1 0.12 0.06 0.04 0.03 0.02 nan nan nan nan ... nan nan nan nan nan nan nan nan nan nan 2020-06-06 1 0.11 0.07 0.04 0.03 nan nan nan nan nan ... nan nan nan nan nan nan nan nan nan nan 2020-06-07 1 0.11 0.06 0.04 nan nan nan nan nan nan ... nan nan nan nan nan nan nan nan nan nan 2020-06-08 1 0.11 0.06 nan nan nan nan nan nan nan ... nan nan nan nan nan nan nan nan nan nan 2020-06-09 1 0.11 nan nan nan nan nan nan nan nan ... nan nan nan nan nan nan nan nan nan nan 2020-06-10 1 nan nan nan nan nan nan nan nan nan ... nan nan nan nan nan nan nan nan nan nan 18 rows × 102 columns 老用户留存老用户第 N 日留存率预估1234567891011121314# 转置s_pred_data = myets.ets(myets.s_ret_old, trend=&#x27;add&#x27;, seasonal=&#x27;add&#x27;, seasonal_periods=7, damped=False, use_boxcox=False, smoothing_level=0.5, smoothing_slope=None, smoothing_seasonal=None, damping_slope=None)pred_ret_old_result = myets.predict_compare(s_pred_data, s_ret_old.loc[&#x27;2020-03-01&#x27;:], u&#x27;老用户第N日留存-ETS&#x27;) RMSE=0.127854181665 1df_show(pred_ret_old_result,0,15) shape=(102, 4) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } f_ratio_origin f_ratio_predict f_ratio_diff f_ratio_rate f_date 2020-05-27 0.01 nan nan nan 2020-05-28 0.01 nan nan nan 2020-05-29 0.01 nan nan nan 2020-05-30 0.01 nan nan nan 2020-05-31 0.01 nan nan nan 2020-06-01 0.01 0.01 0.00 0.00 2020-06-02 0.01 0.01 0.00 0.12 2020-06-03 0.01 0.01 0.00 0.17 2020-06-04 0.01 0.01 0.00 0.23 2020-06-05 0.01 0.01 0.00 0.12 2020-06-06 0.01 0.01 0.00 0.13 2020-06-07 0.01 0.01 0.00 0.08 2020-06-08 0.01 0.01 0.00 0.06 2020-06-09 0.01 0.01 0.00 0.11 2020-06-10 nan 0.01 nan nan 新老用户留存率矩阵1234567891011# 备份修改pred_ret_old_ratio = myets.pred_ret_old(trend=&#x27;add&#x27;, seasonal=&#x27;add&#x27;, seasonal_periods=7, damped=False, use_boxcox=False, smoothing_level=0.5, smoothing_slope=None, smoothing_seasonal=None, damping_slope=None)pred_ret_old_ratio .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 3 4 5 6 7 8 9 ... 93 94 95 96 97 98 99 100 101 102 2020-02-29 1.00 0.05 0.05 0.04 0.04 0.04 0.04 0.03 0.03 0.03 ... 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 1 rows × 103 columns 12ratio_matrix = pd.concat([pred_ret_old_ratio, pred_ret_new_ratio]).sort_index().sort_index(axis=1)df_show(ratio_matrix, 10,10) shape=(103, 103) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 3 4 5 6 7 8 9 ... 93 94 95 96 97 98 99 100 101 102 2020-02-29 1.00 0.05 0.05 0.04 0.04 0.04 0.04 0.03 0.03 0.03 ... 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 2020-03-01 1.00 0.14 0.06 0.04 0.03 0.02 0.02 0.02 0.02 0.01 ... 0.00 0.00 0.00 0.01 0.00 0.01 0.01 0.00 0.00 nan 2020-03-02 1.00 0.10 0.05 0.03 0.03 0.02 0.02 0.02 0.01 0.01 ... 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 nan nan 2020-03-03 1.00 0.08 0.04 0.03 0.02 0.02 0.02 0.02 0.02 0.01 ... 0.01 0.00 0.00 0.00 0.00 0.01 0.01 nan nan nan 2020-03-04 1.00 0.07 0.04 0.03 0.02 0.02 0.02 0.02 0.01 0.01 ... 0.01 0.00 0.00 0.01 0.01 0.01 nan nan nan nan 2020-03-05 1.00 0.06 0.03 0.02 0.02 0.02 0.02 0.02 0.01 0.01 ... 0.01 0.00 0.01 0.01 0.01 nan nan nan nan nan 2020-03-06 1.00 0.06 0.03 0.02 0.02 0.02 0.02 0.02 0.01 0.01 ... 0.00 0.00 0.01 0.01 nan nan nan nan nan nan 2020-03-07 1.00 0.06 0.03 0.02 0.02 0.02 0.02 0.01 0.01 0.01 ... 0.01 0.00 0.00 nan nan nan nan nan nan nan 2020-03-08 1.00 0.05 0.03 0.02 0.02 0.02 0.02 0.01 0.01 0.01 ... 0.00 0.00 nan nan nan nan nan nan nan nan 2020-03-09 1.00 0.06 0.03 0.02 0.02 0.02 0.02 0.02 0.01 0.01 ... 0.01 nan nan nan nan nan nan nan nan nan 2020-06-01 1.00 0.11 0.06 0.04 0.03 0.02 0.01 0.01 0.01 0.01 ... nan nan nan nan nan nan nan nan nan nan 2020-06-02 1.00 0.11 0.06 0.04 0.02 0.02 0.01 0.01 0.01 nan ... nan nan nan nan nan nan nan nan nan nan 2020-06-03 1.00 0.11 0.06 0.03 0.02 0.02 0.01 0.01 nan nan ... nan nan nan nan nan nan nan nan nan nan 2020-06-04 1.00 0.11 0.06 0.03 0.03 0.02 0.02 nan nan nan ... nan nan nan nan nan nan nan nan nan nan 2020-06-05 1.00 0.12 0.06 0.04 0.03 0.02 nan nan nan nan ... nan nan nan nan nan nan nan nan nan nan 2020-06-06 1.00 0.11 0.07 0.04 0.03 nan nan nan nan nan ... nan nan nan nan nan nan nan nan nan nan 2020-06-07 1.00 0.11 0.06 0.04 nan nan nan nan nan nan ... nan nan nan nan nan nan nan nan nan nan 2020-06-08 1.00 0.11 0.06 nan nan nan nan nan nan nan ... nan nan nan nan nan nan nan nan nan nan 2020-06-09 1.00 0.11 nan nan nan nan nan nan nan nan ... nan nan nan nan nan nan nan nan nan nan 2020-06-10 1.00 nan nan nan nan nan nan nan nan nan ... nan nan nan nan nan nan nan nan nan nan 20 rows × 103 columns DAU留存矩阵12dau_matrix = ratio_matrix.mul(pred_dau_new, axis=&#x27;index&#x27;)df_show(dau_matrix, 10,10) shape=(103, 103) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 3 4 5 6 7 8 9 ... 93 94 95 96 97 98 99 100 101 102 2020-02-29 273385370.00 12958148.00 12452616.00 11797162.00 11073535.00 10472504.00 9798707.00 9117300.00 8284139.00 8039563.00 ... 2699356.59 2715180.35 2626011.84 2671154.75 2519520.16 2264857.58 2199788.72 2234150.08 2249973.83 2160805.32 2020-03-01 1475032.00 204291.93 94549.55 58411.27 43218.44 34663.25 29648.14 25223.05 22420.49 20650.45 ... 6936.95 5220.64 7032.82 7539.64 4889.92 7497.01 7614.17 7092.76 7053.06 nan 2020-03-02 1347642.00 134494.67 65495.40 44202.66 34364.87 27896.19 23448.97 21966.56 19675.57 18866.99 ... 9554.92 9098.68 9092.78 8151.67 6763.07 11073.35 9959.14 9417.70 nan nan 2020-03-03 1112908.00 87029.41 47632.46 33943.69 26932.37 21924.29 20700.09 18362.98 17917.82 16359.75 ... 6765.79 4993.47 4390.72 4932.99 4972.60 7680.72 7071.00 nan nan nan 2020-03-04 993211.00 70021.38 38238.62 27015.34 20956.75 19268.29 16785.27 16189.34 14798.84 14600.20 ... 6376.45 3963.54 4519.98 5838.69 5006.08 7176.47 nan nan nan nan 2020-03-05 985703.00 62197.86 33612.47 23065.45 20206.91 17348.37 16559.81 14982.69 14588.40 12814.14 ... 6147.21 4536.31 6059.73 6628.64 5874.86 nan nan nan nan nan 2020-03-06 936955.00 55655.13 29139.30 22955.40 18739.10 17146.28 15272.37 14991.28 13304.76 11993.02 ... 4524.75 3964.23 4799.75 5515.87 nan nan nan nan nan nan 2020-03-07 889821.00 49029.14 28296.31 20821.81 17618.46 15304.92 14593.06 13169.35 11834.62 11389.71 ... 4509.68 3406.76 4220.58 nan nan nan nan nan nan nan 2020-03-08 780080.00 42358.34 23090.37 17629.81 14821.52 13573.39 12013.23 10999.13 10219.05 9594.98 ... 3668.65 2760.97 nan nan nan nan nan nan nan nan 2020-03-09 727798.00 42503.40 23944.55 17903.83 15429.32 12809.24 11353.65 11062.53 10261.95 9461.37 ... 5160.16 nan nan nan nan nan nan nan nan nan 2020-06-01 563557.69 62103.07 35575.73 20969.61 14305.90 10089.09 7795.20 6727.99 5750.08 5095.59 ... nan nan nan nan nan nan nan nan nan nan 2020-06-02 518941.51 56246.99 32305.73 19031.49 12288.68 9177.43 7599.00 6267.09 5469.77 nan ... nan nan nan nan nan nan nan nan nan nan 2020-06-03 517735.65 58635.20 32593.13 18076.83 12242.16 9718.19 7724.17 6392.72 nan nan ... nan nan nan nan nan nan nan nan nan nan 2020-06-04 486268.62 54515.05 29297.18 17014.05 12271.39 9261.54 7372.38 nan nan nan ... nan nan nan nan nan nan nan nan nan nan 2020-06-05 463959.35 54259.31 28966.35 17484.08 11792.88 8750.09 nan nan nan nan ... nan nan nan nan nan nan nan nan nan nan 2020-06-06 431672.77 48169.79 28347.88 16167.69 10923.33 nan nan nan nan nan ... nan nan nan nan nan nan nan nan nan nan 2020-06-07 417944.87 47617.30 27000.65 15774.89 nan nan nan nan nan nan ... nan nan nan nan nan nan nan nan nan nan 2020-06-08 416701.86 45919.82 26305.16 nan nan nan nan nan nan nan ... nan nan nan nan nan nan nan nan nan nan 2020-06-09 394114.05 42717.21 nan nan nan nan nan nan nan nan ... nan nan nan nan nan nan nan nan nan nan 2020-06-10 411632.31 nan nan nan nan nan nan nan nan nan ... nan nan nan nan nan nan nan nan nan nan 20 rows × 103 columns DAU 预估1234567891011121314# df_result[&#x27;ret_old&#x27;] = pd.Series(dau_matrix.loc[base_date-timedelta(1)].values, # index=(base_date + timedelta(i-1) for i in dau_matrix.columns))ets_result = myets.df_result.copy()ets_result[&#x27;dau_origin&#x27;] = dau.loc[pred_start_date:pred_end_date]ets_result[&#x27;dau_sum&#x27;] = [np.sum(np.diag(np.fliplr(np.array(dau_matrix)), d)) for d in range(myets.n_forcast - 1, -1, -1)]ets_result[&#x27;dau_new&#x27;] = dau_matrix[0]ets_result[&#x27;ret_old&#x27;] = dau_matrix.iloc[0, -n_forcast:].valuesets_result[&#x27;ret_new&#x27;] = [np.sum(np.diag(np.fliplr(np.array(dau_matrix.iloc[1:-1,1:-1])), d)) for d in range(myets.n_forcast - 1, -1, -1)]ets_result[&#x27;check&#x27;] = ets_result[&#x27;dau_new&#x27;] + ets_result[&#x27;ret_old&#x27;] + ets_result[&#x27;ret_new&#x27;] - ets_result[&#x27;dau_sum&#x27;]# df_result = df_result.astype(int)df_show(ets_result, 10, 10) shape=(10, 6) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } dau_origin dau_sum dau_new ret_old ret_new check 2020-06-01 3994272 3922560.87 563557.69 2699356.59 659646.59 0.00 2020-06-02 3546456 3925123.98 518941.51 2715180.35 691002.12 0.00 2020-06-03 3248834 3830069.65 517735.65 2626011.84 686322.16 -0.00 2020-06-04 3129509 3864890.96 486268.62 2671154.75 707467.59 0.00 2020-06-05 3216982 3691592.09 463959.35 2519520.16 708112.58 0.00 2020-06-06 2895258 3331318.71 431672.77 2264857.58 634788.36 0.00 2020-06-07 2926685 3222451.35 417944.87 2199788.72 604717.76 0.00 2020-06-08 3023940 3298679.82 416701.86 2234150.08 647827.88 -0.00 2020-06-09 2911481 3327172.93 394114.05 2249973.83 683085.04 0.00 2020-06-10 2737987 3258783.04 411632.31 2160805.32 686345.40 0.00 2020-06-01 3994272 3922560.87 563557.69 2699356.59 659646.59 0.00 2020-06-02 3546456 3925123.98 518941.51 2715180.35 691002.12 0.00 2020-06-03 3248834 3830069.65 517735.65 2626011.84 686322.16 -0.00 2020-06-04 3129509 3864890.96 486268.62 2671154.75 707467.59 0.00 2020-06-05 3216982 3691592.09 463959.35 2519520.16 708112.58 0.00 2020-06-06 2895258 3331318.71 431672.77 2264857.58 634788.36 0.00 2020-06-07 2926685 3222451.35 417944.87 2199788.72 604717.76 0.00 2020-06-08 3023940 3298679.82 416701.86 2234150.08 647827.88 -0.00 2020-06-09 2911481 3327172.93 394114.05 2249973.83 683085.04 0.00 2020-06-10 2737987 3258783.04 411632.31 2160805.32 686345.40 0.00 验证1result = myets.predict_compare(ets_result[&#x27;dau_sum&#x27;], dau.loc[&#x27;2020-03-01&#x27;:], u&#x27;ETS-DAU&#x27;) RMSE=0.147793852021 12arima_result = get_dau_arima()result = myets.predict_compare(arima_result.loc[&#x27;2020-06-01&#x27;: &#x27;2020-06-10&#x27;], dau.loc[&#x27;2020-03-01&#x27;:], u&#x27;ETS-DAU&#x27;) RMSE=0.378640685005]]></content>
      <categories>
        <category>数据科学</category>
      </categories>
      <tags>
        <tag>数据科学</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[行为金融学概述]]></title>
    <url>%2FInvest%2FInvest%2F%E8%A1%8C%E4%B8%BA%E9%87%91%E8%9E%8D%E5%AD%A6%E6%A6%82%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[金融市场归根结底是由人组成的，人是非理性的，是人就会犯错。就像是一场实力相当的球赛，想要赢，制胜的法则有两条：自己少犯错，同时抓住对手每一次犯错的机会发起攻击。行为金融学就是要告诉你，在金融市场上，你怎么才能少犯错，同时又该如何利用大多数人的错误，反向操作，来制定交易策略。大多数人都在犯的错误会在短期内影响价格的涨跌，而从长期来看，价格又会回归价值。掌握了这样的规律，不但可能预测市场，甚至可能战胜市场。 可惜的是行为金融学非常新锐，很多行为金融学家又都冲进了投资市场做实战，所以直至今日还没有一本权威的教科书，理论框架也没有现成的。本文框架是根据《陆蓉·行为金融学》整理而成，它凝结了许多顶尖行为金融学家的智慧。此前，你可能接触过行为金融学的不少概念、心理现象和投资启示，但它们更像是一片片绚烂的树叶，是碎片化的信息，不能有效地为你所用。而建立起学科框架后，今后你再收集树叶，它就会变成你知识体系的一部分。 什么是行为金融学行为金融学（Behavioral Finance，简称BF）是金融学、心理学、行为学、社会学等学科相交叉的边缘学科，力图揭示金融市场的非理性行为和决策规律。 传统金融学 VS 行为金融学讲一个芝加哥大学流传的笑话：有人惊呼，地上有一百块钱，传统金融学家说，这不可能，地上不应该有一百元钱的，有的话早被捡走了；而行为金融学家说，怎么不可能啊，他跑过去一看，果然有，开开心心地捡走了 100 块。传统金融学研究的是，市场“应该是”什么样，均衡来看，地上不应该有一百元，这是长期的趋势和规律。而行为金融学研究的是，真实市场“实际是”什么样，行为金融学认为，不能用“应该是”的理论去指导实际决策，这会出问题，就像监管者面对市场时，不能认为市场从长期看总是对的，就不管它了，要是早点干预市场，说不定金融危机就不会发生。 看待任何领域事物，“应该是”的视角让你把握长远基准，“实际是”的视角帮助你理解当下正在发生的事情。 如果把传统金融学比作一棵大树，树有根、有干、有枝、有叶，越是根上的理论约基础，任何学术进步都是在这棵树上的某个部位又生了新芽、发了新叶。那么，行为金融学是在树的什么位置与传统金融学分开了呢？答案是根！它们的理论基础不同，有各自的理论体系，长成了两棵完整的大树。然而，二者的树冠部分却是重合的，也就是说传统金融学研究哪些问题，行为金融学同样研究这些问题，只不过两者出发点不同，结论也就大相径庭。 行为金融学并没有取代传统金融学，两个理论都在发展，但是相对于传统金融学，行为金融学更加实战，有意思的是，理查德·塞勒常常被称为“临床经济学家”。行为金融学最擅长的是投资实战，指定交易策略，我们现在常常见到的所谓“量化交易策略”绝大多数都是以行为金融学理论为基础的。 有效市场假说 VS 错误定价传统金融学和行为金融学之间最根本的分歧在于有效市场假说与错误定价之间的分歧： 有效市场假说：传统金融学认为，金融学只是经济学基本理论在金融市场的展现而已，而经济学的最基本假设，或者说树的根是 —— 人是理性的。基于个人利益最大化的假定，所有人的决策都是可以用数学计算出来的，就像机器人一样精确。在均衡市场条件下，任何时刻观察到的金融资产价格都是那个时刻所有信息的准确反映，这就是“有效市场假说”，它是传统金融学根上的理论，提出这一理论的经济学家尤金·法玛在 2013 年获得了诺贝尔经济学奖 错误定价：行为金融学认为，金融学从根本上属于社会科学，研究的是与人相关的社会运行规律，金融市场买什么、卖什么的决策都是由人做出来的，而人的行为往往受到认知错误、心理偏差和情绪等非理性的因素的影响。金融学应该先从人出发来研究问题，而不是从数学出发来研究问题，而人是怎么想的、怎么做的，这是心理学的研究领域，“行为金融学”中的“行为”二字取自心理学中的“行为主义”，它是行为金融学的理论基础。金融资产价格常常会偏离其内在价值，也就是错误定价。心理学教授丹尼尔·卡尼曼和阿莫斯·特沃斯基系统地从心理学出发来研究金融学，开创了行为金融学科，2002 年，卡尼曼获得了诺贝尔经济学奖，这标志着这个学科正式被学术界所认可，可惜的是特沃斯基英年早逝，错过了这份荣耀 人是理性的 VS 人是非理性的传统金融学认为人是理性的，怎么理解人是理性的？人做任何决策其实都可以分为两个步骤：① 了解你需要决策的每一个对象，这叫认知；② 对他们进行比较和选择，这叫决策。传统金融学认为理性人可以正确认知他所面对的每个决策对象，并且在决策时，会选择对自己效用最大的那个，这叫“期望效用最大化”，这是经济学的最基本定理。举个例子来说，如果你是个消费者，双十一、广告应该对你没有影响；如果你是个企业家，就应该不会错过赚钱的良机；如果你是个投资者，那你就应该拥有计算机一样的记忆和计算能力。因为每个人都是理性人，所以市场决策一定会让所有的资源都得到最优的配置。 但是在行为金融学这里，观点就相反了。行为金融学认为由于情绪干扰和认知错误无处不在，你根本没有办法准确认知你的决策对象，并且你的决策并不取决于效用的绝对值，而是要看跟谁比。所有的决策都是比较而言的，比如说，我给你发一万块钱奖金，你高不高兴？你说，不好说，得看看我的同事拿到了多少钱，这就是决策非理性，也叫“前景理论”，它是行为金融学第一个获得诺贝尔奖的理论。 随机性偏差 VS 系统性偏差传统金融学家看到行为金融学家的这些研究结果，也开始让步，他们也承认认知和决策是有非理性的，但是他们说，这有什么关系呢？这些非理性是随机发生的，随机就是可以相互抵消的，因此最后均衡的结果不变，价格还是正确的。行为金融学对此又是怎么回应的呢？他们借鉴社会心理学研究结果，社会心理学认为，人都有理性的一面和非理性的一面，如果非理性的一面被统一了以后，心理群体的智商甚至比单个理性人的智商还要低，非理性的心理群体会产生系统性合力。 什么叫系统性？系统性和随机性不同，随机是大家互为不同方向，因而可以互相抵消，而系统性是同方向的。比如，现在股票价格已经跌得很惨了，但是大家会同时产生一种恐慌情绪，都不愿意买，全都在卖，这就叫系统性。社会心理所产生的的系统性影响使价格不会像传统金融学预期的那样迅速修复，而是会持续下跌，所以股票崩盘就是这么回事。 充分套利 VS 有限套利传统金融学说，好吧好吧，我也相信社会心理学有一定的道理，但是那有什么关系呢？我还有一个法宝 —— 套利，套利可以消除一切价格偏差。什么是套利呢？简单地说，低买高卖，就是套利。 获诺贝尔经济学奖的斯蒂芬·罗斯就曾说过依据经典的话：“要教会一只鹦鹉金融学，你只需要交给它一个单词‘套利’”。套利是保证传统金融学所有理论成立的基础。世界上哪怕只有一个人是理性的，都能够完成套利，都能够纠正价格偏差，价格还是对的。这是为什么呢？因为在传统金融学看来，套利有三个条件：零成本、无风险、正收益。我问你，如果有一件事儿，不要钱、还能赚到钱、又没风险，你愿意用多少钱去做这件事？理论上，你应该动用全世界的财富来做这件事，知道这个套利机会消失。所以，传统金融学说，哪怕只有一个人是理性的，套利也可以使得价格恢复到价值，所以我的理论还是成立的。 此时，行为金融学又是怎么回应的呢？它说，真实市场的套利不可能零成本、无风险、正收益，真实的套利是有局限的，称之为有限套利。 套利是有成本的：因为无论在哪个市场上买卖，两边都是需要交保证金的，并且你卖出的那个商品的收入是不可以提取出来用于买入的； 套利是有风险的：假定上海铜比伦敦铜便宜，你卖了伦敦铜来买上海铜，希望两个市场铜的几个趋于一致，后来，那个便宜的上海铜更加便宜了，你卖的那个伦敦铜更加贵了，这就会导致套利存在风险，你的套利会失败； 套利的收益也是不能保证的：因为套利不一定能坚持到省力的那个时刻，中间由于保证金不够，很可能就已经被迫平仓出局了 有限套利使得理性人不敢去套利，从而套利机会一直存在，价格错误很正常。我把有限套利称为行为金融学的一条腿，在套利不能纠正价格偏差的情况下，才有必要了解价格偏差酒精是怎样形成的，这时造成价格偏差的，也就是行为金融学的另一条腿 —— 心理学就站立起来了。 行为金融学如何影响市场参与者在金融学看来，市场上只有三种人：投资者、融资者、监管者，你必是其中之一。接下来会用三个故事告诉你，金融市场上的这三类人是怎么被行为金融学改变的。 投资者：人是不是真的有智慧战胜市场“猩猩与基金经理”是传统金融学经常引用的华尔街的一个著名实验，实验者找来一只会扔飞镖的大猩猩，让它扔飞镖来决定买什么股票，再找一些知名投资专家来精挑细选他们认为最应该买的股票，过一段时间比较猩猩和投资专家的业绩。你猜怎么着？两者真差不多，猩猩还略胜一筹。 需要强调一下，这个实验是真的，它证明了传统金融学的观点：市场是不可预测的，没有任何人、没有任何一种交易策略可以持续地打败市场，即“有效市场假说”理论。这个故事也告诉你，如果你做股票的话，那就买大盘，也就是股票指数，这就是最优组合，这个理论叫做投资组合理论，获得了 1990 年的诺贝尔经济学奖，提出这个理论的学者叫哈里·马科维茨，被人尊称为现代投资之父。 但是，到了行为金融学这里，这个故事就不成立了，为什么呢？猩猩与基金经理的实验样本量太少，这个结论不具有代表性。金融市场是由人构成的，是人就会有行为规律，人的行为是可以预测的，因此由一个个人所构成的市场就有规律可循，战胜市场是完全可能的。买什么、卖什么有讲究；什么时候买、什么时候卖也有讲究。 融资者：有没有更简单的方法借到钱按照传统金融学理论，融资者想要筹到更多的钱，要靠提升公司业绩，让公司更值钱，让投资人有信息，愿意借更多的钱给公司，但是听完这个故事，你就会知道，在股市上有更简单的方法让自己借到钱。 股票“高送转”是股市的一个特有现象，很受投资者追捧，它指的是，上市公司给每个持有其股票的股东发放一种福利，这个福利叫做股票股利。什么意思呢？公司业绩好，赚到了钱，就会把一部分钱分给股东，这就是股利，也就是我们常说的分红。股利一般都是发现金的，还有一种股利，不发现金，而是发股票，这种就称之为股票股利，如果发放比例高，就称为“高转送”。 投资者心想，这是好事啊，买这家股票还多得了，因此很愿意投资，但是他们其实什么也没得到，这就像分蛋糕，蛋糕就那么大，本来十个人，每人分一份，现在每人再多送一份，一个蛋糕被切成 20 份，那份量不就变小了吗？ 然而，奇怪的是投资者却特别喜欢这种做法，因为他们不是机器人，很容易被骗，他们把股票当成青菜来买了。10 元钱他们觉得贵，10 送 10 之后相当于变成 5 元了，他们觉得便宜，但是他们不知道的是股票总值没有变化，每股价值却缩水一半。这种现象叫做“低股价幻觉”，融资者利用投资者这种非理性就可以达到不用提升业绩就多融资的目的。 这个故事想要说明的是，真正的融资市场并不是像传统金融学讲得那样，人都是理性的，实际上在真实的市场上，时间、方式、怎样回馈投资者等，都是非常重要的。 监管者：市场到底要不要管第三个故事是关于监管者的，讲的是“监管理念与金融危机”。格林斯藩是任期最长的美联储主席，格林斯藩深受传统金融监管思想的影响，长期奉行的监管原则是 —— 尽量不要过多插手市场，当好裁判就好了。直到 2007 年金融危机爆发，格林斯藩从不承认金融危机和他有什么关系，直到 2008 年格林斯藩首次在纽约时报上公开表态，承认自己可能犯了一个错误，他说，“我可能高估了投资者的理性，低估了机构的贪婪”。 对于监管者来说，市场是管还是不管，你相信传统金融学放任自流，认为市场可以自我修复；还是相信行为金融学，市场经常会失灵，政府的积极干预是必要的，这对于整个金融市场来说，结果完全不同。 认知非理性人对任何事物的认知过程都可以分为四个阶段：信息收集、信息加工、信息输出和信息反馈。从认知心理学的角度看，人在处理信息的整个过程中都存在认知偏差。举例来说，你要做股票投资，需要了解公司，总要看企业的财务报表、听听企业和行业的新闻吧，这些都是信息的收集，在第一阶段你就会犯错；而当这家公司的所有信息都收集到之后，你的阅读理解过程，比如，读报表、听新闻时，你的内心就开始有判断了，这就睡信息加工阶段，这一阶段你又会犯错；信息输出阶段是指，在信息加工的基础上你做出对这家公司买或卖的决策，把信息输出出去，这一步你又会犯错；信息反馈阶段是指，你再做出决策后，看到决策的结果，赚钱了还是亏钱了，是会不断学习反省的，理性人总是会吸取教训的吧？但是，你的反省真的是对的吗？后面会看到，你真的会吸取经验教训，还是会重蹈覆辙。 信息收集阶段的认知偏差信息来源有很多，上网、刷微博、看微信、与朋友聊天……，这些渠道来源可以被归纳为两类： 一类来源于所有被你记住的信息：在收集这类信息的时候最容易犯“易记性偏差” 另一类是还没有被你记住的信息：在收集这类信息的时候最容易犯“首位效应”和“末位效应”偏差 易记性偏差易记性偏差是指，人有一种行为习惯，就是搜集信息时喜欢先在自己的记忆库中去找信息，容易被你记住的你会认为它是真的。比如，给你几个股票的名字，让你说哪只股票比较好，其中有些股票是你熟悉的，还有些是你不熟悉的，你会从熟悉的股票中找出那个你认为最好的，不太可能说你不熟悉的。别说普通投资者会犯这个错，就连专业的证券分析师都会犯错，他们推荐的往往都是自己经过调研的公司，而没有调研过的是否真的就不够好呢？ 那么，哪些信息容易被你记住呢？心理学家研究发现，事物的新近性、显著性、生动性、可想象性等影响人的记忆。金融业内都知道，每一年都有新财富最佳分析师评比，分析师到了这个时候会非常活跃，希望进到这个榜单中，实际上人还少那个人，为什么上不上榜就这么重要？因为上榜会影响别人的认知，让人因为熟悉而信任。此外，你有没有注意过，无论哪家证券公司，他们的分析师报告都给常注重形式，目的就是吸引人的关注，从而影响人的决策。 首位效应首位效应是指，你在做决策时首次到达的信息会被富裕最大的权重。你对一家公司仔细调研后，形成了对这家公司的一个判断，之后就不太可能改变这个判断了，如果你觉得它是一家好公司，后面又出了坏消息，你会为它辩解；如果你觉得它是家坏公司，后来出了好消息，你也不会相信这个好消息。 末位效应末位效应是指，最后到达的信息在做决策时被赋予了最大的权重。举例来说，你认真听一下下面这两句话，你觉得哪只股票好些呢？ 我给你推荐一只股票，A 股票收益不错，但有风险！我再给你推荐一只股票，B 股票有风险，但收益不错！ 首位效应和末位效应都能影响人的决策，那么我们在现实中怎么选择呢？这要看信息到达的速度和强度，如果首位效应还没对你的决策产生影响，而你又收到一个信息，很可能末位效应会占主导。比如你在看几张财务报表来评价一家公司，很可能最后看的那张报表信息对你的影响更大。 信息加工阶段的认知偏差在这个阶段，人最容易犯的错误就是代表性偏差：人们喜欢把事物分为典型的几个类别，然后在对事物进行概率估计时，过分强调这种典型类别的重要性，而不顾有关其他潜在可能性的证据，并习惯用小样本去判断大样本。 例如，你看到某位基金经理连续获得金牛奖就立即做出判断：都获得金牛奖了，那他一定是一个好基金经理，事实上你忘记了要得出正确结论，还需要很多决定性的其他信息，比如他这几次成功是偶然的，不能归于能力，如果时间放长一些，或者考虑到公司、团队、工作经历的偶然性等因素，这种随机性就会消失。也就是说，你没观察到的因素太多，代表性特征的信息量不足以做出决策。 再例如，你看到一家公司连续 3 年利润都翻番，然后立即决定买入它的股票，连续 3 年利润翻番，是一个好公司的代表性特征，但是这并不意味着这家公司真的就是一家好公司，这家公司还有好多信息都被你忽略了，比如说公司高管近期需要减持股票，业绩可能是有益调整出来的，再比如这家公司未来的盈利机会消失，业绩不能持续。 再例如，你一位平时很靠谱的朋友给你推荐了一只股票，出于对他的信任，你就立刻买入了，犯的也是代表性偏差的错误，这里面的代表性特征是你朋友靠谱，你太看重这个代表性特征了，实际上一只股票好不好，被你朋友推荐这个因素实在是太不重要了，你忽略了好公司的其他信息。 再比如，一种投资中常见的骗局，假定一个人吹嘘自己是推荐股票的大神，荐股从没错过，看看他是如何让你相信他说的话是真的：这个人第一周向 800 个人发出 800 条信息，其中 400 条说某只股票涨，400 条说跌；第二周，他向其中说对的 400 人再发一条微信，其中 200 条说这只股票涨，200 条说这只股票跌；第三周他再像说对的 200 人发信息，其中 100 人说股票会涨，100 人说股票会跌。你会发现，3 轮 之后最后总有 100 人，发现他连续三次都说对了，简直神奇，就信了这个人，跟随他投资。 信息输出阶段的认知偏差在这个阶段，人最容易犯的错误就是过度自信：人对自己能力的认知超过了自己的实际水平。 过度自信的表现过度自信是人与生俱来的性格特点，几乎每个人都认为自己比一般人聪明、比一般人漂亮、比一般人技艺高超；在工作中，我们总觉得自己比别人工作效率高，总觉得自己能力高；创业者总觉得自己的项目是最好的，自己的公司成功率最高；投资者总觉得自己选择的股票具备上涨潜力…… 90% 的男性认为自己的驾驶技术优于 50% 的男性！90% 的女性认为自己的外貌优于 50% 的女性！ 过度自信的原因过度自信的原因有很多，包括： 从生物进化角度来看：过度自信可以让自己显得比实际上更聪明和强壮，增加个体传宗接待的机会 从社会环境来看：你会发现，自信的人比不自信的人能获得更多的工作机会、交友机会 从个人成长轨迹来看：大多数人被家庭过度保护，逐渐滋长过度自信 知识幻觉：人们倾向认为自己对未来的预测会随着信息的丰富而更准确，尽管有价值的信息可以改善预测的准确度，但是自信程度可能提高得更快，而实际上，信息最多只能有限地改善预测准确度；对于投资新手来说，搜集未过滤的信息比较容易，然而由于他们无法准确理解这些信息，很可能会被知识错觉愚弄而做出糟糕的决定，所以他们最好是使用过滤后的信息，直到在这方面获得了足够多的经验 控制错觉：人们常常相信自己对一些不可控事件的后果具有影响力，产生这种错觉的主要因素包括选择、结果序列、任务熟悉程度、信息以及积极参与 选择：做出一些积极的选择可能会引发控制错觉，例如自己选择彩票号码的人往往会自认为中奖机会要大一些 结果序列：早期积极的结果会给人更大的控制错觉 任务熟悉程度：人们对任务的熟悉程度越高就越觉得自己能够控制它 信息：投资者得到的信息数量越多，控制错觉也越大 积极参与：一个人参与任务的程度越高，那么他觉得自己掌控着事态的感觉也会同比例增强 过往的成功：过度自信的心理来自于过往的成功经历，“自我归因”的心理偏差让人们将成功归因于自己天赋异禀，而将失败归咎于时运不济；在牛市时期，个人投资者往往将成功归功于自己的能力，从而出现更多过度自信的行为，随着牛市消退和熊市来临，过度自信减弱，投资者的行为逐渐逆转；金融市场上有一句话，“没有经历过一个完整牛熊市的人不是一个合格的投资者”，因为只有经历过一个完整的牛熊市，才能更好地平衡自己的投资心态和决策； 过度自信的危害过度自信造成投资者过度交易、冒险交易的错误决策，并最终导致投资组合受损： 过度自信导致过度交易：过度自信会使投资者频繁交易，把钱浪费在支付佣金上；如果同时有很多投资者受到过度自信的影响，这种心理会导致大批投资者增加交易，增加证券交易总量，调查发现高收益率的月份会出现较高的交易量，在股市下滑之后，股票的交易量也较平时更少 过度自信导致买错股票：过度自信的投资者更相信自己对股票的估值，而较少被其他人的观点左右，导致投资者卖出表现好的股票，买入表现差的股票 过度自信导致冒险交易：过度自信的投资者通常会对风险作出误判，毕竟，如果投资者自信地认为所选的股票会获得高收益，他们怎么会看到风险呢？一方面他们倾向于买入高风险股票，另一方面他们倾向于分散程度较低的投资组合，甚至借钱进行杠杆投资 过度自信的应对站在第三方的立场来看，评判会准确得多，或者站在一个更高的视角，或者置身事外。 信息反馈阶段的认知偏差人在信息反馈阶段，会出现几种常见的认知偏差，包括自我归因、后见之明、认知失调、确认性偏差、神奇式思考，这些都是心理学的研究结果。也就是说，你已经看到结果了，但对待结果的态度却与机器人不同，这些认知偏差会让你重蹈覆辙而不是越来越理性。 自我归因自我归因是指人倾向于把好的结果归于自己的天赋异禀，而把坏的结果归咎于时运不济。 你去看上市公司的年报、季报时就会发现，在总结业绩时，如果业绩上涨了，一般都会将原因归结为公司对时长把握正确、决策合理、风险控制得当等；如果业绩下滑，一般会将原因归结为本期出现了不可预知的宏观经济或国际市场的新情况，或者公司突然发生了非可控事件等等。 自我归因导致你没有找打盈利和亏损的真正原因，实际上你归因于自身能力的盈利，很可能是因为大盘普遍上涨了，也有可能只是短期的运气而已。你归因于外界环境的亏损，例如政府为何不来救市、那个基金经理的推荐我再也不相信的了等，其实也不是你亏损的真正原因，真正的原因可能在于你收集信息的偏差、你理解信息的片面和你买入卖出决策的冲动。 总之，自我归因偏差使得你不知道自己为何会盈利，也不知道自己为何会亏损，下次投资时并不会变得更聪明，而是重蹈覆辙。 后见之明后见之明是指当事情已经出现结果之后，误以为自己早就知道结果的一种幻觉。 例如，你身边可能有位炒股的朋友，有一天他在微信朋友圈晒出他对行情的猜测，还附上曾经猜测的时间，他确实在之前就猜对了，不过他可能猜了很多次，只有这一次是准的，他只是把这一次结果拿出来了而已，他甚至自己也忘记了那些曾经猜错的经历。 后见之明使人们在估计风险时过度乐观，从而导致投资失败。 认知失调认知失调是指，当事情出现的结果与预期不一致时，人感到一种不协调的痛苦，为了避免这种痛苦，可能会对事实视而不见或故意歪曲。 例如，有人花多年积蓄买了房子，就不允许别人提这个房子降价的信息，因为降价会让自己感到认知失调，非常痛苦。所以宁愿选择不听、不看。刚刚买车的人在购买之后会有选择地避免阅读其他车型的广告，而仅仅关注他自己车子的广告。 投资也是一样，买入之后，就不去看坏消息，卖出之后，就不去看好消息。因为这些不利消息都会让自己感到痛苦，而且投资者常常有一种共同的特点，那就是只关注自己所投资的那个资产的消息，但这并不意味着其他消息没有发生，而是你在人为规避而已。 认知失调让你收集不到全面的反馈。 确认性偏差确认性偏差是指，人一旦形成一种观念，就会从肯定自己的一面寻找证据，而不倾向于从否定自己的一面寻找证据。 在金融市场上，比如你买了一只股票，你满眼看到的、满耳听到的都是支持你买入的好消息；你卖出一只股票，你看到的、听到的也都是支持你卖出的消息。 实际情况是，正面和反面的信息都有，你只是有目的地选择对自己有利的信息，通常你总能找到你想要的信息，但这并不代表你的决策是对的 神奇式思考神奇式思考是将相关性误以为因果关系导致，比如古代人们常常“卜以决疑”，在重大决策前占卜。 在金融市场上，因为不可控情形太多，这种认知偏差就更常见了，比如公司金融里，有些公司的某些投资或管理决策刚好是在业绩提升前做的，公司管理者就会不断重复这些决策，即使这些决策与业绩根本无关。 大多数笃信技术分析得人，会根据技术指标来进行投资，盈利后认为某些技术指标对于指导投资非常有用，这很可能就是一种神奇式思考。 决策非理性认知和决策是两个步骤，认知是对单个对象的了解过程，决策是对两个或多个对象的比较选择过程。即使你能准确认识你的投资对象，决策过程也有可能出错。 行为金融学中的第一个诺奖理论 —— 前景理论。 前景理论期望收益最大首先通过一个故事来了解人是如何做决策的：小明要给自己的双人床买一床被子，他在商场看到一款喜欢的被子正在打折，正常售价是超大码豪华双人被 1000 元，豪华双人被 600 元，普通双人被 400 元，但是现在所有的尺码都只卖 300 元，限时一周。你猜小明会买哪种被子？最后小明买了一床超大码豪华双人被，虽然很值，但和他的床一点都不搭。 回忆一下，你是不是也做过很多和小明一样的决策？我们该怎么理解这种决策方式呢？ 17 世纪，一些数学家开始思考人在面对选择的时候是怎么决策的，当时公认的想法是，如果有几个选择，能够获得最大财富/收益的那个选择是最好的，这也正是小明所采取的的策略，但是这种观点受到了圣彼得堡悖论的挑战。 圣彼得堡悖论圣彼得堡悖论（St Peterburg Paradox）是数学家丹尼尔·伯努利（Daniel Bernoulli）的堂兄尼古拉·伯努利（Nicolaus Bernoulli）在 1738 年提出的一个概率期望值悖论，它来自于一种掷币游戏，即圣彼得堡游戏： 设定掷出正面或者反面为成功，游戏者如果第一次投掷成功，得奖金 2 元，游戏结束；第一次若不成功，继续投掷，第二次成功得奖金 4 元，游戏结束；这样，游戏者如果投掷不成功就反复继续投掷，直到成功，游戏结束。如果第 n 次投掷成功，得奖金 2 的 n 次方元，游戏结束。 按照概率期望值的计算方法，将每一个可能结果的得奖值乘以该结果发生的概率即可得到该结果奖值的期望值。游戏的期望值即为所有可能结果的期望值之和。随着 n 的增大，以后的结果虽然概率很小，但是其奖值越来越大，每一个结果的期望值均为 1，所有可能结果的得奖期望值之和，即游戏的期望值，将为“无穷大”：$E(H)=\sum{1}^{\infty}P(H_n)Pay(H_n)=\sum{n=1}^{\infty }\frac{1}{2^n}2^n=\infty$。按照概率的理论，多次试验的结果将会接近于其数学期望。但是实际的投掷结果和计算都表明，多次投掷的结果，其平均值最多也就是几十元。正如 Hacking（1980）所说：“没有人愿意花 25 元去参加一次这样的游戏。”这就出现了计算的期望值与实际情况的“矛盾”，问题在哪里? 实际在游戏过程中，游戏的收费应该是多少？决策理论的期望值准则在这里还成立吗？ 丹尼尔·伯努利在 1738 年的论文里对这个悖论进行了解答，提出了效用的概念以挑战以金额期望值为决策标准，论文主要包括两条原理： 边际效用递减原理：一个人对于财富的占有多多益善，即效用函数一阶导数大于零；随着财富的增加，满足程度的增加速度不断下降，效用函数二阶导数小于零； 最大效用原理：在风险和不确定条件下，个人的决策行为准则是为了获得最大期望效用值而非最大期望金额值；如果用奖金的对数函数作为效用测度函数，所有结果的效用期望值将为一个有限值 $E(H)=\sum{1}^{\infty}P(H_n)log_2(Pay(H_n)))=\sum{n=1}^{\infty }\frac{n}{2^n}=2$； 需要说明的是，边际效用递减原理并没有真正消解圣彼得堡悖论，只要适当增大奖金金额，悖论就还在。圣彼得堡悖论的根源在于样本均值与总体均值的差异，以及我们对于“无穷大”的理解，根据伯努利大数定律，当样本容量趋于无穷时，样本均值依概率收敛于总体期望。在大量实验以后，圣彼得堡游戏收益实际样本均值 $X$ 可以近似表示为 $X\approx \frac{lgN}{lg2}$，可见当实验次数趋向无穷大的时候，样本均值也趋向无穷大。比如 100 万次实验的平均值等于 $\frac{6}{0.301}\approx 19.9$，要使样本均值达到 1000 元，实验次数就要达到 $10^332$ 次，这时有可能出现的最高投掷次数约为 1000 次左右，相应的最高赔付金额为 $2^1000\approx 10^301$，已经是天文数字了。由此可见，圣彼得堡悖论的产生是因为平均收益趋向于无穷大的速度太慢了，即使理论上期望收益为无穷大，实践中也远没有足够时间和财力支撑平均收益达到可观水平。 事实上，圣彼得堡悖论并没有驳倒期望收益最大原理，但是人们对它的思考引出了“效用”的重要概念。 期望效用理论数学家冯·诺依曼和经济学家摩根斯特恩在 1944 年正式提出了“期望效用理论”（Expected utility theory）。该理论假定人都是理性的，各人主观追求的效用函数不同、对各种可能性发生所认为的主观概率不同，导致了判断和决策的因人而异。但为保持理性，效用函数必须具有一致性，同一个结果有同样的效用，主观概率也必须满足贝叶斯定理等概率论基本原理。 假设做某个风险决策前，人具有总财富为 $W$，而该决策做出后会导致 $n$ 种可能的结果，第 $i$ 种结果使财富变为 $W + x_i$，发生的概率为 $p_i$，人的财富效用函数为 $U(W)$，效用函数一般假设为二阶导数小于 0 的一致性风险厌恶的形式，例如取作对数函数 $U(W) = ln(W)$，那么人会采取该决策的充要条件是： \sum_{i}U(W + X_i)\times p_i > U(W)期望效用理论认为能够获得最大效用的选择是最好的选择，使用效用的概念代替财富的概念，是决策理论的重大突破，期望效用理论成为经济学最根基的理论之一。 阿莱悖论期望效用理论受到了阿莱悖论（Allais Paradox）的挑战，1952年，法国经济学家、诺贝尔经济学奖获得者阿莱作了一个著名的实验： 首先对 100 人测试所设计的赌局： 赌局 A：100％ 的机会得到 100 万元； 赌局 B：10％ 的机会得到 500 万元，89％ 的机会得到 100 万元，1％ 的机会什么也得不到； 实验结果：绝大多数人选择 A 而不是 B，即赌局 A 的期望值（100万元）虽然小于赌局 B 的期望值（139万元），但是 A 的效用值大于 B 的效用值，即 $1.00U(1m) &gt; 0.89U(1m) + 0.01U(0) + 0.1U(5m)$。 然后阿莱使用新赌局对这些人继续进行测试： 赌局 C：11％ 的机会得到 100 万元，89％ 的机会什么也得不到； 赌局 D：10％ 的机会得到 500 万元，90％ 的机会什么也得不到； 实验结果：绝大多数人选择 D 而非 C，即赌局 C 的期望值（11万元）小于赌局 D 的期望值（50万元），而且 C 的效用值也小于 D 的效用值，即 $0.89U(0) + 0.11U(1m) &lt; 0.9U(0) + 0.1U(5m)$，进一步整理得到 $1.00U(1m) &lt; 0.89U(1m) + 0.01U(0) + 0.1U(5m)$，这与第一次实验结果矛盾。 阿莱悖论的另一种表述是：按照期望效用理论，风险厌恶者应该选择 A 和 C；而风险喜好者应该选择 B 和 D，然而实验中的大多数人选择 A 和 D。 前景理论阿莱悖论的提出，促使大家思考，期望效用理论可能并不符合人的实际决策过程，那么实际的决策选择是怎样的呢？在所有研究实际决策的理论中，行为金融学家卡尼曼和特沃斯基提出的前景理论（prospect theory）最有名，并因此获得了 2002 年的诺贝尔经济学奖。前景理论认为个人基于参考点的不同，会有不同的风险态度。前景理论可以用于对风险和收益的关系进行实证研究，是行为经济学的重大成果之一。 前景理论将人在做决策时的效用得失函数表示为： \Delta U = \frac{\sum_{i}V(x_i - r)\times W(p_i)}{\sum_{i}W(p_i)}其中： $x_i$：表示第 $i$ 种可能的收益； $p_i$：表示第 $i$ 种可能发生的概率； $r$：表示参考点（reference point）； $V$：表示价值函数（value function），描述了效用价值随财富损益的变化趋势； $W$：表示概率权重函数（probability weighting function），$W(p)$ 一般取做 $W(p)=p^\alpha,\ 0&lt;\alpha\leq 1$，反映人对小概率的“过敏程度”； 价值曲线 $V(x)$ 是一条穿过参考点 $(r, 0)$ 的 S 型曲线： 参考点：参考点是价值函数的拐点，人们在评价事物时，总要与一定的参考物做比较，投资者心理上对损益的判断取决于实际资产价格和参考点的偏差；当对比的参考物不同时，即使相同的事物也会得到不同的比较结果，因此参考点是一种主观评价标准； 收益部分是下凹函数：参考点右侧的价值曲线称为收益部分，收益价值函数是下凹函数，当收益增加时，投资者所感觉到的满足感（正效用）并不会同比例增大； 损失部分是下凸函数：参考点左侧的价值曲线称为损失部分，损失价值函数是下凸函数，当损失加剧时，投资者所感觉到的不适感（负效用）也不会同比例放大； 损失函数比收益函数更加陡峭：收益和损失的这种不对称性，表明相对于收益所带来的满足感，等值的损失会为投资者带来更大的不适感； 锚点效应（Anchoring effect）前景理论认为人本身就是通过变化来体验生活的，投资者对得失的判断依赖于对参考点（锚点）的选择。例如公司这个月给你发了一万元的奖金，你高不高兴？传统期望效用理论认为，你当然知道自己是否高兴，因为这一万元绝对值带给人的效用是确定的。但是你可能会说，等一下，我得先看看同事拿了多少钱，如果他们拿了五千，我就高兴，如果他们都拿了三万，我就不高兴，此时参考点就是同事的奖金水平。 既然参考点很重要，那么哪些因素会影响参考点的位置呢？研究发现，参考点与以下几个因素有关： 参考点与历史水平有关：投资者总是会将现在的价格与成本价对比，称之为“往回看”，“我已经赚了20%，可以出手了” “等我解套了就走”……这些都是在“往回看”，而理性人是“往前看”！历史水平的参考点，除了成本价之外，还有最高价、最低价等，“摊薄成本”是将参考点定在近期的高点或低点而做出的决策； 参考点与期望水平有关：期望水平是指心理预期的一个水平，比如你买房子的成本价是 100 万，但预期水平可能是 500 万，那出手价格的参考点就是 500 万，而不再是成本价 100 万； 参考点与身边人的决策有关：将决策的参考点定为同行或朋友的决策，这称之为决策的“同群效应”，很多人投资不是去看投资对象，而是看朋友、专家买了什么、卖了什么，然后跟随决策； 卡尼曼和特沃斯基指出：”可以通过改变参考点的方法来操纵人们的决策“。比如商场在促销时，一件 3千元的衣服，如果原价出售，就不如以 3 万原价、现在 1 折出售销售的好。很多人买东西，不是看绝对价格，或自己是否真的需要，而是看是否便宜，便宜需要参考点，而参考点往往被卖价所利用。同样，公司的人力资源决策也是一样，在总额不变的情况下，固定工资的效果就不如工资稳步上涨给员工带来的效用高，工资的绝对水平不是幸福（幸福是效用的生活化表达）的决定因素，与谁比才重要！ 损失厌恶（loss aversion）前景理论最重要也是最有用的发现之一是: 当我们做有关收益和有关损失的决策时表现出的不对称性。对此，就连传统经济学的坚定捍卫者保罗，萨缪尔森也不得不承认：“增加 100 元收入所带来的效用， 小于失去 100 元所带来的效用。 损失厌恶的心理，一方面会使投资者尽量避免做出让自己损失的决定，另一方面会使投资者在发生损失时，难以止损。在熊市中，损失厌恶让你不想把亏损变成既成事实，风险偏好又让你想要搏一把，就这样，在熊市中，你越亏越狠，越狠又继续亏，最后赔的一塌糊涂。 确定效应（certainty effect）“二鸟在林，不如一鸟在手”，在确定的收益和“赌一把”之间，多数人会选择确定的好处。所谓“见好就收，落袋为安”，称之为“确定效应”。 在对下面两种方案进行选择时，大部分人会选择 A 方案： A 方案: 你有 100% 的可能得到 500元；B 方案: 你有 50% 的可能得到 1000 元，50% 的可能什么也得不到； 多数人处于收益状态时，往往小心翼翼、厌恶风险、喜欢见好就收，害怕失去已有的利润。卡尼曼和特韦斯基将此称为“确定效应”，即处于收益状态时，大部分人都是风险厌恶者。 投资时，多数人的表现是“赔则拖，赢必走”。在股市中，普遍有种“卖出效应”，也就是投资者卖出获利的股票的意向，要远远大于卖出亏损股票的意向。这与“对则持，错即改”的投资核心理念背道而驰。 反射效应（refletion effect）在确定的损失和“赌一把”之间，做一个抉择，多数人会选择“赌一把”，称之为“反射效应”。 在对下面两种方案进行选择时，大部分人会选择 B 方案： 你有 100% 的可能损失 500 元；你有 50% 的可能损失 1000 元，50% 的可能什么也不损失； 多数人处于亏损状态时，会极不甘心，宁愿承受更大的风险来赌一把。卡尼曼和特韦斯鞋为“反射效应”，即处于损失预期时，大多数人变成风险偏好者。 错判概率人在决策时，事件的决策权重并不等于事件发生的概率。 在低概率区域，投资者会被极端损益所吸引，在心中放大低概率，比如担心飞机失事、购买彩票、股票打新等行为；在高概率区又会产生”失去才最美“效应，在心中忽视大概率事件； 人们往往会直接忽视发生概率很低的事件，或者在连续决策过程中，容易认为高概率事件一定会发生； 风险感知人们对风险的感知似乎是不断变化的，在评估当前的风险决策时，过去的结果是一个重要因素： 赌资效应：投资者在结清一笔获利头寸之后，更可能买入高风险股票； 蛇咬效应：投资者在结清一笔亏损头寸之后，风险承受度会降低； 狭隘框架人在决策时没有全局观就是狭隘框架，投资上的狭隘框架在横截面和时间序列两个维度都会出现。 横截面上的狭隘框架主要表现在对资产组合没有全局观，关注单个资产的涨跌，导致没有正确理解风险； 时间序列上的狭隘框架主要表现在频繁清点资产，对每个时间点的资产孤立来看，这回受到参考点很大的影响，从而影响决策； 正确的做法是”向前看“！ 心理账户人的决策取决于心理账户，而非真实账户。投资者会将资金分为不同用途，为其建立各个心理账户，并对不同的心理账户采用不同的风险态度。这样的投资决策有约束自己消费的作用，但并不是最优组合。在生活中，我们对事物的评价取决于将其放在哪个心理账户中进行核算。 选美博弈经济学家凯恩斯的“选美博弈”理论：在一场选美比赛中，很多美人依次出来展示，你作为观众的一员，需要票选出一人，如果你选的这个人获得了最多的选票，成为选美皇后，那你就会得到最多的奖金。请问，你会选你认为最美的那个人吗？不会，因为你的目的是赢得这场比赛，所以你会把票投给那个你认为，别人心目中那个最美的人。而别人也不会把票投给那个自己认为最美的人，而是会把票投给他们觉得，别人心目中那个最美的人。 股票市场就是个选美博弈，你认为别人是怎么想的，比你是怎么想的，更加重要。你应该把钱用来买那个你认为，别人觉得最好的股票。 多花时间研究心理学，与花时间研究投资对象同等重要，甚至更加重要。 典型的交易错误行为金融学和传统金融学的研究领域几乎完全相同，只有研究投资者错误行为的这部分是不同的。因为传统金融学假设理性人是不会犯错的，而行为金融学恰恰认为，投资者行为有错误，是投资失败的重要原因。 分散不足风险本质上是什么呢？“雪上加霜”，也就是这个投资的变动方向与你现有资产的变动方向一致时，就叫风险！ 抵御风险的最佳方式就是将风险分散化：你应该配置与现有资产波动不相关或者最好是反向波动的资产。比如，不再配置房地产这种周期性的股票，而应该配置防御性的视频或医药股票，这会在你最缺钱的时候，反向波动帮助到你，或减少波动，减轻你的损失。 防御性行业的股票对经济周期不敏感，一般经济收缩时配置防御型行业的股票，收缩比其他行业小，经济扩张时配置周期性股票，回避其他股票扩张快。 本地偏差本地偏差是指人们更倾向于买当地的股票。世界上大部分投资者会将绝大多数资产投资于本国市场，也有研究证明了人们更加偏爱本地股。本地偏差无处不在，你会不会对你住所旁边的公司情有独钟？会不会对你学习的那个专业的公司更加关注？如果你所在的但未就是上市公司，你会不会买本公司的股票？自己的工资在这家公司拿，这是个人最大的资产，如果再将投资也配置在同一家公司上，这是极度的集中投资，与分散投资的理念背道而驰，具有巨大的风险。 产生本地偏差的重要原因是人有熟悉偏好，本地偏差违背了分散化投资的原则，是资本配置的错误方式。 简单分散化 你把鸡蛋放在了不同的篮子里，可是却把篮子放在了同一辆车上。 资本配置说的是资金如何在大类资产中进行配置，例如多少比例存银行，多少比例投债券，多少比例买股票等；资产选择，指的是对于某一类资产，例如股票，买那些具体的股票。你觉得资本配置和资产选择哪一步更重要呢？与巴菲特、罗素等齐名，有投资界活传奇之称的加里·布林森增在 1991 年的一篇著名合作研究中指出：“资本配置，即资金如何在股票、债券、银行存款等大类资产中配置，对总收益的影响超过 90%”。 错误的 1/n 法则：无论给出什么样的选择，投资者都会按照 1/n 来出牌，这不是一种理性的资本配置方式； 理性分散化：最优资本配置比例是可以精确计算的，理性人会根据自己的风险偏好，结合不同资产的收益和风险来计算适合自己的配置比例； 如果你想克服资本配置上的非理性，今后就不应该多花时间在资产选择，也就是仔细挑选哪几只股票上，因为你挑来挑去，对你收益的影响有限，你应该多花时间仔细思考在大类资产中的配置比例，这会极大改善你的投资收益。 过度交易个人投资者亏损的主要原因是，交易太多了！不少人自认为对某些股票很熟悉，会反复做同一只股票，买进后再卖出、卖出后再买入，反反复复。事实上，无论你怎样频繁交易，与你买入并持有，总收益是差不多的，但会损耗成本，最终的结果是，来回倒腾不如不动。 卖出行为偏差处置效应，是指投资者不愿意以低于购买价格出售资产的现象。具体表现是，投资者更倾向于卖出盈利的股票，而不愿意卖出亏损的股票。处置效应可以从前景理论得到解释：人在收益和损失区域的风险偏好不同，盈利时，投资者讨厌风险，所以希望落袋为安；亏损时，投资者偏好风险，希望放手一搏。 无论售盈持亏还是售亏持盈都是一种决策错误，正确的做法是“向前看”，卖出未来预期收益最低的资产，不要受买入成本等参考点的影响。 买入行为偏差追涨“涨停敢死队”的获利策略：股票如果发生涨停，并且大概率涨停不止一次，那么追涨停将是一个有效的交易策略。“涨停敢死队”会在当天涨停发生的第一次买入，然后在第二日卖出，所以他们大概率赚了一个涨停。“涨停敢死队”的获利条件：① 要能买到涨停的股票；② 需要能将股票在第二天卖出去 “涨停敢死队”的行为是有意识的，而普通投资者是无意识的，这会导致交易节点不对。 有限关注有限关注，是指投资者买入股票，只是在那些引起关注的股票中进行选择，做广告、新闻、获奖等容易引起关注的事件，就可能引起买入。 在真实市场上，行为金融交易策略主要运作原理就是：一部分投资者要犯错，而另一部分理性人活着说专业机构，利用这种错误来盈利。 摊薄成本摊薄成本，指买入已经持有的股票，特别是当这只股票价格下跌时会补仓，在心理上会感觉成本不断降低，这是一种交易行为偏差，由狭隘框架和参考点效应造成。 羊群效应 社会心理学认为，人都有理性的一面和非理性的一面，当人非理性的一面在群体中被统一了之后，心理群体的智商就会低于单个个体理性人的智商。 羊群效应，是指放弃自己的判断，追随大众的决策。 行业轮动是指，随着经济从繁荣到萧条再到繁荣等周期性变化，在特定的经济时期有特定的行业受益，投资者会根据经济情况配置不同的行业，形成行业轮动现象，这在所有国家的市场中都存在。中国的股市除了行业轮动还有板块轮动现象，板块包括地区、热点话题、热点事件等。板块轮动与经济周期或基本面并没有太多关系，更多的是投资者行为聚集而形成的，也就是上面所提到的羊群效应。 概念股是怎么炒起来的？“领头羊”一般是持仓超过 1000 万的超级大户，而“羊群”为持仓在 50 万以下的小户和 10 万以下的散户。多数炒作模式是，“头痒”直接入场 —— 拉抬股价 —— “羊群”跟进（占60% - 70% 账户）。在概念炒作中，超级大户与中小散户的净买卖行为相反，“头羊”提前建仓，炒作期间净卖出，建仓过程缓慢，而清仓过程很快。而“羊群”的表现刚好相反，他们慢于“头羊”，一般进场时头羊已经建仓完毕，并且小散离场不果断，炒作结束后未完全出货，被套住的比例高。 不轻易跟随所谓的市场热点，不要仅凭技术分析指标操作，减少交易频率可以有效减少损失。 股票大盘的奥秘大盘可预测理论 传统金融学说大盘不可预测，但行为金融学说大盘可预测：股价基准可以通过股息贴现值来计算，长期来看，股价会围绕着这个基准波动，从这个意义上，股价是可以预测的；但是短期来看，噪声太大，股价几乎不可预测，即便如此，这个理论对于判断股市当前系统性风险也是非常有用的。 股票代表的是投资者对上市公司的所有权，具有对上市公司收益的要求权。股东持有股票，可以以获得股息的方式分享企业的收益。所以，你现在愿意用多少钱买股票，应该等于股票未来所有股息的贴现价值，贴现值的意思就是现在的价值。股价会围绕着股息贴现值波动，从这个意义上，股价是可以预测的。 大盘的可预测性只表现在长期，实证研究显示，这个“长期”长达 10 年。大盘短期的噪声太大，导致预测的效果并不好。而长达 10 年的可预测性，在实践中几乎不可操作。虽然股市短期择时非常困难，但对于判断股市当前的系统性风险来说，这个理论是非常有益的。虚拟市场，不能偏离实体经济太远。我们可以通过实体经济的基本趋势，如经济景气的先导指标、企业财务预测等指标找到股票市场波动的基准。如果市场偏离这个基准太远，就应该预测它将会回归实体经济基准线。 股权溢价之谜 股权资产有超常收益但风险不高，这被称为“股权溢价之谜”。股权投资并非零和游戏，无论如何都应该配置一些股票，但实际家庭股票拥有率却很低，这被称为“股票非参与之谜”，这可能是一种投资错误。 股权类资产的收益远远高于其他类资产，按照传统资产定价原理“资产的收益=风险×风险厌恶系数”，风险是指股票收益与消费增长的协方差，在实践中可以观察到消费增长的波动率很小，这么低的波动与高波动股票的协方差当然很低，所以股权资产的风险并不高，那么股权的溢价只能由非常高的风险厌恶系数来解释了，但要得到实验观测到的股票收益，计算出来的风险厌恶系数将会达到 20 以上，合理的风险厌恶系数一般在 2~5 之间，显然这是不合理的，既不能用风险来解释，也不能用风险厌恶系数来解释股票的超高收益，因此被称为“股权溢价之谜”。 股权溢价之谜说明，参与股权投资并非零和游戏，从近百年的数据来看，股权资产的收益很高，年化能达到 10% 以上，所以无论你的风险偏好如何，都应该在战略上配置一些股票。 但是在美国，只有不足 50% 的家庭拥有股票，在其他一些国家，家庭的股票拥有率更低，这被称为“股票非参与之谜”。这可能是因为一次性成本高，包括财富、时间、学习等。研究发现，金融财富的配置与你周围的人际交往圈有关，人们喜欢跟朋友配置相似的资产，越不善交际的家庭越不倾向于持有股票，越不相信他人的人也越不可能持有股票，富有的和受教育程度高的人群则比较爱投资股票。这说明，相对于理性人，“股票非参与之谜”可能是一种投资错误。 量价关系：米勒假说“成交”本质上反映的是意见分歧，你觉得价格高估了，希望卖出，我觉得价格低估了，希望买入，于是一拍即合，成交。所以，交易量反映的是投资者的意见分歧程度，交易量越大，说明意见分歧越大。 米勒假说认为，均衡的时候股价一定是被高估的，价格反映的是乐观者的预期，所有股票由乐观者得。这个假说需要满足两个条件： 投资者有意见分歧：投资者受情绪的影响，可能乐观或悲观，乐观者愿意买、悲观者愿意卖；而新闻、舆情对投资者的情绪影响很大； 市场具有卖空约束：卖空约束并不是指不允许卖空，而是市场对卖空有必要的限制。目前大多数市场对卖空行为都有一定约束，可以卖空的股票或有价证券，卖空的成本一定远高于普通买入的成本，因此除非对结果有绝对把握，否则是不敢出手卖空的； 米勒假说的两个推论： 越是放量，投资者意见分歧越大，股价高估越严重； 越难卖空，股价高估也就越严重； 换手率（交易量除以流通股）是最为稳健且简单的度量投资者意见分歧的指标，当换手率下降之前，你可以买入，当换手率快速下降的时候，则应该果断卖出。 利空出尽是利好，利好出尽是利空。 谣言起则买，新闻起则卖（Buy on the rumors, Sell on the news）：如果一件事还在传闻阶段，则买入，如果已经落实且发布消息了，则应该卖出。 消息是用来利用的，而不是用来信的。 量价关系：博傻理论 博傻理论：在均衡的时候价格会超过所有人的预期，包括乐观者的预期，短期越是频繁交易，资产价格越高。 可以把股票投资想象成一个击鼓传花的过程，每个人对股票价格都有自己的判断，可以向下一个人抛出自己的股票。假设大家对于股票的价值有意见分歧，有人悲观，有人乐观，但都认可长期来看这只股票应该值 200 元。但是短期情况就不同了，如果你认为有人会在短期犯傻，也许可以以 300 元的价格卖给他，那你现在就可能愿意用 250 元的价格来购买股票。你之所以愿意做“傻瓜”，是因为你认为还有比你“更傻”的人，只要快速交易，让接力棒不要停在自己手里，就可以赚钱。 “击鼓传花”的游戏有两个特点： 交易快：博傻会出现在短期快速交易中，因为所有人都会认为股价相对于长期价值来说被高估了，但在短期来看却是合理的，所以，交易快是第一位的，每个人都不想那朵花落在自己手里； 交易量大：因为每个人都采用动态策略，会在短期内大量交易，以免接力棒不停在自己手里，所以短期越是放量，则向“更大的傻瓜”再售期权的价值就越高，价格泡沫越严重； 对于整个市场来说，博傻理论的高交易量-高价格的规律性对于市场有较大的破坏性，要让资金从金融市场回到实体经济中，还需要股利长期投资行为。对于个人投资者，要充分意识到长期持有的风险，不可盲目照搬价值投资理念，也要尽量避免放量的股票，因为股价高估可能更严重。 行为投资策略心理学让我们理解人会犯什么错，传统金融学提供了价格理性的方向，这两者合在一起就构成了行为金融策略发挥作用的基础。行为金融交易策略的制胜条件是投资者会犯错，而错误的价格会向理性的方向收敛。行为金融策略异象的检验需要使用传统金融学的风险模型，这推动了两个学科的共同发展。 行为金融交易策略如何盈利行为金融交易策略就是利用行为金融学原理而制定的交易策略，这个策略成功需要有三个条件： 投资者会犯错：在认知、决策和交易的各个环节，大多数投资者都会犯错，他们的错误是系统性的，会使价格产生错误，价格有错是行为投资策略的前提条件； 需要做与犯错的投资者方向相反的操作：别人恐惧的时候我贪婪，别人贪婪的时候我恐惧，别人都买入的、大家都喜欢的品种，就会让价格高估，那么在策略中就应该卖出这个品种；别人都卖出的、大家都不喜欢的品种，就会让价格低估，那么策略中就应该买入这个品种； 长远看来，价格需要向理性的方向收敛：否则高估的永远高估，低估的永远低估，价格得不到修正，策略也无法盈利； 实际上，没有任何一种交易策略可以一直有效，都只是在一段时间内有效，短则几个月，长则 10 年，策略也需要不断迭代更新。 行为组合策略原理行为金融策略的本质在于，可以找到一些方式，使得你在没有增加风险的情况下，获得更多的盈利。 “异象”是指违背传统金融学风险-收益对应原理的现象，这种现象相当于“免费的午餐”。行为金融交易策略就是在寻找这种异象机会。找到能够获得超额收益的异象因子，是各家行为金融基金的高度机密，他们就靠这些异象因子来赚钱。 检验异象因子的步骤： 将市场所有股票按照异象因子排序 将排好序的股票分成十等份 构建对冲组合：买入觉得低估的 1/10 的股票，卖空觉得高估的 1/10 股票 检验组合风险：看以这个异象因子排序而构建的对冲组合，买入端和卖出端收益上的差异，是不是因为风险产生的，如果不是，那么这个“特征”就是“真异象” 规模溢价和价值溢价规模不同的公司，股价收益不同，这就是“规模效应”。简单地说，价值股就是便宜股，市盈率低的股票更便宜。规模策略的思路就是卖掉小公司股票，买入大公司股票；价值投资策略的思路就是买差公司价值股，因为它的价格被低估，未来向价值收敛能获取高额收益。 股价规律长期反转效应就是涨久必跌、跌久必涨。长期反转策略利用的是长期价格规律，买跌卖涨，利用的是投资者过度反映的心理偏差。 惯性效应就是追涨杀跌，惯性交易策略利用的是短期价格规律，买涨卖跌，利用的是投资者反应不足的心理偏差。 利用财务报表信息制定交易策略投资者要关注盈利质量，应计项目反映了一个公司股票的真实价值，应计越高，企业的盈利质量就越差，这叫做“应计异象”。 好消息的股票会持续走好，而坏消息的股票会持续走差，各种消息按好坏程度好像会长时间漂移一样，这叫盈余公告后漂移，构建投资策略的时候应该按照没有被预期到的盈利增长来排序。 参考 阿莱斯悖论 如何解释圣彼得堡悖论？ 陆蓉·行为金融学]]></content>
      <categories>
        <category>Invest</category>
      </categories>
      <tags>
        <tag>Invest</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[韩非子——《二柄》]]></title>
    <url>%2F%E4%B8%AD%E5%9B%BD%E5%93%B2%E5%AD%A6%2F%E4%B8%AD%E5%9B%BD%E5%93%B2%E5%AD%A6%2F%E9%9F%A9%E9%9D%9E%E5%AD%90%E2%80%94%E2%80%94%E3%80%8A%E4%BA%8C%E6%9F%84%E3%80%8B%2F</url>
    <content type="text"><![CDATA[二柄，指刑与赏，即杀戮和奖赏这两种用来治理臣下的权柄。全文分三段。第一段分析并强调了君主掌握和运用赏罚两种权柄的重要性。因为人臣都是“畏诛罚而利庆赏”的，所以刑赏二柄就能发挥作用；而刑赏二柄又关系到国家的安危，所以君主应独自掌握这两种权柄。第二段论述了正确运用刑赏二柄的方法，即“审合刑名”的主张：“功当其事，事当其言，则赏；功不当其事，事不当其言，则罚”，目的是要使臣下不能“越官而有功”，“陈言而不当”。第三段进一步阐述君主在使用刑赏二柄时应注意的问题，即君主必须“掩其情”，“匿其端”，不表露自己的好恶，使臣下没有“缘以侵其主”的依据，不能蒙蔽君主。 明主之所导制其臣者，二柄而已矣。二柄者，刑、德也。何谓刑德？曰：杀戮之谓刑，庆赏之谓德。为人臣者畏诛罚而利庆赏，故人主自用其刑德，则群臣畏其威而归其利矣。故世之奸臣则不然，所恶则能得之其主而罪之，所爱则能得之其主而赏之。今人主非使赏罚之威利出于己也，听其臣而行其赏罚，则一国之人皆畏其臣而易其君，归其臣而去其君矣，此人主失刑德之患也。夫虎之所以能服狗者、爪牙也，使虎释其爪牙而使狗用之，则虎反服于狗矣。人主者、以刑德制臣者也，今君人者、释其刑德而使臣用之，则君反制于臣矣。故田常上请爵禄而行之群臣，下大斗斛而施于百姓，此简公失德而田常用之也，故简公见弑。子罕谓宋君曰：“夫庆赏赐予者，民之所喜也，君自行之；杀戮刑罚者，民之所恶也，臣请当之。”于是宋君失刑而子罕用之，故宋君见劫。田常徒用德而简公弑，子罕徒用刑而宋君劫。故今世为人臣者兼刑德而用之，则是世主之危甚于简公、宋君也。故劫杀拥蔽之主，非失刑德而使臣用之而不危亡者，则未尝有也。 人主将欲禁奸，则审合刑名者，言异事也。为人臣者陈而言，君以其言授之事，专以其事责其功。功当其事，事当其言，则赏；功不当其事，事不当其言，则罚。故群臣其言大而功小者则罚，非罚小功也，罚功不当名也。群臣其言小而功大者亦罚，非不说于大功也，以为不当名也害甚于有大功，故罚。昔者韩昭侯醉而寝，典冠者见君之寒也，故加衣于君之上，觉寝而说，问左右曰：“谁加衣者？”左右对曰：“典冠。”君因兼罪典衣与典冠。其罪典衣、以为失其事也，其罪典冠、以为越其职也。非不恶寒也，以为侵官之害甚于寒。故明主之畜臣，臣不得越官而有功，不得陈言而不当。越官则死，不当则罪，守业其官所言者贞也，则群臣不得朋党相为矣。 人主有二患：任贤，则臣将乘于贤以劫其君；妄举，则事沮不胜。故人主好贤，则群臣饰行以要君欲，则是群臣之情不效；群臣之情不效，则人主无以异其臣矣。故越王好勇，而民多轻死；楚灵王好细腰，而国中多饿人；齐桓公妒而好内，故竖刁自宫以治内，桓公好味，易牙蒸其子首而进之；燕子哙好贤，故子之明不受国。故君见恶则群臣匿端，君见好则群臣诬能。人主欲见，则群臣之情态得其资矣。故子之托于贤以夺其君者也，竖刁、易牙因君之欲以侵其君者也，其卒子哙以乱死，桓公虫流出户而不葬。此其故何也？人君以情借臣之患也。人臣之情非必能爱其君也，为重利之故也。今人主不掩其情，不匿其端，而使人臣有缘以侵其主，则群臣为子之、田常不难矣。故曰：去好去恶，群臣见素。群臣见素，则大君不蔽矣。]]></content>
      <categories>
        <category>中国哲学</category>
      </categories>
      <tags>
        <tag>中国哲学</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[韩非子——《孤愤》]]></title>
    <url>%2F%E4%B8%AD%E5%9B%BD%E5%93%B2%E5%AD%A6%2F%E4%B8%AD%E5%9B%BD%E5%93%B2%E5%AD%A6%2F%E9%9F%A9%E9%9D%9E%E5%AD%90%E2%80%94%E2%80%94%E3%80%8A%E5%AD%A4%E6%84%A4%E3%80%8B%2F</url>
    <content type="text"><![CDATA[“孤愤”是指法家思想家在与当权重臣斗争中的孤特之势和悲愤的心情。本文揭示了当时诸侯国内存在的维护君主专权的“智法之士”和结党营私盗窃国柄的“当涂之人”的尖锐对立，着重分析了“当涂之人”如何利用各种有利条件与“智法之士”争取控制君主，而君主由于受“当涂之人”的欺骗和蒙蔽，以至于失势亡国的严重局面，这也是“智法之士”无法得到君主了解和信任，往往遭受杀戮和迫害，并产生强烈孤立无援和悲愤之感的原因。文章逻辑严密而言词犀利，带有强烈的感情色彩，读后令人扼腕。司马迁曾以此篇和《说难》为韩非在秦国遭囚禁所作，并用以激励自己发愤著书，完成其历史巨著——《史记》的创作。 智术之士，必远见而明察，不明察不能烛私；能法之士，必强毅而劲直，不劲直不能矫奸。人臣循令而从事，案法而治官，非谓重人也。重人也者，无令而擅为，亏法以利私，耗国以便家，力能得其君，此所为重人也。智术之士，明察听用，且烛重人之阴情；能法之士，劲直听用，且矫重人之奸行。故智术能法之士用，则贵重之臣必在绳之外矣。是智法之士与当涂之人，不可两存之仇也。 当涂之人擅事要，则外内为之用矣。是以诸侯不因则事不应，故敌国为之讼。百官不因则业不进，故群臣为之用。郎中不因则不得近主，故左右为之匿。学士不因则养禄薄礼卑，故学士为之谈也。此四助者，邪臣之所以自饰也。重人不能忠主而进其仇，人主不能越四助而烛察其臣，故人主愈弊，而大臣愈重。凡当涂者之于人主也，希不信爱也，又且习故。若夫即主心同乎好恶，固其所自进也。官爵贵重，朋党又众，而一国为之讼。则法术之士欲干上者，非有所信爱之亲，习故之泽也；又将以法术之言矫人主阿辟之心，是与人主相反也。处势卑贱，无党孤特。夫以疏远与近爱信争，其数不胜也；以新旅与习故争，其数不胜也；以反主意与同好争，其数不胜也；以轻贱与贵重争，其数不胜也；以一口与一国争，其数不胜也。法术之士，操五不胜之势，以岁数而又不得见；当涂之人，乘五胜之资，而旦暮独说于前；故法术之士，奚道得进，而人主奚时得悟乎？故资必不胜而势不两存，法术之士焉得不危？其可以罪过诬者，以公法而诛之；其不可被以罪过者，以私剑而穷之。是明法术而逆主上者，不僇于吏诛，必死于私剑矣。 朋党比周以弊主，言曲以便私者，必信于重人矣。故其可以功伐借者，以官爵贵之；其不可借以美名者，以外权重之。是以弊主上而趋于私门者，不显于官爵，必重于外权矣。今人主不合参验而行诛，不待见功而爵禄，故法术之士安能蒙死亡而进其说，奸邪之臣安肯乘利而退其身？故主上愈卑，私门益尊。夫越虽国富兵强，中国之主皆知无益于己也，曰：“非吾所得制也。”今有国者虽地广人众，然而人主壅蔽，大臣专权，是国为越也。智不类越，而不智不类其国，不察其类者也。人主所以谓齐亡者，非地与城亡也，吕氏弗制，而田氏用之。所以谓晋亡者，亦非地与城亡也，姬氏不制，而六卿专之也。今大臣执柄独断，而上弗知收，是人主不明也。与死人同病者，不可生也；与亡国同事者，不可存也。今袭迹于齐、晋，欲国安存，不可得也。 凡法术之难行也，不独万乘，千乘亦然。人主之左右不必智也，人主于人有所智而听之，因与左右论其言，是与愚人论智也。人主之左右不必贤也，人主于人有所贤而礼之，因与左右论其行，是与不肖论贤也。智者决策于愚人，贤士程行于不肖，则贤智之士羞而人主之论悖矣。人臣之欲得官者，其修士且以精洁固身，其智士且以治辩进业。其修士不能以货赂事人，恃其精洁，而更不能以枉法为治，则修智之士，不事左右，不听请谒矣。人主之左右，行非伯夷也，求索不得，货赂不至，则精辩之功息，而毁诬之言起矣。治辩之功制于近习，精洁之行决于毁誉，则修智之吏废，则人主之明塞矣。不以功伐决智行，不以参伍审罪过，而听左右近习之言，则无能之士在廷，而愚污之吏处官矣。 万乘之患，大臣太重；千乘之患，左右太信；此人主之所公患也。且人臣有大罪，人主有大失，臣主之利与相异者也。何以明之哉？曰：主利在有能而任官，臣利在无能而得事；主利在有劳而爵禄，臣利在无功而富贵；主利在豪杰使能，臣利在朋党用私。是以国地削而私家富，主上卑而大臣重。故主失势而臣得国，主更称蕃臣，而相室剖符，此人臣之所以谲主便私也。故当世之重臣，主变势而得固宠者，十无二三。是其故何也？人臣之罪大也。臣有大罪者，其行欺主也，其罪当死亡也。智士者远见，而畏于死亡，必不从重人矣。贤士者修廉，而羞与奸臣欺其主，必不从重人矣。是当涂者之徒属，非愚而不知患者，必污而不避奸者也。大臣挟愚污之人，上与之欺主，下与之收利侵渔，朋党比周，相与一口，惑主败法，以乱士民，使国家危削，主上劳辱，此大罪也。臣有大罪而主弗禁，此大失也。使其主有大失于上，臣有大罪于下，索国之不亡者，不可得也。]]></content>
      <categories>
        <category>中国哲学</category>
      </categories>
      <tags>
        <tag>中国哲学</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[韩非子——《说难》]]></title>
    <url>%2F%E4%B8%AD%E5%9B%BD%E5%93%B2%E5%AD%A6%2F%E4%B8%AD%E5%9B%BD%E5%93%B2%E5%AD%A6%2F%E9%9F%A9%E9%9D%9E%E5%AD%90%E2%80%94%E2%80%94%E3%80%8A%E8%AF%B4%E9%9A%BE%E3%80%8B%2F</url>
    <content type="text"><![CDATA[说（shuì）难，是指游说、说服或进说君主的困难。文章分析了进说君主过程中会遭到的种种困难和危险，认为进说根本的困难在于难以弄清君主的真实心理，从而以适当的话去适应它；如果不根据君主的心理与要求进言，则会存在种种危险。接着文章还正面阐述了进说的具体原则和方法，关键的一点是要说者“知饰所说之所矜而灭其所耻”。文章还列举历史故事和民间传说，强调取得进说成功，一定要迎合君主的心理，获得君主的信任，甚至不惜卑躬屈节，使用种种诡诈的手段。最后，文章把封建君主比为喉下有逆鳞数尺的龙，进说的人存在着随时可能婴龙鳞、遭杀戮的危险，揭露了封建君主喜怒无常的特点。本篇和《孤愤》，司马迁认为是韩非囚秦所作，《史记·韩非列传》则全文录入此篇，但文字略有差异。同时，《韩非子》中前面有《难言》一篇，谈的也是向君主进言的困难，两篇可以互相参看。 凡说之难：非吾知之，有以说之之难也；又非吾辩之，能明吾意之难也；又非吾敢横失，而能尽之难也。凡说之难，在知所说之心，可以吾说当之。 所说出于为名高者也，而说之以厚利，则见下节而遇卑贱，必弃远矣。所说出于厚利者也，而说之以名高，则见无心而远事情，必不收矣。所说阴为厚利而显为名高者也，而说之以名高，则阳收其身而实疏之，说之以厚利，则阴用其言显弃其身矣。此不可不察也。 夫事以密成，语以泄败，未必其身泄之也，而语及所匿之事，如此者身危。彼显有所出事，而乃以成他故，说者不徒知所出而已矣，又知其所以为，如此者身危。规异事而当，知者揣之外而得之，事泄于外，必以为己也，如此者身危。周泽未渥也，而语极知，说行而有功则德忘，说不行而有败则见疑，如此者身危。贵人有过端，而说者明言礼义以挑其恶，如此者身危。贵人或得计而欲自以为功，说者与知焉，如此者身危。强以其所不能为，止以其所不能已，如此者身危。故与之论大人则以为闲己矣，与之论细人则以为卖重，论其所爱则以为藉资，论其所憎则以为尝己也。径省其说则以为不智而拙之，米盐博辩则以为多而交之。略事陈意则曰怯懦而不尽，虑事广肆则曰草野而倨侮。此说之难，不可不知也。 凡说之务，在知饰所说之所矜而灭其所耻。彼有私急也，必以公义示而强之。其意有下也，然而不能己，说者因为之饰其美而少其不为也。其心有高也，而实不能及，说者为之举其过而见其恶而多其不行也。有欲矜以智能，则为之举异事之同类者，多为之地，使之资说于我，而佯不知也以资其智。欲内相存之言，则必以美名明之，而微见其合于私利也。欲陈危害之事，则显其毁诽而微见其合于私患也。誉异人与同行者，规异事与同计者。有与同污者，则必以大饰其无伤也；有与同败者，则必以明饰其无失也。彼自多其力，则毋以其难概之也；自勇其断，则无以其谪怒之；自智其计，则毋以其败穷之。大意无所拂悟，辞言无所系縻，然后极骋智辩焉，此道所得亲近不疑而得尽辞也。伊尹为宰，百里奚为虏，皆所以干其上也，此二人者，皆圣人也，然犹不能无役身以进，如此其污也。今以吾言为宰虏，而可以听用而振世，此非能仕之所耻也。夫旷日离久，而周泽既渥，深计而不疑，引争而不罪，则明割利害以致其功，直指是非以饰其身，以此相持，此说之成也。 昔者郑武公欲伐胡，故先以其女妻胡君以娱其意。因问于群臣：“吾欲用兵，谁可伐者？”大夫关其思对曰：“胡可伐。”武公怒而戮之，曰：“胡，兄弟之国也，子言伐之何也？”胡君闻之，以郑为亲己，遂不备郑，郑人袭胡，取之。宋有富人，天雨墙坏，其子曰：“不筑，必将有盗。”其邻人之父亦云。暮而果大亡其财，其家甚智其子，而疑邻人之父。此二人说者皆当矣，厚者为戮，薄者见疑，则非知之难也，处知则难也。故绕朝之言当矣，其为圣人于晋，而为戮于秦也。此不可不察。 昔者弥子瑕有宠于卫君。卫国之法，窃驾君车者罪刖。弥子瑕母病，人闲往夜告弥子，弥子矫驾君车以出，君闻而贤之曰：“孝哉，为母之故，忘其刖罪。”异日，与君游于果园，食桃而甘，不尽，以其半啗君，君曰：“爱我哉，忘其口味，以啗寡人。”及弥子色衰爱弛，得罪于君，君曰：“是固尝矫驾吾车，又尝啗我以馀桃。”故弥子之行未变于初也，而以前之所以见贤，而后获罪者，爱憎之变也。故有爱于主则智当而加亲，有憎于主则智不当见罪而加疏。故谏说谈论之士，不可不察爱憎之主而后说焉。夫龙之为虫也，柔可狎而骑也，然其喉下有逆鳞径尺，若人有婴之者则必杀人。人主亦有逆鳞，说者能无婴人主之逆鳞，则几矣。]]></content>
      <categories>
        <category>中国哲学</category>
      </categories>
      <tags>
        <tag>中国哲学</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[韩非子——《难势》]]></title>
    <url>%2F%E4%B8%AD%E5%9B%BD%E5%93%B2%E5%AD%A6%2F%E4%B8%AD%E5%9B%BD%E5%93%B2%E5%AD%A6%2F%E9%9F%A9%E9%9D%9E%E5%AD%90%E2%80%94%E2%80%94%E3%80%8A%E9%9A%BE%E5%8A%BF%E3%80%8B%2F</url>
    <content type="text"><![CDATA[《难势》是围绕势治理论进行辨难。“难”（nàn）是辨难，“势”指君主的地位和权力，即权势。文章首先引用慎到的观点，以为君主的势是制服众人的根本条件，君主利用好势就能治理好国家。文章接着按照儒家的看法，对慎到的势治学说加以驳斥，提出治理国家靠贤才的主张。韩非则针对儒家的看法进行驳难，认为君主必须“抱法处势”，才能使国家长治久安，不仅否定了儒家“贤治”的主张，而且维护和发展了慎到的“势治”学说。 慎子曰：“飞龙乘云，腾蛇游雾，云罢雾霁，而龙蛇与蚓蚁同矣，则失其所乘也。贤人而诎于不肖者，则权轻位卑也；不肖而能服于贤者，则权重位尊也。尧为匹夫不能治三人，而桀为天子能乱天下，吾以此知势位之足恃，而贤智之不足慕也。夫弩弱而矢高者，激于风也；身不肖而令行者，得助于众也。尧教于隶属而民不听，至于南面而王天下，令则行，禁则止。由此观之，贤智未足以服众，而势位足以诎贤者也。” 应慎子曰：飞龙乘云，腾蛇游雾，吾不以龙蛇为不托于云雾之势也。虽然，夫释贤而专任势，足以为治乎？则吾未得见也。夫有云雾之势，而能乘游之者，龙蛇之材美也。今云盛而蚓弗能乘也，雾醲而蚁不能游也，夫有盛云醲雾之势而不能乘游者，蚓蚁之材薄也。今桀、纣南面而王天下，以天子之威为之云雾，而天下不免乎大乱者，桀、纣之材薄也。且其人以尧之势以治天下也，其势何以异桀之势也，乱天下者也。夫势者，非能必使贤者用已，而不肖者不用已也，贤者用之则天下治，不肖者用之则天下乱。人之情性，贤者寡而不肖者众，而以威势之利济乱世之不肖人，则是以势乱天下者多矣，以势治天下者寡矣。夫势者，便治而利乱者也，故《周书》曰：“毋为虎傅翼，将飞入邑，择人而食之。”夫乘不肖人于势，是为虎傅翼也。桀、纣为高台深池以尽民力，为炮烙以伤民性，桀、纣得乘四行者，南面之威为之翼也。使桀、纣为匹夫，未始行一而身在刑戮矣。势者，养虎狼之心，而成暴乱之事者也，此天下之大患也。势之于治乱，本末有位也，而语专言势之足以治天下者，则其智之所至者浅矣。夫良马固车，使臧获御之则为人笑，王良御之而日取千里，车马非异也，或至乎千里，或为人笑，则巧拙相去远矣。今以国位为车，以势为马，以号令为辔，以刑罚为鞭厕，使尧、舜御之则天下治，桀、纣御之则天下乱，则贤不肖相去远矣。夫欲追速致远，不知任王良；欲进利除害，不知任贤能；此则不知类之患也。夫尧、舜亦治民之王良也。 复应之曰：其人以势为足恃以治官。客曰“必待贤乃治”，则不然矣。夫势者，名一而变无数者也。势必于自然，则无为言于势矣。吾所为言势者，言人之所设也。今日尧、舜得势而治，桀、纣得势而乱，吾非以尧、桀为不然也。虽然，非一人之所得设也。夫尧、舜生而在上位，虽有十桀、纣不能乱者，则势治也；桀、纣亦生而在上位，虽有十尧、舜而亦不能治者，则势乱也。故曰：“势治者，则不可乱；而势乱者，则不可治也。”此自然之势也，非人之所得设也。若吾所言，谓人之所得势也而已矣，贤何事焉？何以明其然也？客曰：“人有鬻矛与楯者，誉其楯之坚，物莫能陷也，俄而又誉其矛曰：‘吾矛之利，物无不陷也。’人应之曰：‘以子之矛陷子之楯何如？’其人弗能应也。”以为不可陷之楯，与无不陷之矛，为名不可两立也。夫贤之为势不可禁，而势之为道也无不禁，以不可禁之势，此矛楯之说也；夫贤势之不相容亦明矣。且夫尧、舜、桀、纣千世而一出，是比肩随踵而生也，世之治者不绝于中。吾所以为言势者，中也。中者，上不及尧、舜，而下亦不为桀、纣。抱法处势则治，背法去势则乱。今废势背法而待尧、舜，尧、舜至乃治，是千世乱而一治也。抱法处势而待桀、纣，桀、纣至乃乱，是千世治而一乱也。且夫治千而乱一，与治一而乱千也，是犹乘骥駬而分驰也，相去亦远矣。夫弃隐栝之法，去度量之数，使奚仲为车，不能成一轮。无庆赏之劝，刑罚之威，释势委法，尧、舜户说而人辩之，不能治三家。夫势之足用亦明矣，而曰必待贤则亦不然矣。且夫百日不食以待粱肉，饿者不活；今待尧、舜之贤乃治当世之民，是犹待粱肉而救饿之说也。夫曰良马固车，臧获御之则为人笑，王良御之则日取乎千里，吾不以为然。夫待越人之善海游者以救中国之溺人，越人善游矣，而溺者不济矣。夫待古之王良以驭今之马，亦犹越人救溺之说也，不可亦明矣。夫良马固车，五十里而一置，使中手御之，追速致远，可以及也，而千里可日致也，何必待古之王良乎！且御，非使王良也，则必使臧获败之；治，非使尧、舜也，则必使桀、纣乱之。此味非饴蜜也，必苦莱亭历也。此则积辩累辞，离理失术，两末之议也，奚可以难，失道理之言乎哉！客议未及此论也。]]></content>
      <categories>
        <category>中国哲学</category>
      </categories>
      <tags>
        <tag>中国哲学</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[推荐系统（七）—— 问诊科室推荐（基于ItemCF）]]></title>
    <url>%2F%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%2F%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%2F%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%EF%BC%88%E4%B8%83%EF%BC%89%E2%80%94%E2%80%94%20%E9%97%AE%E8%AF%8A%E7%A7%91%E5%AE%A4%E6%8E%A8%E8%8D%90%EF%BC%88%E5%9F%BA%E4%BA%8EItemCF%EF%BC%89%2F</url>
    <content type="text"><![CDATA[问题定义：TOP-N 推荐2020 年 11 月初腾讯健康上线了一个新首页版本，首页新增“健康推荐”区域，支持为用户个性化推荐不同工具和内容。当前健康推荐区域共提供 9 个推荐位，分多个 tab 页进行展示，每个 tab 页展示两个推荐位。当前推荐位排序依赖于用户历史行为和规则设定进行刷新。 当前问诊服务占据了其中一个推荐位，用户点击问诊服务即进入相应科室的医生列表。问诊科室众多，但对于每个用户来说，问诊推荐处只能展示一个科室的内容，如何将合适的科室推荐给合适的用户显得尤为重要：一方面可以帮助用户发现他们可能感兴趣的科室，另一方面也能将问诊科室推送给对它们感兴趣的用户。 设 $U$ 代表腾讯健康所有潜在用户，$I$ 代表腾讯健康当前所有可访问的问诊科室，$f(u,i)$ 代表将科室 $i$ 推荐给用户 $u$ 所带来的效用，则腾讯健康问诊科室推荐问题可以被形式化定义为： \forall u \in U, {i}'=\underset{i\in I}{argmax}f(u,i)我们的核心目的是提高用户在问诊推荐模块的点击率，可以将用户 $u$ 对科室 $i$ 的点击倾向或兴趣度作为效用函数 $f(u,i)$。 问题求解：ItemCF由于问诊场景的特殊性，我们一般认为用户的问诊行为主要取决于用户的内在需要，而和其他用户的行为无关。即使和用户 $u$ 相似的其他用户访问了某个科室 $i$，也并不会增加用户 $u$ 访问科室 $i$ 的可能性，UserCF 方法在此场景下的应用受限。因此，本文尝试使用基于内容的协同过滤算法来为每位用户生成 TOP-1 的最佳推荐，该方法基于以下理念： 为用户推荐那些和他历史行为最相关的科室； 基于 ItemCF 进行科室推荐的困难在于对腾讯健康所提供“商品”缺乏明确的实体定义，腾讯健康提供的“商品”更多的是一系列不同质的虚拟服务，比如资讯、挂号、问诊、绑卡等，如何定义出和问诊科室相关的“商品”是解决问题的关键。事实上，商品实体存在与否无关紧要，可观测、可区分的行为集合才是重要的。可以将“科室相关的行为集合”看做是问诊科室推荐问题中的“商品”，其中，直接用于衡量用户对科室兴趣度的行为集合称为“推荐/目标商品”，间接影响用户对“目标商品”兴趣度的行为集合称为“辅助商品”。 收集 User-Item 反馈数据腾讯健康可观测到的科室相关行为整理如下： 详情可参考腾讯健康-科室相关行为，以用户访问业务子类相关行为为例，科室信息可能以不同方式散落于扩展字段中 json 串、url 参数中： 遍历近半年的用户行为日志，归纳出以下正则抽取现有各场景中出现的科室信息： 1&quot;&quot;&quot;\W(?:departmentid|deptid|deptinnerid|seconddeptid|deptname)(?:&quot;:&quot;|=)(\d+|[\u4e00-\u9fa5a-z/]+)\W&quot;&quot;&quot; 融合问诊订单数据、挂号订单数据、医生关注数据，得到如下“用户-科室反馈表”： 我们基于时间维度（f_date）将“用户-科室反馈表”划分为训练集和验证集： 训练集（20200704 ≤ f_date ≤ 20201230）：用于模型训练、生成推荐结果； 验证集（20210101 ≤ f_date ≤ 20210118）：用于验证推荐结果 计算 User-Item 评分矩阵基于 User-Item 反馈数据可以计算出每位用户 $u$ 对每个内容 $i$ 的“评分”，可以简单地用二值变量 $0-1$ 来表示用户是否对各项内容存在正反馈，也可以针对不同类型的用户反馈，给予不同的权重评分 $r_{ui}$： R = \left[ \begin{matrix} r_{11} & r_{12} & \cdots & r_{1\left | I \right |} \\ r_{21} & r_{22} & \cdots & r_{2\left | I \right |} \\ \vdots & \vdots & \ddots & \vdots \\ r_{\left | U \right |1} & r_{\left | U \right |2} & \cdots & r_{\left | U \right |\left | I \right |} \\ \end{matrix} \right]其中： r_{ui}= \left\{\begin{matrix} 0 & \mbox{用户 u 对内容 i 无正反馈}\\ 1 & \mbox{else} \end{matrix}\right.或： r_{ui}= \left\{\begin{matrix} 0 & \mbox{用户 u 对内容 i 无正反馈}\\ w_{i} & \mbox{else}\ w_i\ \mbox{为内容 i 的权重} \end{matrix}\right.在科室推荐问题中，我们对用户的不同行为赋以不同权值，权值越大代表该行为越能反映用户对科室的正反馈程度： ID 类别 权值 1 访问大类 1 2 访问子类 2 3 关注医生 5 4 挂号订单 10 5 问诊订单 10 除了行为类型能够反映用户对科室的倾向值，用户的访问频次也是一个重要的因素，我们将用户在不同行为上的访问天数进行加权求和作为用户-科室评分： r_{ui} = \sum_{t=1}^{5} d_{uit} \times w_t其中： $d_{uit}$：用户 $u$ 在科室 $i$ 上发生第 $t$ 类行为的天数； $w_t$：我们所定义的第 $t$ 类行为的权重； 基于“用户-科室反馈表”（2020.07.04 ~ 2020.12.30 数据作为训练集）计算得到如下“用户-科室评分表”： 计算 Item-Item 相似矩阵基于用户-内容评分矩阵可以计算出内容-内容之间的相似矩阵： W = \left[ \begin{matrix} w_{11} & w_{12} & \cdots & w_{1\left | I \right |} \\ w_{21} & w_{22} & \cdots & w_{2\left | I \right |} \\ \vdots & \vdots & \ddots & \vdots \\ w_{\left | I \right |1} & w_{\left | I \right |2} & \cdots & w_{\left | I \right |\left | I \right |} \\ \end{matrix} \right]其中，可以通过余弦相似度来计算不同内容间的相似性： w_{ij}=\frac{\left | N(i) \bigcap N(j) \right |}{\sqrt{\left | N(i) \right | \left | N(j) \right |}} $N(i)$：表示对内容 $i$ 有过正反馈的用户集合 或： w_{ij}=\frac{\vec{r}_i\cdot \vec{r}_j}{\left | \vec{r}_i \right |\left | \vec{r}_j \right |}=\frac{\sum_{u \in U}r_{ui}r_{uj}}{\sqrt{\sum_{u\in U}r_{ui}^2}\sqrt{\sum_{u\in U}r_{uj}^2}} $\vec{r}_i$：表示内容 $i$ 对应的用户评分向量 通过余弦相似度计算“科室-科室相似度”，参见科室相似度： 计算 User-Item 兴趣矩阵在得到内容之间的相似度后，可以计算用户-内容的兴趣度矩阵： P = \left[ \begin{matrix} p_{11} & p_{12} & \cdots & p_{1\left | I \right |} \\ p_{21} & p_{22} & \cdots & p_{2\left | I \right |} \\ \vdots & \vdots & \ddots & \vdots \\ p_{\left | U \right |1} & p_{\left | U \right |2} & \cdots & p_{\left | U \right |\left | I \right |} \\ \end{matrix} \right]其中： p(u,i)=\sum_{j \in S(i,K)\bigcap N(u)}r_{uj}w_{ji} $S(i,K)$：和内容 $i$ 最相似的 $K$ 个内容集合（不包括 $i$） $N(u)$：用户 $u$ 有过正反馈的内容集合 “用户-科室兴趣度”计算结果如下： 生成科室推荐回到最初对问题的定义，我们基于以上 ItemCF 方法得到了一组问题的解： \forall u \in U, {i}'=\underset{i\in I}{argmax}\ p(u,i)当前推荐场景下，每次只会为用户推荐一个科室，系统优先为我推荐的科室为“口腔修复科”。事实上，我在 1 月 3 日做了智齿拔除手术，此前我曾尝试在腾讯健康预约牙医，可惜预约已满，没有预约成功。 冷启动处理对于新用户或新科室冷启动问题的思考: 对于新用户推荐：可以通过为新用户推荐热门科室，作为兜底方案； 对于新科室推荐：可以根据科室信息，将科室推荐给访问过类似科室的用户； 代码实现代码逻辑代码实现逻辑参见工程 SparkV3 com.tencent.csig.healthy.model.recommend.itgdept，其中涉及到的表 UML 参考下图： 任务调度相关任务基于 Tesla 按天进行调度： 效果评价用户满意度调研了几个同事对以上推荐结果的满意度，因为腾讯健康的同事多为测试用户，在腾讯健康上的行为不能代表他们的真实意图，有待调研实际场景下用户对推荐结果的看法： 离线指标离线指标主要关注推荐的准确率、召回率和覆盖率，$R(u)$ 代表基于训练集为用户 $u$ 生成的科室列表，$T(u)$ 是用户在测试集上实际访问的科室列表： 准确率（Precision）： Precision=\frac{\sum_{u \in U}\left | R(u)\bigcap T(u) \right |}{\sum_{u \in U}\left | R(u) \right |} 召回率（Recall）： Recall=\frac{\sum_{u \in U}\left | R(u)\bigcap T(u) \right |}{\sum_{u \in U}\left | T(u) \right |} 覆盖率（Coverage）： Coverage=\frac{\left | \bigcup _{u \in U} R(u)\right |}{\left | I \right |} 命中率（Hit Rate）：如果测试用户 u 实际访问的 Item 出现在了系统为其推荐的 TOP-N 列表中，称为一次命中；$hits$ 为命中的测试集用户数，$\left | U \right|$ 为测试集总用户数， HR = \frac{hits}{\left | U \right |} 加权命中率（Average Reciprocal Hit Rank）：假设 $p{u1},…,p{uh}$ 是 hits 的位置，即测试用户 u 实际访问的 item 在 TOP-N 推荐列表中的位置，$1\leq p_{ui}\leq N$，ARHR 衡量了一个 item 被推荐的强度 ARHR = \frac{1}{\left | U \right |}\sum_{u \in U}\sum_{i=1}^{h}\frac{1}{p_{ui}}我们收集了验证集（ 2021.01.01 ~ 2021.01.18）中，有过科室反馈行为且被系统推荐的用户作为测试集，我们分别在不同的推荐列表长度（N）下，对本文所介绍的方法进行了离线验证，验证结果如下： 算法对科室的覆盖度都达到了 100%，所有标准科室都有机会被推荐出来； 对于 TOP-1 推荐，算法的准确率和命中率有不错的表现，达到 73%； 随着推荐列表长度的增大，推荐准确率逐渐下降，TOP-10推荐准确率只有14%，说明用户对推荐列表尾部科室兴趣不大； 算法命中率和加权命中率无论在 TOP-1 还是 TOP-10 推荐中均有较高取值，说明推荐结果能够很好的命中用户对科室的兴趣； 在线指标在线指标可以通过实施 AB 实验对比不同推荐策略对用户点击率的影响： CTR = \frac{\mbox{科室点击人数}}{\mbox{科室曝光人数}}\times 100\%TODO 定义与问诊科室访问相关的用户行为和权重；— done 用户历史行为数据清洗，关联标准问诊科室；— done 算法实现和测试；— done 离线评测与优化；— done 线上评测与优化； 法务风险评估：用到了用户的文章阅读、词条点击、医生关注等数据，有潜在法务风险； 参考[1] Sparse Linear Methods for Top-N Recommender Systems[2] 之前简单整理过一些推荐系统相关的笔记，可做参考]]></content>
      <categories>
        <category>推荐系统</category>
      </categories>
      <tags>
        <tag>推荐系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mac OSX 配置JAVA环境]]></title>
    <url>%2Funcategorized%2FTools%2FMac-OSX-%E9%85%8D%E7%BD%AEJAVA%E7%8E%AF%E5%A2%83%2F</url>
    <content type="text"><![CDATA[安装 JAVA 环境下载安装JDK检查一下是不是已经安装了Java： 点击更多信息后直接跳转到JDK下载页，下载完 jdk 后直接双击点开，按照提示走完步骤。 12345# 检查java是否安装成功➜ ~ java -versionjava version &quot;1.8.0_211&quot;Java(TM) SE Runtime Environment (build 1.8.0_211-b12)Java HotSpot(TM) 64-Bit Server VM (build 25.211-b12, mixed mode) 配置环境变量查看JAVA_HOME安装路径： 12345➜ ~ /usr/libexec/java_home -VMatching Java Virtual Machines (1): 1.8.0_211, x86_64: &quot;Java SE 8&quot; /Library/Java/JavaVirtualMachines/jdk1.8.0_211.jdk/Contents/Home/Library/Java/JavaVirtualMachines/jdk1.8.0_211.jdk/Contents/Home 如果使用了oh_my_zsh，打开.zshrc配置文件： 123456$ cd$ vi .zshrc# 编辑export JAVA_HOME=&quot;/Library/Java/JavaVirtualMachines/jdk1.8.0_211.jdk/Contents/Home&quot;export PATH=&quot;.$PATH:$JAVA_HOME/bin&quot;export CLASS_PATH=&quot;$JAVA_HOME/lib&quot; 下载安装 IntelliJ IDEA公司IT服务下载地址 配置maven安装maven：12345678910# 安装brew install maven# 验证mvn -v➜ conf mvn -vApache Maven 3.6.2 (40f52333136460af0dc0d7232c0dc0bcf0d9e117; 2019-08-27T23:06:16+08:00)Maven home: /usr/local/Cellar/maven/3.6.2/libexecJava version: 1.8.0_211, vendor: Oracle Corporation, runtime: /Library/Java/JavaVirtualMachines/jdk1.8.0_211.jdk/Contents/Home/jreDefault locale: zh_CN, platform encoding: UTF-8OS name: &quot;mac os x&quot;, version: &quot;10.14&quot;, arch: &quot;x86_64&quot;, family: &quot;mac&quot; mac配置maven 配置环境变量：如果是用homebrew安装的，则会自动配置，否则将Maven home的路径添加到M2_HOME 123# 配置maven环境变量export M2_HOME=&quot;/usr/local/Cellar/maven/3.6.2/libexec&quot;export PATH=$PATH:$M2_HOME/bin mac配置settings.xml：如果两者都存在，它们的内容将被合并，并且用户范围的settings.xml会覆盖全局的settings.xml 全局配置，对操作系统的所有使用者生效：/usr/local/Cellar/maven/3.6.2/libexec/conf/settings.xml 用户配置，只对当前操作系统的使用者生效：${user.home}/.m2/settings.xml intelliJ配置maven intelliJ配置maven：Preferences-&gt;Build,Execution,Deployment-&gt;Build Tools-&gt;Mavent 导入maven项目如果已经打开了一个项目，可以选择File-&gt;Close Project关闭当前项目，回到主界面，点击Import Project，或者点击 File-&gt;New-&gt;Project From Existing Sources...： 选择主pom文件，然后确定： 勾选“auto”自动导入maven项目，先不要next，还要选择环境配置： 在跳出来的对话框页面，第一个选择到本地maven的插件，后面两个是setting.xml文件和映射到的仓库地址，可以默认。如果有自己的settings文件，可以选择 override 然后指定自己的settings.xml或者将自己的settings.xml文件拷贝到默认路径下： 点击OK-&gt;一路NEXT： 然后等一段时间，项目会自己从maven上下载依赖，并构建工程。等待工程构建完毕后打开View-&gt;Tool Windows-&gt; Maven Projects，右侧就会打开一个maven工程的构建窗口，打开目录树big_dataàLifecycle双击package，就会执行jar包的打包动作： IDEA下方的信息窗口显示BUILD SUCCESS，则表示代码打包成功，可以上传到TDW平台运行，其中出现问题导致项目构建失败或者打包失，多半都是环境配置问题。]]></content>
  </entry>
  <entry>
    <title><![CDATA[Git简明教程]]></title>
    <url>%2F%E5%B7%A5%E5%85%B7%2FTools%2FGit%E7%AE%80%E6%98%8E%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[Git 配置Git 诞生于2005年，是一种分布式版本控制系统（Distributed Version Control System，简称 DVCS），客户端并不只提取最新版本的文件快照，而是把代码仓库完整地镜像下来，任何一处协同工作用的服务器发生故障，事后都可以用任何一个镜像出来的本地仓库恢复。 Mac 安装 Xcode Command Line Tools 时会自带 git，也可以通过 homebrew 来安装： 123456$ brew install git# 查看版本$ git versiongit version 2.20.1 (Apple Git-117)# 查看帮助$ git help 检查配置信息： 1$ git config --list 配置账号邮箱： 12$ git config --global user.name &quot;John Doe&quot;$ git config --global user.email johndoe@example.com 本地仓库创建本地仓库有两种方法来获取Git仓库： 1234567# 将现有目录初始化为仓库$ cd &lt;folder&gt;$ git init# 从仓库url克隆现有仓库，自定义仓库名称为new_repo_name$ cd &lt;folder&gt;$ git clone [url] [new_repo_name] 如果要删除本地仓库，只需要删除仓库下的 .git 目录即可将git仓库转化为普通目录： 1$ rm -rf .git 本地仓库的状态查看当前分支文件的状态： 123$ git statusOn branch masternothing to commit, working directory clean Git 的三种区域/文件： 仓库区：用来保存项目元数据和对象数据库的地方； 缓存区：保存了下次将要提交的文件列表信息的文件； 工作区：从git仓库中取出放在磁盘上用于使用或修改的文件； 文件的四类状态： 已跟踪/未跟踪：根据文件是否被纳入了版本控制来分(从第一次add开始跟踪到最后一次rm结束跟踪)； 已修改/未修改：自上次提交之后是否做了修改； 已缓存/未缓存：修改后是否添加到了缓存； 已提交/未提交：添加到了缓存之后是否提交到仓库； 添加文件至缓存添加文件git add &lt;file&gt;：将工作区的指定内容添加到下一次的提交列表(缓存区)，在不同条件下add有三种功能 如果是未被跟踪的文件：开始跟踪新文件并将其添加到缓存区； 如果是已跟踪的文件：将已跟踪文件放到缓存区； 如果是合并时有冲突的文件：将有冲突的文件标记为已解决状态； 示例： 12345678910111213# 将file添加到下次提交的列表中(缓存区)$ git add file# 将多个文件加入缓存区$ git add readme.txt ant.txt# 将.c文件加入缓存区$ git add *.c# 将当前目录下所有未跟踪或修改的文件放入缓存区$ git add .$ git add *# 将整个工作区下所有未跟踪或修改的文件放入缓存区$ git add -A 忽略文件可以通过创建一个名为.gitignore的文件，列出 add 命令要忽略的文件格式，格式规范如下 所有已空行或以#开头的行会被忽略 可以使用标准的glob模式匹配（shell所使用的简化了的正则表达式） *匹配零或多个任意字符 [abc]匹配任何一个列在方括号中的字符 ?匹配一个任意字符 ** 表示匹配任意中间目录 匹配模式可以以（/）开头防止递归 匹配模式可以以（/）结尾指定目录 要忽略指定模式以外的文件或目录，可以在模式前加上惊叹号（!）取反 12345678910# 忽略.a后缀类型的文件*.a# 不忽略lib.a，即使上面忽略了.a类型的文件!lib.a # 忽略当前目录下的TODO文件/TODO# 忽略build目录下的所有文件build/# 忽略doc目录下的所有pdf文件doc/**/*.pdf 需要注意的是.gitignore的文件只对那些尚未被跟踪的文件有用，而对已经被跟踪的文件无效，所以要养成一开始就设置好.gitignore文件的习惯，以免将来误提交这类无用的文件。即使这样，有事在项目开发过程中，突然心血来潮想把某些目录加入到忽略规则，只修改.gitignore文件是不行的，还要把本地缓存删除，然后再重新添加： 123$ git rm -r --cached .$ git add .$ git commit -m &quot;update .gitignore&quot; 对比文件查看已修改但未暂存的变化：git diff 用于比较当前工作区与缓存区的差异，打印那些在工作区做出了修改但尚未添加到缓存区的内容 12345$ git diffdiff --git a/CONTRIBUTING.md b/CONTRIBUTING.mdindex 8ebb991..643e24f 100644--- a/CONTRIBUTING.md+++ b/CONTRIBUTING.md 查看已缓存但未提交的变化：git diff --staged 用于比较当前缓存区和仓库的差异，打印那些已添加到缓存区但尚未提交到仓库的内容 1$ git diff --staged 取消暂存git restore --staged &lt;file&gt;：取消对文件file的暂存 123456789$ git restore --staged text.txt位于分支 demo您的分支领先 &#x27;origin/files&#x27; 共 1 个提交。 （使用 &quot;git push&quot; 来发布您的本地提交）尚未暂存以备提交的变更： （使用 &quot;git add &lt;文件&gt;...&quot; 更新要提交的内容） （使用 &quot;git restore &lt;文件&gt;...&quot; 丢弃工作区的改动） 修改： text.txt 取消修改git checkout -- &lt;file&gt;：自上次提交后对文件进行了修改但还没有添加到缓存，则可以通过该命令取消对文件file的修改，也就是用上次提交时的文件覆盖工作区中的文件； 1234567$ git checkout -- CONTRIBUTING.md$ git statusOn branch masterChanges to be committed: (use &quot;git reset HEAD &lt;file&gt;...&quot; to unstage) renamed: README.md -&gt; README 移除文件需要从已跟踪文件清单中移除该文件，然后提交，git rm 可以完成此项工作并连带从工作目录中删除指定的文件 1234567# 使用git rm移除文件$ git rm PROJECTS.md# 如果该文件新的修改已缓存但尚未提交，则必须进行强制删除$ git rm -f PROJECTS.md# 也可以手动删除后提交$ rm PROJECTS.md$ git commit -a -m &quot;delete file&quot; 提交文件至仓库提交文件git commit -m &quot;&quot;：将已缓存的文件提交至仓库 示例： 12345678# 会启动shell 的环境变量 $EDITOR 所指定的文本编辑器以便输入本次提交的说明$ git commit # 将提交信息与命令放在同一行$ git commit -m &quot;***&quot;# 跳过使用暂存区域，提交之前不再需要 git add 文件$ git commit -a -m &#x27;added new benchmarks&#x27; 查看提交历史git log：按提交时间列出所有的更新，最近的更新排在最上面； 123456789101112131415161718192021$ git log# SHA-1 校验和commit ca82a6dff817ec66f44342007202690a93763949# 作者Author: Scott Chacon &lt;schacon@gee-mail.com&gt;# 提交时间Date: Mon Mar 17 21:52:11 2008 -0700# 提交说明 changed the version numbercommit 085bb3bcb608e1e8451d4b2432f8ecbe6306e7e7Author: Scott Chacon &lt;schacon@gee-mail.com&gt;Date: Sat Mar 15 16:40:33 2008 -0700 removed unnecessary testcommit a11bef06a3f659402fe7563abf99ad00de2209e6Author: Scott Chacon &lt;schacon@gee-mail.com&gt;Date: Sat Mar 15 10:31:28 2008 -0700 first commit 如果要显示每次提交的内容差异，可以加上选项-p： 12345678910111213141516171819202122# -2 表示仅显示最近两次提交$ git log -p -2commit ca82a6dff817ec66f44342007202690a93763949Author: Scott Chacon &lt;schacon@gee-mail.com&gt;Date: Mon Mar 17 21:52:11 2008 -0700 changed the version numberdiff --git a/Rakefile b/Rakefileindex a874b73..8f94139 100644--- a/Rakefile+++ b/Rakefile@@ -5,7 +5,7 @@ require &#x27;rake/gempackagetask&#x27; spec = Gem::Specification.new do |s| s.platform = Gem::Platform::RUBY s.name = &quot;simplegit&quot;- s.version = &quot;0.1.0&quot;+ s.version = &quot;0.1.1&quot; s.author = &quot;Scott Chacon&quot; s.email = &quot;schacon@gee-mail.com&quot; s.summary = &quot;A simple gem for using Git in Ruby code.&quot; 每行显示一次提交：--pretty=oneline 1234$ git log --pretty=onelineca82a6dff817ec66f44342007202690a93763949 changed the version number085bb3bcb608e1e8451d4b2432f8ecbe6306e7e7 removed unnecessary testa11bef06a3f659402fe7563abf99ad00de2209e6 first commit 展示分支合并历史：--graph 1234567891011$ git log --pretty=format:&quot;%h %s&quot; --graph* 2d3acf9 ignore errors from SIGCHLD on trap* 5e3ee11 Merge branch &#x27;master&#x27; of git://github.com/dustin/grit|\| * 420eac9 Added a method for getting the current branch.* | 30e367c timeout code and tests* | 5a09431 add timeout protection to grit* | e1193f8 support for heads with slashes in them|/* d6016bc require time for xmlschema* 11d191e Merge branch &#x27;defunkt&#x27; into local 分支管理Git 中的分支可以看做是由仓库快照组成的反向链表，每个分支名就是指向对应分支的头指针(HEAD是指向当前分支头指针的指针)，在该分支中的每次提交都会在对应链表头部插入一个快照节点，并将分支的头指针向前移动。 查看分支： 123456789101112131415# 查看本地分支，*标识的是当前分支$ git branch iss53* master testing# 查看所有分支$ git branch -a# 查看远程分支$ git branch -r# 查看每一个分支的最后一次提交$ git branch -v# 查看已经合并或尚未合并到当前分支的分支$ git branch --merged$ git branch --no-merged 分支创建git branch &lt;branch_name&gt;：创建一个名为branch_name的新分支，这会在当前所在的提交队形上创建一个指针 1$ git branch testing 分支切换git checkout &lt;branch_name&gt;：切换到一个已存在的分支，之后对文件的修改、添加、提交都在在新的分支上进行的； 1$ git branch testing 分支合并git merge &lt;branch_name&gt;：将另一个分支合并到当前分支上来，根据当前主分支（当前分支）和副分支（被合并的分支）头指针间的关系，有四种情形： 副分支是主分支的直接上游：不会发生任何变化 12$ branch checkout issue53$ branch merge master 主分支是副分支的直接上游：将主分支快进(fast-ward)到副分支的位置 12$ branch checkout master$ branch merge issue53 主分支和副分支分叉不冲突：Git 会使用两个分支的末端所指的快照（C4 和 C5）以及这两个分支的工作祖先（C2），做一个简单的三方合并，并自动创建一个新的提交指向它，这称作一次合并提交 123456$ git checkout masterSwitched to branch &#x27;master&#x27;$ git merge iss53Merge made by the &#x27;recursive&#x27; strategy.index.html | 1 +1 file changed, 1 insertion(+) 主分支和副分支分叉冲突：如果你在两个不同的分支中，对同一个文件的同一个部分进行了不同的修改，此时Git做了合并但不会自动创建一个新的提交，需要等到你手动解决了冲突之后再进行手动提交 1234567891011121314151617181920212223# 冲突合并$ git merge iss53Auto-merging index.htmlCONFLICT (content): Merge conflict in index.htmlAutomatic merge failed; fix conflicts and then commit the result.# 包含冲突待解决的文件都会以未合并状态标识出来，Git 会在有冲突的文件中加入标准的冲突解决标记，这样你可以打开这些包含冲突的文件然后手动解决冲突，&lt;&lt;&lt;后面和&gt;&gt;&gt;&gt;后面表示发生冲突的两个分支文件，======上下是发生冲突的内容&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD:index.html&lt;div id=&quot;footer&quot;&gt;contact : email.support@github.com&lt;/div&gt;=======&lt;div id=&quot;footer&quot;&gt; please contact us at support@github.com&lt;/div&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; iss53:index.html# 冲突解决后的样子&lt;div id=&quot;footer&quot;&gt;please contact us at email.support@github.com&lt;/div&gt;# 如果你对结果感到满意，并且确定之前有冲突的的文件都已经暂存了，这时你可以输入 git commit 来完成合并提交$ git add *$ git commit -m &quot;解决冲突&quot; 删除分支12$ git branch -d hotfixDeleted branch hotfix (3a0874c). 分支策略常见的利用分支进行开发的工作流程： 渐进稳定分支（长期分支）：只在 master 分支上保留完全稳定的代码，还有一些名为 develop 或者 next 的平行分支，被用来做后续开发或者测试稳定性——这些分支不必保持绝对稳定，但是一旦达到稳定状态，它们就可以被合并入 master 分支了，本博客采用的也是一种长期分支策略，用master分支存放静态网页，用files分支存放网站原始文件； 特性分支（短期分支）：特性分支被用来实现单一特性或其相关工作，考虑这样一个例子，你在 master 分支上工作到 C1，这时为了解决一个问题而新建 iss91 分支，在 iss91 分支上工作到 C4，然而对于那个问题你又有了新的想法，于是你再新建一个 iss91v2 分支试图用另一种方法解决那个问题，接着你回到 master 分支工作了一会儿，你又冒出了一个不太确定的想法，你便在 C10 的时候新建一个 dumbidea 分支，并在上面做些实验。 你的提交历史看起来像下面这个样子： 远程仓库远程仓库是指托管在因特网或其他网络中的你的项目的版本库，与他人协作涉及管理远程仓库以及根据需要推送或拉取数据。 查看远程仓库查看你已经配置的远程仓库服务器，可以运行 git remote 命令，克隆的仓库默认将origin作为远程仓库的名字： 1234567891011121314151617181920212223242526272829303132$ git remoteorigin# 可以指定选项 -v，会显示需要读写远程仓库使用的 Git 保存的简写与其对应的 URL$ git remote -vorigin https://github.com/schacon/ticgit (fetch)origin https://github.com/schacon/ticgit (push)# 查看某个远程仓库更多信息➜ .git git:(files) git remote show origin* 远程 origin 获取地址：git@github.com:liketea/liketea.github.io.git 推送地址：git@github.com:liketea/liketea.github.io.git HEAD 分支：files 远程分支： demo 已跟踪 files 已跟踪 master 已跟踪 ss 已跟踪 tt 已跟踪 ttt 已跟踪 xx 已跟踪 xxx 已跟踪 为 &#x27;git pull&#x27; 配置的本地分支： demo 与远程 files 合并 files 与远程 files 合并 tt 与远程 demo 合并 为 &#x27;git push&#x27; 配置的本地引用： demo 推送至 demo (本地已过时) files 推送至 files (最新) ss 推送至 ss (最新) tt 推送至 tt (可快进)py 添加远程仓库git remote add &lt;shortname&gt; &lt;url&gt;： 添加一个新的远程 Git 仓库，同时指定一个你可以轻松引用的简写 12345678$ git remoteorigin$ git remote add pb https://github.com/paulboone/ticgit$ git remote -vorigin https://github.com/schacon/ticgit (fetch)origin https://github.com/schacon/ticgit (push)pb https://github.com/paulboone/ticgit (fetch)pb https://github.com/paulboone/ticgit (push) 修改远程仓库有三种方法来修改远程仓库： 直接修改本地仓库下的.git/config文件: 123url = http://xxx.com/Name/project.git改为url = git@xxx.com/Name/project.git 修改命令: 12git remote set-url origin [url]例如：git remote set-url origin gitlab@gitlab.chumob.com:php/hasoffer.git 先删后加 12git remote rm origingit remote add origin [url] 配置SSH 公钥许多 Git 服务器都使用 SSH 公钥进行认证，需要在本地生成SSH 公钥，然后将公钥发送给 Git 服务器管理员（配置到github中）。 生成SSH 公钥 123456789101112131415# 默认情况下，用户的 SSH 密钥存储在其 ~/.ssh 目录下$ cd ~/.ssh# .pub 文件是你的公钥，另一个则是私钥$ lsid_dsa id_dsa.pub# 如果没有则需要生成：首先 ssh-keygen 会确认密钥的存储位置（默认是 .ssh/id_rsa），然后它会要求你输入两次密钥口令。如果你不想在使用密钥时输入口令，将其留空即可$ ssh-keygen# 获取公钥$ cat id_dsa.pubssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEAklOUpkDHrfHY17SbrmTIpNLTGK9Tjom/BWDSUGPl+nafzlHDTYW7hdI4yZ5ew18JH4JW9jbhUFrviQzM7xlELEVf4h9lFX5QVkbPppSwg0cda3Pbv7kOdJ/MTyBlWXFCR+HAo3FXRitBqxiX1nKhXpHAZsMciLq8V6RjsNAQwdsdMFvSlVK/7XAt3FaoJoAsncM1Q9x5+3V0Ww68/eIFmb1zuUFljQJKprrX88XypNDvjYNby6vw/Pb0rwert/EnmZ+AW4OZPnTPI89ZPmVMLuayrD2cE86Z/il8b+gw3r3+1nKatmIkjn2so1d01QraTlMqVSsbxNrRFi9wrf+M7Q== schacon@mylaptop.local 配置公钥：github -&gt; personal settings -&gt; SSH and GPG keys 远程分支&amp;跟踪分支本地仓库和远程仓库各自可能包含多个分支，理清几个概念： 本地分支(local branch)：本地仓库中的普通分支； 远程分支(remote branch)：远程仓库中的普通分支； 远程跟踪分支(remote-tracking branch)：本地仓库中自动记录远程分支状态的分支(远程主机名/远程分支名，如origin/master)，其指向只会在用户使用git fetch等指令时移动到对应远程仓库最新位置，用户无法直接改变其指向； 跟踪分支(tracking branch)：如果在某个本地分支 L 与某个远程分支 R 之间建立了映射关系，则称 L 为 R的跟踪分支，称 R 为 L 的上游分支(upstream)，当使用git pull时会按照对应远程分支的指向移动跟踪分支; 在 L 和 R 之间建立映射关系： 1git branch --set-upstream-to=origin/R L 本地仓库推送到远程仓库git push 的一般形式为 git push &lt;远程主机名&gt; &lt;本地分支名&gt; &lt;远程分支名&gt;：将指定的本地分支推送到指定远程主机上指定的远程分支，需注意两点： 如果远程分支不存在则自动创建，但不会自动创建跟踪，如需建立跟踪关系可添加选项-u： 123456789101112131415161718192021222324252627282930# 远程分支不存在，自动创建但不会跟踪➜ liketea.github.io git:(demo) ✗ git push origin tt:xx总共 0 （差异 0），复用 0 （差异 0）remote:remote: Create a pull request for &#x27;xx&#x27; on GitHub by visiting:remote: https://github.com/liketea/liketea.github.io/pull/new/xxremote:remote:remote:remote: GitHub found 1 vulnerability on liketea/liketea.github.io&#x27;s default branch (1 moderate). To find out more, visit:remote: https://github.com/liketea/liketea.github.io/network/alertsremote:To github.com:liketea/liketea.github.io.git * [new branch] tt -&gt; xx # 远程分支不存在，自动创建并跟踪➜ liketea.github.io git:(demo) ✗ git push origin tt:xxx -u总共 0 （差异 0），复用 0 （差异 0）remote:remote: Create a pull request for &#x27;xxx&#x27; on GitHub by visiting:remote: https://github.com/liketea/liketea.github.io/pull/new/xxxremote:remote:remote:remote: GitHub found 1 vulnerability on liketea/liketea.github.io&#x27;s default branch (1 moderate). To find out more, visit:remote: https://github.com/liketea/liketea.github.io/network/alertsremote:To github.com:liketea/liketea.github.io.git * [new branch] tt -&gt; xxx分支 &#x27;tt&#x27; 设置为跟踪来自 &#x27;origin&#x27; 的远程分支 &#x27;xxx&#x27;。 如果远程分支已存在，只有当远程分支是本地分支的直接上游才能push成功，本地分支直接merge到远程分支，否则需要先将远程分支拉取到本地，在本地合并/解决冲突后重新提交、推送，如需强制推送则添加选项-f: 1234567891011121314151617181920212223➜ liketea.github.io git:(tt) git push origin tt:filesTo github.com:liketea/liketea.github.io.git ! [rejected] tt -&gt; files (non-fast-forward)error: 推送一些引用到 &#x27;git@github.com:liketea/liketea.github.io.git&#x27; 失败提示：更新被拒绝，因为推送的一个分支的最新提交落后于其对应的远程分支。提示：检出该分支并整合远程变更（如 &#x27;git pull ...&#x27;），然后再推送。详见提示：&#x27;git push --help&#x27; 中的 &#x27;Note about fast-forwards&#x27; 小节。# 强制推送➜ liketea.github.io git:(files) git push -f枚举对象: 14, 完成.对象计数中: 100% (14/14), 完成.使用 12 个线程进行压缩压缩对象中: 100% (10/10), 完成.写入对象中: 100% (10/10), 5.12 KiB | 5.12 MiB/s, 完成.总共 10 （差异 8），复用 0 （差异 0）remote: Resolving deltas: 100% (8/8), completed with 4 local objects.remote:remote: GitHub found 1 vulnerability on liketea/liketea.github.io&#x27;s default branch (1 moderate). To find out more, visit:remote: https://github.com/liketea/liketea.github.io/network/alertsremote:To github.com:liketea/liketea.github.io.git + 62bc43b...ca67c31 files -&gt; files (forced update) 比起git push的一般格式外，更常用的是它的简略模式。 省略远程分支——推送到同名分支git push &lt;remote&gt; &lt;local_branch&gt;：将指定本地分支推送到指定远程主机上的同名分支，如果同名分支不存在则创建，新建的远程分支不会自动与本地分支建立映射 12345# 推送至同名远程分支$ git push &lt;远程主机名&gt; &lt;local_branch&gt;# 推送时创建本地分支与远程同名分支的映射$ git push -u &lt;远程主机名&gt; &lt;local_branch&gt; 省略本地分支——删除远程分支git push &lt;remote&gt; :&lt;remote_branch&gt;：省略了本地分支名，相当于推送了一个空的本地分支到远程分支，实际效果是删除了指定的远程分支 1$ git &lt;远程主机名&gt; :&lt;remote_branch&gt; 省略本地分支和远程分支——当前分支到上游同名分支git push &lt;remote&gt;：将当前分支推送到指定主机上的当前分支所跟踪的的远程分支(上游分支)，如果主机上没有上游分支则失败，如果有上游分支但与当前分支不同名也会失败 12345678910111213141516171819# 没有上游分支➜ liketea.github.io git:(tt) ✗ git push originfatal: 当前分支 tt 没有对应的上游分支。为推送当前分支并建立与远程上游的跟踪，使用 git push --set-upstream origin tt# 有上游分支但不同名，产生歧义➜ liketea.github.io git:(demo) ✗ git push originfatal: 您当前分支的上游分支和您当前分支名不匹配，为推送到远程的上游分支，使用 git push origin HEAD:files为推送至远程同名分支，使用 git push origin HEAD为了永久地选择任一选项，参见 &#x27;git help config&#x27; 中的 push.default。 省略远程主机名、本地仓库名和远程仓库名——当前分支到上游唯一同名分支git push：将当前分支推送到其跟踪的唯一的远程上游分支，远程主机不唯一会失败，没有上游分支则失败，有上游分支但不同名也会失败 12345➜ liketea.github.io git:(tt) ✗ git pushfatal: 当前分支 tt 没有对应的上游分支。为推送当前分支并建立与远程上游的跟踪，使用 git push --set-upstream origin tt 从远程仓库拉取到本地仓库git fetch 的一般形式为 git fetch &lt;remote&gt; &lt;remote_branch&gt; &lt;local_branch&gt;：将指定远程主机上指定分支的更新拉取到本地指定分支，如果本地分支不存在则自动创建，但不会自动创建跟踪。 123➜ liketea.github.io git:(files) ✗ git fetch origin files:xyz来自 github.com:liketea/liketea.github.io * [新分支] files -&gt; xyz 默认情况下，git合并命令拒绝合并没有共同祖先的历史。当两个项目的历史独立地开始时，这个选项可以被用来覆盖这个安全。由于这是一个非常少见的情况，因此没有默认存在的配置变量，也不会添加。如果要拉取没有共同祖先的分支，直接拉取会出错，可以使用如下命令来合并： 1234$ git pullfatal: 拒绝合并无关的历史$ git pull origin master --allow-unrelated-histories 从本地仓库推送更新到远程仓库git push 用于将本地分支的更新推送到远程分支，其完整语法为： 12# 将本地指定分支的更新推送到指定远程主机的指定分支名$ git push &lt;远程主机名&gt; &lt;本地分支名&gt;:&lt;远程分支名&gt; 如果省略远程分支名，则将本地指定分支的更新推送到指定主机上的同名分支(注意不是所跟踪的远程分支)，如果远程仓库中不存在同名分支则会被新建(注意他们之间的映射关系不会自动创建)： 1$ git push &lt;远程主机名&gt; &lt;本地分支名&gt; 如果省略了本地分支名，相当于推送了一个空的本地分支到远程分支，实际效果是删除了指定的远程分支： 123456789$ git push &lt;远程主机名&gt; :&lt;远程分支名&gt;➜ liketea.github.io git:(demo) ✗ git push origin :demoremote:remote: GitHub found 1 vulnerability on liketea/liketea.github.io&#x27;s default branch (1 moderate). To find out more, visit:remote: https://github.com/liketea/liketea.github.io/network/alertsremote:To github.com:liketea/liketea.github.io.git - [deleted] demo 如果同时省略本地分支名和远程分支名，则将当前分支的更新推送到其跟踪且同名的远程分支： 1$ git push &lt;远程主机名&gt; 如果省略远程主机名、远程仓库名和本地仓库名，则将当前分支的更新推送到其所跟踪且同名的远程分支 1$ git push 从远程仓库拉取更新到本地仓库git fetch 从远程分支拉取更新到本地分支，其完整语法为： 12# 将指定远程仓库中指定分支的更新拉取到本地仓库指定分支中，如果本地分支不存在则被创建，如果本地仓库存在且是`fast forword`则合并两个分支，否则拒绝拉取$ git fetch &lt;远程主机名&gt; &lt;远程分支名&gt;:&lt;本地分支名&gt; 从远程仓库拉取数据：git fetch 会拉取远程仓库最新数据到本地仓库中对应的远程分支，它并不会自动合并或修改你当前的工作，当准备好时必须手动合并到你的分支中；如果你有一个分支设置为跟踪一个远程分支，则可以使用 git pull 来自动抓取并合并远程分支到当前分支 12345678910# 先抓取远程仓库数据到远程分支，再手动合并远程分支到当前分支# 抓取远程服务器origin上的默认分支$ git fetch origin# 抓取远程服务器origin上的指定分支master$ git fetch origin master# 将远程分支与当前分支合并$ git merge origin/master# 直接抓取并合并远程仓库中的数据到当前分支$ git pull 参考 Git 中文文档]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>教程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[曾广贤文：下集]]></title>
    <url>%2FLife%2FLife%2F%E6%9B%BE%E5%B9%BF%E8%B4%A4%E6%96%87%EF%BC%9A%E4%B8%8B%E9%9B%86%2F</url>
    <content type="text"><![CDATA[前人俗语，言浅理深。 补遗增广，集成书文。 世上无难事，只怕不专心。 成人不自在，自在不成人; 金凭火炼方知色，与人交财便知心。 乞丐无粮，懒惰而成。 勤俭为无价之宝，节粮乃众妙之门。 省事俭用，免得求人。 量大祸不在，机深祸亦深。 善为至宝深深用，心作良田世世耕。 群居防口，独坐防心。 体无病为富贵，身平安莫怨贫。 败家子弟挥金如土，贫家子弟积土成金。 富贵非关天地，祸福不是鬼神。 安分贫一时，本分终不贫。 不拜父母拜干亲，弟兄不和结外人。 人过留名，雁过留声。 择子莫择父，择亲莫择邻。 爱妻之心是主，爱子之心是亲。 事从根起，藕叶连心。 祸与福同门，利与害同城。 清酒红人脸，财帛动人心! 宁可荤口念佛，不可素口骂人。 有钱能说话，无钱话不灵。 岂能尽如人意?但求不愧吾心。 不说自己井绳短，反说他人箍井深。 恩爱多生病，无钱便觉贫。 只学斟酒意，莫学下棋心。 孝莫假意，转眼便为人父母。 善休望报，回头只看汝儿孙! 口开神气散，舌出是非生! 弹琴费指甲，说话费精神。 千贯买田，万贯结邻。 人言未必犹尽，听话只听三分。 隔壁岂无耳，窗外岂无人? 财可养生须注意，事不关己不劳心。 酒不护贤，色不护病; 财不护亲，气不护命! 一日不可无常业，安闲便易起邪心! 炎凉世态，富贵更甚于贫贱; 嫉妒人心，骨肉更甚于外人! 瓜熟蒂落，水到渠成。 人情送匹马，买卖不饶针! 过头饭好吃，过头话难听! 事多累了自己，田多养了众人。 怕事忍事不生事自然无事; 平心静心不欺心何等放心! 天子至尊不过于理，在理良心天下通行。 好话不在多说，有理不在高声! 一朝权在手，便把令来行。 甘草味甜人可食，巧言妄语不可听。 当场不论，过后枉然。 贫莫与富斗，富莫与官争! 官清难逃猾吏手，衙门少有念佛人! 家有千口，主事一人。 父子竭力山成玉，弟兄同心土变金。 当事者迷，旁观者清。 怪人不知理，知理不怪人。 未富先富终不富，未贫先贫终不贫。 少当少取，少输当赢! 饱暖思淫欲，饥寒起盗心! 蚊虫遭扇打，只因嘴伤人! 欲多伤神，财多累心! 布衣得暖真为福，千金平安即是春。 家贫出孝子，国乱显忠臣! 宁做太平犬，莫做离乱人! 人有几等，官有几品。 理不卫亲，法不为民。 自重者然后人重，人轻者便是自轻。 自身不谨，扰乱四邻。 快意事过非快意，自古败名因败事。 伤身事莫做，伤心话莫说。 小人肥口，君子肥身。 地不生无名之辈，天不生无路之人。 一苗露水一苗草，一朝天子一朝臣。 读未见书，如得良友;见已读书，如逢故人。 福满须防有祸，凶多料必无争。 不怕三十而死，只怕死后无名。 但知江湖者，都是薄命人。 不怕方中打死人，只知方中无好人。 说长说短，宁说人长莫说短; 施恩施怨，宁施人恩莫施怨。 育林养虎，虎大伤人。 冤家抱头死，事要解交人。 卷帘归乳燕，开扇出苍蝇。 爱鼠常留饭，怜蛾灯罩纱。 人命在天，物命在人。 奸不通父母，贼不通地邻。 盗贼多出赌博，人命常出奸情。 治国信谗必杀忠臣，治家信谗必疏其亲。 治国不用佞臣，治家不用佞妇。 好臣一国之宝，好妇一家之珍。 稳的不滚，滚的不稳。 儿不嫌母丑，狗不嫌家贫。 君子千钱不计较，小人一钱恼人心。 人前显贵，闹里夺争。 要知江湖深，一个不做声。 知止自当出妄想，安贫须是禁奢心。 初入行业，三年事成; 初吃馒头，三年口生。 家无生活计，坐吃如山崩。 家有良田万顷，不如薄艺在身; 艺多不养家，食多嚼不赢。 命中只有八合米，走遍天下不满升。 使心用心，反害自身。 国家无空地，世上无闲人。 妙药难医怨逆病，混财不富穷命人。 耽误一年春，十年补不清; 人能处处能，草能处处生。 会打三班鼓，也要几个人。 人不走不亲，水不打不浑。 三贫三富不到老，十年兴败多少人! 买货买得真，折本折得轻; 不怕问到，只怕倒问。 人强不如货强，价高不如口便。 会买买怕人，会卖卖怕人。 只只船上有梢公，天子足下有贫亲。 既知莫望，不知莫向。 在一行，练一行; 穷莫失志，富莫癫狂。 天欲令其灭亡，必先让其疯狂。 梢长人胆大，梢短人心慌。 隔行莫贪利，久炼必成钢。 瓶花虽好艳，相看不耐长。 早起三光，迟起三慌。 未来休指望，过去莫思量; 时来遇好友，病去遇良方。 布得春风有夏雨，哈得秋风大家凉。 晴带雨伞，饱带饥粮。 满壶全不响，半壶响叮当。 久利之事莫为，众争之地莫往。 老医迷旧疾，朽药误良方; 该在水中死，不在岸上亡。 舍财不如少取，施药不如传方。 倒了城墙丑了县官，打了梅香丑了姑娘。 燕子不进愁门，耗子不钻空仓。 苍蝇不叮无缝蛋，谣言不找谨慎人。 一人舍死，万人难当。 人争一口气，佛争一炷香。 门为小人而设，锁乃君子之防。 舌咬只为揉，齿落皆因眶。 硬弩弦先断，钢刀刃自伤。 贼名难受，龟名难当。 好事他人未见讲，错处他偏说得长。 男子无志纯铁无钢，女子无志烂草无瓤。 生男欲得成龙犹恐成獐，生女欲得成凤犹恐成虎。 养男莫听狂言，养女莫叫离母。 男子失教必愚顽，女子失教定粗鲁。 生男莫教弓与弩，生女莫教歌与舞。 学成弓弩沙场灾，学成歌舞为人妾。 财交者密，财尽者疏。 婚姻论财，夫妻之道。 色娇者亲，色衰者疏。 少实胜虚，巧不如拙。 百战百胜不如无争，万言万中不如一默。 有钱不置怨逆产，冤家宜解不宜结。 近朱者赤，近墨者黑。 一个山头一只虎，恶龙难斗地头蛇。 出门看天色，进门看脸色。 商贾买卖如施舍，买卖公平如积德。 天生一人，地生一穴。 家无三年之积不成其家，国无九年之积不成其国。 男子有德便是才，女子无才便是德。 有钱难买子孙贤，女儿不请上门客。 男大当婚女大当嫁，不婚不嫁惹出笑话。 谦虚美德，过谦即诈。 自己跌倒自己爬，望人扶持都是假。 人不知己过，牛不知力大。 一家饱暖千家怨，一物不见赖千家。 当面论人惹恨最大，是与不是随他说吧! 谁人做得千年主，转眼流传八百家。 满载芝麻都漏了，还在水里捞油花! 皇帝坐北京，以理统天下。 五百年前共一家，不同祖宗也同华! 学堂大如官厅，人情大过王法。 找钱犹如针挑土，用钱犹如水推沙! 害人之心不可有，防人之心不可无! 不愁无路，就怕不做。 须向根头寻活计，莫从体面下功夫! 祸从口出，病从口入。 药补不如肉补，肉补不如养补。 思虑之害甚于酒色，日日劳力上床呼疾。 人怕不是福，人欺不是辱。 能言不是真君子，善处方为大丈夫! 为人莫犯法，犯法身无主。 姊妹同肝胆，弟兄同骨肉。 慈母多误子，悍妇必欺夫! 君子千里同舟，小人隔墙易宿。 文钱逼死英雄汉，财不归身恰是无。 妻子如衣服，弟兄似手足。 衣服补易新，手足断难续。 盗贼怨失主，不孝怨父母。 一时劝人以口，百世劝人以书。 我不如人我无其福，人不如我我常知足! 捡金不忘失金人，三两黄铜四两福。 因祸得福，求赌必输。 一言而让他人之祸，一忿而折平生之福。 天有不测风云，人有旦夕祸福。 不淫当斋，淡饱当肉。 缓步当车，无祸当福。 男无良友不知己之有过，女无明镜不知面之精粗。 事非亲做，不知难处。 十年易读举子，百年难淘江湖! 积钱不如积德，闲坐不如看书。 思量挑担苦，空手做是福。 时来易借银千两，运去难赊酒半壶。 天晴打过落雨铺，少时享过老来福。 与人方便自己方便，一家打墙两家好看。 当面留一线，过后好相见。 入门掠虎易，开口告人难。 手指要往内撇，家丑不可外传。 浪子出于祖无德，孝子出于前人贤。 货离乡贵，人离乡贱。 树挪死，人挪活。 在家千日好，出门处处难。 三员长者当官员，几个明人当知县? 明人自断，愚人官断。 人怕三见面，树怕一墨线。 村夫硬似铁，光棍软如棉。 不是撑船手，怎敢拿篙竿! 天下礼仪无穷，一人知识有限。 一人不得二人计，宋江难结万人缘。 家有三亩田，不离衙门前，乡间无强汉，衙门就饿饭。 人人依礼仪，天下不设官。 衙门钱，眼睛钱; 田禾钱，千万年。 诗书必读，不可做官。 为人莫当官，当官皆一般。 换了你我去，恐比他还贪。 官吏清廉如修行，书差方便如行善。 靠山吃山，种田吃田。 吃尽美味还是盐，穿尽绫罗还是棉。 一夫不耕，全家饿饭，一女不织，全家受寒。 金银到手非容易，用时方知来时难。 先讲断，后不乱，免得藕断丝不断。 听人劝，得一半。 不怕慢，只怕站。 逢快莫赶，逢贱莫懒。 谋事在人，成事在天! 长路人挑担，短路人赚钱。 宁卖现二，莫卖赊三。 赚钱往前算，折本往后算。 小小生意赚大钱，七十二行出状元。 自己无运至，却怨世界难。 胆大不如胆小，心宽甚如屋宽。 妻贤何愁家不富，子孙何须受祖田。 是儿不死，是财不散。 财来生我易，我去生财难。 十月滩头坐，一日下九滩。 结交一人难上难，得罪一人一时间。 借债经商，卖田还债; 赊钱起屋，卖屋还钱。 修起庙来鬼都老，拾得秤来姜卖完。 不嫖莫转，不赌莫看。 节食以去病，少食以延年。 豆腐多了是包水，梢公多了打烂船。 无口过是，无眼过难。 无身过易，无心过难。 不会凫水怨河湾，不会犁田怨枷担。 他马莫骑，他弓莫挽。 要知心腹事，但听口中言。 宁在人前全不会，莫在人前会不全。 事非亲见，切莫乱谈。 打人莫打脸，骂人莫骂短。 好言一句三冬暖，话不投机六月寒。 人上十口难盘，帐上万元难还。 放债如施，收债如讨。 告状讨钱，海底摸盐。 衙门深似海，弊病大如天。 银钱莫欺骗，牛马不好变。 好汉莫被人识破，看破不值半文钱。 狗咬对头人，雷打三世冤。 不卖香烧无剩钱，井水不打不满边。 事宽则园，太久则偏。 高人求低易，低人求高难。 有钱就是男子汉，无钱就是汉子难。 人上一百，手艺齐全。 难者不会，会者不难。 生就木头造就船，砍的没得车的圆。 心不得满，事不得全。 鸟飞不尽，话说不完。 人无喜色休开店，事不遂心莫怨天。 选婿莫选田园，选女莫选嫁奁。 红颜女子多薄命，福人出在丑人边。 人将礼义为先，树将花果为园。 临危许行善，过后心又变。 天意违可以人回，命早定可以心挽。 强盗口内出赦书，君子口中无戏言。 贵人语少，贫子话多。 快里须斟酌，耽误莫迟春。 读过古华佗，不如见症多。 东屋未补西屋破，前帐未还后又拖。 今年又说明年富，待到明年差不多。 志不同己，不必强合。 莫道坐中安乐少，须知世上苦情多。 本少利微强如坐，屋檐水也滴得多。 勤俭持家富，谦恭受益多。 细处不断粗处断，黄梅不落青梅落。 见钱起意便是贼，顺手牵羊乃为盗。 要做快活人，切莫寻烦恼。 要做长寿人，莫做短命事。 要做有后人，莫做无后事。 不经一事，不长一智。 宁可无钱使，不可无行止。 栽树要栽松柏，结交要结君子。 秀才不出门，能知天下事。 钱多不经用，儿多不耐死。 弟兄争财家不穷不止，妻妾争风夫不死不止。 男人有志，妇人有势。 夫人死百将临门，将军死一卒不至。 天旱误甲子，人穷误口齿。 百岁无多日，光阴能几时? 父母养其身，自己立其志。 待有余而济人，终无济人之日; 待有闲而读书，终无读书之时。 此书传后世，句句必精读，其中礼和义，奉劝告世人。 勤奋读，苦发奋，走遍天涯如游刃。]]></content>
      <categories>
        <category>Life</category>
      </categories>
      <tags>
        <tag>Life</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[曾广贤文：上集]]></title>
    <url>%2FLife%2FLife%2F%E6%9B%BE%E5%B9%BF%E8%B4%A4%E6%96%87%EF%BC%9A%E4%B8%8A%E9%9B%86%2F</url>
    <content type="text"><![CDATA[昔时贤文，诲汝谆谆。 集韵增广，多见多闻。 观今宜鉴古，无古不成今。 知己知彼，将心比心。 酒逢知己饮，诗向会人吟。 相识满天下，知心能几人? 相逢好似初相识，到老终无怨恨心。 近水知鱼性，近山识鸟音。 易涨易退山溪水，易反易覆小人心。 运去金成铁，时来铁似金。 读书须用意，一字值千金。 逢人且说三分话，未可全抛一片心。 有意栽花花不发，无心插柳柳成荫。 画虎画皮难画骨，知人知面不知心。 钱财如粪土，仁义值千金。 流水下滩非有意，白云出岫本无心。 当时若不登高望，谁信东流海洋深? 路遥知马力，日久见人心。 两人一般心，无钱堪买金; 一人一般心，有钱难买针。 相见易得好，久住难为人。 马行无力皆因瘦，人不风流只为贫。 饶人不是痴汉，痴汉不会饶人。 是亲不是亲，非亲却是亲。 美不美，乡中水;亲不亲，故乡人。 莺花犹怕春光老，岂可教人枉度春? 相逢不饮空归去，洞口桃花也笑人。 红粉佳人休使老，风流浪子莫教贫。 在家不会迎宾客，出门方知少主人。 黄芩无假，阿魏无真。 客来主不顾，自是无良宾。 良宾方不顾，应恐是痴人。 贫居闹市无人问，富在深山有远亲。 谁人背后无人说，哪个人前不说人? 有钱道真语，无钱语不真。 不信但看筵中酒，杯杯先劝有钱人。 闹里挣钱，静处安身。 来如风雨，去似微尘。 长江后浪推前浪，世上新人赶旧人。 近水楼台先得月，向阳花木早逢春。 古人不见今时月，今月曾经照古人。 先到为君，后到为臣。 莫道君行早，更有早行人。 莫信直中直，须防仁不仁。 山中有直树，世上无直人。 自恨枝无叶，莫怨太阳偏。 一切都是命，半点不由人。 一年之计在于春，一日之计在于寅。 一家之计在于和，一生之计在于勤。 责人之心责己，恕己之心恕人。 守口如瓶，防意如城。 宁可人负我，切莫我负人。 再三须慎意，第一莫欺心。 虎身犹可近，人毒不堪亲。 来说是非者，便是是非人。 远水难救近火，远亲不如近邻。 有酒有肉多兄弟，急难何曾见一人? 人情似纸张张薄，世事如棋局局新。 山中也有千年树，世上难逢百岁人。 力微休负重，言轻莫劝人。 无钱休入众，遭难莫寻亲。 平生不做皱眉事，世上应无切齿人。 士者国之宝，儒为席上珍。 若要断酒法，醒眼看醉人。 求人须求大丈夫，济人须济急时无。 渴时一滴如甘露，醉后添杯不如无。 久住令人贱，频来亲也疏。 酒中不语真君子，财上分明大丈夫。 出家如初，成佛有余。 积金千两，不如明解经书。 养子不教如养驴，养女不教如养猪。 有田不耕仓廪虚，有书不读子孙愚。 仓廪虚兮岁月乏，子孙愚兮礼仪疏。 听君一席话，胜读十年书。 人不通今古，马牛如襟裾。 茫茫四海人无数，哪个男儿是丈夫? 白酒酿成缘好客，黄金散尽为收书。 救人一命，胜造七级浮屠。 城门失火，殃及池鱼。 庭前生瑞草，好事不如无。 欲求生富贵，须下死工夫。 百年成之不足，一旦坏之有余。 人心似铁，官法如炉。 善化不足，恶化有余。 水至清则无鱼，人太急则无智。 知者减半，愚者全无。 在家由父，出嫁从夫。 痴人畏妇，贤女敬夫。 是非终日有，不听自然无。 竹篱茅舍风光好，道院僧房终不如。 宁可正而不足，不可邪而有余。 宁可信其有，不可信其无。 命里有时终须有，命里无时莫强求。 道院迎仙客，书堂隐相儒。 庭栽栖凤竹，池养化龙鱼。 结交须胜己，似我不如无。 但看三五日，相见不如初。 人情似水分高下，世事如云任卷舒。 会说说都是，不会说无理。 磨刀恨不利，刀利伤人指; 求财恨不多，财多害自己。 知足常足，终身不辱; 知止常止，终身不耻。 有福伤财，无福伤己。 差之毫厘，失之千里。 若登高必自卑，若涉远必自迩。 三思而行，再思可矣。 动口不如亲为，求人不如求己。 小时是兄弟，长大各乡里。 嫉财莫嫉食，怨生莫怨死。 人见白头嗔，我见白头喜。 多少少年郎，不到白头死。 墙有缝，壁有耳。 好事不出门，坏事传千里。 若要人不知，除非己莫为。 为人不做亏心事，半夜敲门心不惊。 贼是小人，智过君子。 君子固穷，小人穷斯溢矣。 富贵多忧，贫穷自在。 不以我为德，反以我为仇。 宁可直中取，不可曲中求。 人无远虑，必有近忧。 知我者谓我心忧，不知我者谓我何求? 晴天不肯去，直待雨淋头。 成事莫说，覆水难收。 是非只为多开口，烦恼皆因强出头。 忍得一时之气，免得百日之忧。 近来学得乌龟法，得缩头时且缩头。 惧法朝朝乐，欺公日日忧。 人生一世，草长一春。 黑发不知勤学早，转眼便是白头翁。 月过十五光明少，人到中年万事休。 儿孙自有儿孙福，莫为儿孙做马牛。 人生不满百，常怀千岁忧。 今朝有酒今朝醉，明日愁来明日忧。 路逢险处须回避，事到临头不自由。 人贫不语，水平不流。 一家养女百家求，一马不行百马忧。 有花方酌酒，无月不登楼。 三杯通大道，一醉解千愁。 深山毕竟藏猛虎，大海终须纳细流。 惜花须检点，爱月不梳头。 大抵选她肌骨好，不搽红粉也风流。 受恩深处宜先退，得意浓时便可休。 莫待是非来入耳，从前恩爱反为仇。 留得五湖明月在，不愁无处下金钩。 休别有鱼处，莫恋浅滩头。 去时终须去，再三留不住。 忍一句，息一怒，饶一着，退一步。 三十不豪，四十不富，五十将来寻死路。 生不认魂，死不认尸。 一寸光阴一寸金，寸金难买寸光阴。 父母恩深终有别，夫妻义重也分离。 人生似鸟同林宿，大难来时各自飞。 人善被人欺，马善被人骑。 人无横财不富，马无夜草不肥。 人恶人怕天不怕，人善人欺天不欺。 善恶到头终有报，只盼来早与来迟。 黄河尚有澄清日，岂能人无得运时? 得宠思辱，居安思危。 念念有如临敌日，心心常似过桥时。 英雄行险道，富贵似花枝。 人情莫道春光好，只怕秋来有冷时。 送君千里，终有一别。 但将冷眼观螃蟹，看你横行到几时。 见事莫说，问事不知。 闲事莫管，无事早归。 假缎染就真红色，也被旁人说是非。 善事可做，恶事莫为。 许人一物，千金不移。 龙生龙子，虎生虎儿。 龙游浅水遭虾戏，虎落平原被犬欺。 一举首登龙虎榜，十年身到凤凰池。 十年寒窗无人问，一举成名天下知。 酒债寻常处处有，人生七十古来稀! 养儿防老，积谷防饥。 鸡豚狗彘之畜，无失其时，数口之家，可以无饥矣。 当家才知盐米贵，养子方知父母恩。 常将有日思无日，莫把无时当有时。 树欲静而风不止，子欲养而亲不待。 时来风送滕王阁，运去雷轰荐福碑。 入门休问荣枯事，且看容颜便得知。 官清司吏瘦，神灵庙祝肥。 息却雷霆之怒，罢却虎豹之威。 饶人算之本，输人算之机。 好言难得，恶语易施。 一言既出，驷马难追。 道吾好者是吾贼，道吾恶者是吾师。 路逢侠客须呈剑，不是才人莫献诗。 三人行必有我师焉。 择其善者而从之，其不善者而改之。 欲昌和顺须为善，要振家声在读书。 少壮不努力，老大徒伤悲。 人有善愿，天必佑之。 莫饮卯时酒，昏昏醉到酉。 莫骂酉时妻，一夜受孤凄。 种麻得麻，种豆得豆。 天眼恢恢，疏而不漏。 做官莫向前，作客莫在后。 宁添一斗，莫添一口。 螳螂捕蝉，岂知黄雀在后? 不求金玉重重贵，但愿儿孙个个贤。 一日夫妻，百世姻缘。 百世修来同船渡，千世修来共枕眠。 杀人一万，自损三千。 伤人一语，利如刀割。 枯木逢春犹再发，人无两度再少年。 未晚先投宿，鸡鸣早看天。 将相顶头堪走马，公候肚内好撑船。 富人思来年，穷人想眼前。 世上若要人情好，赊去物品莫取钱。 生死有命，富贵在天。 击石原有火，不击乃无烟。 人学始知道，不学亦徒然。 莫笑他人老，终须还到老。 和得邻里好，犹如拾片宝。 但能守本分，终身无烦恼。 大家做事寻常，小家做事慌张。 大家礼义教子弟，小家凶恶训儿郎。 君子爱财，取之有道。 贞妇爱色，纳之以礼。 善有善报，恶有恶报。 不是不报，时候未到。 万恶淫为首，百行孝当先。 人而无信，不知其可也。 一人道虚，千人传实。 凡事要好，须问三老。 若争小利，便失大道。 家中不和邻里欺，邻里不和说是非。 年年防饥，夜夜防盗。 学者是好，不学不好。 学者如禾如稻，不学如草如蒿。 遇饮酒时须防醉，得高歌处且高歌。 因风吹火，用力不多。 不因渔夫引，怎能见波涛? 无求到处人情好，不饮任他酒价高。 知事少时烦恼少，识人多处是非多。 进山不怕伤人虎，只怕人情两面刀。 强中更有强中手，恶人须用恶人磨。 会使不在家富豪，风流不用衣着佳。 光阴似箭，日月如梭。 天时不如地利，地利不如人和。 黄金未为贵，安乐值钱多。 为善最乐，作恶难逃。 羊有跪乳之恩，鸦有反哺之情。 孝顺还生孝顺子，忤逆还生忤逆儿。 不信但看檐前水，点点滴滴旧窝池。 隐恶扬善，执其两端。 妻贤夫祸少，子孝父心宽。 已覆之水，收之实难。 人生知足时常足，人老偷闲且是闲。 处处绿杨堪系马，家家有路通长安。 既坠釜甑，反顾何益。 见者易，学者难。 厌静还思喧，嫌喧又忆山。 自从心定后，无处不安然。 莫将容易得，便作等闲看。 用心计较般般错，退后思量事事宽。 道路各别，养家一般。 由俭入奢易，从奢入俭难。 知音说与知音听，不是知音莫与谈。 点石化为金，人心犹未足。 信了赌，卖了屋。 他人观花，不涉你目。 他人碌碌，不涉你足。 谁人不爱子孙贤，谁人不爱千钟粟。 奈五行，不是这般题目。 莫把真心空计较，儿孙自有儿孙福。 书到用时方恨少，事非经过不知难。 天下无不是的父母，世上最难得者兄弟。 与人不和，劝人养鹅;与人不睦，劝人架屋。 但行好事，莫问前程。不交僧道，便是好人。 河狭水激，人急计生。 明知山有虎，莫向虎山行。 路不铲不平，事不为不成。 无钱方断酒，临老始读经。 点塔七层，不如暗处一灯。 堂上二老是活佛，何用灵山朝世尊。 万事劝人休瞒昧，举头三尺有神明。 但存方寸土，留与子孙耕。 灭却心头火，剔起佛前灯。 惺惺多不足，蒙蒙作公卿。 众星朗朗，不如孤月独明。 兄弟相害，不如友生。 合理可作，小利不争。 牡丹花好空入目，枣花虽小结实多。 欺老莫欺小，欺人心不明。 勤奋耕锄收地利，他时饱暖谢苍天。 得忍且忍，得耐且耐，不忍不耐，小事成灾。 相论逞英豪，家计渐渐退。 贤妇令夫贵，恶妇令夫败。 一人有庆，兆民咸赖。 人老心未老，人穷志莫穷。 人无千日好，花无百日红。 黄蜂一口针，橘子两边分。 世间痛恨事，最毒淫妇心。 杀人可恕，情理不容。 乍富不知新受用，乍贫难改旧家风。 座上客常满，杯中酒不空。 屋漏更遭连夜雨，行船又遇打头风。 笋因落箨方成竹，鱼为奔波始化龙。 记得少年骑竹马，转眼又是白头翁。 礼义生于富足，盗贼出于赌博。 天上众星皆拱北，世间无水不朝东。 士为知己者死，女为悦己者容。 色即是空，空即是色。 君子安贫，达人知命。 良药苦口利于病，忠言逆耳利于行。 顺天者昌，逆天者亡。 有缘千里来相会，无缘对面不相逢。 有福者昌，无福者亡。 人为财死，鸟为食亡。 夫妻相和好，琴瑟与笙簧。 红粉易妆娇态女，无钱难作好儿郎。 有子之人贫不久，无儿无女富不长。 善必寿老，恶必早亡。 爽口食多偏作病，快心事过恐遭殃。 富贵定要依本分，贫穷不必再思量。 画水无风空作浪，绣花虽好不闻香。 贪他一斗米，失却半年粮。 争他一脚豚，反失一肘羊。 龙归晚洞云犹湿，麝过春山草木香。 平生只会说人短，何不回头把己量? 见善如不及，见恶如探汤。 人穷志短，马瘦毛长。 自家心里急，他人未知忙。 贫无达士将金赠，病有高人说药方。 触来莫与竞，事过心清凉。 秋来满山多秀色，春来无处不花香。 凡人不可貌相，海水不可斗量。 清清之水为土所防，济济之士为酒所伤。 蒿草之下或有兰香，茅茨之屋或有侯王。 无限朱门生饿殍，几多白屋出公卿。 酒里乾坤大，壶中日月长。 拂石坐来春衫冷，踏花归去马蹄香。 万事前身定，浮生空自忙。 叫月子规喉舌冷，宿花蝴蝶梦魂香。 一言不中，千言不用。 一人传虚，百人传实。 万金良药，不如无疾。 千里送鹅毛，礼轻情义重。 世事如明镜，前程暗似漆。 君子怀刑，小人怀惠。 架上碗儿轮流转，媳妇自有做婆时。 人生一世，如驹过隙。 良田万顷，日食一升。 大厦千间，夜眠八尺。 千经万典，孝义为先。 天上人间，方便第一。 一字入公门，九牛拔不出。 八字衙门向南开，有理无钱莫进来。 欲求天下事，须用世间财。 富从升合起，贫因不算来。 近河不得枉使水，近山不得枉烧柴。 家无读书子，官从何处来? 慈不掌兵，义不掌财。 一夫当关，万夫莫开。 万事不由人计较，一生都是命安排。 白云本是无心物，却被清风引出来。 慢行急行，逆取顺取。 命中只有如许财，丝毫不可有闪失。 人间私语，天闻若雷。 暗室亏心，神目如电。 一毫之恶，劝人莫作。一毫之善，与人方便。 亏人是祸，饶人是福，天眼恢恢，报应甚速。 圣贤言语，神钦鬼服。 人各有心，心各有见。 口说不如身逢，耳闻不如目见。 见人富贵生欢喜，莫把心头似火烧。 养兵千日，用在一时。 国清才子贵，家富小儿娇。 利刀割体疮犹使，恶语伤人恨不消。 公道世间唯白发，贵人头上不曾饶。 有才堪出众，无衣懒出门。 为官须作相，及第必争先。 苗从地发，树由枝分。 宅里燃火，烟气成云。 以直报怨，知恩报恩。 红颜今日虽欺我，白发他时不放君。 借问酒家何处有，牧童遥指杏花村。 父子和而家不退，兄弟和而家不分。 一片云间不相识，三千里外却逢君。 官有公法，民有私约。 平时不烧香，临时抱佛脚。 幸生太平无事日，恐防年老不多时。 国乱思良将，家贫思良妻。 池塘积水须防旱，田地深耕足养家。 根深不怕风摇动，树正何愁月影斜。 争得猫儿，失却牛脚。 愚者千虑，必有一得，智者千虑，必有一失。 始吾于人也，听其言而信其行。 今吾于人也，听其言而观其行。 哪个梳头无乱发，情人眼里出西施。 珠沉渊而川媚，玉韫石而山辉。 夕阳无限好，只恐不多时。 久旱逢甘霖，他乡遇故知;洞房花烛夜，金榜题名时。 惜花春起早，爱月夜眠迟。 掬水月在手，弄花香满衣。 桃红李白蔷薇紫，问着东君总不知。 教子教孙须教义，栽桑栽柘少栽花。 休念故乡生处好，受恩深处便为家。 学在一人之下，用在万人之上。 一日为师，终生为父。 忘恩负义，禽兽之徒。 劝君莫将油炒菜，留与儿孙夜读书。 书中自有千钟粟，书中自有颜如玉。 莫怨天来莫怨人，五行八字命生成。 莫怨自己穷，穷要穷得干净;莫羡他人富，富要富得清高。 别人骑马我骑驴，仔细思量我不如， 待我回头看，还有挑脚汉。 路上有饥人，家中有剩饭。 积德与儿孙，要广行方便。 作善鬼神钦，作恶遭天遣。 积钱积谷不如积德，买田买地不如买书。 一日春工十日粮，十日春工半年粮。 疏懒人没吃，勤俭粮满仓。 人亲财不亲，财利要分清。 十分伶俐使七分，常留三分与儿孙， 若要十分都使尽，远在儿孙近在身。 君子乐得做君子，小人枉自做小人。 好学者则庶民之子为公卿，不好学者则公卿之子为庶民。 惜钱莫教子，护短莫从师。 记得旧文章，便是新举子。 人在家中坐，祸从天上落。 但求心无愧，不怕有后灾。 只有和气去迎人，哪有相打得太平。 忠厚自有忠厚报，豪强一定受官刑。 人到公门正好修，留些阴德在后头。 为人何必争高下，一旦无命万事休。 山高不算高，人心比天高。 白水变酒卖，还嫌猪无糟。 贫寒休要怨，宝贵不须骄。 善恶随人作，祸福自己招。 奉劝君子，各宜守己。 只此呈示，万无一失。]]></content>
      <categories>
        <category>Life</category>
      </categories>
      <tags>
        <tag>Life</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 操作 MySQL：MySQLdb]]></title>
    <url>%2FPython%2FPython%2FPython%E6%93%8D%E4%BD%9CMySQL%EF%BC%9AMySQLdb%2F</url>
    <content type="text"><![CDATA[快速指引MySQLdb 是用于Python链接Mysql数据库的接口，它实现了 Python 数据库 API 规范 V2.0，基于 MySQL C API 上建立的。 MySQLdb安装在命令行中执行： 12$ pip install mysql-python MySQLdb使用MySQLdb Tutorials MySQLdb 导入方式1234#!/usr/bin/python# -*- coding: UTF-8 -*-import MySQLdbimport time MySQLdb 一般流程Python通过MySQLdb操作MySQL数据库的一般过程： 配置数据库参数-&gt;连接数据库-&gt;获取游标对象-&gt;构造SQL语句-&gt;执行SQL语句-&gt;提交事务(write)-&gt;关闭连接 123456789101112131415161718192021222324252627282930313233343536# 1. 配置数据库参数：参数字典conf = &#123; &#x27;host&#x27;: &#x27;127.0.0.1&#x27;, &#x27;port&#x27;: 3306, &#x27;user&#x27;: &#x27;root&#x27;, &#x27;db&#x27;: &#x27;didi&#x27;, &#x27;charset&#x27;: &#x27;utf8&#x27;&#125;# 2. 连接数据库：返回连接对象connconn = MySQLdb.connect(**conf)print conn# 3. 获取游标对象：cursorcursor = conn.cursor(MySQLdb.cursors.DictCursor)print cursor# 4. 构造SQL语句和参数序列:sql_table_str = &#x27;g_team_combine_activity_city_map&#x27;sql_fields = [&#x27;id&#x27;, &#x27;activity_id&#x27;, &#x27;city_id&#x27;, &#x27;create_time&#x27;]sql_fields_str = &#x27;(&#123;&#125;)&#x27;.format(&#x27;,&#x27;.join(sql_fields))sql_values_str = &#x27;(&#123;&#125;)&#x27;.format(&#x27;,&#x27;.join([&#x27;%s&#x27;] * len(sql_fields)))sql_stmt = &quot;&quot;&quot;INSERT INTO &#123;&#125; &#123;&#125; VALUES &#123;&#125;&quot;&quot;&quot;.format(sql_table_str, sql_fields_str, sql_values_str)sql_data = [2000,0,0,0]print sql_stmt, sql_data# 5. 执行SQL语句：返回受影响的条数，如果插入失败则回滚事务try: print cursor.execute(sql_stmt, sql_data) conn.commit()except Exception as e: conn.rollback() print e# 6. 关闭连接conn.close() &lt;_mysql.connection open to &#39;127.0.0.1&#39; at 7f87bf2df620&gt; &lt;MySQLdb.cursors.DictCursor object at 0x10c13af90&gt; INSERT INTO g_team_combine_activity_city_map (id,activity_id,city_id,create_time) VALUES (%s,%s,%s,%s) [2000, 0, 0, 0] (1062, &quot;Duplicate entry &#39;2000&#39; for key &#39;PRIMARY&#39;&quot;) 连接DB1234567891011# 端口是int，其他都是字符串# MySQLdb.connect(host=主机, port=端口, user=用户名, passwd=密码, db=数据库, charset=&#x27;utf8&#x27;)conf = &#123; &#x27;host&#x27;: &#x27;127.0.0.1&#x27;, &#x27;port&#x27;: 3306, &#x27;user&#x27;: &#x27;root&#x27;, &#x27;db&#x27;: &#x27;didi&#x27;, &#x27;charset&#x27;: &#x27;utf8&#x27;&#125;conn = MySQLdb.connect(**conf) 获取游标12# 传入游标类型：决定了返回记录的类型，字典最常用cursor = conn.cursor(MySQLdb.cursors.DictCursor) 执行SQL原型：cursor.execute(query, args=None) query查询语句和args数据参数需配合使用，总体上有三种使用方式： 完整查询语句+数据参数None：query是一句完整可执行的SQL语句字符串 + args为None； 占位符查询语句+数据参数序列：query中含有占位符%s，args 为一个序列，执行时会先使用args中的值依次替换query中的占位符； 关键字占位符查询语句+数据参数字典：query中含有关键字占位符%(key)s ，args 为一个字典，执行时会先使用args按关键字替换query中的占位符； 后两种方式是将参数作为最终要插入的值，然后代入到查询语句的，而不是先代入再用SQL语句计算其值。 查询执行查询语句123456# 方式一sql1 = &#x27;select * from &#123;0&#125; where &#123;1&#125; &lt; &#123;2&#125;&#x27;.format( &#x27;g_team_combine_activity_city_map&#x27;, &#x27;id&#x27;, 80)# 返回受影响的记录行数cursor.execute(sql1) 6L 获取查询结果每次获取查询结果，游标会自动后移。 123# 逐条获取记录：返回一个字典，每次获取，游标下移，无数据返回Nonedata1 = cursor.fetchone()print data1 &#123;&#39;city_id&#39;: 1L, &#39;activity_id&#39;: 1L, &#39;create_time&#39;: 1556865052L, &#39;id&#39;: 1L&#125; 123# 获取指定数目的记录：返回一个字典元组，每次读取，游标都会移动data2 = cursor.fetchmany(2)print data2 (&#123;&#39;city_id&#39;: 3L, &#39;activity_id&#39;: 2L, &#39;create_time&#39;: 1556868188L, &#39;id&#39;: 2L&#125;, &#123;&#39;city_id&#39;: 1L, &#39;activity_id&#39;: 45L, &#39;create_time&#39;: 1505969474L, &#39;id&#39;: 5L&#125;) 123# 返回全部结果，字典元组data3 = cursor.fetchall()data3 (&#123;&#39;activity_id&#39;: 71L, &#39;city_id&#39;: 35L, &#39;create_time&#39;: 1506402761L, &#39;id&#39;: 79L&#125;,) 插入单条插入‘INSERT INTO table_name (字段1，字段2,…,字段n)’ VALUES (值1，值2，…，值n) 1234567891011121314151617181920212223# 方式一：构造出完整SQL语句，注意这里的unix_timestamp(now())会被SQL执行sql4 = &quot;&quot;&quot;INSERT INTO g_team_combine_activity_city_map( id, activity_id, city_id, create_time) VALUES(11, 1, 1, unix_timestamp(now()))&quot;&quot;&quot;# 方式二：占位符查询语句+数据参数序列，注意这里传递给%s的是最终的结果，因为如果传入SQL函数则不会在SQL中执行sql_stmt1 = &quot;&quot;&quot;INSERT INTO g_team_combine_activity_city_map( id,activity_id,city_id,create_time) VALUES(%s, %s, %s, %s)&quot;&quot;&quot;sql_data1 = [2, 2, 3, time.time()]# 方式三：关键字占位符查询语句+数据参数字典sql_stmt2 = &quot;&quot;&quot;INSERT INTO g_team_combine_activity_city_map( id,activity_id,city_id,create_time) VALUES(%(id)s, %(activity_id)s, %(city_id)s, %(create_time)s)&quot;&quot;&quot;sql_data2 = &#123; &#x27;id&#x27;: 3, &#x27;activity_id&#x27;: 5, &#x27;city_id&#x27;: 7, &#x27;create_time&#x27;: time.time()&#125; 1234567# 执行SQL语句，提交事务，如果提交失败则回滚try: cursor.execute(sql_stmt2, sql_data2) conn.commit()except Exception as e: conn.rollback() print e 批量插入123456789101112131415161718# 普通方式sql_stmt4 = &quot;&quot;&quot;INSERT INTO g_team_combine_activity_city_map (id,activity_id,city_id,create_time) VALUES &#123;&#125;&quot;&quot;&quot;.format(&#x27;,&#x27;.join([&#x27;(%s)&#x27; % &#x27;,&#x27;.join([&#x27;%s&#x27;] * 4)] * 3))sql_data4 = [(1210,0,0,0), (1211,0,0,0), (1212,0,0,0)]sql_data4 = [tt for t in sql_data4 for tt in t]try: cursor.execute(sql_stmt4, sql_data4) conn.commit()except Exception as e: conn.rollback() print e 12345678910111213141516# executemany，用于多行插入，效率比逐行插入更高sql_stmt3 = &quot;&quot;&quot;INSERT INTO g_team_combine_activity_city_map (id,activity_id,city_id,create_time) VALUES (%s, %s, %s, %s)&quot;&quot;&quot;sql_data3 = [(1205,0,0,0), (1206,0,0,0), (1207,0,0,0)]try: cursor.executemany(sql_stmt3, sql_data3) conn.commit()except Exception as e: conn.rollback() print e 更新操作12345678910111213sql_table_str = &#x27;g_team_combine_activity_city_map&#x27;sql_key_dict = &#123;&#x27;city_id&#x27;:999, &#x27;create_time&#x27;: time.time()&#125;sql_eval_str = &#x27;,&#x27;.join(&#x27;&#123;&#125; = &#123;&#125;&#x27;.format(item[0],item[1]) for item in key_dict.items())sql_cond_str = &#x27;&#123;&#125;&gt;&#123;&#125;&#x27;.format(&#x27;id&#x27;, 1100)sql_stmt4 = &quot;&quot;&quot;UPDATE &#123;0&#125; SET &#123;1&#125; WHERE &#123;2&#125;&quot;&quot;&quot;.format(table_str, eval_str, cond_str)try: cursor.execute(sql_stmt4) conn.commit()except Exception as e: conn.rollback() print e 删除操作12345678910sql_table_str = &#x27;g_team_combine_activity_city_map&#x27;sql_cond_str = &#x27;id &gt; 1200&#x27; sql_stmt5 = &quot;&quot;&quot;DELETE FROM &#123;&#125; WHERE &#123;&#125;&quot;&quot;&quot;.format(sql_table_str, sql_cond_str)try: cursor.execute(sql_stmt5) conn.commit()except Exception as e: conn.rollback() print e 创建数据库表1234567891011121314151617181920212223242526sql_stmt6 = &quot;&quot;&quot;CREATE TABLE `t_data_status` ( `f_id` int(10) NOT NULL AUTO_INCREMENT COMMENT &#x27;自增ID&#x27;, `f_app_id` varchar(32) NOT NULL COMMENT &#x27;合作方id&#x27;, `f_busi_id` varchar(32) NOT NULL DEFAULT 0 COMMENT &#x27;内部业务id&#x27;, `f_src_data_set` varchar(60) NOT NULL DEFAULT &#x27;&#x27; COMMENT &#x27;数据源,即分表名称, 包括基础表和业务基础数据表&#x27;, `f_des_data_set` varchar(60) NOT NULL DEFAULT &#x27;&#x27; COMMENT &#x27;数据目标,即各个副本表名称, 包括基础表和业务基础数据表&#x27;, `f_dataset_type_id` int(2) NOT NULL COMMENT &#x27;数据源类型dic_dataset_type f_id，字典:医院基础数据，科室基础数据, 医院挂号业务数据等, 通过dic_dataset_type的type知道这条数据是不是基础数据&#x27;, `f_type` int(1) NOT NULL COMMENT &#x27;数据类型，0基础数据, 1业务数据, 冗余字段，便于检索&#x27;, `f_data_num` int(11) NOT NULL DEFAULT &#x27;0&#x27; COMMENT &#x27;数据总量&#x27;, `f_relation_id` varchar(32) NOT NULL DEFAULT &#x27;&#x27; COMMENT &#x27;业务生成的unique id，标识基础表和星形表的关联, 例如医院基础数据和医院挂号业务数据两个数据源就需要共用同一个唯一id&#x27;, `f_status` tinyint NOT NULL COMMENT &#x27;状态 1 成功 0 失败&#x27;, `f_create_time` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT &#x27;数据方创建时间&#x27;, `f_version` int(10) NOT NULL DEFAULT 1 COMMENT &#x27;数据在当天的版本&#x27;, `f_deal_status` tinyint NOT NULL DEFAULT &#x27;-1&#x27; COMMENT &#x27;状态 -1 未处理 1 成功 0 失败 状态位2表示处理中..&#x27;, `f_deal_time` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT &#x27;处理数据时的时间&#x27;, PRIMARY KEY (`f_id`)) ENGINE=InnoDB AUTO_INCREMENT=0 DEFAULT CHARSET=utf8;&quot;&quot;&quot;try: cursor.execute(sql_stmt6) conn.commit()except Exception as e: conn.rollback() print e (1050, &quot;Table &#39;t_data_status&#39; already exists&quot;)]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>教程</tag>
        <tag>Python</tag>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python IO：OJ 输入输出]]></title>
    <url>%2FPython%2FPython%2FPython%20IO%3A%20OJ%20%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA%2F</url>
    <content type="text"><![CDATA[一些公司的在线笔试会承包给第三方，第三方平台通常会采用 OJ(Online Judge) 判题系统，OJ 并不会像 LeetCode 那样给定函数声明，而是需要我们自己去处理输入和输出。 OJ 平台的输入数据有各种各样的格式，再加上 python2.x 和 python3.x 在输入输出方法上又有很大差异，这使得在 OJ 平台上使用Python处理输入输出显得有点复杂，因此有必要做一下整理。 python2.7输入python2.7会默认将所有输入读取为字符串。 sys.stdin标准输入sys.stdin会读入输入的所有字符，包括每行结尾的换行符。 方式一：读入所有字符，返回字符串，如果没有任何输入则返回’’。 1234567In [5]: content = sys.stdin.read()abcd10 11 12 13ab bc cd de^DIn [7]: contentOut[7]: &#x27;abcd\n10 11 12 13\nab bc cd de\n&#x27; 方式二：按行读入所有字符，返回由行字符串组成的迭代器，如果没有任何输入则返回[]。 123456In [8]: lines = sys.stdin.readlines()abcd10 11 12 13ab bc cd de^DIn [9]: linesOut[9]: [&#x27;abcd\n&#x27;, &#x27;10 11 12 13\n&#x27;, &#x27;ab bc cd de&#x27;] 以上两种读入方式在OJ中基本用不到，更常用到的是第三种方式。 方式三：读入一行，返回该行字符串，如果没有任何输入则返回’’ 12345In [16]: line = sys.stdin.readline()abcdIn [17]: lineOut[17]: &#x27;abcd\n&#x27; 通过标准输入读取数据的通用实例： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263# 1. 读入一个字符串In [21]: line = sys.stdin.readline().strip()abcIn [22]: lineOut[22]: &#x27;abc&#x27;# 2. 读入一个浮点数In [23]: line = float(sys.stdin.readline().strip())3.14In [24]: lineOut[24]: 3.14# 3. 读入一个字符串列表In [26]: line = sys.stdin.readline().strip().split()ab bc cdeIn [27]: lineOut[27]: [&#x27;ab&#x27;, &#x27;bc&#x27;, &#x27;cde&#x27;]# 4. 读入一个整数列表In [28]: line = map(int,sys.stdin.readline().strip().split())10 11 12In [29]: lineOut[29]: [10, 11, 12]# 5. 读入k行整数列表In [31]: lines = []In [32]: for i in xrange(3): ...: line = map(int,sys.stdin.readline().strip().split()) ...: lines.append(line) ...:1 2 34 5 67 8 9In [33]: linesOut[33]: [[1, 2, 3], [4, 5, 6], [7, 8, 9]]# 6. 读入若干行整数列表In [36]: lines = []In [37]: count = 0In [38]: while True: ...: line = sys.stdin.readline().strip() ...: if not line: ...: break ...: line = map(int, line.split()) ...: lines.append(line) ...: count += 1 ...:1 2 34 5 67 8 910 11 12In [39]: linesOut[39]: [[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]] raw_input()和input()可以看到，通过标准输入读取每行数据需要手动去除行尾的换行符，而且代码量较大。python2.7提供了两个专门用于接收整行输入的函数raw_input()和input()，代码简介且会自动去除行尾的换行符， raw_input()：将用户整行输入作为字符串返回，如果用户输入为空则返回’’； input()：将用户整行输入作为可执行的表达式，返回表达式结果，等价于eval(raw_input())，如果用户输入为空，则出错； 建议：强烈建议统一使用raw_input()读取输入；但如果输入为单个数值时使用input()会比较方便。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970# 1. 读入一个字符串In [45]: line = raw_input()abcIn [46]: lineOut[46]: &#x27;abc&#x27;# 2. 读入一个数值In [1]: line = int(raw_input())3In [2]: lineOut[2]: 3In [47]: line = input()3.14In [48]: lineOut[48]: 3.14In [49]: type(line)Out[49]: float# 3. 读入一个字符串列表In [50]: line = raw_input().split()ab bc cdeIn [51]: lineOut[51]: [&#x27;ab&#x27;, &#x27;bc&#x27;, &#x27;cde&#x27;]# 4. 读入一个整数列表In [52]: line = map(int, raw_input().split())10 11 12In [53]: lineOut[53]: [10, 11, 12]# 5. 读入k行整数列表In [55]: for i in xrange(3): ...: line = map(int,raw_input().split()) ...: lines.append(line) ...:1 2 34 5 67 8 9In [56]: linesOut[56]: [[1, 2, 3], [4, 5, 6], [7, 8, 9]]# 6. 读入若干行整数列表In [57]: lines = []In [58]: count = 0In [59]: while True: ...: line = raw_input() ...: if not line: ...: break ...: line = map(int,line.split()) ...: lines.append(line) ...: count += 1 ...:1 2 34 5 6 78 9 1011 12 13In [60]: linesOut[60]: [[1, 2, 3], [4, 5, 6, 7], [8, 9, 10], [11, 12, 13]] 输出print 语句python2.7的print为表达式语句，括号可带可不带，将对象的文本形式打印到sys.stdout，并在各项之间添加一个空格，并在行末添加一个换行符。如果不希望在行末添加换行符，则可以在最后一个元素后面加上逗号(注意此时并没有自动添加空格)，如果再次使用print进行输出时，默认会自动在新的输出内容和旧的输出之间添加空格。 1234567print 33,34print 3533 3435print 33,34,print 3533 34 35 相比python2，python3的print函数可定制化更强，如果要在python2中使用python3中的print语句，可以通过以下方式实现： 1234567891011In [68]: from __future__ import print_functionIn [69]: print 2 File &quot;&lt;ipython-input-69-9d8034018fb9&gt;&quot;, line 1 print 2 ^SyntaxError: invalid syntaxIn [70]: print(1,2,3,sep=&#x27;&#x27;,end=&#x27;&#x27;)123 格式化字符串python支持两种形式的格式化字符串：%语句和format函数。 format函数更加强大、灵活，一般格式为： &#39;&#123;位置/关键字:填充-对齐-宽度-类型&#125;ohers&#123;...&#125;&#39;.format(x,y,...) 格式串：每个格式串用花括号括起来 format函数：参数个数必须与格式串个数相同 以函数传参的方式将format接受到的参数按照位置或关键字对应到每个格式串，然后每个参数按照格式串的格式替换到字符串中 格式串的格式：位置/关键字:填充-对齐-宽度-类型 123456789# 填充-对齐-宽度-类型，注意会对浮点数进行四舍五入x = &#x27;s1:&#123;key:+^10s&#125; s2:&#123;2: &lt;10s&#125; int:&#123;0:,d&#125; float:&#123;1:0&gt;10.3f&#125;&#x27;.format(100000,1.3456,&#x27;word2&#x27;,key=&#x27;word1&#x27;)print xs1:++word1+++ s2:word2 int:100,000 float:000001.346# 进制转换y = &#x27;&#123;0:b&#125; &#123;0:o&#125; &#123;0:x&#125; &#123;0:X&#125;&#x27;.format(10)print y1010 12 a A python3.5python3相对python2有很多不同，出于OJ实践考虑，暂时只需要了解以下内容即可。 输入python3.中的标准输入与python2.x并无太大区别。 python3.x中没有raw_input()函数，但是python3.x中的input()函数等价于python2.x中的raw_input()函数。 123&gt;&gt;&gt; input()2 3 4&#x27;2 3 4&#x27; 输出python3.x中print()是一个打印函数，必须带括号，其原型为： 1print(value, ..., sep=&#x27; &#x27;, end=&#x27;\n&#x27; 多个对象间默认以空格分隔 行末默认使用换行符 123print(23,34,sep=&#x27;,&#x27;,end=&#x27;,&#x27;)print(35)23,34,35 OJ平台常见的输入输出模式OJ平台的输入数据通常会有多组，并且格式多种多样。注意审题，千万不要在输入输出格式上出错，否则很难通过反馈结果找到这种错误。 只有一组输入输入： 13 2 输出： 15 代码：只需要处理一组测试用例即可 12a,b = map(int, raw_input().split())print a + b 预先知道输入数据的组数输入：已知有k组测试数据以及每组数据的输入格式 1232 322 33... 输出： 123555... 代码：在3.1的基础上循环k次即可 123for _ in xrange(k): a,b = map(int, raw_input().split()) print a + b 预先不知道数据的组数输入：可能有多组测试数据 1232 322 33... 输出：对于每组数据，将结果单独作为一行输出 123555... 代码：一直读到文件末尾，raw_input会引发EOFError错误，而sys.stdin.readline()却不会引发EOFError错误，而是返回空串’’。 12345678910111213141516171819202122# 方式一：逐行读入，读到末尾结束while True: # 注意此处不能strip，否则就变成了遇到空行结束 line = sys.stdin.readline() if not line: break a,b = map(int,line.strip()) print a+b # 方式二：整体读入for line in sys.stdin.readlines(): a,b = map(int,line.strip()) print a+b# 方式三：通过raw_input()报错识别文件末尾try: while True: line = raw_input() a,b = map(int,line.strip()) print a + bexcept EOFError: pass 总结: 选择python2.x中的raw_input()逐行读入，不知道数据组数时，通过EOFError结束。` python2和python3的选择：为了避免混淆，建议只使用一种版本来刷OJ 逐行读入和整体读入：逐行读入有更好的灵活性，使用逐行读入的方式就可以满足python在OJ上的所有输入要求 raw_input和readline：readline需要手动处理行末换行符，raw_input读到文件末尾会报错 raw_input和input：在python2中只有输入是单个数值时，才能用input，简单一些]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>教程</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 操作 Excel：openpyxl]]></title>
    <url>%2FPython%2FPython%2FPython%E6%93%8D%E4%BD%9CExcel%EF%BC%9Aopenpyxl%2F</url>
    <content type="text"><![CDATA[快速指引用python读写excel的强大工具：openpyxl。本文只整理了openpyxl中那些使用最频繁的操作，其余的可自行搜索或查看官方文档，或者中文文档。 openpyxl安装1$ pip install openpyxl openpyxl使用1234#!/usr/bin/python# -*- coding: UTF-8 -*-import openpyxl as opxfrom openpyxl.utils import get_column_letter workbook级操作12345678910# 创建一个workbook对象，默认只含有一个sheetwb = opx.Workbook()print wb# 加载已有的Workbook文件，返回一个Workbook对象wb_exist = opx.load_workbook(&#x27;./learn.xlsx&#x27;)print wb_exist# 关闭workbook，如果Workbook已打开则关闭，只会影响到read_only和write_only模式wb_exist.close() &lt;openpyxl.workbook.workbook.Workbook object at 0x10966fa50&gt; &lt;openpyxl.workbook.workbook.Workbook object at 0x10968ee90&gt; 12# 保存Workbookwb.save(&#x27;./5-3.xlsx&#x27;) Worksheet级操作获取Worksheet对象12345678910111213141516# 激活第一个worksheetws = wb.active# 创建新的worksheet，如果表名已被占用则在表名后加123ws1 = wb.create_sheet(&#x27;Sheet1&#x27;)ws2 = wb.create_sheet(&#x27;sheet2&#x27;)print ws, ws1, ws2# 获取Worksheet对象ws1 = wb[&#x27;Sheet1&#x27;]print ws1# 遍历所有worksheetsfor wse in wb.worksheets: print wse.title &lt;Worksheet &quot;Sheet&quot;&gt; &lt;Worksheet &quot;Sheet1&quot;&gt; &lt;Worksheet &quot;sheet2&quot;&gt; &lt;Worksheet &quot;Sheet1&quot;&gt; Sheet Sheet1 sheet2 获取Worksheet对象的属性12345678# 返回所有worksheets的名字print wb.sheetnames# 获取Worksheet的名字print ws.title# 获取Worksheet最大行和最大列，初始时只有一行一列print ws.max_column,ws.max_row [u&#39;Sheet0&#39;, u&#39;Sheet1&#39;, u&#39;sheet2&#39;, u&#39;Sheet0 Copy&#39;] Sheet0 1 1 操作Worksheet12345678910# 更改Worksheet的表名ws.title = &#x27;Sheet0&#x27;print ws.title# 更改表名背景颜色ws.sheet_properties.tabColor = &quot;1072BA&quot;# 复制worksheetws_copy = wb.copy_worksheet(ws)print ws_copy Sheet0 &lt;Worksheet &quot;Sheet0 Copy1&quot;&gt; Worksheet行列操作123# 在Worksheet的max_row后面追加一行数据，序列默认从第一列添加，不足则补Nonews.append([1,2])tuple(ws.values) ((1, 2),) 123# 也可传入字典，key对应了列ws.append(&#123;1:3, &#x27;C&#x27;: 5&#125;)tuple(ws.values) ((1, 2, None), (3, None, 5)) Worksheet插入/删除行或列123# 在第3行前面插入行，如果参数超出了当前范围则什么也不做ws.insert_rows(1)tuple(ws.values) ((None, None, None), (1, 2, None), (3, None, 5)) 12ws.delete_rows(idx=3,amount=1)tuple(ws.values) ((None, None, None), (1, 2, None)) Cell级操作获取Cell对象123456789101112131415161718192021# 获取单个Cellcell_a1 = ws[&#x27;A1&#x27;]cell_a1 = ws.cell(row=1, column=1)print cell_a1# 当一个worksheet在内存中被创建时，它不包含任何Cell，只有当Cell被首次访问时才被创建for r in range(3): for c in range(2): ws.cell(row=r+1, column=c+1)# 获取单行Cell、获取单列Cell，返回一个单元格对象组成的元组print ws[1], len(ws[1])print ws[&#x27;A&#x27;], len(ws[&#x27;A&#x27;])# 获取多行多列print &#x27;前三列&#x27;, ws[&#x27;A&#x27;:&#x27;C&#x27;]print &#x27;前两行：&#x27;, ws[1:2]# 获取一个区域print &#x27;A1:C2：&#x27;,ws[&#x27;A1&#x27;:&#x27;C2&#x27;] &lt;Cell u&#39;Sheet0&#39;.A1&gt; (&lt;Cell u&#39;Sheet0&#39;.A1&gt;, &lt;Cell u&#39;Sheet0&#39;.B1&gt;, &lt;Cell u&#39;Sheet0&#39;.C1&gt;) 3 (&lt;Cell u&#39;Sheet0&#39;.A1&gt;, &lt;Cell u&#39;Sheet0&#39;.A2&gt;, &lt;Cell u&#39;Sheet0&#39;.A3&gt;) 3 前三列 ((&lt;Cell u&#39;Sheet0&#39;.A1&gt;, &lt;Cell u&#39;Sheet0&#39;.A2&gt;, &lt;Cell u&#39;Sheet0&#39;.A3&gt;), (&lt;Cell u&#39;Sheet0&#39;.B1&gt;, &lt;Cell u&#39;Sheet0&#39;.B2&gt;, &lt;Cell u&#39;Sheet0&#39;.B3&gt;), (&lt;Cell u&#39;Sheet0&#39;.C1&gt;, &lt;Cell u&#39;Sheet0&#39;.C2&gt;, &lt;Cell u&#39;Sheet0&#39;.C3&gt;)) 前两行： ((&lt;Cell u&#39;Sheet0&#39;.A1&gt;, &lt;Cell u&#39;Sheet0&#39;.B1&gt;, &lt;Cell u&#39;Sheet0&#39;.C1&gt;), (&lt;Cell u&#39;Sheet0&#39;.A2&gt;, &lt;Cell u&#39;Sheet0&#39;.B2&gt;, &lt;Cell u&#39;Sheet0&#39;.C2&gt;)) A1:C2： ((&lt;Cell u&#39;Sheet0&#39;.A1&gt;, &lt;Cell u&#39;Sheet0&#39;.B1&gt;, &lt;Cell u&#39;Sheet0&#39;.C1&gt;), (&lt;Cell u&#39;Sheet0&#39;.A2&gt;, &lt;Cell u&#39;Sheet0&#39;.B2&gt;, &lt;Cell u&#39;Sheet0&#39;.C2&gt;)) 遍历行/列/单元格对象遍历用 ws.iter_rows 和 ws.iter_cols 就够了！！ 123# 按行列号遍历每一行，带min max参数时不受已激活范围的影响for row in ws.iter_rows(min_row=1,max_row=2,min_col=1,max_col=3): print row (&lt;Cell u&#39;Sheet0&#39;.A1&gt;, &lt;Cell u&#39;Sheet0&#39;.B1&gt;, &lt;Cell u&#39;Sheet0&#39;.C1&gt;) (&lt;Cell u&#39;Sheet0&#39;.A2&gt;, &lt;Cell u&#39;Sheet0&#39;.B2&gt;, &lt;Cell u&#39;Sheet0&#39;.C2&gt;) 123# 遍历每一列，不带min max参数时，只返回激活范围内的单元格for col in ws.iter_cols(): print col (&lt;Cell u&#39;Sheet0&#39;.A1&gt;, &lt;Cell u&#39;Sheet0&#39;.A2&gt;, &lt;Cell u&#39;Sheet0&#39;.A3&gt;) (&lt;Cell u&#39;Sheet0&#39;.B1&gt;, &lt;Cell u&#39;Sheet0&#39;.B2&gt;, &lt;Cell u&#39;Sheet0&#39;.B3&gt;) (&lt;Cell u&#39;Sheet0&#39;.C1&gt;, &lt;Cell u&#39;Sheet0&#39;.C2&gt;, &lt;Cell u&#39;Sheet0&#39;.C3&gt;) 1234# 行优先遍历每个单元格，ws.iter_rows()和ws.rows效果相同，但前者可自定义参数for row in ws.iter_rows(): for cell in row: print cell &lt;Cell u&#39;Sheet0&#39;.A1&gt; &lt;Cell u&#39;Sheet0&#39;.B1&gt; &lt;Cell u&#39;Sheet0&#39;.C1&gt; &lt;Cell u&#39;Sheet0&#39;.A2&gt; &lt;Cell u&#39;Sheet0&#39;.B2&gt; &lt;Cell u&#39;Sheet0&#39;.C2&gt; &lt;Cell u&#39;Sheet0&#39;.A3&gt; &lt;Cell u&#39;Sheet0&#39;.B3&gt; &lt;Cell u&#39;Sheet0&#39;.C3&gt; 1234# 遍历区域for row in ws[&#x27;a1:c2&#x27;]: for cell in row: print cell &lt;Cell u&#39;Sheet0&#39;.A1&gt; &lt;Cell u&#39;Sheet0&#39;.B1&gt; &lt;Cell u&#39;Sheet0&#39;.C1&gt; &lt;Cell u&#39;Sheet0&#39;.A2&gt; &lt;Cell u&#39;Sheet0&#39;.B2&gt; &lt;Cell u&#39;Sheet0&#39;.C2&gt; 123# 遍历值for row in ws.iter_rows(min_row=1,max_col=2,values_only=True): print row (None, None) (1, 2) (None, None) 获取单元格的属性12cell = ws[&#x27;A1&#x27;]print cell.value None 修改Cell的属性修改单元格的值123456789# 修改单元格的值ws[&#x27;A1&#x27;] = 0print ws[&#x27;A1&#x27;].value# 修改一个区域的值，需要逐个赋值for r in ws[&#x27;a1&#x27;:&#x27;c3&#x27;]: for c in r: c.value = 3print tuple(ws.values) 0 ((3, 3, 3), (3, 3, 3), (3, 3, 3)) 修改单元格的格式12345678910111213141516171819202122232425262728293031323334353637383940单元格的默认属性:&gt;&gt;&gt; font = Font(name=&#x27;Calibri&#x27;,... size=11,... bold=False,... italic=False,... vertAlign=None,... underline=&#x27;none&#x27;,... strike=False,... color=&#x27;FF000000&#x27;)&gt;&gt;&gt; fill = PatternFill(fill_type=None,... start_color=&#x27;FFFFFFFF&#x27;,... end_color=&#x27;FF000000&#x27;)&gt;&gt;&gt; border = Border(left=Side(border_style=None,... color=&#x27;FF000000&#x27;),... right=Side(border_style=None,... color=&#x27;FF000000&#x27;),... top=Side(border_style=None,... color=&#x27;FF000000&#x27;),... bottom=Side(border_style=None,... color=&#x27;FF000000&#x27;),... diagonal=Side(border_style=None,... color=&#x27;FF000000&#x27;),... diagonal_direction=0,... outline=Side(border_style=None,... color=&#x27;FF000000&#x27;),... vertical=Side(border_style=None,... color=&#x27;FF000000&#x27;),... horizontal=Side(border_style=None,... color=&#x27;FF000000&#x27;)... )&gt;&gt;&gt; alignment=Alignment(horizontal=&#x27;general&#x27;,... vertical=&#x27;bottom&#x27;,... text_rotation=0,... wrap_text=False,... shrink_to_fit=False,... indent=0)&gt;&gt;&gt; number_format = &#x27;General&#x27;&gt;&gt;&gt; protection = Protection(locked=True,... hidden=False) 1from openpyxl.styles import PatternFill, Border, Side, Alignment, Protection, Font 123456789101112131415161718# 以修改单元格的字体为例font = Font(name=&#x27;Calibri&#x27;, size=16, bold=False, italic=False, vertAlign=None, underline=&#x27;none&#x27;, strike=False, color=&#x27;FF000000&#x27;)alignment=Alignment(horizontal=&#x27;left&#x27;, vertical=&#x27;center&#x27;, text_rotation=0, wrap_text=False, shrink_to_fit=False, indent=0)ws[&#x27;a1&#x27;] = 9999ws[&#x27;a1&#x27;].font = fontws[&#x27;a1&#x27;].alignment = alignment 1wb.save(&#x27;./learn_openpyxl.xlsx&#x27;) 改进模式有时，您需要打开或写入非常大的XLSX文件，而OpenPYXL中的常见例程将无法处理该负载。幸运的是，有两种模式使您能够以（接近）恒定的内存消耗来读写无限量的数据。 只读模式12wb_read = opx.load_workbook(filename=&#x27;./learn_openpyxl.xlsx&#x27;, read_only=True)wb_read &lt;openpyxl.workbook.workbook.Workbook at 0x1096f4110&gt; 12ws_read = wb_read.activeprint ws_read.max_row,ws_read.max_column 3 3 12for row in ws_read.iter_rows(values_only=True): print row (9999L, 3L, 3L) (3L, 3L, 3L) (3L, 3L, 3L) 12wb_read.close()wb_read &lt;openpyxl.workbook.workbook.Workbook at 0x1096f4110&gt; 只写模式 与普通工作簿不同，新创建的只写工作簿不包含任何工作表；必须使用 create_sheet() 方法。 在只写工作簿中，只能使用 append() . 不能在任意位置用 cell() 或 iter_rows() . 它能够导出无限量的数据（甚至超过了Excel的实际处理能力），同时将内存使用量保持在10MB以下。 只写工作簿只能保存一次。之后，每次试图将工作簿或append（）保存到现有工作表时，都会引发 openpyxl.utils.exceptions.WorkbookAlreadySaved 例外。 在添加单元格之前，必须创建实际单元格数据之前出现在文件中的所有内容，因为在此之前必须将其写入文件。例如， freeze_panes 应在添加单元格之前设置。 123# load_workbook本地读取的Excel没有只写权限wb_write = opx.Workbook(write_only=True)wb_write &lt;openpyxl.workbook.workbook.Workbook at 0x1096cba10&gt; 1234# 只写模式的Workbook创建后，没有sheet，需要手动创建print wb_write.worksheetsws_write = wb_write.create_sheet()print ws_write [] &lt;WriteOnlyWorksheet &quot;Sheet&quot;&gt; 12# 只写模式的worksheet没有cell属性，也不能通过索引来获取单元格ws_write.cell --------------------------------------------------------------------------- AttributeError Traceback (most recent call last) &lt;ipython-input-39-5164431d4a6c&gt; in &lt;module&gt;() 1 # 只写模式的worksheet没有cell属性，也不能通过索引来获取单元格 ----&gt; 2 ws_write.cell AttributeError: &#39;WriteOnlyWorksheet&#39; object has no attribute &#39;cell&#39; 1ws_write.append([1,2,3]) 123456789# 如果希望为单元格添加格式或注释，可以使用WriteOnlyCellfrom openpyxl.cell import WriteOnlyCellfrom openpyxl.comments import Commentfrom openpyxl.styles import Fontcell = WriteOnlyCell(ws, value=&quot;hello world&quot;)cell.font = Font(name=&#x27;Courier&#x27;, size=36)cell.comment = Comment(text=&quot;A comment&quot;, author=&quot;Author&#x27;s Name&quot;)ws.append([cell, 3.14, None]) 1wb_write.save(&#x27;./write_only.xlsx&#x27;)]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>教程</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python 基础：核心语法图例]]></title>
    <url>%2FPython%2FPython%2Fpython%20%E5%9F%BA%E7%A1%80%EF%BC%9A%E6%A0%B8%E5%BF%83%E8%AF%AD%E6%B3%95%E5%9B%BE%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[对象模型 数据类型核心类型 迭代器 格式化字符串 表达式 语句 函数]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>教程</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python 标准库：二分查找 bisect]]></title>
    <url>%2FPython%2FPython%2Fpython%20%E6%A0%87%E5%87%86%E5%BA%93%EF%BC%9A%E4%BA%8C%E5%88%86%E6%9F%A5%E6%89%BE%20bisect%2F</url>
    <content type="text"><![CDATA[1import bisect 使用Bisect二分查找模块前必须保证列表已经是有序的，bisect模块提供以下功能： bisect：返回待插入元素的插入位置，保证数组仍然有序； bisect(a,x,lo=0,hi=len(a)):如果相同，则插入右侧，满足all(val &gt; x for val in a[i:hi])，all(val &lt;= x for val in a[lo:i]) bisect_left:如果相同则插入左侧，all(val &lt; x for val in a[lo:i])， all(val &gt;= x for val in a[i:hi]) bisect_right:同bisect insort：将给定元素插入到有序表中，保证数组仍然有序； insort(a,x,lo=0,hi=len(a)):等价于a.insert(bisect.bisect(a, x, lo, hi), x) insort_left:等价于a.insert(bisect.bisect_left(a, x, lo, hi), x) insort_right:等价于insort 1a = [2,3,4,5,6,6,10] bisect1bisect.bisect(a,6) 6 1bisect.bisect(a,6.1) 6 1bisect.bisect_left(a,6) 4 1bisect.bisect_left(a,6.1) 6 1bisect.bisect_right(a,6) 6 1bisect.bisect_right(a,6.1) 6 insort1a = [2,3,4,5,6,6,10] 12bisect.insort(a,6)a [2, 3, 4, 5, 6, 6, 6, 10] 12bisect.insort(a,6.1)a [2, 3, 4, 5, 6, 6, 6, 6.1, 10] 12bisect.insort_left(a,4)a [2, 3, 4, 4, 5, 6, 6, 6, 6.1, 10] 12bisect.insort_right(a,10)a [2, 3, 4, 4, 5, 6, 6, 6, 6.1, 10, 10] 常用查询操作12345678910111213141516171819202122232425262728293031323334def index(a, x): &#x27;返回第一个等于x的元素下标&#x27; i = bisect_left(a, x) if i != len(a) and a[i] == x: return i raise ValueErrordef find_lt(a, x): &#x27;返回最后一个小于x的元素下标&#x27; i = bisect_left(a, x) if i: return a[i-1] raise ValueErrordef find_le(a, x): &#x27;返回最右侧小于等于x的元素下标&#x27; i = bisect_right(a, x) if i: return a[i-1] raise ValueErrordef find_gt(a, x): &#x27;Find leftmost value greater than x&#x27; i = bisect_right(a, x) if i != len(a): return a[i] raise ValueErrordef find_ge(a, x): &#x27;Find leftmost item greater than or equal to x&#x27; i = bisect_left(a, x) if i != len(a): return a[i] raise ValueError]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>教程</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python 标准库：容器类型 collections]]></title>
    <url>%2FPython%2FPython%2Fpython%20%E6%A0%87%E5%87%86%E5%BA%93%EF%BC%9A%E5%AE%B9%E5%99%A8%E7%B1%BB%E5%9E%8B%20collections%2F</url>
    <content type="text"><![CDATA[collections提供了一组高效的容器数据类型，可以作为Python通用容器(tuple、list、dict)的补充。熟练掌握这些容器类型，不仅可以让我们写出的代码更加Pythonic，也可以提高我们程序的运行效率。 构造方法 说明 namedtuple() 创建拥有命名域的元组 deque 创建双端队列 Counter 创建计数器 OrderedDict 创建有序字典 defaultdict 创建具有默认值的字典 命名元组——namedtuple()我们可以为元组中的每个位置起一个名字，名字会被作为对象的属性域，增强了代码的可读性。此外，命名元组并不会为每个实例创建，所以命名元组是轻量级的，并不会比普通元组差。 1from collections import namedtuple 1234# 定义一个命名元组，并创建一个实例point = namedtuple(&#x27;Point&#x27;,[&#x27;x&#x27;,&#x27;y&#x27;])p = point(3,4)p Point(x=3, y=4) 123# 可以通过实例的属性名访问元组中不同的域，也可以像普通元祖一样使用print p.x,p.yprint p[0],p[-1] 3 4 3 4 12# 从一个已有的序列创建point._make([3,4]) Point(x=3, y=4) 12# 转化为字典p._asdict() OrderedDict([(&#39;x&#39;, 3), (&#39;y&#39;, 4)]) 双向列表——dequeclass collections.deque([iterable[, maxlen]]) 使用list存储数据时，按索引访问元素很快（O(1)），但是插入和删除元素就很慢了（O(n)），deque是为了高效实现插入和删除操作的双向列表，适合用于队列和栈： dequeue在两端的操作O(1)，方便高效的实现各种栈和队列 dequeue在中间的操作O(n)，如需随机存取还应使用普通列表 1from collections import deque 123# 创建双端列表Q = deque(range(10))Q deque([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) 12345# 默认右端操作Q.append(10)print QQ.pop()print Q deque([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) deque([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) 12345# 支持左端操作Q.appendleft(-1)print QQ.popleft()print Q deque([-1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) deque([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) 1234# 数组旋转Q = deque([1,2,3,4,5])Q.rotate(2)Q deque([4, 5, 1, 2, 3]) 1234567891011# 实现一个无尽换换的跑马灯import sysimport timefancy_loading = deque(&#x27;&gt;--------------------&#x27;)while True: print &#x27;\r%s&#x27; % &#x27;&#x27;.join(fancy_loading), fancy_loading.rotate(1) sys.stdout.flush() time.sleep(0.1) 计数器——Counter1from collections import Counter 创建计数器class collections.Counter([iterable-or-mapping]) 可以由任意的可迭代对象来创建计数器，Counter会对可迭代对象中的相同对象计数统计，返回一个计数字典，以不同对象为key，以该对象出现次数为value。 12a = Counter()a Counter() 12b = Counter(range(3))b Counter(&#123;0: 1, 1: 1, 2: 1&#125;) 12c = Counter(&#x27;hello&#x27;)c Counter(&#123;&#39;e&#39;: 1, &#39;h&#39;: 1, &#39;l&#39;: 2, &#39;o&#39;: 1&#125;) 12d = Counter(a=1,b=2.1)d Counter(&#123;&#39;a&#39;: 1, &#39;b&#39;: 2.1&#125;) 12x = Counter(&#123;&#x27;a&#x27;:1,&#x27;b&#x27;:0,&#x27;c&#x27;:-2&#125;)x Counter(&#123;&#39;a&#39;: 1, &#39;b&#39;: 0, &#39;c&#39;: -2&#125;) Counter对象是字典的子类Counter对象与内置dict的显著差异是Counter会自动为新的键值创建默认值0 1234# 不存在的键，默认返回0，但是如果只是访问不会自动添加新的键，等价于x.get(&#x27;d&#x27;,0)print x[&#x27;d&#x27;]x[&#x27;d&#x27;] += 1print x 0 Counter(&#123;&#39;a&#39;: 1, &#39;d&#39;: 1, &#39;b&#39;: 0, &#39;c&#39;: -2&#125;) 12# 存在性&#x27;a&#x27; in x True 1234# 返回视图print x.keys()print x.values()print x.items() [&#39;a&#39;, &#39;c&#39;, &#39;b&#39;, &#39;d&#39;] [1, -2, 0, 1] [(&#39;a&#39;, 1), (&#39;c&#39;, -2), (&#39;b&#39;, 0), (&#39;d&#39;, 1)] Counter的常用方法12x = Counter(&#123;&#x27;a&#x27;:2,&#x27;b&#x27;:0,&#x27;c&#x27;:-2&#125;)x Counter(&#123;&#39;a&#39;: 2, &#39;b&#39;: 0, &#39;c&#39;: -2&#125;) elements()elements方法用户迭代地展示Counter内的所有元素，按元素的计数重复该元素，如果该元素的计数小于1，那么Counter就会忽略该元素，不进行展示。 1list(x.elements()) [&#39;a&#39;, &#39;a&#39;] most_common([k])most_common函数返回Counter中次数最多的k个元素，如果N没有提供或者是None，那么就会返回所有元素。 1x.most_common() [(&#39;a&#39;, 2), (&#39;b&#39;, 0), (&#39;c&#39;, -2)] 1x.most_common(2) [(&#39;a&#39;, 2), (&#39;b&#39;, 0)] clear()清除所有元素的统计次数 12x.clear()x Counter() subtract([iterable-or-mapping])substract方法接收一个可迭代或者可映射的对象，针对每个元素减去参数中的元素对应的次数。 12x.subtract(&#x27;abc&#x27;)x Counter(&#123;&#39;a&#39;: 1, &#39;b&#39;: -1, &#39;c&#39;: -3&#125;) update([iterable-or-mapping])update方法的功能和substract方法的功能正好相反，它接收一个可迭代或者可映射的对象，针对每个元素加上参数中的元素对应的次数。 12x.update(&#x27;bcd&#x27;)x Counter(&#123;&#39;a&#39;: 1, &#39;b&#39;: 0, &#39;c&#39;: -2, &#39;d&#39;: 1&#125;) Counter对象运算注意：Counter对象的运算会自动忽略掉小于等于0的元素，也常常通过这种方法去除Counter对象中小于等于0的元素。 123x = Counter(&#123;&#x27;a&#x27;:1,&#x27;b&#x27;:0,&#x27;c&#x27;:-2,&#x27;d&#x27;:3&#125;)y = Counter(&#123;&#x27;a&#x27;:2,&#x27;d&#x27;:1,&#x27;e&#x27;:2&#125;)x,y (Counter(&#123;&#39;a&#39;: 1, &#39;b&#39;: 0, &#39;c&#39;: -2, &#39;d&#39;: 3&#125;), Counter(&#123;&#39;a&#39;: 2, &#39;d&#39;: 1, &#39;e&#39;: 2&#125;)) 1x + y Counter(&#123;&#39;a&#39;: 3, &#39;d&#39;: 4, &#39;e&#39;: 2&#125;) 1x - y Counter(&#123;&#39;d&#39;: 2&#125;) 1x &amp; y Counter(&#123;&#39;a&#39;: 1, &#39;d&#39;: 1&#125;) 1x | y Counter(&#123;&#39;a&#39;: 2, &#39;d&#39;: 3, &#39;e&#39;: 2&#125;) 123# 仅在python3支持+xCounter(&#123;&#x27;a&#x27;: 1, &#x27;d&#x27;: 3&#125;) --------------------------------------------------------------------------- TypeError Traceback (most recent call last) &lt;ipython-input-35-a5890c994b78&gt; in &lt;module&gt;() 1 # 仅在python3支持 ----&gt; 2 +x TypeError: bad operand type for unary +: &#39;Counter&#39; 有序字典——orderedDict使用dict时，Key是无序的。在对dict做迭代时，我们无法确定Key的顺序。如果要保持Key的顺序，可以用OrderedDict 1from collections import OrderedDict 123456od = OrderedDict()od[&#x27;a&#x27;] = 1od[&#x27;c&#x27;] = 1od[&#x27;b&#x27;] =2for k in od: print k,od[k] a 1 c 1 b 2 默认类型字典——defaultdictclass collections.defaultdict([default_factory[, ...]])¶ 使用dict时，如果引用的Key不存在，就会抛出KeyError。如果希望key不存在时，为key设置一个默认值，就可以用defaultdict。 1from collections import defaultdict 如果我们想从无到有构建一个字典{key:list}，通常有以下三种方式，可以看到使用defaultdict用时最少。 12345678910111213141516171819202122232425# 使用get追加的方式最慢，因为每次都生成新的副本，O(n)start = time.time()dic_1 = &#123;&#125;for i in range(100000): dic_1[i%2] = dic_1.get(i%2,[]) + [i] print &#x27;%.3f&#x27;%(time.time() - start)25.481# 使用setdefault的方式，很快start = time.time()dic_3 = &#123;&#125;for i in range(100000): dic_3.setdefault(i%2,[]).append(i)print &#x27;%.3f&#x27;%(time.time() - start)0.080# 使用defaultdict的方式start = time.time()dic_2 = defaultdict(list)for i in range(100000): dic_2[i%2].append(i)print &#x27;%.3f&#x27;%(time.time() - start)0.065 值得注意的是，一旦我们引用了defaultdict中的某个key，defaultdict就会自动添加该key并为之赋予对应类型的默认值，因此使用x in dict时要格外注意： 123456789101112x = collections.defaultdict(int)print xprint 2 in xprint x[2]print xprint 2 in xdefaultdict(&lt;type &#x27;int&#x27;&gt;, &#123;&#125;)False0defaultdict(&lt;type &#x27;int&#x27;&gt;, &#123;2: 0&#125;)True 引用8.3. collections — High-performance container datatypes]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>教程</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《大秦帝国之裂变》语录]]></title>
    <url>%2FLife%2FLife%2F%E5%A4%A7%E7%A7%A6%E5%B8%9D%E5%9B%BD%E4%B9%8B%E8%A3%82%E5%8F%98%2F</url>
    <content type="text"><![CDATA[第一集：少梁之战秦兵：赳赳老秦，复我河山。血不流干，死不休战。 第二集：秦军撤兵少梁之战，秦献公身中狼毒箭，魏国丞相公孙痤被俘，秦军撤兵。 第三集：献公传位献公召见公子虔 献公：跟公父说实话，你想不想做国君？ 公子虔：公父让我做，我便做，公父不让我做，我便不做。 献公：好，为父没有看错你，铮铮铁骨，定国柱石。 公子虔：公父说过，秦国疲弱，祸在内斗，惟有一心，方能强国。 献公：老大啊，你说实话，为父有没有把你和渠梁分过嫡庶？ 公子虔：公父何出此言啊，儿臣周岁丧母，国后娘抱儿养儿，历尽了艰辛万苦，爱儿如同亲生，养儿胜于亲娘，此恩是天高地厚，儿臣纵是禽兽也九死不能报万一，儿臣又怎会生出嫡庶异心之分。 献公：你娘是你娘啊。。。 公子虔：公父待儿，倚重之心天地可鉴，庶出疏离之感，儿臣从未有过。 献公：好，好，老大，为父那就放开说话了。老大，你说，假如你是农夫，是工匠，或者是个军士，你看谁做国君更好？ 公子虔：渠梁更合适。儿臣绝无虚言。 献公：老大，为父还是要你立誓明心啊。 公子虔：公父是信不过儿臣吗？ 献公：不，老大，不是不信任你，实在是国家公器不能寄予一言，即使是骨肉兄弟，也得是公心为先 公子虔：（自断手指中。。。） 献公：老大，老大，我看看，哎呀。。。你呀。。。 献公召见渠梁 献公：渠梁啊，公父有话跟你说，不要分心，公父的路要走完了，我决定立你为太子，即刻继国君之位。 渠梁：公父，这。。。 献公：听我说完！这是老大的血誓，他若有二心，你就可以公诸朝野，人人得以诛之。 渠梁：公父，我与大哥素来同心同德，为何让大哥如此折磨自己？ 献公：渠梁啊，千万要记住，同德易，同心难，大德大节，求同就更难，历来公室内乱，哪个不是骨肉相残？赢虔，他是内明之人，你要倚重他，血誓，只防万一。 渠梁：公父放心，公父教诲渠梁铭记在心，如果食言，死不得入赢氏太庙。 第四集：秦国释放公孙痤渠梁阻止荧玉杀公孙痤 渠梁：小妹，你可以不认我这个哥哥，但你不能不认我这个国君。 荧玉：你为何不让我杀公叔老贼？ 渠梁：私仇滥杀，乱国乱局。 荧玉：君仇国恨自古一体，何谓私仇？ 渠梁：无视国家存亡，惟泄一己怨恨，就是私仇。 荧玉：得先君之位忘先君之仇，这才是天地难容！ 渠梁：公器不存私念，公父若在，也会如此。左庶长，公叔痤不能杀。 嬴虔：左庶长？我是你大哥！ 渠梁：事涉国家，没有大哥，只有君臣。 嬴虔：老秦人同仇敌忾，我看哪个国君敢护仇？ 渠梁：父仇为私，不能公报！娘，大哥，小妹，大臣将士们，渠梁少入军旅，和父老兄弟并肩血战沙场，脚下踩过万千老秦人的尸骨，哪个不是做梦都想着复仇啊，杀一个俘虏，就能证明我们老秦人的血性吗？今日杀了公孙痤，只能解一时之恨，然而必然引起山东六国集体攻秦，到时候邦国灭亡，尸骨成山，谁来复仇啊，复个鸟！说到底，只有保住国家，保住百姓，保住将士，秦国才能复仇，才能翻身，才能雪耻！ 卫鞅救师求见公子虔公子虔：什么事那么重要，别绕弯子，说吧。卫鞅：原本申明非秦公不言，念公子虔是秦国栋梁，新君兄长，说也无妨。公子虔：直说，实说。卫鞅：在下入秦，一非官派，二非私托，只为一事，救回老丞相以全师生之交。公子虔：就凭你啊。卫鞅：卫鞅救师，也救秦国。公子虔：哈哈哈。。。魏人救秦？笑话。卫鞅：上将军未免浅见，在下明言，魏国朝野，庞涓力主灭秦，而丞相公叔图谋称霸，二人势同水火。庞涓奇袭骊山，就是要夺回公叔明正典刑，一举剪除政敌，从而全力灭秦，今奇袭未成，庞涓自然退而求其次，希望借秦人之手杀死我师，若秦国当真如此，无异于助了庞涓一臂之力，加重了自己的危机处境。眼下，唯有一策，即可救我师，又可救秦国。公子虔：说下去。卫鞅：这一策便是，秦国放人、割地、求和。使魏国一时无攻秦口实，日后再做图谋，如此，两全，否则秦国危在旦夕。公子虔：卖国求私，还大言侃侃，天下一奇啊。卫鞅：在下并非魏人，更非魏臣，何谈卖国。公子虔：带入大营。。。 渠梁谈和公孙痤公孙痤：你说，老秦公死了？既然如此，老夫的人头归你了，何时开刀？渠梁：老丞相差矣，渠梁不是要杀你，而是来放你。公孙痤：哈哈哈。。。小子，你敢嘲弄老夫，士可杀，不可辱渠梁：渠梁怎敢侮辱前辈，我诚意放老丞相回归安邑，想当时秦魏两国激战多年，生灵涂炭，死伤无数，渠梁新任国君，只图秦国庶民安居耕牧，并不想两国纠缠交恶，素知老丞相深明大义，嬴渠梁欲与老丞相共谋两国休战歇兵，并无它意。公孙痤：秦公不记杀父之仇？渠梁：父仇为私，和战为公。嬴渠梁若非真心，愿受上天惩罚。公孙痤：好，老夫信你。只是这疆界，不知秦公如何打算？渠梁：以石门大战之前为定。公孙痤：河西高原，秦东地带，函谷关都给魏国？渠梁：嗯。公孙痤：两国邦交作何说法？渠梁：秦国承认魏国霸主地位。公孙痤：纳贡，称臣？渠梁：不称臣，不纳贡。公孙痤：再让一步，会更稳妥一些。渠梁：老丞相，凡事都有底线，秦国穷困，无贡可纳，秦人硬骨，不能折腰。公孙痤：秦公大明！渠梁：二十年后我会夺回来的。公孙痤：共有此心，老夫拭目以待，只是这缓兵之计太过明显渠梁：老丞相，秦国虽穷，却是尚武善战之邦，若魏国执意灭秦，老秦人必将拼死血战，魏国纵然灭秦，也将在惨胜之后元气大伤，那时，魏国必成众矢之的，非但保不住霸主地位，反而被五大强国瓜分，如果魏王不明此理，秦国宁肯血战到底。休息三五日之后，渠梁派出特使护送老丞相返回安邑，不言俘获，只当做魏王议和特使对待，行吗？公孙痤：哎，老夫阅人无数，秦公气量胸怀，数年之后必令天下刮目相看。渠梁：渠梁浅陋，哪敢当此褒奖。公孙痤：可惜老夫来日不多，不能与你并世争雄了。 第五集：说服魏王谈和卫鞅支持公孙痤争霸方略公孙痤：卫鞅，你到酒肆去了？卫鞅：老师，族人有望解禁。公孙痤：酒肆消息你也信？荒唐卫鞅：哈哈。。。断事不在消息源头，在自家评判。公孙痤：算了，不说在这些了，鞅啊，总算是有惊无险。卫鞅：不，险关还未过，庞涓以一统天下为终身大志，铁了心要拿秦国开刀，魏王未必不为所动。公孙痤：庞涓、公子卬都想杀老夫。卫鞅：公子卬不足为虑，争功夺权罢了，庞涓才是真正的大患。公孙痤：这一统天下也是大魏功业所在，老夫若是为了保住性命而反对，公心何在？卫鞅：老师差矣，一统天下如果可行，卫鞅也不会帮老师与庞涓对峙。公孙痤：你说一统天下不可行？卫鞅：时势未到，眼下不可行。公孙痤：为何?卫鞅：天下无一心，七国无独强，未来之势未见分晓，冒然灭国定会招致灾祸。公孙痤：你是说，还是老夫的争霸方略，适合魏国？卫鞅：正是。卫鞅：备车！ 公孙痤说服魏王谈和魏王：那老丞相你说，让本人怎么接受秦国那几块鸡零狗碎？公孙痤：如何对秦，首在长策！魏王：先说办法，再讲长策！公孙痤：老臣的办法分两步走。魏王：第一步。公孙痤：与秦罢兵，接纳割地，河西秦东数十县，再加上函谷关天险，绝非鸡零狗碎。魏王：第二步。公孙痤：待秦地完全划入魏国，等到时机成熟再变灭秦为分秦。魏王：灭秦分秦有何区别？公孙痤：灭秦，魏国独吞；分秦，六国分吞。魏王：那如此一来，我既无后顾之忧，又可稳坐天下霸主了。公孙痤：正是魏王：果然姜是老的辣！ 魏王庞涓密谋分秦策略魏王：本王在庙堂上的运筹，上将军明白吗？庞涓：我王一番指点，臣恍然醒悟魏王：这就对了，你今天想谋兵灭秦固然是上策，但是现在到达嘴边的这块肥肉不能让它滑了，一纸休兵盟书，不费一兵一卒，我大魏便可稳稳占了秦国东部最肥美的土地城池，到哪去找这等好事去？实咬一口当然胜过虚啃三日，治国之道尽在此矣！庞涓：不过，我王，臣以为，先食言于秦国，再食言于五国，只怕有失道义。魏王：道义？现在天下大争，谁讲道义，讲道义干什么？我今天就是要咬秦国这块肥肉，然后啃秦国这块骨头，谁敢奈我何？不服，不服就等你先强大了再说；你弱，你就没有说话的资格，就只能等着做别人砧板上的鱼肉，明白吗？什么叫邦交？下刀之前的动作才叫邦交。庞涓：臣以为，还是打出来的地盘更实在。魏王：打归打，谈归谈嘛，两手并用才能得天下，明白吗？六国分秦，即刻交你放手去做，彻底灭秦之日就是你上将军首功之时。庞涓：臣遵命。 卫鞅推测老师无恙公叔痤：庞涓、公子卬都想杀了老夫取而代之，魏王耳根子又软。卫鞅：魏王醉心权谋，喜好均衡朝局，鞅以为，老师无恙。魏王派庞涓为全权特使，正密谋六国分秦。公叔痤：魏王，你的心也太急了，盟约刚立，就谋分秦，你让老夫有何颜立足于天下，让大魏又有何面目立于天下?卫鞅：更重要的，魏国死盯弱小秦国，丧失了中原扩张的最佳时机。 第六集：六国指地分秦楚王：这商於、武关归我大楚了。韩王：那崤函、关中一带就归我大韩。赵王：陇西、北地全部归赵了。燕王：河西、河套归我大燕。齐王：齐秦相隔太远，本王不争秦地，然则我灭他国，列国不得插手。庞涓：哼，如此分秦，盟主国是一无所得了?众王：魏国已吞肥肉，再分居心何在？天下三王，不能只听魏王的。庞涓：五君之意，是不认我魏国盟主了？众王：庞涓，魏王国书原说，公平分秦，今日公平何在？庞涓：公平？以本上将军谋划，根本不需要六国分秦。 第七集：]]></content>
      <categories>
        <category>Life</category>
      </categories>
      <tags>
        <tag>Life</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[人生的时区]]></title>
    <url>%2FLife%2FLife%2F%E4%BA%BA%E7%94%9F%E7%9A%84%E6%97%B6%E5%8C%BA%2F</url>
    <content type="text"><![CDATA[纽约时间比加州时间早三个小时，New York is 3 hours ahead of California, 但加州时间并没有变慢。but it does not make California slow. 有人22岁就毕业了，Someone graduated at the age of 22, 但等了五年才找到好的工作！but waited 5 years before securing a good job! 有人25岁就当上CEO，Someone became a CEO at 25, 却在50岁去世。and died at 50. 也有人迟到50岁才当上CEO，While another became a CEO at 50, 然后活到90岁。and lived to 90 years. 有人依然单身，Someone is still single, 同时也有人已婚。while someone else got married. 奥巴马55岁就退休，Obama retires at 55, 川普70岁才开始当总统。but Trump starts at 70. 世上每个人本来就有自己的发展时区。Absolutely everyone in this world works based on their Time Zone. 身边有些人看似走在你前面，People around you might seem to go ahead of you, 也有人看似走在你后面。some might seem to be behind you. 但其实每个人在自己的时区有自己的步程。But everyone is running their own RACE, in their own TIME. 不用嫉妒或嘲笑他们。Don’t envy them or mock them. 他们都在自己的时区里，你也是！They are in their TIME ZONE, and you are in yours! 生命就是等待正确的行动时机。Life is about waiting for the right moment to act. 所以，放轻松。So, RELAX. 你没有落后。You’re not LATE. 你没有领先。You’re not EARLY. 在命运为你安排的属于自己的时区里，一切都准时。You are very much ON TIME, and in your TIME ZONE Destiny set up for you.]]></content>
      <categories>
        <category>Life</category>
      </categories>
      <tags>
        <tag>Life</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo + Github Pages 博客搭建教程（一）：博客搭建]]></title>
    <url>%2F%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%2FHexo%2FHexo%20%2B%20Github%20Pages%20%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%95%99%E7%A8%8B%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9A%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[前言随着移动互联网的到来，使用电脑订阅资讯的时代一去不复返，博客时代芳华已逝，但内容创作永不过时。这里我想先谈一下自己对于个人博客的看法。 为什么要写博客： 践行“费曼技巧”：与假想听众一起学习是最佳的学习方式； 建立“云知识库”：大脑的记忆总是模糊、有限而易逝的，博客便是整理过的记忆，清晰、持久而又便于回忆； 为什么要搭建个人博客： 公共博客平台不可控或收费； 收获自主权和归属感； 为什么选择 Hexo + GitPages 搭建个人博客： 轻量：没有麻烦的配置，使用标记语 Markdown ，无需自己搭建服务器； 免费：免费托管 Github 仓库，有 1G 免费空间； 通用：是目前比较流行的方案，社区活跃，不断创新； 准备工作博客搭建过程主要涉及 Hexo 和 Github Pages 两个工具，在开始搭建博客前，首先要完成以下准备工作： 注册 Github 账号：按指示完成注册即可； 创建两个 Github 仓库：一个仓库名为&lt;username&gt;.github.io，用于托管博客的静态文件（public文件夹）、生成 Github Pages 展示页面；另一个仓库名为&lt;username&gt;.github.source 用于文章备份，可兼作图床（source 文件夹）； 本地安装git：先在本地安装 homebrew 软件包管理工具，再通过 brew install git 安装 git 工具，通过git version 查看 git 是否成功安装； 123456# 安装homebrew$ /usr/bin/ruby -e &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)&quot;# 安装git$ brew install git$ git versiongit version 2.15.0 配置git公钥：Git 服务器都会选择使用 SSH 公钥来进行授权，配置方法也很简单，在本地通过 ssh-keygen 生成密钥对之后，将./.ssh/id_rsa.pub 中的公钥添加到github账号SSH key； 安装node.js：brew install node安装，node -v检查是否安装成功； 123$ brew install node$ node -vv8.4.0 安装Hexo：在安装好 git 和 node.js 之后就可以使用npm install -g hexo-cli 安装 hexo 了，可通过 hexo version 查看hexo 版本信息； 1234$ npm install -g hexo-cli$ hexo versionhexo-cli: 1.1.0os: Darwin 18.0.0 darwin x64 本系列教程基于操作系统 macOS Mojave，windows 或Linux 也可作参考。 搭建博客利用 Hexo 和 Github Pages 搭建博客的基本原理：首先通过 Hexo 将 Markdown 文件渲染生成静态网页，再将静态站点托管到 Github 仓库，利用 Github Pages 服务以网页形式显示仓库内容。 创建站点成功安装 Hexo 后，请执行下列命令，Hexo 将会在指定文件夹中新建所需要的文件： 1234# 在目标文件夹中创建新站点，Hexo会从hexo-starter代码库clone代码到目标文件夹$ hexo init &lt;folder&gt;$ cd &lt;folder&gt;$ npm install 创建站点后的文件目录： 12345678.├── node_modules ├── scaffolds ├── source ├── themes ├── _config.yml ├── package-lock.json └── package.json node_modules：node依赖包； scaffolds：模版文件夹，用户可自定义markdown模板，默认包含了以下三种模板 page：页面模板； post：文章模板； draft：草稿模板； source：资源文件夹，存放用户所有资源； themes：主题文件夹； _config.yml：在 Hexo 中有两份主要的配置文件，其名称都是 _config.yml。 其中，一份位于站点根目录下，主要包含 Hexo 本身的配置，称作站点配置文件；另一份位于主题目录下，这份配置由主题作者提供，主要用于配置主题相关的选项，称作主题配置文件； 生成静态文件使用 Hexo 生成静态文件快速而且简单： 1$ hexo generate Hexo 能够监视文件变动并立即重新生成静态文件，在生成时会比对文件的 SHA1 checksum，只有变动的文件才会写入：1$ hexo generate --watch 完成后部署，让 Hexo 在生成完毕后自动部署网站，以下四个命令的作用是相同的： 1234$ hexo generate --deploy$ hexo deploy --generate$ hexo g -d$ hexo d -g 调试站点Hexo 3.0 把服务器独立成了个别模块，您必须先安装 hexo-server 才能使用： 1$ npm install hexo-server --save 安装完成后，输入以下命令以启动服务器： 1$ hexo server 在服务器启动期间，Hexo 会监视文件变动并自动更新，您无须重启服务器，您的网站会在 http://localhost:4000 下启动，效果如下： 如果希望服务器只处理 public 文件夹内的文件，而不会处理文件变动，可以在静态模式下启动服务器，此模式通常用于生产环境（production mode）下： 1$ hexo server -s 如果希望以调试模式启动服务器： 1$ hexo server --debug 如果您想要更改端口，或是在执行时遇到了 EADDRINUSE 错误，可以在执行时使用 -p 选项指定其他端口，如下： 1$ hexo server -p 5000 部署站点将我们的站点部署到github上，需要三个步骤： 安装 hexo-deployer-git：该命令需在站点文件目录下执行； 1$ npm install hexo-deployer-git --save 修改配置：修改配置文件_config.yml中的 deploy 配置； 12345678910111213# type表示服务器类型，repo表示仓库地址，branch和message可省略deploy: type: git repo: &lt;repository url&gt; # https://bitbucket.org/JohnSmith/johnsmith.bitbucket.io branch: [branch] # published message: [message]# 您可同时使用多个 deployer，Hexo 会依照顺序执行每个 deployerdeploy: type: git repo: type: heroku repo: Hexo 一键部署：如果出现错误请先检查以上各步骤是否正确设置； 1$ hexo deploy 站点成功部署后，站点目录下的 public 文件夹会被同步到相应的 github 仓库中，可以在仓库的 Settings 下找到 GitHub Pages 网页的地址（默认为 &lt;github用户名&gt;.github.io），网页默认效果如下： 至此，一个简易的个人博客就搭建完成了。但是距离一个简洁大方、美观实用的博客系统还有很多地方需要优化。 Hexo 常用命令现将 Hexo 常用命令整理如下，更详细的说明参见Hexo官方文档-指令： 命令 简写 说明 hexo init [folder] 新建一个网站。如果没有设置 folder ，Hexo 默认在目前的文件夹建立网站 hexo new [layout] &lt;title&gt; hexo n [layout] &lt;title&gt; 新建一篇文章。如果没有设置 layout 的话，默认使用 _config.yml 中的 default_layout 参数代替。如果标题包含空格的话，请使用引号括起来 hexo generate hexo g 生成静态文件 -d, --deploy文件生成后立即部署网站-w, --watch监视文件变动 hexo publish [layout] &lt;filename&gt; 发表草稿，将草稿移动至_post文件夹 hexo server hexo s 启动服务器 -p, --port重设端口；-s, --static只使用静态文件；-l, --log启动日记记录，使用覆盖记录格式； hexo deploy hexo d 部署网站，-g, --generate部署之前预先生成静态文件 hexo clean 清除缓存文件 (db.json) 和已生成的静态文件 (public)。在某些情况（尤其是更换主题后），如果发现您对站点的更改无论如何也不生效，您可能需要运行该命令 hexo list 列出网站资料，Available types: page, post, route, tag, category hexo version 显示Hexo版本 hexo --draft 显示 source/_drafts 文件夹中的草稿文章 hexo migrate &lt;type&gt; 从其他博客系统 迁移内容 参考 Hexo官方文档 GitHub+Hexo 搭建个人网站详细教程 Hexo+Github Pages+Next博客搭建 mac环境下搭建hexo+github pages+next个人博客]]></content>
      <categories>
        <category>搭建博客</category>
      </categories>
      <tags>
        <tag>搭建博客</tag>
        <tag>教程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[费曼技巧：以教促学]]></title>
    <url>%2Funcategorized%2FLife%2F%E8%B4%B9%E6%9B%BC%E6%8A%80%E5%B7%A7%2F</url>
    <content type="text"><![CDATA[中学时，常常喜欢和别人讨论一些深奥的物理或哲学问题，这些在大多数人看来枯燥乏味的内容，在我却是甘之如饴。但要找到一位志同道合或是愿意倾听、一同讨论的人是很难的，为了维系本就为数不多的几位“听众”，在每次读书时，我总会无意识地假想自己该如何把当前所学讲给他们听，设想他们可能会提出怎样的质疑，我又该如何应答，通过这种方式，逼迫自己反复斟酌逻辑、简化表达，用尽可能简单的语言向他们解释清楚最深奥的道理……到了本科，对哲学的兴趣愈加浓厚，读了从古希腊到康德时期的很多经典著作，将所读所想整理成了两册笔记，笔记本便成了我的忠实听众……后来读了计算机方面的研究生，计算机领域知识浩如烟海且日新月异，好在整个行业奉行开源共享的精神，我也慢慢学会了将自己的学习笔记和生活感悟搬到博客上来…… 以上便是我在学生生涯的不同时期所采用的三种自学方法：讨论、笔记、博客。虽然不同时期所凭借的学习媒介不同，但却有着一脉相承的共同原则： 与“假想听众”一同学习，反复斟酌逻辑、简化表达，直至听众完全听懂，便是真正学会。 直至近日在网上看到了对“费曼技巧”的解释，才发现自己一直默默践行的学习方法原来早就有了名字，至于费曼本人是否提出过“费曼技巧”，网上以讹传讹，无从考证，当然名称本就无关紧要，重要的是其内涵是否指向一种行之有效的学习方法。 只有教会别人，才算真正学会 著名物理学家马克斯·普朗克因为量子物理学研究而获得了诺贝尔奖，他在德国各地演讲。每次他的司机都会无意聆听讲座，不久后他就将演讲摸索的一清二楚了，于是他提议，在下个城市演讲时可以进行角色互换。于是在下个演讲中，普朗克同意并伪装成司机坐在观众群中。司机站在他的位置进行演讲……演讲取得了圆满成功。全场响起热烈的欢呼声和掌声！司机能像普朗克本人一样绘声绘色地进行演讲。但是，一位教授询问有关这个报告中的内容。司机当然不知道如何回答这个问题！尽管如此，他还是保持冷静，并且回答道：“我很惊讶，您提出这么简单的问题，即使我的司机也知道，我会让他回答这个问题”。 你认为这个故事的主旨是什么呢？是的，司机的头脑非常敏捷。但这不是重点，这里描述了两种不同的知识： 真正的“普朗克知识”：普朗克能真正了解报告中内容的深层意思（knowing something），并能向一个原本不了解报告内容的人解释明白； 表面的“司机知识”：司机只是掌握了关于专业名称和关系方面的知识（knowing the name of something），他能在恰当的时刻运用这些句子，正确地在上下文中使用专业术语，并能准确无误地呈现内容，但涉及对内容的理解方面……他却什么都不懂； 表面的“司机知识”只能用于“鹦鹉学舌”，只有具备真正的“普朗克知识”才叫“真正学会”。不幸的是，大多数人都在专注于表面的“司机知识”而不自知，那么检验是否“真正学会”的标准又是什么呢？ 说他知道自己的想法但却无法表达的人, 通常并不知道他自己的想法。 -- Mortimer Adler 如果你不能解释它，就说明你还不够理解它。 -- Albert Einstein 也就是说：只有能教会别人，才说明掌握了真正的“普朗克知识”，才算真正学会了。 只有教会别人，才能真正学会这里我无意玩弄文字游戏，只怪汉语博大精深……“教会”和“学会”在上一句中表结果，意指能否教会别人是验证自己是否真正学会的标准；在本句中表过程，意指只有通过教会别人的这个过程，自己才能真正学会。 one can only learn by teaching. -- Wheeler 费曼技巧是一种“以教促学”的学习方法：在学习某个主题时，假想自己要把它教给一个对其毫无所知的人，思考应该如何表达，在思考过程中不断发现、弥补漏洞，组织、简化表达，以致不断加深对主题的理解，当把他教会的时候自己也就真正学会了。 费曼技巧的四个步骤人们一般将费曼技巧总结为四个步骤： 确定主题：选择一个你想了解的概念，写在纸上； 教给孩子：在白纸上写下你对这个概念的解释，就好像你正在教导一位新接触这个概念的学生一样。当你这样做的时候，你会更清楚地意识到关于这个概念你理解了多少，以及是否还存在理解不清的地方； 发现漏洞：无论何时你感觉卡壳了，都要回到原始的学习资料并重新学习让你感到卡壳的那部分，直到你领会得足够顺畅，顺畅到可以在纸上解释这个部分为止； 组织简化：最终的目的，是用你自己的语言，而不是学习资料中的语言来解释概念。如果你的解释很冗长或者令人迷惑，那就说明你对概念的理解可能并没有你自己想象得那么顺畅； 费曼技巧流程图： 费曼技巧的评价在我看来，践行“费曼技巧”有以下好处: 激发学习动力：“假想听众”的存在会产生学习的责任感和使命感； 自检学习效果：人易自欺欺人，“假想听众”可以更客观地检验学习的真实效果； 完善知识体系：发现、弥补漏洞的过程就是建立知识体系的过程； 训练表达能力：“说不出来”在于没有形成自己的一套逻辑体系和语言体系； 参考 费曼技巧：学习任何东西的最佳方式 费曼技巧视频 号称终极快速学习法的费曼技巧，究竟是什么样的学习方法？]]></content>
  </entry>
  <entry>
    <title><![CDATA[Hexo + Github Pages 博客搭建教程（二）：主题定制]]></title>
    <url>%2F%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%2FHexo%2FHexo%20%2B%20Github%20Pages%20%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%95%99%E7%A8%8B%EF%BC%88%E4%BA%8C%EF%BC%89%EF%BC%9A%E4%B8%BB%E9%A2%98%E5%AE%9A%E5%88%B6%2F</url>
    <content type="text"><![CDATA[Hexo 安装主题的方式非常简单，只需要将主题文件拷贝至站点目录的 themes 目录下，然后修改下配置文件即可。 这里推荐 Next 主题，该主题简洁大方美观，集成度高，定制性强，官方文档完备，十分适合新手使用。下面以 NexT 主题为例，介绍主题切换、配置方法。 NexT 主题安装下载主题NexT 主题下载有两种方式： 官网下载：前往 NexT 版本发布页面，找到 Source code 点击下载，下载后解压到站点目录的 themes 目录下； git clone：直接将源码克隆到站点目录的 themes 目录下； 12$ cd your-hexo-site$ git clone https://github.com/iissnan/hexo-theme-next themes/next 启用主题打开站点配置文件，找到 theme 字段，并将其值更改为 next 即可； 1theme: next 验证主题启动本地站点服务器，使用浏览器访问 http://localhost:4000 当你看到站点的外观与下图所示类似时即说明你已成功安装 NexT 主题，这是 NexT 默认的 Scheme —— Muse。 NexT 主题定制主题风格千变万化且各有所好，建议在充分参考 NexT 官方文档和他人博客风格的前提下，选择最适合自己的风格，总的原则是：简洁、实用。下面列举了 NexT 主题常见的定制方法，如果有更多定制需求，可自行百度。 说明： 配置文件：“站点配置文件”是指站点目录下的 _config.yml 文件，“主题配置文件”是指NexT 主题文件下的 _config.yml 文件。 配置生效：如果修改了站点配置文件，可能需要先清理静态文件，再重新生成静态文件，最后重新启动服务器，才会生效，如果还不行，可能需要多次生成部署； 123$ hexo clean$ hexo g$ hexo s 整体定制选择 SchemeNexT 提供了多种不同的外观风格 Scheme，几乎所有的配置都可以在 Scheme 之间共用，可通过更改 主题配置文件，搜索 Scheme 关键字，将你需用启用的 scheme 前面注释 # 去除即可。 将默认的 Muse 主题注释掉，取消 Mist 的注释 12345678# ---------------------------------------------------------------# Scheme Settings# ---------------------------------------------------------------# Schemes# scheme: Musescheme: Mist# scheme: Pisces# scheme: Gemini 查看新 Scheme 的效果: 设置语言观察以上截图，发现首页标签为英文，编辑站点配置文件，将 language 设置成你所需要的语言（简体中文对应zh-Hans）: 1language: zh-Hans 设置字体Hexo 默认字体大小为 14px，如果感觉字体太小，可打开 \themes\next\source\css\ _variables\base.styl文件，将 font-size-base 改成16px即可，如下所示： 12// Font size$font-size-base = 16px 更改网页图标 FaviconNexT 提供了默认的网页图标，可通过以下步骤进行自定义设置： 制作 Favicon 图标：可以先在 EasyIcon 中找到一张你所喜欢的 ico 图标，然后通过比特虫在线生成两张ico图标（大小分别为16x16和32x32），将图标放在 /themes/next/source/images ； 修改主题配置文件： 12345favicon: small: /images/feather-16x16.ico medium: /images/feather-32x32.ico apple_touch_icon: /images/apple-touch-icon-feather.png safari_pinned_tab: /images/apple-touch-icon-feather.png 修改后的实际效果： 隐藏网页底部 powered By Hexo在主题配置文件中，修改如下配置： 12345678# Hexo link (Powered by Hexo). powered: false theme: # Theme &amp; scheme info link (Theme - NexT.scheme). enable: false # Version info of NexT after scheme info (vX.X.X). version: true 修改网页头部图片 修改格式文件：打开 hexo\themes\next\source\css\_schemes\Mist\_header.styl，将第一行 background: 后的内容改为如下形式： 1.header &#123; background: url(&#x27;https://likeitea-1257692904.cos.ap-guangzhou.myqcloud.com/liketea_blog/top_pic.jpg&#x27;); &#125; 显示效果：并不简洁美观，建议采用默认背景 添加 README.md 文件如果希望在项目中添加 README.md 文件，又不想对该文件进行渲染，可在 Hexo 目录下的 source 根目录下添加一个 README.md 文件，修改站点配置文件 _config.yml，将 skip_render 参数的值设置为 1skip_render: README.md 保存退出即可，再次使用 hexo d 命令部署博客的时候就不会再渲染 README.md 这个文件了。 侧边栏定制侧边栏出现位置和时机默认情况下，侧栏仅在文章页面（拥有目录列表）时才显示，并放置于右侧位置。可以通过修改主题配置文件中的 sidebar 字段来控制侧栏的行为。侧栏的设置包括两个部分，其一是侧栏的位置， 其二是侧栏显示的时机。 设置侧边栏位置： 123# left 靠左，right靠右sidebar: position: left 设置侧边栏显示时机： 123# post是默认行为，在文章页显示；另外几个选项是always、hide、removesidebar: display: post 设置站点信息 设置头像：编辑 主题配置文件，修改字段 avatar，值设置成头像的链接地址，可以是完整URI，也可以是本地文件路径（站点目录下的 source/images 或者主题目录下的 source/uploads）。 1234# Sidebar Avatar# in theme directory(source/images): /images/avatar.gif# in site directory(source/uploads): /uploads/avatar.gifavatar: /images/avatar.png 设置作者昵称：编辑 站点配置文件， 设置 author 为你的昵称； 设置站点描述：编辑 站点配置文件， 设置 description 字段为你的站点描述。站点描述可以是你喜欢的一句签名； 建站时间：编辑 主题配置文件，新增字段 since:年份； 修改作者头像并旋转如果希望将作者头像修改为圆形，并且实现鼠标悬浮于图片上时图片旋转的效果，可按照以下步骤进行修改： 修改代码：打开\themes\next\source\css\_common\components\sidebar\sidebar-author.styl，在里面添加如下代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354.site-author-image &#123; display: block; margin: 0 auto; padding: $site-author-image-padding; max-width: $site-author-image-width; height: $site-author-image-height; border: $site-author-image-border-width solid $site-author-image-border-color; /* 头像圆形 */ border-radius: 80px; -webkit-border-radius: 80px; -moz-border-radius: 80px; box-shadow: inset 0 -1px 0 #333sf; /* 设置循环动画 [animation: (play)动画名称 (2s)动画播放时长单位秒或微秒 (ase-out)动画播放的速度曲线为以低速结束 (1s)等待1秒然后开始动画 (1)动画播放次数(infinite为循环播放) ]*/ /* 鼠标经过头像旋转360度 */ -webkit-transition: -webkit-transform 1.0s ease-out; -moz-transition: -moz-transform 1.0s ease-out; transition: transform 1.0s ease-out;&#125;img:hover &#123; /* 鼠标经过停止头像旋转 -webkit-animation-play-state:paused; animation-play-state:paused;*/ /* 鼠标经过头像旋转360度 */ -webkit-transform: rotateZ(360deg); -moz-transform: rotateZ(360deg); transform: rotateZ(360deg);&#125;/* Z 轴旋转动画 */@-webkit-keyframes play &#123; 0% &#123; -webkit-transform: rotateZ(0deg); &#125; 100% &#123; -webkit-transform: rotateZ(-360deg); &#125;&#125;@-moz-keyframes play &#123; 0% &#123; -moz-transform: rotateZ(0deg); &#125; 100% &#123; -moz-transform: rotateZ(-360deg); &#125;&#125;@keyframes play &#123; 0% &#123; transform: rotateZ(0deg); &#125; 100% &#123; transform: rotateZ(-360deg); &#125;&#125; 效果如图： 设置RSSNexT 中 RSS 有三个设置选项，满足特定的使用场景，更改 主题配置文件：设定 rss 字段的值： false：禁用 RSS，不在页面上显示 RSS 连接。 留空：使用 Hexo 生成的 Feed 链接，你可以需要先安装 hexo-generator-feed 插件。 1$ npm install hexo-generator-feed 具体的链接地址：适用于已经烧制过 Feed 的情形。 重启服务器，侧边栏效果如图： 侧边栏社交链接侧边栏社交链接的修改包括链接和图标两部分，打开主题配置文件： 123456789101112131415# 社交链接，键值格式是“显示文本: 链接地址 || 图标名”social: GitHub: https://github.com/your-user-name Twitter: https://twitter.com/your-user-name 微博: http://weibo.com/your-user-name 豆瓣: http://douban.com/people/your-user-name 知乎: http://www.zhihu.com/people/your-user-name # 社交图标，键值格式是“显示文本: Font Awesome 图标名称”，enable 选项用于控制是否显示图标social_icons: enable: true # Icon Mappings GitHub: github Twitter: twitter 微博: weibo 侧边友情链接友情链接可用于提供一些关联网站地址，或者推荐文章。 编辑 主题配置文件：友情链接包含两个部分，① 友情链接名称、图标；② 每个友情链接的名称、地址； 12345678910# Blog rolls# 友情链接本身的名字和图标links_icon: planelinks_title: 友情链接# 友情链接显示格式：竖、横links_layout: block#links_layout: inline# 友情链接名称、地址links: likew: https://likew.bitcron.com 侧边栏最终效果： 首页定制设置菜单菜单配置包括三个部分： 菜单项：名称和链接，名称指在内部文件中使用的变量名； 菜单项文本：菜单在页面上显示的文字； 菜单项图标：NexT 主题默认集成了识别Font Awesome图标的方法，只需要在里面找到想要图标的名称，去掉前缀icon- 就可以拿过来直接使用，可以满足绝大的多数的场景，同时无须担心在 Retina 屏幕下 图标模糊的问题； 编辑主题配置文件，修改以下内容： 设定菜单项和菜单项图标：menu下的设置项格式为菜单项名称: 链接路径 || 菜单项图标； 12345678menu: home: / || home about: /about/ || user tags: /tags || tags categories: /categories/ || th archives: /archives/ || archive schedule: /schedule/ || calendar sitemap: /sitemap.xml || sitemap 设置菜单项文本：如果要修改菜单项文本，可打开 NexT 主题目录下的 languages/&#123;language&#125;.yml（{language} 为你所使用的语言），进行编辑修改； 123456789menu: home: 首页 archives: 归档 categories: 分类 tags: 标签 about: 关于 search: 搜索 commonweal: 公益404 something: 有料 保留你需要的菜单项后，实际效果如图： 添加标签页面新建“标签”页面，并在菜单中显示“标签”链接，在标签页展示站点所有标签，如果所有文章都没有标签则标签页将是空的。 配置方法： 新建页面：在终端窗口下，定位到 Hexo 站点目录下，使用 hexo new page 新建一个页面，命名为 tags ； 123456789# 切换到站点目录$ cd your-hexo-site# 新建tags页面，会在source目录下自动生成tags文件夹$ hexo new page tags# tags目录下包含index文件夹和index.md文件$ tree ./source/tags ./source/tags ├── index └── index.md 设置页面类型：编辑刚才新建的index.md文件，将页面类型设置为tags， 主题将自动为这个页面显示标签云，一般标签页不显示评论； 123456---title: 标签date: 2019-03-12 16:15:10type: &quot;tags&quot;comments: false--- 修改菜单：在菜单中添加链接，编辑主题配置文件，添加 tags 到 menu 中，如下: 1234menu: home: / archives: /archives tags: /tags 设置文章标签：在文章 front-matter 添加 tags 字段，可设置文章的标签： 12345678---title: 测试标签页date: 2019-03-12 16:34:39tags: - 标签1 - 标签2 - 标签3--- 实际效果：启动本地站点服务器，使用浏览器访问 http://localhost:4000 ，标签页如图所示。 添加分类页面“分类”和“标签”都可以用来对文章进行管理，区别在于分类多级、有序而标签同级、无序。分类是一种更加精细的文件管理方式，可以按照主题结构树来管理文章，标签则是一种平面化的管理方式，适合于快速定位到一些关键词。 新建页面：在站点目录下创建分类页面 12$ cd your-hexo-site$ hexo new page categories 设置页面类型：打开 categories/index.md 文件，修改页面类型，一般分类页面不显示评论： 12345title: 分类date: 2014-12-22 12:39:04type: &quot;categories&quot;comments: false--- 修改菜单：打开主题配置文件，修改 menu 下的categories 为 /categories 1234menu: home: / archives: /archives categories: /categories 实际效果：分类页面显示分类树结构，但不会显示具体分类下的文章。点击具体的分类类别，可显示该类别下的所有文章： 首页文章以摘要形式显示 修改主题配置文件：打开主题配置文件，找到如下位置，修改 1234# 其中length代表显示摘要的截取字符长度auto_excerpt: enable: true length: 150 修改后的效果如图： 设置首页文章显示篇数 安装插件： 123npm install --save hexo-generator-indexnpm install --save hexo-generator-archivenpm install --save hexo-generator-tag 修改站点配置文件：在 站点配置文件 中，添加如下内容，其中 per_page 字段是你希望设定的显示篇数。index，archive 及 tag 开头分表代表主页，归档页面和标签页面 123456789101112index_generator: path: &#x27;&#x27; per_page: 10 order_by: -date # 反之，date时间早的在前面archive_generator: per_page: 20 yearly: true monthly: truetag_generator: per_page: 10 文章页定制设置代码高亮NexT 使用 Tomorrow Theme 作为代码高亮，共有5款主题供你选择。 NexT 默认使用的是 白色的 normal 主题，可选的值有 normal，night， night blue， night bright， night eighties： 更改 highlight_theme 字段，将其值设定成你所喜爱的高亮主题（默认的就挺好），例如： 12345# Code Highlight theme# Available value:# normal | night | night eighties | night blue | night bright# https://github.com/chriskempson/tomorrow-themehighlight_theme: normal 修改文章底部带 # 号的标签修改模板 /themes/next/layout/_macro/post.swig，搜索 rel=&quot;tag&quot;&gt;#，将 # 换成&lt;i class=&quot;fa fa-tag&quot;&gt;&lt;/i&gt;，效果如图： 文章顶部显示更新时间123456# Post meta display settings 文章显示信息post_meta: item_text: true created_at: true updated_at: true categories: true 开启打赏功能NexT 支持微信打赏和支付宝打赏，只需要在 主题配置文件 中填入微信或支付宝收款二维码图片地址（绝对地址从source目录起）即可开启该功能。 123reward_comment: 坚持原创技术分享，您的支持将鼓励我继续创作！wechatpay: /path/to/wechat-reward-imagealipay: /path/to/alipay-reward-image 实际效果: 设置动画效果设置加载动画NexT 默认开启动画效果，效果使用 JavaScript 编写，因此需要等待 JavaScript 脚本完全加载完毕后才会显示内容。 如果您比较在乎速度，可以将 use_motion 设置为 false 来关闭加载动画； 添加顶部加载条修改主题配置文件 _config.yml 将 pace设为 true 就行了，你还可以换不同样式的加载条，如下图： 1234567891011121314151617181920212223# Progress bar in the top during page loading.pace: true# Themes list:#pace-theme-big-counter# 跳跳球#pace-theme-bounce#pace-theme-barber-shop# 原子转#pace-theme-center-atom#pace-theme-center-circle#pace-theme-center-radar# 进度条#pace-theme-center-simple#pace-theme-corner-indicator#pace-theme-fill-left#pace-theme-flash#pace-theme-loading-bar# pace-theme-mac-osx# 简约式#pace-theme-minimal# For example# pace_theme: pace-theme-center-simplepace_theme: pace-theme-minimal 实际效果： 点击出现红心效果 添加 js 文件：在 hexo\themes\next\source\js\src\ 目录下新增文件 love.js，文件内容如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546!function(e, t, a) &#123; function n() &#123; c(&quot;.heart&#123;width: 10px;height: 10px;position: fixed;background: #f00;transform: rotate(45deg);-webkit-transform: rotate(45deg);-moz-transform: rotate(45deg);&#125;.heart:after,.heart:before&#123;content: &#x27;&#x27;;width: inherit;height: inherit;background: inherit;border-radius: 50%;-webkit-border-radius: 50%;-moz-border-radius: 50%;position: fixed;&#125;.heart:after&#123;top: -5px;&#125;.heart:before&#123;left: -5px;&#125;&quot;), o(), r() &#125; function r() &#123; for (var e = 0; e &lt; d.length; e++) d[e].alpha &lt;= 0 ? (t.body.removeChild(d[e].el), d.splice(e, 1)) : (d[e].y--, d[e].scale += .004, d[e].alpha -= .013, d[e].el.style.cssText = &quot;left:&quot; + d[e].x + &quot;px;top:&quot; + d[e].y + &quot;px;opacity:&quot; + d[e].alpha + &quot;;transform:scale(&quot; + d[e].scale + &quot;,&quot; + d[e].scale + &quot;) rotate(45deg);background:&quot; + d[e].color + &quot;;z-index:99999&quot;); requestAnimationFrame(r) &#125; function o() &#123; var t = &quot;function&quot; == typeof e.onclick &amp;&amp; e.onclick; e.onclick = function(e) &#123; t &amp;&amp; t(), i(e) &#125; &#125; function i(e) &#123; var a = t.createElement(&quot;div&quot;); a.className = &quot;heart&quot;, d.push(&#123; el: a, x: e.clientX - 5, y: e.clientY - 5, scale: 1, alpha: 1, color: s() &#125;), t.body.appendChild(a) &#125; function c(e) &#123; var a = t.createElement(&quot;style&quot;); a.type = &quot;text/css&quot;; try &#123; a.appendChild(t.createTextNode(e)) &#125; catch (t) &#123; a.styleSheet.cssText = e &#125; t.getElementsByTagName(&quot;head&quot;)[0].appendChild(a) &#125; function s() &#123; return &quot;rgb(&quot; + ~~(255 * Math.random()) + &quot;,&quot; + ~~(255 * Math.random()) + &quot;,&quot; + ~~(255 * Math.random()) + &quot;)&quot; &#125; var d = []; e.requestAnimationFrame = function() &#123; return e.requestAnimationFrame || e.webkitRequestAnimationFrame || e.mozRequestAnimationFrame || e.oRequestAnimationFrame || e.msRequestAnimationFrame || function(e) &#123; setTimeout(e, 1e3 / 60) &#125; &#125;(), n()&#125;(window, document); 修改配置：在文件 hexo\themes\next\layout\_layout.swig 底部的 &lt;/body&gt; 标签上一行增加： 12&lt;!-- 页面点击小红心 --&gt;&lt;script type=&quot;text/javascript&quot; src=&quot;/js/src/love.js&quot;&gt;&lt;/script&gt; 实际效果: 设置背景动画编辑 主题配置文件， 搜索 canvas_nest 或 three_waves，根据您的需求设置值为 true 或者 false 即可（会干扰读者注意力，建议关闭）： 12345678# Canvas-nestcanvas_nest: true# three_waves# three_waves: true# canvas_lines# canvas_lines: true# canvas_sphere# canvas_sphere: True Canvas-nest 的实际效果： Next 的其他样式设置 去除 Next 主题图片的灰色边框 Hexo下表格的美化和优化 Hexo之NexT主题代码块MacPanel特效配置 表格宽度根据内容自适应：可以尝试将 source/css/_common/scaffolding/tables.styl 中的 table-layout: fixed; 改为 table-layout: auto; 第三方插件使用静态站点拥有一定的局限性，因此我们需要借助于第三方服务来扩展站点的功能。 以下是 NexT 目前支持的第三方服务，你可以根据你的需求集成一些功能进来。 添加站内搜索配置方法如下： 安装 hexo-generator-searchdb，在站点的根目录下执行以下命令： 1npm install hexo-generator-searchdb --save 编辑站点配置文件，新增以下内容到任意位置： 12345search: path: search.xml field: post format: html limit: 10000 编辑主题配置文件，启用本地搜索功能： 123# Local searchlocal_search: enable: true 站点搜索效果： 添加评论区NexT 支持多款评论系统： 多说：已挂； 畅言：需要备案； Disqus：已墙； valine：LeanCloud提供的后端云服务，可用于统计网址访问数据，分为开发版和商用版，只需要注册生成应用App ID和App Key即可使用； 来必力：来自韩国，使用邮箱注册； Gitment：一款基于 GitHub Issues 的评论系统，支持在前端直接引入，不需要任何后端代码。可以在页面进行登录、查看、评论、点赞等操作，同时有完整的 Markdown / GFM 和代码高亮支持，尤为适合各种基于GitHub Pages的静态博客或项目页面； DISQUS编辑 主题配置文件， 将 disqus 下的 enable 设定为 true，同时提供您的 shortname，count 用于指定是否显示评论数量： 1234disqus: enable: false shortname: count: true 需要注册 disqus 账号后才可参与评论，效果如下： gitmentGitment 使用 GitHub Issues 作为评论系统，在接入 Gitment 前，需要获得 GitHub 的授权，获得相应的客户端id和客户端私钥，以备站点使用。gitment 的配置方法: 创建 oAuth App ：github首页 &gt; settings &gt; Developer settings &gt; OAuth Apps &gt; New oAuth App，Homepage URL 和 Authorization callback URL 都写上你的 github 博客首页地址（如果绑定了个人域名，则写完整的新域名地址），比如 https://liketea.xyz，点击 Register application即可完成注册，生成 Client ID 和 Client Secret； 修改 主题配置文件 ：最重要的就是 github_repo，可以是你托管博客静态文件的仓库，也可以是新仓库，注意是仓库名称而不是仓库地址； 12345678910111213gitment: enable: true mint: true # RECOMMEND, A mint on Gitment, to support count, language and proxy_gateway count: true # Show comments count in post meta area lazy: false # Comments lazy loading with a button cleanly: true # Hide &#x27;Powered by ...&#x27; on footer, and more language: zh-CN # Force language, or auto switch by theme github_user: liketea # MUST HAVE, Your Github ID github_repo: liketea.github.io # MUST HAVE, The repo you use to store Gitment comments client_id: a52dc...0f3b156f7c # MUST HAVE, Github client id for the Gitment client_secret: 2307d156a...a8495b1b68a3a3ae # EITHER this or proxy_gateway, Github access secret token for the Gitment proxy_gateway: # Address of api proxy, See: https://github.com/aimingoo/intersect redirect_protocol: # Protocol of redirect_uri with force_redirect_protocol when mint enabled 重新部署：通过 hexo g -d 重新部署站点，进入一篇文章的评论区； 登入授权：点击登入，对评论区进行授权； 解决无法登陆的问题：如果点击授权之后，评论区一直在转圈圈，但是登录不进去 打开浏览器的调试功能，发现报了个错误~点击后面的网址，一路点击高级、继续前往 然后你会发现依旧访问不了，不过不用理会，此时gitment已经可以登录啦~ 最后初始化评论：每一篇文章都需要进行初始化才能开始评论，目前还没有较好的一键初始化方法； 来必力以上评论系统，要么已经挂掉了、要么被墙了、要么各种BUG、要么原始界面奇丑，通过各种尝试发现还是来必力最好用： 注册登陆 来必力 获取你的 LiveRe UID：点击安装免费的city版本，安装成功后点击代码管理，复制其中的 data-uid 字段 编辑主题配置文件，填写 livere_uid：将复制的 data-uid 12# You can get your uid from https://livere.com/insight/myCode (General web site)livere_uid: &quot;MTAyMC80M...Tc0NQ==&quot; 实际效果：来必力支持使用已有社交网站(SNS)账号登录 添加统计分析统计文章阅读次数LeanCloud 可以统计单篇文章阅读次数，配置过程如下: 注册 LeanCloud：完成邮箱激活，进入控制台页面； 创建应用：创建一个新应用，名称无所谓，点击应用进入； 创建名称为Counter的Class：名称必须为Counter； 查看应用 key：在所创建的应用的设置-&gt;应用key 中查看 app_id 和app_key ； 修改主题配置文件： 1234leancloud_visitors: enable: true app_id: m135LmdEWo9GrD-gzGzoHsz app_key: CWheAQhgeEYEa1nDYn 重新部署 Hexo 博客：重新部署后便可以在博客主页以及每篇文章中显示阅读次数，如图所示 说明： 为了安全，设置网站的安全域名：设置——安全中心——Web安全域名； 记录文章访问量的唯一标识符是文章的发布日期以及文章的标题，因此请确保这两个数值组合的唯一性，如果你更改了这两个数值，会造成文章阅读数值的清零重计； 添加文章阅读量排行榜 新建排行页面：在根目录路径下，执行 hexo new page &quot;rank&quot;； 编辑主题配置文件：加上菜单 rank 和它的 icon: 12menu: rank: /rank/ || signal 在语言文件中加上菜单 rank，以中文为例，在 /themes/next/languages/zh_Hans.yml 中添加： 1234567891011menu: home: 首页 archives: 归档 categories: 分类 tags: 标签 about: 关于 search: 搜索 schedule: 日程表 sitemap: 站点地图 commonweal: 公益404 rank: 排行榜 编辑~/source/top/index.md：必须将里面的里面的 app_id 和 app_key 替换为你的 LeanCloud 在主题配置文件中的值；必须替换里面博客的链接；1000 是显示文章的数量，其它可以自己看情况更改； 12345678910111213141516171819202122232425262728---title: 排行榜comments: false---&lt;div id=&quot;hot&quot;&gt;&lt;/div&gt;&lt;script src=&quot;https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js&quot;&gt;&lt;/script&gt;&lt;script&gt;AV.initialize(&quot;app_id&quot;, &quot;app_key&quot;);&lt;/script&gt;&lt;script type=&quot;text/javascript&quot;&gt; var time=0 var title=&quot;&quot; var url=&quot;&quot; var query = new AV.Query(&#x27;Counter&#x27;); query.notEqualTo(&#x27;id&#x27;,0); query.descending(&#x27;time&#x27;); query.limit(1000); query.find().then(function (todo) &#123; for (var i=0;i&lt;1000;i++)&#123; var result=todo[i].attributes; time=result.time; title=result.title; url=result.url; var content=&quot;&lt;p&gt;&quot;+&quot;&lt;font color=&#x27;#1C1C1C&#x27;&gt;&quot;+&quot;【文章热度:&quot;+time+&quot;℃】&quot;+&quot;&lt;/font&gt;&quot;+&quot;&lt;a href=&#x27;&quot;+&quot;https://liketea.github.io/&quot;+url+&quot;&#x27;&gt;&quot;+title+&quot;&lt;/a&gt;&quot;+&quot;&lt;/p&gt;&quot;; document.getElementById(&quot;hot&quot;).innerHTML+=content &#125; &#125;, function (error) &#123; console.log(&quot;error&quot;); &#125;);&lt;/script&gt; 实际效果： 统计站点访问次数NexT 集成了“不蒜子服务”，可以统计站点 uv（全站访客人次）、pv（全站点击次数）和单页面 pv，配置更加方便。 编辑 主题配置文件 中的 busuanzi_count 配置项：当 enable: true 时，代表开启全局开关，若site_uv 、site_pv 、page_pv 的值均为 false 时，不蒜子仅作记录而不会在页面上显示 123456789101112131415busuanzi_count: # count values only if the other configs are false enable: true # custom uv span for the whole site site_uv: true site_uv_header: &lt;i class=&quot;fa fa-user&quot;&gt;&lt;/i&gt; site_uv_footer: 人次 # custom pv span for the whole site site_pv: true site_pv_header: &lt;i class=&quot;fa fa-eye&quot;&gt;&lt;/i&gt; site_pv_footer: 次 # custom pv span for one page only page_pv: false page_pv_header: &lt;i class=&quot;fa fa-eye&quot;&gt;&lt;/i&gt; page_pv_footer: 次 禁用单页面pv：不蒜子的 单页面pv 默认显示在文章标题下边，但是却不会在站点主页显示，因此我们还是选择用 LeanCloud 来显示 单页面pv 而使用不蒜子来显示 站点uv, pv ，实际效果如图所示： 如果发现页面中的统计数据都不显示： 那是因为不蒜子统计的js文件找不到了，官方给出了相应的方法，即只需要更改 next 主题下的不蒜子 插件的 js 引用链接即可。进入 hexo 博客项目的 themes 目录下，在 next 主题目录中的 layout/_third-party/analytics/ 下找到 busuanzi-counter.swig 文件，将: 1&lt;script async src=&quot;https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js&quot;&gt;&lt;/script&gt; 替换为如下代码既可： 12&lt;script async src=&quot;//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js&quot;&gt;&lt;/script&gt; 统计文章字数 安装插件:在根目录下安装 hexo-wordcount，运行： 1$ npm install hexo-wordcount --save 修改主题配置文件：然后在主题的配置文件中，配置如下 12345678# Post wordcount display settings# Dependencies: https://github.com/willin/hexo-wordcountpost_wordcount: item_text: true # 是否显示项目文字 wordcount: true # 是否显示统计字数 min2read: true # 是否显示阅读时长（分钟） totalcount: true # 是否显示总字数 separated_meta: true 实际效果: 添加分享服务jiathis 已关闭服务，我采用了 needmoreshare2，其配置过程如下： 编辑主题配置文件中的 needmoreshare2 ：将enable 设置为 true： 123456789101112131415161718needmoreshare2: enable: true # 底部提交分享按钮 postbottom: enable: false options: iconStyle: box boxForm: horizontal position: bottomCenter networks: Weibo,Wechat,Douban,QQZone,Twitter,Facebook # 左侧悬浮分享按钮 float: enable: true options: iconStyle: box boxForm: horizontal position: topRight networks: Weibo,Wechat,Douban,QQZone,Twitter,Facebook 实际效果如图，包含了微博、微信、QQ、豆瓣等众多渠道： 以上只有微信点击无法分享，只需修改themes\next\source\lib\needsharebutton\needsharebutton.js： 1234# 把var imgSrc = &quot;https://api.qinco.me/api/qr?size=400&amp;content=&quot; + encodeURIComponent(myoptions.url);# 改为var imgSrc = &quot;http://api.qrserver.com/v1/create-qr-code/?size=150x150&amp;data=&quot; + encodeURIComponent(myoptions.url); 该分享方式为生成博客的二维码，手机扫码之后即可分享: 为博客添加其他功能文章加密访问 修改代码：打开主题文件夹/layout/_partials/head.swig在首句后面插入以下代码： 12345678910&lt;script&gt; (function()&#123; if(&#x27;&#123;&#123; page.password &#125;&#125;&#x27;)&#123; if (prompt(&#x27;请输入文章密码&#x27;) !== &#x27;&#123;&#123; page.password &#125;&#125;&#x27;)&#123; alert(&#x27;密码错误&#x27;); history.back(); &#125; &#125; &#125;)();&lt;/script&gt; 为文章设置密码：在需要加密的文章里加进password: 你要设的密码，像这样： 1234567---title: 13date: 2019-03-14 21:06:14tags:categories:password: 123456--- 实际效果：点击文章，需要输入正确密码 文章置顶 修改 hero-generator-index 插件：把文件：node_modules/hexo-generator-index/lib/generator.js 内的代码替换为： 1234567891011121314151617181920212223242526272829&#x27;use strict&#x27;;var pagination = require(&#x27;hexo-pagination&#x27;);module.exports = function(locals)&#123; var config = this.config; var posts = locals.posts; posts.data = posts.data.sort(function(a, b) &#123; if(a.top &amp;&amp; b.top) &#123; // 两篇文章top都有定义 if(a.top == b.top) return b.date - a.date; // 若top值一样则按照文章日期降序排 else return b.top - a.top; // 否则按照top值降序排 &#125; else if(a.top &amp;&amp; !b.top) &#123; // 以下是只有一篇文章top有定义，那么将有top的排在前面（这里用异或操作居然不行233） return -1; &#125; else if(!a.top &amp;&amp; b.top) &#123; return 1; &#125; else return b.date - a.date; // 都没定义按照文章日期降序排 &#125;); var paginationDir = config.pagination_dir || &#x27;page&#x27;; return pagination(&#x27;&#x27;, posts, &#123; perPage: config.index_generator.per_page, layout: [&#x27;index&#x27;, &#x27;archive&#x27;], format: paginationDir + &#x27;/%d/&#x27;, data: &#123; __index: true &#125; &#125;);&#125;; 在文章中添加 top 值：数值越大文章越靠前 1234567---title: 13date: 2017-05-22 22:45:48tags: categories: top: 100--- 添加网易云音乐我将网易云音乐播放器放在了侧边栏，想要听的朋友可以手动点击播放，配置方法如下： 生成外链播放器：在网页版网易云音乐中搜索我们想要插入的音乐或歌单，然后点击“生成外链播放器” 设置ifame插件参数：选择 310x90，取消勾选自动播放 插入代码：将网易云音乐插件生成的 HDML 代码插入到文件 themes\next\layout\_macro\sidebar.swig中 &lt;/aside&gt;上一行，类似于以下代码 12345# auto=0 禁止网页打开自动播放&lt;div id=&quot;music163player&quot;&gt; &lt;iframe frameborder=&quot;no&quot; border=&quot;0&quot; marginwidth=&quot;0&quot; marginheight=&quot;0&quot; width=280 height=86 src=&quot;//music.163.com/outchain/player?type=2&amp;id=38358214&amp;auto=0&amp;height=66&quot;&gt; &lt;/iframe&gt;&lt;/div&gt; 实际效果：可操作播放、暂停，上/下一曲，如果尺寸是 310x413 则可以展开播放列表 参考 hexo的next主题个性化教程:打造炫酷网站 NexT官方文档 NexT主题个性化设置]]></content>
      <categories>
        <category>搭建博客</category>
      </categories>
      <tags>
        <tag>搭建博客</tag>
        <tag>教程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo + Github Pages 博客搭建教程（三）：文章写作]]></title>
    <url>%2F%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%2FHexo%2FHexo%20%2B%20Github%20Pages%20%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%95%99%E7%A8%8B%EF%BC%88%E4%B8%89%EF%BC%89%EF%BC%9A%E6%96%87%E7%AB%A0%E5%86%99%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[写作流程使用 HEXO 写作博客的一般流程： 创建文章：通过 hexo new [layout] &lt;title&gt; 创建一篇新的文章； 编辑文章：通过本地 Markdown 编辑器完成博客内容的写作； 发布文章：将博客发布到你的服务器； 创建文章你可以通过下列命令来创建一篇新文章： 1$ hexo new [layout] &lt;title&gt; 布局Hexo 有三种默认布局：post、page 和 draft，它们分别对应不同的路径，而您自定义的其他布局和 post 相同，都将储存到 source/_posts 文件夹： 布局 翻译 路径 post 发表 source/_post page 页面 source draft 草稿 source/_drafts 草稿草稿（draft）默认不会显示在页面中，但你可以通过以下三种方式中的任意一种来预览草稿： 发布草稿：执行 hexo publish [layout] &lt;title&gt; 可以将草稿从source/_drafts 文件夹移动到 source/_posts 文件夹； 在执行时加上 --draft 参数； 把 render_drafts 参数设为 true ：这将在页面中显示全部草稿，一般不这么用； 模板在新建文章时，Hexo 会根据 scaffolds 文件夹内相对应的文件来建立文件，例如： 1$ hexo new photo &quot;My Gallery&quot; 在执行这行指令时，Hexo 会尝试在 scaffolds 文件夹中寻找 photo.md，并根据其内容建立文章，以下是您可以在模版中使用的变量： 变量 描述 layout 布局 title 标题 date 文件建立日期 文件名Hexo 默认以标题做为文件名称，但您可编辑站点配置文件中的 new_post_name 参数来改变默认的文件名称，举例来说，设为 :year-:month-:day-:title.md 可让您更方便的通过日期来管理文章： 变量 描述 :title 标题（小写，空格将会被替换为短杠） :year 建立的年份，比如， 2015 :month 建立的月份（有前导零），比如， 04 :i_month 建立的月份（无前导零），比如， 4 :day 建立的日期（有前导零），比如， 07 :i_day 建立的日期（无前导零），比如， 7 编辑文章HEXO 文章编写采用 Markdown 语法进行写作，一级标题太大，建议从二级标题开始。 Front-matterFront-matter 是指文章头部以 --- 分隔的区域，用于指定个别文件的变量。以下是预先定义的参数，您可在模板中使用这些参数值并加以利用： 参数 描述 默认值 layout 布局 title 标题 date 建立日期 文件建立日期 updated 更新日期 文件更新日期 comments 开启文章的评论功能 true tags 标签（不适用于分页） categories 分类（不适用于分页） permalink 覆盖文章网址 只有文章支持分类和标签，分类具有顺序性和层次性，以下是文章分类和标签的一个例子： 12345678# 文章在Diary/Life下categories: - Diary - Life# 文章有两个标签PS3、Gamestags: - PS3 - Games 一个文章可以有多个标签，多个分类，用中括号就可以达到并列效果: 1234# 文章同时在Diary类别和Life类别下categories: - [Diary] - [Life] 插入图片俗话说“一图胜千言”，但在博客中插入图片一直是件让人头疼的事，在博客中插入图片大体有两种方式： 作为本地文件引用：首先将图片同代码文件一起上传至站点服务器，然后以相对路径形式引用；HEXO 提供了三种图片引用方式： 将图片放在 source/images 文件夹中。然后通过类似于 ![](/images/image.jpg) 的方法访问它们； 将 _config.yml 文件中的 post_asset_folder 选项设为 true ，Hexo将会在你每一次通过 hexo new [layout] &lt;title&gt; 命令创建新文章时自动创建一个文件夹，这个资源文件夹将会有与这个 markdown 文件一样的名字，将所有与你的文章有关的资源放在这个关联文件夹中之后，你可以通过相对路径来引用它们； 以上两种方式会导致图片无法在主页或归档页显示的问题，为此， HEXO) 推出了使用标签插件来引用图片的方式，但这种方式的可移植性不强，也不推荐； 作为外链引用：首先将图片上传至图床（储存图片的服务器）生成外链，然后以外链形式引用；这种方式可大大简化图片管理，节约宝贵的服务器资源，加快图片加载速度，推荐使用； 发布文章发布文章很简单，只需要经过“生成”和“部署”两步就行了： 12$ hexo g$ hexo d 你也可以通过以下一行命令来完成“生成”和“部署”两个过程： 1$ hexo g -d 如果你希望在正式发布前，先在本地查看发布后的实际效果并进行调试，也可以在“生成”步骤之后，通过以下命令打开本地服务器，您的网站会在 http://localhost:4000 下启动。在服务器启动期间，Hexo 会监视文件变动并自动更新，您无须重启服务器。 123$ hexo s# 或者进入调试模式$ hexo s --debug 如果你的文章符合以下两种情况之一，那么以上过程将不会对你的文章进行处理，你的文章也就不会被发布出去： 文章存放在草稿文件夹 source/_drafts 中； 文章名称被添加到站点配置文件中的 skip_render； 在实际写作中，更常用的写作流程如下： 123456789# 1. 创新草稿$ hexo n draft &lt;title&gt;# 2. 编辑文章# 3. 发布文章$ hexo publish &lt;title&gt;# 4. 渲染文章$ hexo g# 5. 部署文章$ hexo d 写作工具“工欲善其事，必先利其器”，好的写作工具能够让博客写作变得更简单、更有趣。 Mweb 编辑器Mweb 是 Mac上一款非常好用的 Markdown 写作、记笔记、静态博客生成软件，极力推荐。 Mweb 有以下一些优点: 简洁的外观； 丰富的快捷键； 丰富的扩展语法； 轻松编辑表格； 图片的快速插入与统一管理； 图片自动上传图床功能； 可设置图片居左/中/右； 一键生成/上传博客； 支持各种导出格式； 图床国内外有众多图床可供选择，图床可分为两种： 公共图床：利用公共服务的图片上传接口，来提供图片外链的服务，比如 Imgur 图床、微博图床、SM.MS 图床等等； 私有图床：利用各大云服务商提供的存储空间或者自己在 VPS 上使用开源软件来搭建图床，如七牛云、又拍云、阿里云 OSS、腾讯云 COS、自建图床工具 Lychee，此外 Github 仓库也可作为少量图片的图床使用，但不推荐这么做； 关于各种图床的优劣可以参考文章盘点一下免费好用的图床，公共图床不可控也不安全、私有图床大多需要网站备案，在尝试了多个图床之后，我最后选择了腾讯云 COS 。 腾讯 COS 配置通过腾讯云&gt;控制台&gt;对象存储&gt;存储同列表&gt;创建存储桶，如下： 有两个关键的配置不能忽略： 设置访问权限：将访问权限应设置为公有读私有写； 设置防盗链：开启之后即使其他人获取到链接也无法访问相应图片； PicGo图片上传工具PicGo 是一款开源跨平台的免费图片上传工具以及图床相册管理软件，它能帮你快速地将图片上传到微博、又拍云、阿里云 OSS、腾讯云 COS、七牛、GitHub、sm.ms、Imgur 等常见的免费图床网站或云存储服务上，并自动复制图片的链接到剪贴板里，使用上非常高效便捷。 关于 PicGo 的下载安装和配置使用的详细过程请参照PicGo指南。PicGo 支持剪切板上传和拖拽上传两种方式： 以PicGo配置腾讯COS为例： 参考 腾讯对象存储 PicGo指南]]></content>
      <categories>
        <category>搭建博客</category>
      </categories>
      <tags>
        <tag>搭建博客</tag>
        <tag>教程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo + Github Pages 博客搭建教程（四）：域名绑定]]></title>
    <url>%2F%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%2FHexo%2FHexo%20%2B%20Github%20Pages%20%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%95%99%E7%A8%8B%EF%BC%88%E5%9B%9B%EF%BC%89%EF%BC%9A%E5%9F%9F%E5%90%8D%E7%BB%91%E5%AE%9A%2F</url>
    <content type="text"><![CDATA[购买域名Github Pages 的默认域名是 &lt;your_name&gt;.github.io， 如果想将默认域名修改为个人域名，首先你要拥有一个自己的域名，如果还没有，可在以下几个网站购买： 阿里云域名注册 腾讯云域名注册 GoDaddy 绑定域名如果你已经购买了域名，绑定已有域名需要分别在“域名解析服务商”和 “Github” 两边进行设置。 域名解析配置将域名和其他域名进行绑定，让你可以通过不同域名访问同一个网站。因为我是在腾讯云上买的域名，就用腾讯云的域名解析服务了。 腾讯云控制台 &gt; 云解析 &gt; 解析 &gt; 修改 &gt; 添加记录： 主机记录：@ 表直接解析主域名； 记录类型：CNAME 将域名指向另一个域名，再由另一个域名提供 ip ； 线路类型：选默认； 记录值：填写一个域名，即你原有的Github Pages 访问地址； TTL：缓存默认时间，默认600s； Github 设置进入你的仓库 &lt;your_name&gt;.github.io ，点击 Settings ，在 Github Pages设置项中将 Custom domain 设置为你的个人域名： 这时会在你的仓库目录下自动生成一个 CNME 文件，里面存放着你的个人域名。 上面的方式有一个问题，那就是你每次部署站点时 CNME都会自动消失，还需要你手动再设置一遍，所以为了方便，可以直接将 CNME 存放在 source 目录下，每次部署就会一同上传了。 完成以上设置之后，可以在浏览器中输入你自己的域名即可访问你的博客了： 参考 Github pages 绑定个人域名 腾讯云COS]]></content>
      <categories>
        <category>搭建博客</category>
      </categories>
      <tags>
        <tag>搭建博客</tag>
        <tag>教程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo + Github Pages 博客搭建教程（五）：问题解答]]></title>
    <url>%2F%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%2FHexo%2FHexo-Github-Pages-%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%95%99%E7%A8%8B%EF%BC%88%E4%BA%94%EF%BC%89%EF%BC%9A%E9%97%AE%E9%A2%98%E8%A7%A3%E7%AD%94%2F</url>
    <content type="text"><![CDATA[百度和Google收录使用 GitHub + Hexo 搭建的博客，默认只能你自己能看到，别人是无法通过百度、谷歌等搜索引擎搜索到的: 可以手动将自己的博客站点提交给百度、谷歌的搜索引擎，这样就可以通过百度或谷歌搜索到自己的博客内容了： 百度收录百度无法搜索到博客信息，是因为 Github Pages 屏蔽了百度爬虫。 验证站点登录百度搜索资源平台&gt;用户中心&gt;站点管理&gt;添加网站，输入网站域名，选择站点属性，到第三步“验证网站”： 有三种不同的验证方式：文件验证、HTML标签验证、CNAME验证。这里我们选择文件验证，下载验证文件到本地，放置在 themes/next/source目录下，执行生成和部署命令： 1$ hexo g -d 然后点击完成验证即可： 添加站点地图站点地图（sitemap）可以告诉搜索引擎网站上有哪些可供抓取的网页，以便搜索引擎可以更加智能地抓取网站。 安装百度和谷歌站点地图生成插件：cd 到你的站点目录，执行以下命令 12$ npm install hexo-generator-baidu-sitemap --save$ npm install hexo-generator-sitemap --save 修改 url：修改站点配置文件 _config.yml 中的 url 参数: 1234url: http://jonzzs.cn # 修改成你博客的首页地址root: /permalink: :year/:month/:day/:title/permalink_defaults: 修改配置文件：修改站点配置文件 _config.yml，添加以下内容： 12345# 自动生成sitemapsitemap: path: sitemap.xmlbaidusitemap: path: baidusitemap.xml 生成和部署：执行生成和部署命令后，进入public目录，你会发现里面有 sitemap.xml 和baidusitemap.xml 两个文件，这就是生成的站点地图，前者用来提交给谷歌，后者用来提交给百度 1$ hexo g -d 自动推送站点地图：在百度资源搜索平台，找到链接提交，这里我们可以看到有两种提交方式，自动提交和手动提交，自动提交又分为主动推送、自动推送和sitemap，自动推送配置最简单，将主题配置文件下的 baidu_push 设置为 true： 12# Enable baidu push so that the blog will push the url to baidu automatically which is very helpful for SEObaidu_push: true 百度收录网站到此配置结束，只需要等待百度收录，这个过程会比较久。 谷歌收录验证站点登陆Google网站站长 &gt; 进入Search Console &gt; 添加资源： 我们选择 HTML 文件上传的方式验证，下载验证文件到本地，放置在 themes/next/source目录下，执行生成和部署命令： 1$ hexo g -d 部署完成之后，进行验证即可： 添加站点地图安装插件、修改配置文件在上述百度收录过程已经做过了，现在只需要点击前往站点资源页面 &gt; 点击站点地图，添加新的站点地图 sitemap.xml： 即可完成谷歌收录网站，只需要等待谷歌收录，这个过程会比较久，成功收录后的效果如下： github 托管博客原始文件Hexo生成的博客静态网页会被自动托管到 github，我们同样可以通过 github 来实现对博客原始文件的版本管理，这样我们就可以随时随地地将博客迁移至新的电脑，在新的电脑上继续我们的创作了。 Hexo生成的文件里面是有一个.gitignore的，所以它的本意应该也是想我们把这些文件放到GitHub上存放的，但如果用额外一个仓库来存放这些文件会显得冗余，我们完全可以在同一个仓库中使用不同分支来分别存放生成的静态网页和原始文件（分支名可以自己决定）： master 分支：用于存放网页发布的静态文件，当执行 hexo d 时实际上是将生成的静态网页push到github远程仓库的master分支中去，并使用master分支的文件通过git pages生成博客页面，由于这些过程都是Hexo自动完成的，我们没有必要在本地仓库显式地创建 master 分支； files 分支：用于存放原始文件(博客文件、图片、配置文件等)，由于我们需要手动管理这个分支，可以将 github 中的files分支设置为默认分支，这样每次通过 git pull 合并远程分支时默认都是从origin/files分支获取的； 以上是git管理博客文件的答题思路，下面来看看在具体场景下的操作。 将博客原始文件同步到github 默认情况下，Hexo 博客目录应该已经是一个本地 git 仓库了(包含.git)，且已经将静态网页所存放的远程仓库添加为了远程仓库，否则你需要先将静态网页关联的github仓库添加为博客本地仓库的远程仓库： 1234567# 创建本地仓库$ git init$ git add -A$ git commit -m &quot;创建本地仓库&quot;# 添加远程仓库$ git remote$ git remote add origin https://github.com/paulboone/ticgit 创建新的分支： 1234# 创建新的分支$ git branch files# 创建远程分支$ git push origin files:files 修改 .gitignore 文件：默认.gitignore文件中过滤了以下文件，这些文件都是被动生成的，不用托管 1234567891011# mac版本文件.DS_StoreThumbs.dbdb.json*.log# npm依赖包，不用托管，npm install 会自动下载node_modules/# hexo g 生成的静态网页文件public/# hexo d 生成的版本管理文件.deploy*/% 新增忽略规则：如果在提交了之后，希望在.gitignore文件中添加新的过滤规则，除了修改.gitignore文件外，还需要清除缓存后重新添加并提交 123git rm -r --cached .git add .git commit -m &#x27;update .gitignore&#x27; 将子仓库转化为正常文件：如果你是通过 git clone 拷贝的 themes/next，next目录就会作为一个子仓库嵌套在博客仓库内，在push或pull时会发现next目录虽有但是空的，这是因为外层仓库是不会跟踪内层仓库的变化的，这需要将内层仓库转化为普通目录，除此之外还需要将next目录移出-提交-移入-提交，否则博客仓库也还是无法将其纳入版本控制 1234567891011$ cd themes/next# 删除 .git 文件$ rm -rf .git# 移出next后$ git add -A$ git commit -m &quot;移出next&quot;# 移入next$ git add -A $ git commit -m &quot;移入next&quot;# 同步到远程仓库$ git push origin files:files 本地分支只有一个files，但有两个远程分支 origin/files 和 origin/master： 在新电脑上部署博客先安装好homebrew，再安装 node 和 hexo： 12345678910# 安装node$ brew install node$ node -vv8.4.0# 安装 hexo$ npm install -g hexo-cli$ hexo versionhexo-cli: 1.1.0os: Darwin 18.0.0 darwin x64 克隆远程仓库到本地： 1$ git clone *** 进入仓库目录，安装依赖：不要 hexo init 会覆盖博客配置文件 _config.yml，npm install 安装过程可能会报错，忽略就行 12345678# 安装npm依赖$ cd &lt;folder&gt;$ npm install# 安装启动服务$ npm install hexo-server --save# 安装部署服务$ npm install hexo-deployer-git --save 新的工作流123456789# 1. 创建文章$ hexo new filename# 2. 编辑文章# 3. 发布文章$ hexo g -d# 4. 上传原始文件$ git commit -a -m &quot;更新了一篇文章&quot;$ git push hexo next主题解决无法显示数学公式https://blog.csdn.net/yexiaohhjk/article/details/82526604 问题Hexo 默认使用 hexo-renderer-marked 引擎渲染网页，该引擎会把一些特殊的 markdown 符号转换为相应的 html 标签，比如在 markdown 语法中，下划线_代表斜体，会被渲染引擎处理为&lt;em&gt;标签。 因为类 Latex 格式书写的数学公式下划线_表示下标，有特殊的含义，如果被强制转换为&lt;em&gt;标签，那么 MathJax 引擎在渲染数学公式的时候就会出错，类似的语义冲突的符号还包括*, &#123;, &#125;, \\等。 解决 更换 Hexo 的 markdown 渲染引擎：hexo-renderer-kramed 引擎是在默认的渲染引擎 hexo-renderer-marked 的基础上修改了一些 bug ，两者比较接近，也比较轻量级。执行下面的命令即可，先卸载原来的渲染引擎，再安装新的： 12npm uninstall hexo-renderer-marked --savenpm install hexo-renderer-kramed --save escape、em 变量修改：引擎后行间公式可以正确渲染了，但是这样还没有完全解决问题，行内公式的渲染还是有问题，因为 hexo-renderer-kramed 引擎也有语义冲突的问题。接下来到博客根目录下，找到node_modules\kramed\lib\rules\inline.js，把第11行的 escape 变量的值以及第20行的em变量做相应的修改 12//escape: /^\\([\\`*&#123;&#125;\[\]()#$+\-.!_&gt;])/,escape: /^\\([`*\[\]()#$+\-.!_&gt;])/, 在 Next 主题中开启 MathJax 开关：如果使用了主题了，别忘了在主题（Theme）中开启 MathJax 开关，下面以 next 主题为例，介绍下如何打开 MathJax 开关。进入到主题目录，找到 _config.yml 配置问题，把 math 默认的 false 修改为true： 123456# MathJax Supportmathjax: enable: true per_page: false engine: mathjax cdn: //cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML 如果希望只有在用到公式的页面才加载 Mathjax，这样不需要渲染数学公式的页面的访问速度就不会受到影响，可以将per_page设置为true，然后再需要加载mathjax的md文件头部加上mathjax: true： 123456---title: index.htmldate: 2018-07-05 12:01:30tags:mathjax: true-- 最终效果： 在手机无法打开博客博客在某些网络中无法访问，原因可能是 gitpage 被墙，解决办法： 在站长工具-DNS查询输入博客网址如liketea.xyz，查询响应 IP 修改电脑 hosts 文件，以 MAC为例： 1234(base) ➜ ~ sudo vim /etc/hostsPassword:# 将查询到的 IP 和域名写到这里185.199.109.153 liketea.xyz 刷新博客网站即可正常访问。 参考 Hexo博客提交百度和Google收录 百度资源平台 Google网站站长]]></content>
      <categories>
        <category>搭建博客</category>
      </categories>
      <tags>
        <tag>搭建博客</tag>
        <tag>教程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习：理论基础（一）—— 概率论]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%9A%E7%90%86%E8%AE%BA%E5%9F%BA%E7%A1%80%EF%BC%88%E4%B8%80%EF%BC%89%E2%80%94%E2%80%94%20%E6%A6%82%E7%8E%87%E8%AE%BA%2F</url>
    <content type="text"><![CDATA[待补充…]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习：理论基础（二）—— 信息论]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%9A%E7%90%86%E8%AE%BA%E5%9F%BA%E7%A1%80%EF%BC%88%E4%BA%8C%EF%BC%89%E2%80%94%E2%80%94%20%E4%BF%A1%E6%81%AF%E8%AE%BA%2F</url>
    <content type="text"><![CDATA[惊奇度惊奇度：某事件所携带的信息量可以被看做该事件发生时所给我们带来的惊讶程度。 我们希望能够将这种惊奇度/信息量进行量化，一个合理的假设是：事件的惊奇度/信息量只取决于事件发生的概率。用$S(p)$表示由概率为$p$的事件发生以后所产生的惊奇程度，假定$S(p)$对一切$0&lt;p&lt;=1$有定义。下面我们从$S(p)$应满足的条件出发确定$S(p)$的形式： 公理1：$S(1)=0$，当听到一个必然事件发生时，不会感到任何惊奇 公理2：$S(p)$是$p$的严格连续递减函数，事件发生的概率越大，惊奇度越小 公理3：$S(pq)=S(p)+S(q)$，对于独立事件E和F，假设E发生的惊奇度为$S(p)$，F发生的惊奇度为$S(q)$，则二者同时发生的惊奇度等于二者分别发生的惊奇度之和 容易验证，对数函数可同时满足这四个条件： S(p) = -log p当底数取2时，惊奇度/信息量的单位可用比特表示。 信息熵信息熵：某随机变量x所带来的平均惊奇度(所携带的平均信息量) 离散随机变量的熵： H(x)=-\sum_{x} p(x)logp(x) 规定$0log0=0$ 当p(x)为均匀分布时，信息熵达到最大值，如$p(x)=\frac{1}{n}$，则$H(x)=lgn$ 同等情况下x的取值越多，熵也越大 连续随机变量的熵(微分熵): H(x)=-\int p(x)lnp(x)dx 当p(x)为高斯分布时，微分熵达到最大$H(x)=\frac{1}{2}(1+ln(2\pi \sigma ^2))$ 微分熵随着分布宽度$\sigma ^2$的增加而增加； 微分熵可取负值； 我们可以从不同的视角来理解随机变量的熵: 平均惊奇度：观察x时的平均惊讶程度； 平均信息量：发送x时传输的平均信息量； 不确定度：对x取值的不确定程度； 最短编码长度：假设我们要对随机变量x的各种取值进行前缀编码，则随机变量x的熵等于最短平均编码长度； 联合熵与条件熵随机变量$X,Y$的联合熵$H(X,Y)$描述了随机变量$X,Y$联合分布的不确定度： H(X,Y)=-\sum_{x}\sum_{y}p(x,y)logp(x,y)给定X的条件下，Y的条件熵(conditional entropy)为： \begin{align*} H(Y\mid X)&=\sum_{x}H(Y|{X=x})\cdot p(X=x)\\ &=-\sum_{x}\sum_{y}p(y\mid x)logp(y\mid x)p(x)\\ &=-\sum_{x}\sum_{y}p(x,y)logp(y\mid x) \end{align*}定理1:$X$和$Y$的联合熵可分解为$X$的熵加上给定$X$条件下$Y$的条件熵 H(X,Y)=H(X)+H(Y\mid X)证明： \begin{aligned} H(X,Y)&=-\sum_{i}\sum_{j}p(x_{i},y_{j})logp(x_{i},y_{j}) \\ & =-\sum_{i}\sum_{j}p(y_{i})p(x_{i}|y_{j})[logp(y_{i})+logp(x_{i}|y_{j})] \\ & = -\sum_{j}p(y_{j})logp(y_{j})\sum_{i}p(x_{i}|y_{i})-\sum_{j}p(y_{j})\sum_{i}p(x_{i}|y_{i})logp(x_{i}|y_{i})\\ & =H(Y)+H(X\mid Y) \end{aligned}定理2：当另一个随机变量$X$被观测到后，$Y$的不确定度在平均意义下减少，当二者独立时等号成立： H(Y\mid X) \leqslant H(Y)交叉熵-相对熵-互信息交叉熵(Cross-Entropy)：度量两个概率分布间的差异性信息。交叉熵等于我们用近似的分布$q(x)$来表示真实分布$p(x)$的平均编码长度，则： C(p,q)=-\sum p(x)logq(x) $C(p,q)\geqslant H(p)$：交叉熵以真实熵为下界，熵代表了最短编码长度； 最小交叉熵等价于极大似然：我们用$q(x\mid \theta)$来作为真实分布$p(x)$的近似，关于p(x)的期望可以通过已观察到的服从分布$p(x)$的有限训练点$x_n$的加和作为近似，而后者正是极大似然的表达式； C(p,q)=-\sum p(x)logq(x) = -\frac{1}{N}\sum_{n=1}^{N} logq(x_n\mid \theta )相对熵(Relative Enrtopy)：交叉熵减去真实熵，表示近似概率分布$q(x)$对真实分布$p(x)$的相对熵；相对熵又称为KL散度(Kullback and Leibler,1951)，与交叉熵类似，同样可以度量两个随机分布的差异性； \begin{align*} KL(p\parallel q)&=C(p,q)-H(p)\\ &=-\sum p(x)logq(x)-(-\sum p(x)logp(x))\\ &=-\sum p(x)log\frac{q(x)}{p(x)} \end{align*} $KL(p\parallel q)\geqslant 0$ $KL(p\parallel q)\neq KL(q\parallel p)$ 互信息(mutual information):用$p(x)p(y)$来作为$p(x,y)$的近似的KL散度，散度越大说明二者差异越大，x,y越不独立，互信息可以看做已知其中一个随机变量而造成的另一个随机变量的不确定性的减少。 \begin{align*} I(X,Y)&=KL(p(x,y)\parallel p(x)p(y))\\ &=-\sum_{x}\sum_{y} p(x,y)log\frac{p(x)p(y)}{p(x,y)}\\ &=-\sum_{x}\sum_{y}p(x,y)log\frac{p(x)}{p(x,y)}-\sum_{x}\sum_{y}p(x,y)logp(y)\\ &=\sum_{x}\sum_{y}p(x,y)logp(y\mid x)-\sum_{y}p(y)logp(y)\\ &=H(Y)-H(Y\mid X)\\ \end{align*} $I(X,Y)=I(Y,X)\geqslant 0$：当X,Y相互独立时等号成立 现在，我们可以总结上述所有和熵相关的公式: \left\{\begin{matrix} \begin{align*} &S(p)=-lgp\\ &H(X)=-\sum_{x}p(x)lgp(x)\\ &H(Y\mid X)=\sum_{x}H(Y\mid X=x)p(X=x)=\sum_{x}\sum_{y}p(x,y)lgp(y\mid x)\\ &H(X,Y)=-\sum_{x}\sum_{y}p(x,y)lgp(x,y)\\ &C(p,q)=-\sum_{x}p(x)lgq(x)\\ &KL(p\parallel q)=C(p,q)-H(p)=-\sum_{x}p(x)lg\frac{q(x)}{p(x)}\\ &I(X,Y)=KL(p(x,y)\parallel p(x)p(y))=H(Y)-H(Y\mid X)=H(X)-H(X\mid Y)\\ &H(Y\mid X)=H(Y)-I(X,Y)\\ &H(X,Y)=H(X)+H(Y)-I(X,Y) \end{align*} \end{matrix}\right.编码定理通信中的编码问题（最小期望码长）：假设一个离散型随机变量X取值于$\lbrace x{1}, \cdot \cdot \cdot ,x{N}\rbrace$，其相应概率为$\lbrace p(x{1}), \cdot \cdot \cdot ,p(x{N})\rbrace$，设计一个编码系统，将$x{i}$编成$n{i}$位的二进制序列，通过一个通信网络将从A处传送到B处，为避免混乱，要求编码后的序列不能出现一个序列是另一个序列的延伸。如何设计编码系统使得最终的期望码长最小。 引理1:为了将$X$的可能取值编码成0-1序列，且任何一个序列都不能是另一序列的延伸，其充要条件为： \sum_{i=1}^{N}\left ( \frac{1}{2} \right )^{n_{i}}\leqslant 1证明: 记$w{j}$为$x{i}$中编码长度为j的个数，$j=1,2,3…$，显然有： w_{1}2^{n-1}+w_{2}2^{n-2}+\cdot \cdot \cdot +w_{n-1}2+w_{n}\leqslant 2^{n}两边同除以$2^{n}$得： \sum_{j=1}^{n}w_{j}\left ( \frac{1}{2}\right )^{j}=\sum_{i=1}^{N}\left ( \frac{1}{2} \right )^{n_{i}}\leqslant 1无噪通道编码定理无噪通道编码定理:假设每个信号单位从位置A到位置B的过程没有发生错误，则编码的期望码长不小于随机变量的信息熵: \sum_{i=1}^{N}n_{i}p\left ( x_{i} \right )\geqslant H(X)=-\sum_{i=1}^{N}p\left ( x_{i} \right )\log p\left ( x_{i} \right )证明：记$p{i}=p(x{i})$，$q{i}=2^{-n{i}}/\sum{j=1}^{N}2^{-n{j}}$，则有$\sum{i=1}^{N}p{i}=\sum{i=1}^{N}q{i}=1$ \begin{aligned} -\sum_{i=1}^{N}p_{i}\log(\frac{p_{i}}{q_{i}})&=-\log e \sum_{i=1}^{N}p_{i}\ln (\frac{p_{i}}{q_{i}})\\ &=\log e \sum_{i=1}^{N}p_{i}\ln (\frac{q_{i}}{p_{i}})\\ &\leqslant \log e \sum_{i=1}^{N}p_{i}(\frac{q_{i}}{p_{i}}-1)\\ &=\log e (\sum_{i=1}^{N}p_{i}-\sum_{i=1}^{N}q_{i})=0 \end{aligned}由此可得： \begin{aligned} -\sum_{i=1}^{N}p\left ( x_{i} \right )\log p\left ( x_{i} \right )&\leqslant - \sum_{i=1}^{N}p_{i}\log q_{i}\\ &= \sum_{i=1}^{N}n_{i}p_{i}+\log(\sum_{j=1}^{N}2^{-n_{j}})\\ &\leqslant\sum_{i=1}^{N}n_{i}p_{i} \end{aligned}定理:对于大部分随机变量$X$，不存在一组编码系统使得期望码长达到下界$H(X)$，但是总存在一个编码系统，使得期望码长与$H(X)$之间的误差小于1 证明:取$n{i}=\left \lceil -\log p(x{i}) \right \rceil$，即： -\log p(x_{i}) \leqslant n_{i}\leqslant -\log p(x_{i}) +1代入期望码长公式$L=\sum{i=1}^{N}n{i}p(x_{i})$得： -\sum_{i=1}^{N}p\left ( x_{i} \right )\log p\left ( x_{i} \right )\leqslant L\leqslant -\sum_{i=1}^{N}p\left ( x_{i} \right )\log p\left ( x_{i} \right )+1 H(X)\leqslant L< H(X)+1有噪通道编码定理假设每个信号单位的传送是独立的，且以概率p正确地从A处传送到B处，这样的通信系统称为二进制对称通道。若不经过处理直接传送便会发生误传，一种减少误传信号的方法是将信号重复多次，在译码时按多数原则进行翻译。 假设p=0.8，通过将信号重复3次进行编码译码。如000、001、010、100都代表0，111，110，101，011代表1。此时，传输一位错误的概率为： 0.2^{3}+3\times 0.2^{2}\times 0.8=0.104错误率由0.2减小到0.104，事实上，只要重复足够多次可以将误传概率变得任意小，但是这种方法是以牺牲传输效率为代价的（降低到1/3）。但值得庆幸的是将传输错误概率减小到0的同时，传输效率并不会减小到0，这正是香农在信息论中提出的含噪声编码定理。 有噪通道编码定理:: 只要信息传输效率小于通道容量，必存在一类编码使信息传输错误概率可以任意小。通道容量可由以下公式计算： C^{*}=1+p\log p+(1-p)\log(1-p)$C^{*}\leqslant 1$,当$p=1$或$p=0$时等号成立。有噪声编码定理又称香农第二定理，是编码存在定理。 最大熵原理最大熵原理认为, 学习概率模型时, 在所有可能的概率模型(分布) 中, 熵最大的模型是最好的模型。直观地, 最大熵原理认为要选择的概率模型首先必须满足已有的事实, 即约束条件. 在没有更多信息的情况下, 那些不确定的部分都是等可能的. 最大熵原理通过熵的最大化来表示等可能性. 等可能不容易操作, 而熵则是一个可优化的数值指标。 引用 概率论基础 PRML]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习：理论基础（三）—— 决策论]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%9A%E7%90%86%E8%AE%BA%E5%9F%BA%E7%A1%80%EF%BC%88%E4%B8%89%EF%BC%89%E2%80%94%E2%80%94%20%E5%86%B3%E7%AD%96%E8%AE%BA%2F</url>
    <content type="text"><![CDATA[概率论研究如何量化和计算不确定性，而决策论则研究如何在不确定性中做出最优决策，决策论研究的问题一般可分为两个阶段： 推断阶段(inference stage)：确定联合概率分布$p(x,y)$或后验概率分布$p(y\mid x)$；虽然联合概率分布能够给出问题完整的概率描述，但并不是必要的； 决策阶段(decision stage)：在给定概率分布的前提下，进行最优决策；一旦推断问题解决了决策问题就会变得非常简单，甚至不值一提； 我们可以通过概率论中的贝叶斯定理进行决策： \begin{align*} p(y\mid x)&=\frac{p(x,y)}{p(x)}\\ &=\frac{p(y)p(x\mid y)}{\int_{y}p(x,y)dy} \end{align*}期望损失最小化损失函数(loss function)：是对于所有可能的决策可能产生的损失的一种整体的度量，通常又称代价函数(cost function)，记做$L(y,\hat{y})$，其中y是实际值，$\hat{y}$是模型预测值。我们的目标是最小化整体的损失，有些学者不考虑损失函数，而是考虑效用函数(utility function)，目标是最大化效用函数，如果让损失函数等于效用函数的相反数的话，这两个概念是等价的。 期望损失最小化原则：在进行决策时通常希望决策所带来的期望损失达到最小。 分类问题损失矩阵通常用损失矩阵来描述k分类问题的损失函数： L = [L_{i,j}]_{k\times k}其中$L_{i,j}$表示将实际为i类的样本预测为j类所带来的损失，对于0-1损失有： L_{i,j}=\left\{\begin{matrix} 0 &,i = j \\ 1 & ,i\neq j \end{matrix}\right.通常我们可以为不同情形赋予不同的损失值，比如在疾病预测时，将正常人预测为病人损失通常较小，而将病人预测为正常损失通常较大。 分类问题描述分类问题的目标就是要找到某种规则，将输入空间切分为不同的决策区域$R_k$，并将决策区域$R_k$中的所有点都划分到类别$c_k$，使得期望损失达到最小。即： f(x)=\sum_{k}[x\in R_k]c_k其中： \begin{align*} (R,c) &= arg\ \underset{R_k,c_k,k=1,2,...}{min} \mathbb{E}(L)\\ &=arg\ \underset{R_k,c_k,k=1,2,...}{min}\sum_{k}\sum_{j}\int_{R_j}L_{k,j}p(x,c_k)dx \end{align*}对于给定的x，目标就变成了： \begin{align*} \hat{y}&= arg\ \underset{c_j}{min} \sum_{k}L_{kj}p(x,c_k)\\ &=arg\ \underset{c_j}{min} \sum_{k}L_{kj}p(c_k\mid x)p(x)\\ &=arg\ \underset{c_j}{min} \sum_{k}L_{kj}p(c_k\mid x) \end{align*}对于0-1损失函数： \begin{align*} \hat{y}&=arg\ \underset{c_j}{min} \sum_{k}L_{kj}p(c_k\mid x)\\ &=arg\ \underset{c_j}{min} \sum_{k\neq j}p(c_k\mid x)\\ &=arg\ \underset{c_j}{min} (1-p(c_j\mid x))\\ &=arg\ \underset{c_j}{max} \ p(c_j\mid x) \end{align*}即，0-1损失下的期望风险最小化等价于最大后验概率策略。 拒绝选项拒绝选项：当不同类别的联合概率相差不大时，类别的归属相对不稳定，在这种情况下避免做出自动化决策，而将决策权留给人类专家，能够降低模型的分类错误率。 具体地，可以通过引入一个决策阈值$\theta$，拒绝最大后验概率小于$\theta$的那些输入。 回归问题闵可夫斯基损失函数闵可夫斯基损失函数是平方损失的一种推广： L_q=\left | y-y(x) \right |^q y:输入x对应的真实标签； y(x):输入x对应的预测值； 问题描述回归问题的目标就是对于每个输入x找到一个估计值$y(x)$，使得期望损失最小。 以平方误差为例： \begin{align*} \mathbb{E}(L)&=\int \int L(y,y(x))p(x,y)dxdy\\ &=\int \int (y-y(x))^2p(x,y)dxdy \end{align*}两侧对x求导得： \frac{\partial \mathbb{E}(L)}{\partial y(x)}=2\int (y(x)-y)p(x,y)dy=0 y(x)=\frac{\int yp(x,y)dy}{p(x)}=\int yp(y\mid x)dy=\mathbb{E}_t(t\mid x)即，最小化期望平方损失的回归函数由条件均值给出，值得说明的是当q=1时得到的是条件中位数，当$q\rightarrow 0$时得到的是条件众数。 期望平方损失下的偏差-方差分解：其中的交叉项对dy积分可消去 \begin{align*} \mathbb{E}(L)&=\int \int (y(x)-y)^2dxdy\\ &=\int \int(y(x)-\mathbb{E}(y\mid x)+\mathbb{E}(y\mid x)-y)^2dxdy\\ &=\int(\int (y(x)-\mathbb{E}(y\mid x)+\mathbb{E}(y\mid x)-y)^2p(y\mid x)dy) p(x)dx\\ &=\int(\int ((y(x)-\mathbb{E}(y\mid x))^2+(\mathbb{E}(y\mid x)-y)^2+2(y(x)-\mathbb{E}(y\mid x))(\mathbb{E}(y\mid x)-y)p(y\mid x)dy) p(x)dx\\ &=\int(\int ((y(x)-\mathbb{E}(y\mid x))^2+(\mathbb{E}(y\mid x)-y)^2)p(y\mid x))p(x)dx\\ &=\int (y(x)-\mathbb{E}(y\mid x))^2p(x)dx+\int var(y\mid x)p(x)dx \end{align*} 第一项表示预测值与条件均值的偏差：当$y(x)=\mathbb{E}(y\mid x)$时，取得最小值； 第二项表示条件均值的方差：表示目标数据内在的变化性，可以被看做是噪声，因为与y(x)无关，它表示损失函数的不可减少的最小值； 解决决策问题的三种方法三种方法按照复杂度降低的顺序给出： 生成式模型（generative model）:首先解决确定联合概率分布$p(x,y)$的推断问题，然后通过贝叶斯定理求出后验概率分布$p(y\mid x)$，最后基于后验概率分布使用上述方法进行决策；这种方法可以人工生成出输入空间的数据点，故称生成式模型； 判别式模型（discriminative model）:首先解决确定后验概率分布的推断问题，再基于后验概率分布使用上述方法进行决策； 判别函数：与上述两种概率模型不同，该方法将推断阶段和决策阶段结合到同一个过程，直接从训练数据中寻找一个分类/回归函数y(x)； 到某个函数$f(x)$，然后将每个输入x直接映射为标签； 三种方法对比： 生成式模型：需要求解的东西最多，能够完整描述概率分布，但需要更多数据，也会浪费计算资源； 判别式模型：只需求出后验概率，有很多理由需要计算后验概率： 最小期望风险：当损失矩阵发生变化时，如果已知后验概率分布，则只需做少量改动即可； 拒绝选项：方便设置阈值； 补偿类先验概率：对样本不均衡重采样后得到的后验概率分布不能作为真实数据的分布，需要乘以原始数据中的先验概率再除以平衡后的数据中的先验概率，最后再进行归一化； 判别函数：模型简单，但无法得到后验概率分布； 参考 模式识别与机器学习.Bishop]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习：理论基础（四）—— 多元微积分]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%9A%E7%90%86%E8%AE%BA%E5%9F%BA%E7%A1%80%EF%BC%88%E5%9B%9B%EF%BC%89%E2%80%94%E2%80%94%20%E5%A4%9A%E5%85%83%E5%BE%AE%E7%A7%AF%E5%88%86%2F</url>
    <content type="text"><![CDATA[实际应用中的函数普遍包含多个变量，当进入高维时，微积分的普遍法则本质上保持原样，虽然必须引入一些新的记号，但幸运的是并不需要彻底改造原有理论，多变量微积分无非是同时在各个方向运用单变量微积分。 向量和矩阵表示法将大大简化多元微积分，并能保持与低维形式上的一致性，本文使用小写字母$x$表示标量，粗体小写字母$\boldsymbol{x}$表示向量，大写字母$X$表示矩阵。 1. 梯度（Gradient）1.1 多元函数定义：$f:\boldsymbol{x}\rightarrow y$，其中$\boldsymbol{x}=(x{1},x{2},…,x_{n})\in \mathbb{R}^{n}$，$y\in \mathbb{R}$，多元函数是从n维空间（n维向量）到一维空间（标量）的映射。 1.2 等高线图定义：对于二元函数$f(x,y)$，曲线$f(x,y)=z{0}$称为函数f在平面$z=z{0}$中的等高线。如果所有等高线$z=z_{0}$都被投射到$x-y$平面上，则得到这个曲面的等高线图。 1.3 梯度（向量）定义：$f$在点$\boldsymbol{x}$处的梯度是由对应维度的偏导构成向量，记作$\bigtriangledown f(\boldsymbol{x})$，读作grad f或del f。 \triangledown f(\boldsymbol{x}) = \begin{bmatrix} \frac{\partial f}{\partial x_{i}} \end{bmatrix}_{n\times 1}性质： 梯度是标量对向量的导数； 梯度是输入空间中的一个偏导向量； 梯度的方向是在输入空间中使函数增长最快的方向； 梯度的大小是函数关于输入向量的最大变化率； 梯度也称为全导数，记做 $f^{‘}(\boldsymbol{x})$； f^{'}(\boldsymbol{x})=\triangledown f(\boldsymbol{x}) = \begin{bmatrix} \frac{\partial f}{\partial x_{i}} \end{bmatrix}_{n\times 1}\\ f(\boldsymbol{x}) \approx f(\boldsymbol{x_{0}})+f^{'}(\boldsymbol{x_{0}})^{T}(\boldsymbol{x}-\boldsymbol{x_{0}})1.4 全微分定义：多元函数全增量$\triangle f$的线性主部，记做$df$。 \begin{align*} \triangle f&=\sum_{i=1}^{n}a_{i}\triangle x_{i}+o(\rho )\\ df&=\sum_{i=1}^{n}\frac{\partial f}{\partial x_{i}}dx_{i} =\triangledown f(\boldsymbol{x})\cdot d\boldsymbol{x} \end{align*}梯度-全导数-全微分-偏导之间的关系如下： \triangledown f(\boldsymbol{x})=f^{'}(\boldsymbol{x})=\frac{\mathrm{d} f}{\mathrm{d} \boldsymbol{x}}=\begin{bmatrix} \frac{\partial f}{\partial x_{i}} \end{bmatrix}_{n\times 1}1.5 方向导数定义：设$\mathbf{\mathit{u}}$为单位向量，$f$在该方向上的方向导数（变化率）记做$\frac{\partial f}{\partial \mathbf{\mathit{u}}}$。 \begin{align*} \frac{\partial f(\boldsymbol{x})}{\partial \boldsymbol{u}}&=\lim_{h\rightarrow 0}\frac{f(\boldsymbol{x}+h\boldsymbol{u})-f(\boldsymbol{x})}{h}\\ &=\triangledown f(\boldsymbol{x})\cdot \boldsymbol{u}\\ &=|\triangledown f(\boldsymbol{x})|cos\theta \end{align*}性质： 方向导数是梯度向量在该方向上的投影； 梯度方向的方向导数最大，即函数在梯度方向上增长最快； 在负梯度方向上，函数下降最快； 2. 海塞矩阵（Hessian Matrix）定义：海塞矩阵是一个多元函数的二阶偏导数构成的对称方阵，描述了函数的局部曲率。 H(f)=\begin{bmatrix} \frac{\partial f}{\partial x_{i} \partial x_{j}} \end{bmatrix}_{n\times n}完整写作： 2.1 多元泰勒展开在工程实际问题的优化设计中，所列的目标函数往往很复杂，为了使问题简化，常常将目标函数在某点邻域展开成泰勒多项式来逼近原函数。 f(\boldsymbol{x})=f(\boldsymbol{x_{0}})+\triangledown f(\boldsymbol{x_{0}})^{T}(\boldsymbol{x}-\boldsymbol{x_{0}})+\frac{1}{2}(\boldsymbol{x}-\boldsymbol{x_{0}})^{T}H(\boldsymbol{x_{0}})(\boldsymbol{x}-\boldsymbol{x_{0}})+... $\triangledown f(\boldsymbol{x{0}})$是$f$在$\boldsymbol{x{0}}$处的梯度 $H(\boldsymbol{x{0}})$是$f$在$\boldsymbol{x{0}}$处的海塞矩阵 2.2 海赛矩阵判定多元函数极值定理：设n元函数$f(x{1},x{2},…,x{n})$在点 $M{0}(a{1},a{2},…,a_{n})$的邻域内有二阶连续偏导，如果f在该点梯度为0，则可以通过该点的二阶偏导即海塞矩阵H判断函数在该点是否取得极值。 如果H为正定矩阵，函数在该点取得极小值； 如果H为负正定矩阵，函数在该点取得极大值； 如果H是半正定或半负定，该点是可疑极值点，需更通过其他方法来判定； 如果H是不定矩阵，该点不是极值点； 3. 矩阵求导 矩阵求导的技术，在统计学、控制论、机器学习等领域有广泛的应用。鉴于看过的一些资料或言之不详、或繁乱无绪，本文来做个科普，以下内容参考： 矩阵求导术（上） 矩阵求导术（下） 3.1 全导数与全微分全导数：标量f对矩阵X的导数为f对X逐元素求导排成与X尺寸相同的矩阵 f^{'}(X)=\begin{bmatrix} \frac{\partial f}{\partial X_{ij}} \end{bmatrix} 对于标量：$f^{‘}(x)=\begin{bmatrix}\frac{\partial f}{\partial x}\end{bmatrix}=\frac{\mathrm{d} f}{\mathrm{d} x}$ 对于向量：$f^{‘}(\boldsymbol{x})=\begin{bmatrix}\frac{\partial f}{\partial x_{i}}\end{bmatrix}=\triangledown f(\boldsymbol{x})$ 全微分：全增量△f的线性主部 df = \sum_{i,j} \frac{\partial f}{\partial X_{ij}}dX_{ij} 对于标量：$df = \frac{\partial f}{\partial x}dx$ 对于向量：$df = \sum{i} \frac{\partial f}{\partial x{i}}dx_{i}$ 对于规模相同的矩阵A和B，有： \text{tr}(A^TB) = \sum_{i,j}A_{ij}B_{ij}因此，全微分等于矩阵导数与$dX$内积的迹（trace） df = \sum_{i,j} \frac{\partial f}{\partial X_{ij}}dX_{ij} = \text{tr}\left(f^{'}(X)^T dX\right) 对于标量：$df = \frac{\partial f}{\partial x}dx=tr(f^{‘}(x)^Tdx)=f^{‘}(x)dx$ 对于向量：$df = \sum{i} \frac{\partial f}{\partial x{i}}dx_{i}=tr(f^{‘}(\boldsymbol{x})^Td\boldsymbol{x})=tr(\triangledown f(\boldsymbol{x})^Td\boldsymbol{x})$ 3.2 标量对矩阵的求导规则3.2.1 一般思路最重要的一点要记住：微积分中对标量求导的结论不适用于对矩阵求导！比如认为AX对X的导数为A，这是没有根据的。我们需要重新建立对矩阵求导的规则，全导数的定义在计算中并不好用，因为它需要将矩阵拆做单个元素，而使用矩阵表示函数的意义正在于矩阵运算更加简洁，因此我们希望找到一种从整体出发的算法。回想一元函数的求导，通常不是从定义出发来求极限，而是先建立初等函数求导、四则运算和复合求导的链式法则，我们也将按照这样的思路来建立对矩阵求导的运算法则。 由： df = \text{tr}(df) = \text{tr}\left(f^{'}(X)^T dX\right)可将矩阵求导分解为两个步骤： 微分化简：通过微分化简提出$dX$ 迹化简：通过迹化简将$dX$移至微分右侧，则微分中$dX$左侧部分既是矩阵导数 3.2.2 矩阵微分运算法则 常矩阵：$dC=\boldsymbol{0}$，0矩阵 加减：$d(X \pm Y )=dX \pm dY$，线性$d(aX \pm C )=adX$ 乘法：$d(XY) = dX Y + X dY $ 转置：$d(X^T) = (dX)^T$ 逆：$dX^{-1} = -X^{-1}dX X^{-1}$。此式可在$XX^{-1}=I$两侧求微分来证明 迹：$d\text{tr}(X) = \text{tr}(dX)$ 行列式： $d|X| = \text{tr}(X^{*} dX)$ ，其中 $X^{*}$表示X的伴随矩阵，在X可逆时又可以写作$d|X|= |X|\text{tr}(X^{-1}dX)$ 逐元素乘法：$d(X\odot Y) = dX\odot Y + X\odot dY，\odot$表示尺寸相同的矩阵X,Y逐元素相乘 逐元素函数：$d\sigma(X) = \sigma’(X)\odot dX ，\sigma(X) = \left[\sigma(X_{ij})\right]$是逐元素运算的标量函数 3.2.3 矩阵微分复合法则对符合函数求导$f^{‘}(U(X))$，其中$U$和$X$均为矩阵，有以下微分复合法则： \begin{align*} df=tr(df)&=tr(f^{'}(U)^TdU)\\ &=tr(f^{'}(U)^Ttr(U^{'}(X)^TdX)) \end{align*}同样不能将对标量求导的复合法则搬到这里，以下公式对矩阵求导不成立： f^{'}(U(X))=f^{'}(U)U^{'}(X)3.2.4 矩阵迹运算法则 标量：$a = \text{tr}(a)$ 转置：$\mathrm{tr}(A^T) = \mathrm{tr}(A)$ 线性：$\text{tr}(A\pm B) = \text{tr}(A)\pm \text{tr}(B)$ 矩阵乘法交换：$\text{tr}(AB) = \text{tr}(BA)$，A B规模相反，两侧都等于$\sum{i,j}A{ij}B_{ji}$，乘法交换迹不变! 矩阵乘法/逐元素乘法交换：$\text{tr}(A^T(B\odot C)) = \text{tr}((A\odot B)^TC)$，A B C规模相同，两侧都等于$\sum{i,j}A{ij}B{ij}C{ij}$ 记忆：前三个通过迹的定义可直观得到，最重要的是矩阵乘法交换： 3.3 重要实例有些资料在计算矩阵导数时，会略过求微分这一步，这是逻辑上解释不通的。 例1：【线性函数】 $f=w^TX$，求$f^{‘}(X)$ 解：$df=d(w^TX)=d(w^T)X+w^TdX=w^TdX=tr(w^TdX)$，因此$f^{‘}(X)=w$ 例2：$f=X^THX$ 解： \begin{align*} df=d(X^THX)&=d(X^T)HX+X^Td(HX)\\ &=d^T(X)HX+X^T(dHX+HdX)\\ &=tr(d^T(X)HX)+tr(X^THdX)\\ &=tr(X^TH^TdX+X^THdX)\\ &=tr((X^TH^T+X^TH)dX) \end{align*}因此，$f^{‘}(X)=(X^TH^T+X^TH)^T=(H+H^T)X$ 特殊地，当H为对称矩阵，$f^{‘}(X)=2HX$ 例3：【线性回归】：$l = |X\boldsymbol{w}- \boldsymbol{y}|^2$，求$l^{‘}(w)$ 解：$l = (X\boldsymbol{w}- \boldsymbol{y})^T(X\boldsymbol{w}- \boldsymbol{y})$，求微分，使用矩阵乘法、转置等法则：$dl = (Xd\boldsymbol{w})^T(X\boldsymbol{w}-\boldsymbol{y})+(X\boldsymbol{w}-\boldsymbol{y})^T(Xd\boldsymbol{w}) = 2(X\boldsymbol{w}-\boldsymbol{y})^TXd\boldsymbol{w}$。对照导数与微分的联系，得到$l^{‘}(w)= 2X^T(X\boldsymbol{w}-\boldsymbol{y})$ 例4【多元logistic回归】：$l = -\boldsymbol{y}^T\log\text{softmax}(W\boldsymbol{x})，求\frac{\partial l}{\partial W}$。 例5【方差的最大似然估计】：已知样本 $ \boldsymbol{x}_1,\dots, \boldsymbol{x}_n \sim N(\boldsymbol{\mu}, \sigma) $ ， 其中 $\sigma $是对称正定矩阵，求方差 $\sigma$ 的最大似然估计。写成数学式是：$l = \log|\sigma|+\frac{1}{n}\sum_{i=1}^n(\boldsymbol{x}_i-\boldsymbol{\bar{x}})^T\sigma^{-1}(\boldsymbol{x}_i-\boldsymbol{\bar{x}})$， 求$\frac{\partial l }{\partial \sigma}$的零点。 例6【二层神经网络】$l = -\boldsymbol{y}^T\log\text{softmax}(W_2\sigma(W_1\boldsymbol{x}))$，求$\frac{\partial l}{\partial W_1}$和$\frac{\partial l}{\partial W_2}$。其中$\boldsymbol{y}$是除一个元素为1外其它元素为0的向量，$\text{softmax}(\boldsymbol{a}) = \frac{\exp(\boldsymbol{a})}{\boldsymbol{1}^T\exp(\boldsymbol{a})}$同例3，$\sigma(\cdot)$是逐元素sigmoid函数$\sigma(a) = \frac{1}{1+\exp(-a)}$ 神经网络的求导术是学术史上的重要成果，还有个专门的名字叫做BP算法，我相信如今很多人在初次推导BP算法时也会颇费一番脑筋，事实上使用矩阵求导术来推导并不复杂。为简化起见，我们推导二层神经网络的BP算法 略…]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习：理论基础（五）—— 矩阵论]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%9A%E7%90%86%E8%AE%BA%E5%9F%BA%E7%A1%80%EF%BC%88%E4%BA%94%EF%BC%89%E2%80%94%E2%80%94%20%E7%9F%A9%E9%98%B5%E8%AE%BA%2F</url>
    <content type="text"><![CDATA[待补充…]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习：理论基础（六）—— 凸优化]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%9A%E7%90%86%E8%AE%BA%E5%9F%BA%E7%A1%80%EF%BC%88%E5%85%AD%EF%BC%89%E2%80%94%E2%80%94%20%E5%87%B8%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[待补充…]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习：优化算法（一）—— 梯度下降]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%9A%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95%EF%BC%88%E4%B8%80%EF%BC%89%E2%80%94%E2%80%94%20%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%2F</url>
    <content type="text"><![CDATA[梯度下降法一般原理梯度下降法(gradient descent)是求解无约束最优化问题的一种最常用的迭代算法。当目标函数是凸函数时，梯度下降法的解是全局最优解。一般情况下，其解不保证是全局最优解，收敛速度也未必是很快的。 无约束最优化问题假设$f(x)$是$\mathbb{R}^n$上具有一阶连续偏导的函数，求解无约束最优化问题： x^*=\underset{x \in \mathbb{R}^n}{min}\ f(x)求解步骤1、选取合适的初值$x_0$，置k=02、计算梯度 $g_k$，若 $\left | g_k \right | &lt; \varepsilon$，则停止计算，得到$x^*=x_k$，否则沿负梯度方向(目标函数下降最快的方向)更新参数 $x\leftarrow x-\eta \nabla_xf(x)$ 各种梯度下降法的变形以下实例，以平方损失函数作为目标函数： \underset{\theta }{min}\ L(\theta) L(\theta) = \frac{1}{2m}\sum_{i=1}^{m}(\hat{y}_i-y_i)^2 $y_i$：第i个样本的实际值 $\hat{y}_i$：第i个样本的预测值 $\theta$：模型参数 在进入正题之前，先通过以下两个动画来感知一下不同梯度下降方法的实际效果： 批量梯度下降/最速梯度下降法(Batch Gradient Descent，BGD)批量梯度下降法（Batch Gradient Descent，简称BGD）是梯度下降法最原始的形式，它的具体思路是每次迭代时都使用所有的样本来来更新模型参数，其数学形式如下： \theta _k = \theta _{k-1} - \eta \nabla_{\theta}L ,\ k=0,1,2... $k$：迭代次数 $\eta $: 学习率 $\nabla_{\theta} L$: 损失函数关于参数的梯度 训练过程示意图： 优点：易于并行实现 缺点： 样本很多时，训练过程很慢； 固定的学习率，太小速度慢，太大容易发生震荡 随机梯度下降法(stochastic gradient descent，SGD)批量梯度下降在每次迭代过程都需要计算所有训练样本，当样本数量很大时训练过程会很慢，一种解决思路是使用随机梯度下降法Stochastic Gradient Descent，简称SGD），每次迭代从样本中随机选取一个样本来更新模型参数（m=1）: \theta _k = \theta _{k-1} - \eta \frac{1}{2}(\hat{y_i}-y_i)\frac{\partial y_i}{\partial \theta}训练过程: 优点：训练速度快 缺点： 不易于并行实现，容易受噪声影响 固定的学习率，太小速度慢，太大容易发生震荡 小批量梯度下降法(Mini-batch Gradient Descent，MBGD)MBGD融合了BGD和SGD的优点，既方便并行实现又不易受到噪声影响，每次迭代从样本中随机选取小批量的样本来更新模型参数（b&lt;m）（是一种用样本来估计总体的方法）: L(\theta) = \frac{1}{2b}\sum_{i=1}^{b}(\hat{y}_i-y_i)^2 \theta _k = \theta _{k-1} - \eta \nabla_{\theta}L ,\ k=0,1,2... 优点：训练速度快、易于并行实现 缺点：固定的学习率，不能适应各个参数、训练过程 实践中的梯度下降一般会线性衰减学习率直到第$\tau $次迭代: \eta _k = (1-\alpha )\eta _0 + \alpha \epsilon _{\tau } $\alpha=\frac{k}{\tau}$：:$\tau$步迭代之后会使学习率保持不变。 动量算法动量算法是一种模拟动力学的梯度下降方法，旨在加速学习，特别是处理高曲率、小但一致的梯度，或是带噪声的梯度。 想象在冰面上滑行的冰球，每当它沿着冰面最陡的部分下降时，它会累积该方向上的滑行速度。与此类似，如果将损失函数值看做是冰面的高度，将损失函数的负梯度看做是冰球所受重力，将更新参数的步长看做是冰球的速度，那么步长会累积指数衰减的历史梯度： \begin{align*} &v\leftarrow \alpha v-\eta \nabla_{\theta}L\\ &\theta \leftarrow \theta +v \end{align*} $v$：速度，累积了指数衰减的历史梯度； $\alpha \in [0,1)$:超参数，一般取0.5,0.9,0.99，决定了历史梯度的贡献衰减的有多快，越大衰减的越慢，之前梯度对现在方向的影响也越大 如果动量算法总是观测到梯度$g$，那么它在方向$-g$上不停加速直至达到最终速度，步长为$\frac{\eta \left | g \right |}{1-\alpha }$，对应着动量算法比梯度下降法快$\frac{1}{1-\alpha }$倍。 优点：训练速度快缺点：引入了额外的超参数 学习率对模型的性能有显著影响，是难以设置的超参数之一。损失函数高度敏感与参数空间中的某些方向，而不敏感于其他方向，动量算法可以一定程度缓解这些问题，但必须引入额外的超参数。改进思路是使用自适应的学习率： 对不同参数设置不同的学习率 随着训练过程自动调整学习率 下面要讲的几种方法正是基于这样的思路对梯度下降算法进行了改进。 AdaGred算法AdaGred算法按照历史梯度和的平方根比例缩放学习率，实现对不同参数学习率有不同的衰减速度，历史偏导较大的参数有较大的衰减速度： \begin{align*} &\theta _{k,i} = \theta _{k-1,i} - \frac{\eta }{\sqrt{G_{k-1,i}+\delta }}g_{k-1,i}\\ &g_{k,i} = \frac{\partial L}{\partial \theta _i}\\ &G_{k,i} = \sum_{j=1}^{k}g_{k,i} \end{align*} $G_{k,i}$：$\theta_i$历史k次偏导和的平方根; $\delta$:平滑项，防止除零操作，一般取值1e−8; RMSProp算法RMSProp算法改变AdaGred算法中梯度累积为指数加权平均，以在非凸设定下效果更好。 G_{k,i} = \rho G_{k-1,i} + (1-\rho)g_{k-1,i}^2Adam]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习：优化算法（三）—— 拉格朗日对偶法]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%9A%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95%EF%BC%88%E4%B8%89%EF%BC%89%E2%80%94%E2%80%94%20%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%AF%B9%E5%81%B6%E6%B3%95%2F</url>
    <content type="text"><![CDATA[在有约束的最优化问题中，常常利用拉格朗日对偶法(Lagrange duality)将原始问题转化为对偶问题，通过求解对偶问题得到原始问题的解。 原始问题假设$f(x),\ c_i(x),\ h_j(x)$是定义在$\mathbb{R}^n$上的连续可微函数，考虑带约束的最优化问题： \begin{align*} \underset{x \in R^n}{min}\ &f(x)\\ s.t.\ &c_i(x)\leqslant 0,\ i=1,2,...,k\\ &h_j(x)=0,\ j=1,2,...,l \end{align*}拉格朗日函数的极小极大问题拉格朗日函数：$\alpha_i,\ \beta_j$为拉格朗日乘子，其中 $\alpha_i\geqslant 0$ L(x,\alpha ,\beta )=f(x)+\sum_{i=1}^{k}\alpha_ic_i(x)+\sum_{j=1}^{l}\beta_jh_j(x)令： \theta_P(x)=\underset{\alpha ,\beta }{max}\ L(x,\alpha ,\beta )有： \theta_P(x)=\left\{\begin{matrix} f(x), &x满足原始问题约束 \\ +\infty, & 其他 \end{matrix}\right.带约束的原始问题可以转化为无约束的拉格朗日函数极小极大问题： \underset{x}{min}\theta_P(x)=\underset{x}{min}\ \underset{\alpha ,\beta }{max}\ L(x,\alpha ,\beta )拉格朗日的极大极小问题(对偶问题)令： \theta_D(x)=\underset{x }{min}\ L(x,\alpha ,\beta )拉格朗日的极大极小问题: \underset{\alpha ,\beta }{max}\theta_D(x)=\underset{\alpha ,\beta }{max}\ \underset{x}{min}\ \ L(x,\alpha ,\beta )KKT 条件定理1（弱对偶性）：如果原始问题和对偶问题都有最优解，则对偶问题的最优值是原始问题最优值的一个下界： \underset{\alpha ,\beta }{max}\ \underset{x}{min}\ \ L(x,\alpha ,\beta )\leqslant \underset{x}{min}\ \underset{\alpha ,\beta }{max}\ L(x,\alpha ,\beta )证明：强者中最弱的也比弱者中最强的要强 \theta_D(x)\leqslant L(x,\alpha ,\beta)\leqslant \theta_P(x)故： \underset{\alpha ,\beta }{max}\ \theta_D(x)\leqslant\underset{x}{min}\ \theta_P(x)定理2（强对偶性）：假设$f(x)$和$c_i(x)$是凸函数，$h_j(x)$是仿射函数，并且不等式约束$c_j(x)$是严格可行的，即存在x对所有i都有$c_i(x)&lt;0$，则$x^$和$\alpha^, \beta^$分别是原始问题和对偶问题的解的充要条件是$x^,\alpha^, \beta^$满足KKT条件： \left\{\begin{matrix} \nabla_xL(x^*,\alpha^*\beta^* )=0 & \\ \nabla_{\alpha }L(x^*,\alpha^*\beta^*)=0 & \\ \nabla_{\beta }L(x^*,\alpha^*\beta^* )=0 & \\ c_i(x^*)\leqslant 0 & i=1,2,...,k\\ h_j(x^*)=0& j=1,2,...,l\\ \alpha_i *\geqslant 0& i=1,2,...,k \\ \alpha_i *c_i(x^*)=0& i=1,2,...,k \end{matrix}\right.$\alpha_i c_i(x^)=0$称为KKT的对偶互补条件，$\alpha_i ,\ c_i(x^*)$必有一个为0.]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习：优化算法（二）——（拟）牛顿法]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%9A%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95%EF%BC%88%E4%BA%8C%EF%BC%89%E2%80%94%E2%80%94%20%E6%8B%9F%E7%89%9B%E9%A1%BF%E6%B3%95%2F</url>
    <content type="text"><![CDATA[牛顿法(Newton method)和拟牛顿法(quasi Newton method)也是求解最优化问题的常用迭代方法。当目标函数是凸函数时，可以得到全局最优解，否则不能保证得到全局最优解，但是其收敛速度相比梯度下降法要快。 假设 $f(x)$ 具有二阶连续偏导，考虑无约束最优化问题： x^*=\underset{x \in \mathbb{R}^n}{min}\ f(x)牛顿法牛顿法原理将$f(x)$在$x_k$附近进行二阶泰勒展开： f(x)=f(x_k)+g_k^T(x-x_k)+\frac{1}{2}(x-x_k)^TH(x_k)(x-x_k) $g(x)$：$f(x)$在x处的一阶偏导 $H(x)$：$f(x)$在x处的二阶偏导矩阵(海赛矩阵) 两边同时取导，得到过点$(x_k,g_k)$的切线方程： g=g_k+H_k(x-x_k)$f(x)$ 取极值的必要条件是在极值处偏数为0，即求$g(x)=0$，该问题可以用牛顿法求解： 1、选取合适的初值$x_0$，置k=02、计算$g_k$，若$\left | g_k \right | &lt; \varepsilon $，则停止计算，得到$x^=x_k$，否则用$g(x)$在当前位置的切线与x轴的交点*更新$x_k$（因此，牛顿法也被称为“切线法”） x_{k+1}=x_k-H_k^{-1}g_k 牛顿法有效性证明因为 $f(x)$ 是凸函数，那么$H_k$为正定矩阵($H_k^{-1}$也是正定矩阵)，即： H_k^{-1}g_k=\lambda g_k,\ \lambda > 0可以保证x更新方向总是沿着$f(x)$的负梯度方向： x_{k+1}=x_k-H_k^{-1}g_k=x_k-\lambda g_k牛顿法优缺点优点： 更快：相比一阶收敛的梯度下降法，牛顿法二阶收敛，收敛速度更快，牛顿法不但考虑了搜索的方向，还用二阶逼近来估计步长 更准：从几何上看，牛顿法是用一个二次曲面来拟合当前位置的局部曲面，而梯度下降则是用一个平面拟合当前局部曲面，所以牛顿法选择的下降路径会更符合真实的最优下降路径 缺点: 牛顿法仍然是一种局部最优化算法，在非凸问题中一般无法取到全局最优解； 每次迭代需要求解目标函数的海赛矩阵的逆矩阵，计算复杂度高； 拟牛顿法拟牛顿法对牛顿法改进的基本思路是：使用某个矩阵作为海赛矩阵或海赛矩阵的逆矩阵的近似，从而避免求解海赛矩阵。 拟牛顿条件：在切线方程中取$x=x_{k+1}$ g_{k+1}-g_k=H_k(x_{k+1}-x_{k})记$yk=g{k+1}-gk,\ \delta_k=x{k+1}-x_k$，则： y_k=H_k\delta_kDFP(Davidon-Fletcher-Powell)算法DFP使用$G_k$作为$H_k^{-1}$的近似，在每次迭代时通过如下公式更新$G_k$： G_{k+1}=G_{k}+P_k+Q_k G_{k+1}y_k=G_{k}y_k+P_ky_k+Q_ky_k为使$G_{k+1}$满足拟牛顿条件，可使： p_ky_k=\delta_k G_{k}y_k=-Q_ky_k容易找到这样的$P_k,\ Q_k$，得到： G_{k+1}=G_k+\frac{\delta_k \delta_k^T}{ \delta_k^T\delta_k}-\frac{G_ky_ky_k^TG_k}{y_k^TG_ky_k}可以证明，如果初始矩阵$G_0$是正定的，则迭代过程中的每个矩阵$G_k$都是正定的。 1、选取初始点$x_0$，取$G_0$为正定对称矩阵2、计算$g_k$，若$\left | g_k \right | &lt; \varepsilon $，则停止计算，得到$x^*=x_k$，否则:（1）计算$p_k=-G_kg_k$（2）一维搜索$\lambda_k$，满足： f(x_k+\lambda_kp_k)=\underset{\lambda\geqslant 0}{min}f(x_k+\lambda p_k)（3）置$x{k+1}=x_k+\lambda p_k$（4）计算$g{k+1}$，更新$G_{k+1}$ BFGS(Davidon-Fletcher-Powell)算法BFGS用$B_k$作为$H_k$的近似，在每次迭代时通过如下公式更新$B_k$： B_{k+1}=B_{k}+P_k+Q_k B_{k+1}\delta_k=B_{k}\delta_k+P_k\delta_k+Q_k\delta_k为使$G_{k+1}$满足拟牛顿条件，可使： p_k\delta_k=y_k B_{k}\delta_k=-Q_k\delta_k容易找到这样的$P_k,\ Q_k$，得到： B_{k+1}=B_k+\frac{y_k y_k^T}{ y_k^T\delta_k}-\frac{B_k\delta_k\delta_k^TB_k}{\delta_k^TB_k\delta_k}可以证明，如果初始矩阵$B_0$是正定的，则迭代过程中的每个矩阵$B_k$都是正定的。 1、选取初始点 $x_0$，取 $B_0$ 为正定对称矩阵2、计算$g_k$，若$\left | g_k \right | &lt; \varepsilon $，则停止计算，得到$x^*=x_k$，否则: （1）由$B_kp_k=-g_k$计算$p_k$（2）一维搜索$\lambda_k$，满足： f(x_k+\lambda_kp_k)=\underset{\lambda\geqslant 0}{min}f(x_k+\lambda p_k)（3）置$x{k+1}=x_k+\lambda p_k$（4）计算$g{k+1}$，更新$B_{k+1}$ 对比梯度下降和牛顿法 更新公式 优点 缺点 梯度下降 $x_{k+1}=x_k-\eta \nabla_xf(x)$ 简单 局部最优、速度慢 牛顿法 $x_{k+1}=x_k-H_k^{-1}g_k$ 更快更准 局部最优、复杂度高 BFP $x_{k+1}=x_k-G_kg_k$ 不用计算海赛矩阵 BFGS $x_{k+1}=x_k-B_kg_k$ 不用计算海赛矩阵]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习：优化算法（五）—— 模拟退火算法]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%9A%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95%EF%BC%88%E4%BA%94%EF%BC%89%E2%80%94%E2%80%94%20%E6%A8%A1%E6%8B%9F%E9%80%80%E7%81%AB%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[算法背景模拟退火算法最早的思想由Metropolis（1953）提出，1983年Kirkpatrick等将其应用于组合优化，有以下优点： 解决NP问题 克服局部最优 克服初值依赖性 物理退火物理退火过程退火：将固体加热到足够高的温度，使分子呈现随机排列状态，然后缓慢降温，使得分子在每一温度时，能够有足够时间找到稳定状态，最后分子以近似最低能量状态排列，达到某种稳定状态。但如果快速降温（淬火，quenching）会导致不是最低能态的非晶体. 升温过程：增强粒子动能，消除系统可能的非均匀态 等温过程：对于与环境换热而温度不变的封闭系统，系统自发朝着自由能降低的方向变化，当自由能达到最低时，系统达到平衡态 冷却过程：粒子热运动减弱并渐趋有序，系统能量逐渐下降，从而得到低能的晶体结构 Boltzman概率分布在温度T，分子停留在状态r满足Boltzman概率分布，其中$Z(T)$为概率分布的标准化因子： P(E_r) = \frac{1}{Z(T)}e^{-\frac{E_r}{KT}}选定两个状态，分子处于这两个状态的概率差可表示为： P(E_1) - P(E_2) = \frac{1}{Z(T)}e^{-\frac{E_1}{KT}}[1-e^{-\frac{E_2-E_1}{KT}}] 同一温度下，能量越小的状态分子处于该状态的概率越大，即分子总是趋向于低能状态； 温度越高，不同能量状态的概率差异性越小，即分子处于各能量状态的随机性越强； 随着温度下降，分子处于低能状态的概率越来越大，当温度趋于0时，分子停留在最低能量状态的概率趋于1； 模拟热平衡——依概率接受新状态Metropolis准则用于模拟固体在恒温下达到热平衡的过程: 在温度T，当前能量状态i→状态j 若$E_j &lt; E_i$，则直接接收j为新的状态 若$E_j &gt;= E_i$，以$p = e^{-\frac{(E_j - E_i)}{KT}}$的概率接受j为当前状态，以$1-p$的概率保持原始状态 模拟退火组合优化与物理退货的相似性： 组合优化 物理退货 可行解 金属状态 最优解 能量最低的状态 目标函数 能量 设定初始温度 升温过程 Metropolis抽样 等温过程 参数下降 冷却过程 问题定义对于最优化问题： \underset{\boldsymbol{ \theta }}{arg}max f(\boldsymbol{ \theta })问题求解代码实现关键参数改进应用]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习：优化算法（四）—— 遗传算法]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%9A%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95%EF%BC%88%E5%9B%9B%EF%BC%89%E2%80%94%E2%80%94%20%E9%81%97%E4%BC%A0%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[算法背景术语界定 算法核心问题定义 argmax f(x_{1}) 编码DNA 定义适应度函数 问题求解算法描述经过N代进化，从末代种群中选出最优个体，每轮迭代： 自然选择 交叉重组 基因突变 算法流程图算法实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134#!/usr/bin/env python# coding:utf-8&quot;&quot;&quot;遗传算法：“在程序里生宝宝, 杀死不乖的宝宝, 让乖宝宝继续生宝宝”问题描述：1. DNA：将每个可行解编码为DNA2. 适应度函数：get_fitness(NDA)-&gt;value，计算每个DNA的适应度进化过程：1. 自然选择（selection）：适应性高的个体更容易存活2. 交叉重组（crossover）：父代基因片段交换产生新的子代3. 基因突变（mutation）：子代有一定概率发生基因突变参数选择：1. 种群大小2. 交叉概率3. 变异概率4. 迭代次数&quot;&quot;&quot;import numpy as npfrom erate import get_expected_rateclass Ga(object): def __init__(self, prob_matrix, for_accept, pop_size=50, cross_rate=0.6, mutate_rate=0.02, max_gen=100): &quot;&quot;&quot; 初始化遗传算法的进化参数，并生成初代DNA种群 :param prob_matrix: 概率矩阵，type = ndarray，shape = (members_num * captains_num) :param for_accept: 每个队长待接收的队员数，ndarray shape = captains_num :param pop_size: 种群大小 :param cross_rate: 交叉概率 :param mutate_rate: 变异概率 :param max_gen: 最大迭代次数 &quot;&quot;&quot; self.L = prob_matrix self.FOR_ACCEPT_NUM = for_accept self.MEMBERS_NUM = self.L.shape[0] self.CAPTAINS_NUM = self.L.shape[1] self.POP_SIZE = pop_size self.DNA_SIZE = L.size self.CROSS_RATE = cross_rate self.MUTATE_RATE = mutate_rate self.MAX_GEN = max_gen self.population = np.random.randint(2, size=(self.POP_SIZE, self.DNA_SIZE)) self.fitness = np.apply_along_axis(self.get_fitness, axis=1, arr=self.population) def get_fitness(self, dna): &quot;&quot;&quot; 适应度：计算单个DNA的适应度 :param dna: ndarray :return: 推荐矩阵ndarray &quot;&quot;&quot; fitness = get_expected_rate(self.L, self.dna_decode(dna), self.FOR_ACCEPT_NUM) return fitness def dna_encode(self, individual): &quot;&quot;&quot; DNA编码：将原始形式的参数编码为二进制形式的NDA列表 :param individual: 存储二进制DNA的矩阵，(pop_size,DNA_size) :return: dna &quot;&quot;&quot; dna = individual.ravel(&#x27;C&#x27;) return dna def dna_decode(self, dna): &quot;&quot;&quot; DNA解码：将二进制形式的DNA解码为原始形式的参数形式 :param dna: 个体dna :return: 原始推荐矩阵 &quot;&quot;&quot; individual = dna.reshape(self.MEMBERS_NUM, -1) return individual def select(self, population, fitness): &quot;&quot;&quot; 自然选择：依适应度高低/概率从原始种群中有放回地抽样出等容量的新种群，原地修改 :param population: 种群矩阵ndarray :param fitness: 种群中每个个体的适应度,ndarray :return: 新的种群ndarray &quot;&quot;&quot; idx = np.random.choice(np.arange(self.POP_SIZE), size=self.POP_SIZE, replace=True, p=fitness / fitness.sum()) population[:] = population[idx] return def crossover(self, population, cross_rate): &quot;&quot;&quot; 交叉重组：给定父代群体，通过奇偶个体以一定概率进行交叉重组产生子代群体 :param population: 给定父代群体，通过交叉重组产生子代群体 :param cross_rate: 交叉重组的概率 :return: 子代群体 &quot;&quot;&quot; for parent1, parent2 in zip(population[::2],population[1::2]): if np.random.rand() &lt; cross_rate: cross_points = np.random.randint(0, 2, self.DNA_SIZE).astype(np.bool) parent1[cross_points], parent2[cross_points] = parent2[cross_points], parent1[cross_points] return def mutate(self, population, mutation_rate): &quot;&quot;&quot; 基因突变：子代DNA中的基因码位有一定概率发生变异 :param population: 待变异的DNA :param mutation_rate: 突变的概率 :return: 变异后的DNA &quot;&quot;&quot; mutate_points = np.random.choice(2, size=population.shape, p=[1-mutation_rate, mutation_rate]).astype(np.bool) population[mutate_points] = 1 ^ population[mutate_points] self.fitness = np.apply_along_axis(self.get_fitness, axis=1, arr=self.population) return def evolution(self): &quot;&quot;&quot; 种群进化过程，迭代&quot;自然选择-交叉重组-基因突变&quot;，退出条件为满足一定的迭代次数 :return: 最优的个体 &quot;&quot;&quot; for gen in range(self.MAX_GEN): print(&#x27;generation: &#123;&#125;，fitness: &#123;&#125;&#x27;.format(gen, self.fitness.max())) self.select(self.population, self.fitness) self.crossover(self.population, self.CROSS_RATE) self.mutate(self.population, self.MUTATE_RATE) print(&#x27;generation: &#123;&#125;，fitness: &#123;&#125;&#x27;.format(gen + 1, self.fitness.max())) res_id = self.fitness.argmax() print(&#x27;近似最优期望组队成功率：&#123;&#125;&#x27;.format(self.fitness[res_id])) print(&#x27;最优的分配方式: \n&#123;&#125;&#x27;.format(self.dna_decode(self.population[res_id]))) return self.fitness[res_id]if __name__ == &quot;__main__&quot;: # 测试 L = np.random.rand(6, 3) for_accept = np.ones(shape=L.shape[1], dtype=int) * (L.shape[0]//2) ga = Ga(L, for_accept) ga.evolution() 算法应用优化方法 其他话题]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习：其它话题（一）—— 类别不平衡问题]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%85%B6%E5%AE%83%E8%AF%9D%E9%A2%98%EF%BC%88%E4%B8%80%EF%BC%89%E2%80%94%E2%80%94%20%E7%B1%BB%E5%88%AB%E4%B8%8D%E5%B9%B3%E8%A1%A1%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[问题类别不平衡（class-imbalance）是指在分类任务中不同类别的训练样本数差别很大的情况。类别不平衡问题在实际数据中是很常见的，比如在癌症检查中可能只有极少部分病人患上了癌症，而其余大多数样本都是健康的个体；又比如欺诈识别，欺诈样本与正常样本的比例可能会达到1：100000。 精度是在评估分类模型性能时最常用的一个指标，如果不同类别的训练样例数目差别很大（正例：负例 = 1:99），学习模型只需要返回一个永远将样本预测为反例的学习器，就能达到99%的精度，然而这样的学习器往往没有价值，这个精度并不能反映学习器的性能，只是反映了类别的分布。 分析以线性分类器为例，在我们使用$y=w^Tx+b$对新样本x进行分类时，事实上是用预测值与一个阈值进行比较，例如通常在y&gt;0.5时盼为正例，否则为反例。y实际表达了是正例的可能性，决策器的分类规则为： \frac{y}{1-y} > 1 \Rightarrow positive当正负样例不同时，令$m^+$表示正例数目，$m^-$表示负例数目，则观测几率为$\frac{m^+}{m^-}$，通常我们假设观测几率代表了真实几率，因此只要分类器的预测几率高于观测几率，就判为正例： \frac{y}{1-y} > \frac{m^+}{m^-} \Rightarrow positive但是我们的分类器是基于第一个式子来进行决策的，因此需要对其观测值进行调整，使得在基于第一个式子来决策时实际是在执行第二个式子： \frac{\hat{y}}{1-\hat{y}} = \frac{y}{1-y} \times \frac{m^-}{m^+}这就是类别不平衡学习的一个基本策略——再缩放，虽然思路简单，但是实际操作却并不平凡，现有技术大体上有三种做法：过采样、欠采样、移动阈值。 再缩放也是代价敏感学习的基础，只需将式中$\frac{m^-}{m^-}$用$\frac{cost^+}{cost^-}$替代即可，其中$cost^+$是将正例判为负例的代价。 接下来简单对常用的过采样和欠采样算法做一说明，学习不均衡学习最好的方式就是学习imbalanced-learn的API。 过采样（oversampling） 原理：增加训练集中少数类样本，以使各类别样本数相对均衡； 实现：简单复制、设置权重、插值产生； 评价：简单对少数派样本重复抽样，容易造成过拟合; 代表算法：SMOTE，通过对训练集中的正例进行插值产生额外的正例; 过采样和交叉验证过采样只能应用于训练集，也就是说，当我们同时需要进行过采样和交叉验证时，应该在划分出训练集和验证集之后，针对新的训练集进行过采样，这样就可以避免将验证集信息“泄露给”训练集，造成过拟合，见下图： 最左边那列表示原始数据，里面包含了少数类下的两个样本。我们拷贝这两个样本作为副本，然后再进行交叉验证。在迭代的过程，我们的训练样本和验证样本会包含相同的数据，如最右那张图所示，这种情况下会导致过拟合或误导的结果，合适的做法应该如下图所示。 也就是说我们每次迭代做交叉验证之前先将验证样本从训练样本中分离出来，然后再对训练样本中少数类样本进行过采样（橙色那块图所示）。在这个示例中少数类样本只有两个，所以我拷贝了三份副本。这种做法与之前最大的不同就是训练样本和验证样本是没有交集的，因而我们获得一个比之前好的结果。即使我们使用其他的交叉验证方法（譬如 k-flod）或其他的过采样方法（如SMOTE），做法也是一样的。 随机上采样随机上采样原理随机过采样是随机在少数样本中抽样生成新副本，与原始数据共同构成新的均衡数据集的方法： 在训练集的少数类集合中随机选中一些样本； 复制这些样本生成样本集E； 将新样本集E添加到原始训练集集中，得到新的训练集； 随机上采样代码示例12345678910111213141516&gt;&gt;&gt; from sklearn.datasets import make_classification&gt;&gt;&gt; X, y = make_classification(n_samples=5000, n_features=2, n_informative=2,... n_redundant=0, n_repeated=0, n_classes=3,... n_clusters_per_class=1,... weights=[0.01, 0.05, 0.94],... class_sep=0.8, random_state=0)&gt;&gt;&gt; from imblearn.over_sampling import RandomOverSampler&gt;&gt;&gt; ros = RandomOverSampler(random_state=0)&gt;&gt;&gt; X_resampled, y_resampled = ros.fit_sample(X, y)&gt;&gt;&gt; from collections import Counter&gt;&gt;&gt; print(sorted(Counter(y_resampled).items()))[(0, 4674), (1, 4674), (2, 4674)]&gt;&gt;&gt; from sklearn.svm import LinearSVC&gt;&gt;&gt; clf = LinearSVC()&gt;&gt;&gt; clf.fit(X_resampled, y_resampled) # doctest : +ELLIPSISLinearSVC(...) SMOTESMOTE(Synthetic Minority Over-sampling Technique,合成少数上采样) 方法并不是采取简单复制样本的策略来增加少数类样本， 而是通过分析少数类样本来创建新的样本的同时对多数类样本进行欠采样。正常来说当我们简单复制样本的时候，训练出来的分类器在预测这些复制样本时会很有信心的将他们识别出来，你为他知道这些复制样本的所有边界和特点，而不是以概括的角度来刻画这些少数类样本。但是，SMOTE 可以有效的强制让分类的边界更加的泛化，一定程度上解决了不够泛化而导致的过拟合问题。在 SMOTE 的论文中用了很多图来进行解释这个问题的原理和解决方案，所以我建议大家可以去看看。 但是，我们有一定必须要清楚的是 使用 SMOTE 过采样的确会提升决策边界，但是却并没有解决前面所提到的交叉验证所面临的问题。 如果我们使用相同的样本来训练和验证模型，模型的技术指标肯定会比采样了合理交叉验证方法所训练出来的模型效果好。 SMOTE样本生成的原理SMOTE和ADASYN采用相同的算法来生成新的样本：对于一个样本$xi$，从它的k邻近样本点中随机选择一个邻居$x{zi}$，随机在它们连线上生成一个新的样本: x_{new} = x_i + \lambda (x_{zi}- x_i)\\ \lambda \in [0,1]SMOTE原理如下图： SMOTE代码样例123456789&gt;&gt;&gt; from imblearn.over_sampling import SMOTE, ADASYN&gt;&gt;&gt; X_resampled, y_resampled = SMOTE().fit_sample(X, y)&gt;&gt;&gt; print(sorted(Counter(y_resampled).items()))[(0, 4674), (1, 4674), (2, 4674)]&gt;&gt;&gt; clf_smote = LinearSVC().fit(X_resampled, y_resampled)&gt;&gt;&gt; X_resampled, y_resampled = ADASYN().fit_sample(X, y)&gt;&gt;&gt; print(sorted(Counter(y_resampled).items()))[(0, 4673), (1, 4662), (2, 4674)]&gt;&gt;&gt; clf_adasyn = LinearSVC().fit(X_resampled, y_resampled) 欠采样（undersampling） 原理：减少训练集中多数类的样本，以使各类别样本数相对均衡； 实现：直接丢弃、设置权重、ENN、集成方法； 评价：随机丢弃多数类样本，可能丢失一些重要信息造成欠拟合; 代表算法： ENN：删除那些类别与其最近的三个近邻样本中的两个或两个以上的样本类别不同的样本； easyEnsemble：利用集成学习机制，将负例划分为若干集合供不同学习器使用，这样对每个学习器来说都进行了欠采样，但在全局看来却不会丢失重要信息; 随机欠采样随机从原始数据集中的多数类中删除一些样本。 ENN随机欠抽样方法未考虑样本的分布情况，采样具有很大的随机性，可能会删除重要的多数类样本信息。针对以上的不足，Wilson 等人提出了一种最近邻规则(edited nearest neighbor: ENN)： 基本思想：删除那些类别与其最近的三个近邻样本中的两个或两个以上的样本类别不同的样本 缺点：因为大多数的多数类样本的样本附近都是多数类，所以该方法所能删除的多数类样本十分有限 easyEnsemble 首先通过从多数类中独立随机抽取出若干子集 将每个子集与少数类数据联合起来训练生成多个基分类器 最终将这些基分类器组合形成一个集成学习系统 EasyEnsemble 算法被认为是非监督学习算法，因此它每次都独立利用可放回随机抽样机制来提取多数类样本 结合过采样和欠采样 SMOTE+ENN SMOTE + Tomek 移动阈值（threshold-moving） 原理：不改变训练集，但将$\frac{\hat{y}}{1-\hat{y}} = \frac{y}{1-y} \times \frac{m^-}{m^+}$嵌入到决策过程中； 重采样后如何预测概率不均衡学习和代价敏感学习参考 处理不均衡数据：下采样、上采样及正确的交叉验证 imbalanced-learn User-Guide 不均衡学习的抽样方法]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习：工作流（一）—— 数据预处理]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%B7%A5%E4%BD%9C%E6%B5%81%EF%BC%88%E4%B8%80%EF%BC%89%E2%80%94%E2%80%94%20%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86%2F</url>
    <content type="text"><![CDATA[模型训练之前的数据准备阶段可以分为三个步骤，虽然这些步骤往往是相互交织、反复迭代的，但是为了指导数据准备阶段有序进行，有必要尝试对各个步骤进行界定： 数据收集：收集所有可能与所研究问题相关的数据； 数据预处理：为满足模型对数据的要求而对原始数据进行的一系列操作； 特征工程：从现有数据中挖掘出有意义的特征，常见的如特征选择、特征抽取； 在做数据预处理时，总的原则可以概括为：以尽可能少的信息损失为代价使数据满足模型的要求。具体来说，评价一个预处理操作的好坏可以从以下两方面分析： 尽可能保留原始数据中的信息； 尽可能减少操作所代入的噪声； 预处理操作所带来的信息量变化$\Delta I $可以用下面式子来表示，其中$\Delta H $表示该操作所带来的信息增益，$\Delta \varepsilon $表示该操作所引入的新的噪声，增加信息增益、减少噪声的操作可以提升数据质量，这是数据处理的核心。 \Delta I = \Delta H - \Delta \varepsilon统计数据分类在统计学中，统计数据主要可分为四种类型，分别是定类数据，定序数据，定距数据，定比变量： 定类数据（Nominal）：名义级数据，数据的最低级，表示个体在属性上的特征或类别上的不同变量，仅仅是一种标志，没有序次关系。例如， ”性别“，”男“编码为1，”女“编码为2。 定序数据（Ordinal）:数据的中间级，用数字表示个体在某个有序状态中所处的位置，不能做四则运算。例如，“受教育程度”，文盲半文盲=1，小学=2，初中=3，高中=4，大学=5，硕士研究生=6，博士及其以上=7。 定距数据（Interval）:具有间距特征的变量，有单位，没有绝对零点，可以做加减运算，不能做乘除运算。例如，温度。 定比变量（Ratio）:数据的最高级，既有测量单位，也有绝对零点，例如职工人数，身高。 一般来说，数据的等级越高，应用范围越广泛，等级越低，应用范围越受限。不同测度级别的数据，应用范围不同。等级高的数据，可以兼有等级低的数据的功能，而等级低的数据，不能兼有等级高的数据的功能 缺失值处理基本术语： 完全数据集/变量/样本：不含缺失值的数据集/变量/样本； 不完全数据集/变量/样本：含有缺失值的数据集/变量/样本； 缺失值产生的原因缺失值的产生的原因多种多样，主要分为机械原因和人为原因： 机器原因：由于机器原因导致的数据保存失败，如机器故障、磁盘损坏等； 人为原因：由于人的主观失误或故意隐瞒造成的数据缺失，如录入人员漏录，或被采访着拒绝透露； 缺失值的分类按缺失机制划分“缺失机制”并非是指造成缺失值的原因，而是描述某个变量是否发生缺失与观测变量值的关系，Rubin (1976)最早将缺失值的机制分为三类： 完全随机缺失（missing completely at random,MCAR）：某变量是否发生缺失与其他变量取值和该变量自身取值都无关，即变量缺失值的出现完全是随机的；例如，我们一般认为性别信息的缺失是MCAR，因为性别信息的缺失与被访者是男是女、或其他变量的取值无明显关系； 随机缺失(missing at random,MAR)：某变量是否缺失与其他变量取值有关而与该变量自身取值无关；例如，在一次测试中，如果IQ达不到最低要求的100分，那么将不能参加随后的人格测验。在人格测验上因为IQ低于100分而产生的缺失值为MAR，因为人格测验产生的缺失只与IQ有关，而与人格测验本身无关； 非随机缺失(missing not at random,MNAR)：某变量是否缺失的分布与该变量自身取值相关；例如，公司新录用了20名员工，由于6名员工表现较差在试用期内辞退，试用期结束后的表现评定中，辞退的6名员工的表现分即为非随机缺失，因为表现分缺失与表现分过低有关； 简单说来： “随机缺失”就是指变量是否缺失和变量自身取值无关，如果也和其他变量取值无关则称“完全随机缺失”； “非随机缺失”就是指变量是否缺失和变量自身取值有关； 数据是否是完全随机缺失可以采用单变量t检验和Little (1988)提出的多元t检验。其原理是，如果变量X的缺失值是完全随机的，那么在X上缺失和非缺失两组样本间在第二个变量Y上的均值差异是不显著的，否则存在某种相关性。多元t检验可在SPSS上运行。然而均值差异比较并非能保证MCAR，因为在MAR和MANR条件先也能产生相等的均值(Enders, 2010)。 目前，缺少检验MAR的有效程序，幸运的是严重违反MAR假设的情况相对较少(Graham et al., 1997, p. 354; Schafer &amp; Graham, 2002)。研究者推荐使用包含辅助变量（Auxiliary Variables，与缺失值相关的因素）的方法减少估计偏差并提高满足MAR假设的可能性(Collins et al., 2001; Rubin, 1996; Schafer, 1997; Schafer &amp; Graham, 2002)。具体来说，在分析缺失值数据时将辅助变量纳入分析过程，但辅助变量并不出现在模型中。 按缺失值所属特征划分 单值缺失：所有的缺失值都是同一属性 任意缺失：缺失值属于不同的属性 单调缺失：对于时间序列类的数据，可能存在随着时间的缺失 缺失值处理方法大多数模型都要求数据集中不能含有缺失值（除了决策树系算法），当数据集中出现缺失值时，首先应尝试从数据源头进行补全（虽然常常被忽视），只有当这些缺失值确实无法被获取到或者获取的成本很高时，才需考虑特殊的缺失值处理方法。缺失值处理大多是基于MAR假设。 对于缺失值的处理，总体上可分为删除和插补两种方式，删除会损失掉一些信息，插补又会引入一些噪声，在选择缺失值处理的方法时总的原则是使得操作所引起的信息损失达到最小，即尽可能少的丢弃原有数据，尽可能少的引入新的噪声： \Delta loss = \Delta H + \Delta noise $\Delta H$：处理操作导致的原有数据的减少； $\Delta noise$：处理操作引入的噪声； 删除当插补所引入的噪声比删除所丢弃的信息还要多的时候，应该选择删除缺失值所在的样本或特征： 样本删除（casewise deletion）：如果某些行（样本）中缺失值很多，而这些样本在总样本中的比例较小时，应舍弃这些样本；有时也叫列表删除（listwise deletion） 特征删除（variable deletion）：如果某些列（特征）中缺失值很多，二这些特征又不是很重要时，应舍弃这些特征；有时也叫配对删除（pairwise deletion） 插补插补相当于预测缺失值，这和监督学习的目标很像，但所采用的方法一般没有后者那么精确。 当删除不完全样本所带来的信息丢失比插补所引入的噪声还要多的时候，应对缺失值进行插补。插补又可分为单值插补和多重插补： 单值插补：为每个缺失值提供一个替代值； 多重插补：为每个缺失值提供多个替代值，从而得到多个完整数据集，然后对每个完整数据集进行统计分析，得到一个最优的替代值；多重插补效果更好，所得结论也更稳定； 单值插补单值插补操作简单，但无法反映缺失值的不确定性，且导致参数估计方差产生偏倚，常用的单重插补方法包括：全局均值插补法、组均值插补法、中位数插补法、众数插补法、回归插补法、末次观测值结转法、热平台插补法、冷平台插补法、比插补、最近邻插补等。 对于标称型特征： 新类别填充：将缺失值NAN作为新的类别处理，引入的噪声较小且保留了原始数据全部信息； 众数填充：用该特征上出现频率最高的值来填充； 条件众数：在监督学习中，如果只有训练数据中有缺失值，则可以用与该样本同label下的条件众数进行填充； 对于连续特征： 新类别填充：先对该特征做离散化，再将NAN作为新的类别处理 统计量填充：用特征均值、分位值（最大最小中位值）、随机数、插值、固定值、上下数据填充缺失值，这种方法会产生估计偏差，最不为方法学者推荐； 条件统计量填充：在监督学习中，如果只有训练数据中有缺失值，则可以用与该样本同label下的条件均值、条件中位数进行填充 回归法：通过其他特征建立回归模型预测缺失值；同样会产生估计偏差； 多重插补按照Landerman 等( 1997) 建议当缺失比例小于2%时, 用均数替代; 当缺失比例在2%—5%之间时, 用最大似然估计替代; 而当缺失比利大于5%时, 用多项回归替代(multiple imputation)。 模型拟合填充单值插补是最容易实现的，也是以前人们经常使用的，但是它对样本存在极大的干扰，尤其是当插补后的值作为解释变量进行回归时，参数的估计值与真实值的偏差很大。相比较而言，极大似然估计和多重插补是两种比较好的插补方法，与多重插补对比，极大似然缺少不确定成分，所以越来越多的人倾向于使用多值插补方法。 KNN算法：对于缺失了特征f的样本s，从其它不缺失特征s的样本集中找到与当前样本距离前k近的样本，取这k个样本在特征f上的平均值/众数作为样本s在特征f上的缺失值； 极大似然估计（Max Likelihood ,ML）：在缺失类型为随机缺失的条件下，假设模型对于完整的样本是正确的，那么通过观测数据的边际分布可以对未知参数进行极大似然估计，实际中常采用的计算方法是期望值最大化(Expectation Maximization，EM）。该方法比删除个案和单值插补更有吸引力，它一个重要前提：适用于大样本。有效样本的数量足够以保证ML估计值是渐近无偏的并服从正态分布。但是这种方法可能会陷入局部极值，收敛速度也不是很快，并且计算很复杂。 多重插补（Multiple Imputation，MI）：多值插补的思想来源于贝叶斯估计，认为待插补的值是随机的，它的值来自于已观测到的值。具体实践上通常是估计出待插补的值，然后再加上不同的噪声，形成多组可选插补值。根据某种选择依据，选取最合适的插补值。多重插补方法分为三个步骤：①为每个空值产生一套可能的插补值，这些值反映了无响应模型的不确定性；每个值都可以被用来插补数据集中的缺失值，产生若干个完整数据集合。②每个插补数据集合都用针对完整数据集的统计方法进行统计分析。③对来自各个插补数据集的结果，根据评分函数进行选择，产生最终的插补值。 假设一组数据，包括三个变量Y1，Y2，Y3，它们的联合分布为正态分布，将这组数据处理成三组，A组保持原始数据，B组仅缺失Y3，C组缺失Y1和Y2。在多值插补时，对A组将不进行任何处理，对B组产生Y3的一组估计值（作Y3关于Y1，Y2的回归），对C组作产生Y1和Y2的一组成对估计值（作Y1，Y2关于Y3的回归）。 当用多值插补时，对A组将不进行处理，对B、C组将完整的样本随机抽取形成为m组（m为可选择的m组插补值），每组个案数只要能够有效估计参数就可以了。对存在缺失值的属性的分布作出估计，然后基于这m组观测值，对于这m组样本分别产生关于参数的m组估计值，给出相应的预测即，这时采用的估计方法为极大似然法，在计算机中具体的实现算法为期望最大化法（EM）。对B组估计出一组Y3的值，对C将利用 Y1,Y2,Y3它们的联合分布为正态分布这一前提，估计出一组(Y1，Y2）。 上例中假定了Y1,Y2,Y3的联合分布为正态分布。这个假设是人为的，但是已经通过验证（Graham和Schafer于1999），非正态联合分布的变量，在这个假定下仍然可以估计到很接近真实值的结果。 多重插补和贝叶斯估计的思想是一致的，但是多重插补弥补了贝叶斯估计的几个不足。 (1)贝叶斯估计以极大似然的方法估计，极大似然的方法要求模型的形式必须准确，如果参数形式不正确，将得到错误得结论，即先验分布将影响后验分布的准确性。而多重插补所依据的是大样本渐近完整的数据的理论，在数据挖掘中的数据量都很大，先验分布将极小的影响结果，所以先验分布的对结果的影响不大。 (2)贝叶斯估计仅要求知道未知参数的先验分布，没有利用与参数的关系。而多重插补对参数的联合分布作出了估计，利用了参数间的相互关系。 插补处理只是将未知值补以我们的主观估计值，不一定完全符合客观事实。以下分析都是理论分析，对于缺失值由于它本身无法观测，也就不可能知道它的缺失所属类型，也就无从估计一个插补方法的插补效果。另外这些方法通用于各个领域，具有普遍性，那么针对一个领域的专业的插补效果就不会很理想，正是因为这个原因，很多专业数据挖掘人员通过他们对行业的理解，手动对缺失值进行插补的效果反而可能比这些方法更好。缺失值的插补是在数据挖掘过程中为了不放弃大量的信息，而采用的人为干涉缺失值的情况，无论是那种处理方法都会影响变量间的相互关系，在对不完备信息进行补齐处理的同时，我们或多或少地改变了原始的数据的信息系统，对以后的分析存在潜在的影响，所以对缺失值的处理一定要慎重。 参考机器学习中如何处理缺失数据？ 机器学习预处理之数据值缺失 流行病调查中缺失数据的常用填补方法)]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习：工作流（三）—— 模型优化和融合]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%B7%A5%E4%BD%9C%E6%B5%81%EF%BC%88%E4%B8%89%EF%BC%89%E2%80%94%E2%80%94%20%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96%E5%92%8C%E8%9E%8D%E5%90%88%2F</url>
    <content type="text"><![CDATA[sklearn.model_selection.GridSearchCV，用于自动搜索超参数的最优值。GridSearchCV从之前的grid_search模块移动到了model_selection模块。 网格搜索使用思路： 按照超参数的重要度（依赖于经验和理论）依次对参数进行调优，对于每个参数或每两个参数，调优的过程如下 首先猜测待调优参数的大致范围，其余参数取初始估计值 将该超参数名作为key（字符串），对应的取值范围作为value（列表）构造出param_grid字典，连通其他参数一起传入GridSearchCV构造网格搜索模型。交叉验证中最重要的是确定评估指标metrics和评估方法(传入k-fold 的indices) 在使用GridSearchCV对象fit完整训练集时，会系统的遍历多种参数组合，通过交叉验证确定最佳的参数 通过GridSearchCV对象的bestparams 属性获取最佳参数，bestscore属性获取最佳性能得分 工作原理： 构造原始模型，非调优参数取预估值 构造调优参数字典，交叉验证性能指标和评估方法 将原始模型，交叉验证参数连通其他参数连同其他参数一起传入GridSearchCV构造函数，构造网格搜索模型 使用网格搜索模型fit完整训练集，开始执行网格搜索： 每轮将待调优的参数的一个组合传入分类器 对分类器按照所选的评估方法和评估指标进行交叉验证 记录最佳参数结果，再用最佳参数在完全训练集上refit一个模型并返回该“最佳模型”（如果使用贪心算法分步选取参数的话，得到最佳参数比得到最佳模型更重要） 通过训练好的网格搜索模型获取最佳参数或者直接通过它的predict方法进行预测 缺点： 这个方法适合于小数据集，一旦数据的量级上去了，很难得出结果。此时通常采用一种贪心算法：拿当前对模型影响最大的参数调优，直到最优化；再拿下一个影响最大的参数调优，如此下去，直到所有的参数调整完毕。这个方法的缺点就是可能会调到局部最优而不是全局最优，但是省时间省力，巨大的优势面前，还是试一试吧，后续可以再拿bagging再优化。 参数解读class sklearn.model_selection.GridSearchCV(estimator, param_grid, scoring=None, fit_params=None, n_jobs=1, iid=True, refit=True, cv=None, verbose=0, pre_dispatch=‘2*n_jobs’, error_score=’raise’, return_train_score=True) estimator：所使用的分类器对象，如estimator = XGBClassifier( learning_rate =0.1, n_estimators=500, max_depth=5,min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,objective= &#39;binary:logistic&#39;, nthread=5, scale_pos_weight=1, seed=27)，并且传入除需要确定最佳的参数之外的其他参数。每一个分类器都需要一个scoring参数，或者score方法。 param_grid：传入字典，其中当前调优参数名（字符串）作为字典的key，参数备选取值（列表）作为value scoring：性能指标，有以下几种情形： None，使用分类器默认的性能指标（如果有的话） 单个值：可以是一个表示性能指标的字符串或者是一个可调用的返回性能指标的函数； 多个值：字符串列表或者是一个字典，名字作为key，可调用函数作为value 自定义函数：每个计分器返回一个值，值越大说明性能越好（对于一些损失函数取相反数） n_jobs：线程数，默认为1，取-1跟CPU核数一致。取代了较早版本中是nthred pre_dispatch：指定总共分发的并行任务数。当n_jobs大于1时，数据将在每个运行点进行复制，这可能导致OOM，而设置pre_dispatch参数，则可以预先划分总共的job数量，使数据最多被复制pre_dispatch次 iid：默认True，则假设数据在folds间独立同分布，误差估计为所有样本之和，而非各个fold的平均。 cv：交叉验证参数，有以下几种情形： 默认为None，使用3-k交叉验证 整数，指定fold的折数 产生 trainset,valset indices（训练集-验证集索引列表）的可迭代对象或生成器 refit：默认为True，在网格搜索结束后，用最佳参数结果再次fit一遍全部数据集。 重新训练的模型可以通过GridSearchCV实例的bestestimator属性得到 允许直接通过GridSearchCV实例的predict方法对新数据进行预测。 同样best_index_, best_score_ 和 best_parameters_这些属性只有在refit之后才能访问 verbose：日志冗长度，int： 0：不输出训练过程2：1：偶尔输出 &gt;1：对每个子模型都输出 error_score :‘raise’ (default) or numeric,Value to assign to the score if an error occurs in estimator fitting. If set to ‘raise’, the error is raised. If a numeric value is given, FitFailedWarning is raised. This parameter does not affect the refit step, which will always raise the error. return_train_score:缺省值为True，如果为False,cv_results_属性不会包含训练scores 常用属性其中best_*_属性必须refit=True时才可用： cv_results_:网格搜索结果，ndarray字典，可以被转化为DataFrame best_estimator_:最佳模型，当refit=True时，返回使用最佳参数在完整训练集上重新拟合的模型； best_score_：最佳得分，best_estimator_交叉验证的平均性能评估得分 best_params_ :最佳参数， 在交叉验证中得到的最佳参数 scorer_: function or a dict,网格搜索使用的评估桉树 n_splits_:所用交叉验证的折数 常用方法predict方法只有在refit=True时才可用： fit(X, y=None, groups=None, **fit_params)[source]：重新拟合数据集。参见具体分类器的fit函数 array-like, 训练集shape = [n_samples, n_features] y : array-like, 标签列，shape = [n_samples] array-like, shape = (n_samples,), 可选，用于划分训练集/验证集 predict(X)：调用包含了最佳参数的分类器对测试集X进行预测 predict_log_proba(X)：预测对数概率 predict_proba(X)：预测概率 get_params(deep=True)：获取该模型的参数 set_params(**params)：设置该模型的参数 实例12345678910111213141516171819202122232425&gt;&gt;&gt; from sklearn import svm, datasets&gt;&gt;&gt; from sklearn.model_selection import GridSearchCV&gt;&gt;&gt; iris = datasets.load_iris()&gt;&gt;&gt; parameters = &#123;&#x27;kernel&#x27;:(&#x27;linear&#x27;, &#x27;rbf&#x27;), &#x27;C&#x27;:[1, 10]&#125;&gt;&gt;&gt; svc = svm.SVC()&gt;&gt;&gt; clf = GridSearchCV(svc, parameters)&gt;&gt;&gt; clf.fit(iris.data, iris.target)...GridSearchCV(cv=None, error_score=...,estimator=SVC(C=1.0, cache_size=..., class_weight=..., coef0=...,decision_function_shape=&#x27;ovr&#x27;, degree=..., gamma=...,kernel=&#x27;rbf&#x27;, max_iter=-1, probability=False,random_state=None, shrinking=True, tol=...,verbose=False),fit_params=None, iid=..., n_jobs=1,param_grid=..., pre_dispatch=..., refit=..., return_train_score=...,scoring=..., verbose=...)&gt;&gt;&gt; sorted(clf.cv_results_.keys())...[&#x27;mean_fit_time&#x27;, &#x27;mean_score_time&#x27;, &#x27;mean_test_score&#x27;,...&#x27;mean_train_score&#x27;, &#x27;param_C&#x27;, &#x27;param_kernel&#x27;, &#x27;params&#x27;,...&#x27;rank_test_score&#x27;, &#x27;split0_test_score&#x27;,...&#x27;split0_train_score&#x27;, &#x27;split1_test_score&#x27;, &#x27;split1_train_score&#x27;,...&#x27;split2_test_score&#x27;, &#x27;split2_train_score&#x27;,...&#x27;std_fit_time&#x27;, &#x27;std_score_time&#x27;, &#x27;std_test_score&#x27;, &#x27;std_train_score&#x27;...]]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习：工作流（二）—— 特征工程]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%B7%A5%E4%BD%9C%E6%B5%81%EF%BC%88%E4%BA%8C%EF%BC%89%E2%80%94%E2%80%94%20%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[前言特征工程的重要性特征工程(Feature Engineering)是机器学习界的一个非正式话题，至今还没有一个明确的定义，大多数书籍也都以讲解算法为主，很少提及特征工程，但不可否认的是特征工程很大程度上决定了机器学习实践的成败： Feature engineering is another topic which doesn’t seem to merit any review papers or books, or even chapters in books, but it is absolutely vital to ML success. […] Much of the success of machine learning is actually success in engineering features that a learner can understand. — Scott Locklin, in “Neglected machine learning ideas” 创造新的特征是一件十分困难的事情，需要丰富的专业知识和大量的时间。机器学习应用的本质基本上就是特征工程。 ——Andrew Ng The algorithms we used are very standard for Kagglers. […] We spent most of our efforts in feature engineering.— Xavier Conort, on “Q&amp;A with Xavier Conort” on winning the Flight Quest challenge on Kaggle 虽然机器学习的最终效果是由数据、模型、参数、评价指标等众多因素相互影响的结果，但是好的特征能够为模型和参数的选择提供更大的空间和灵活性，所以在大数据竞赛中经常流行着这样一句话： 数据和特征决定了性能的上限，不同的算法只能去逼近这个上限。 什么是特征工程这里我们尝试给出特征工程的一个定义： 特征工程是利用领域知识或自动化手段将原始特征空间映射到新的特征空间，以使隐藏在数据和问题中的模式能够更好地被呈现给我们所选择的模型，从而提升模型在特定问题上的预测精度的过程。 在特征工程的定义中涉及到了以下概念： 所要解决的问题：分类、回归、标注等… 拿到的原始数据：最初收集到的数据 领域知识：通常新的特征融合了我们关于具体问题和具体数据的领域知识 自动化手段：在没有领域知识的前提下仍然可以通过一些自动化的手段实现特征工程 通过特征工程得到的新的特征空间 所选择的模型：朴素贝叶斯、SVM、决策树等 模型性能的评价指标：均方误差、精度、F1、AUC等 特征工程是一个数据呈现的问题：你必须将你的输入转化为算法能够理解的东西。 特征工程是一门艺术：数据是各种各样的，特征工程没有通用有效的方法，必须不断地从实践中学习什么时候该用哪种特征工程方法。 feature engineering is manually designing what the input x’s should be. — Tomasz Malisiewicz 特征变换离散特征 one-hot编码 类别合并 连续特征标准化标准化是特征无量纲化的一种方法，前提是特征服从正态分布，标准化后可以将其转化为标准正态分布。 标准化 归一化 离散化 对数变换 特征组合求积取出不同特征进行加减乘除产生新的备选特征 将原始特征组合成一组具有物理意义或统计意义的特征； 有时可以通过自动化手段对特征进行加减乘除来产生新的特征，然后评估所有特征的重要度，选择重要程度高的特征来做训练，但不建议使用自动循环来对所有特征获取相关的操作，这会导致“特征爆炸”。 两个特征相加：你想要通过预售数据来预测收入，通过sales_blue_pens和sales_black_pens相加，你可以得到sales_pens的总销量； 两个特征相减：同样也可以根据房屋的建造时间（house_built_date ）和购买时间差（house_purchase_date）来得到房屋购买时的年限（house_age_at_purchase） 两个特征相乘：当你要进行售价测试的时候，你可以通过售价price和指示器变量conversion相乘来得到新的特征earnings。 两个特征相除：当你在对市场竞争对手分析时，可以通过点击率（n_clicks）和网页打开次数（n_impressions)相除来得到点击率click_through_rate来更好的分析对手数据。 特征拆分 方法描述：将原始特征拆分为多个特征；比如将身份证信息拆分为地域和年龄。 适用场景：单特征蕴含多种信息； 影响评估：需要领域知识 特征选择特征选择（feature selection）：从给定特征集合中选择出相关特征子集的过程 相关特征：对当前学习任务有用的特征；需要在特征选择中留下的特征； 无关特征：对当前学习任务没什么用的特征；需要在特征选择中去掉的特征； 冗余特征：它们所包含的特征能从其他特征中推演出来；只有在冗余特征能够对应于完成任务所需的“中间概念”时，冗余特征才是有益的，否则建立去掉； 特征选择的作用： 降维：减轻维数灾难，提高计算效率； 降噪：无关特征相当于数据中的噪声； 特征选择的步骤：子集搜索和子集评价共同构成完整的特征选择方法 子集搜索：获取特征子集的环节； 子集评价：评价特征子集的环节； 特征选择的分类：按照特征选择与模型训练的时机关系 过滤式选择：是在模型训练之前进行的特征选择 首先对特征重要度进行排序； 选择其中前k个最重要的特征作为特征子集； 模型训练； 包裹式选择：是在模型训练之后进行的特征选择 通过某种方式产生不同的特征子集； 通过交叉验证选出性能最好的特征子集； 嵌入式选择：是在模型训练过程中进行的特征选择 在模型训练过程中自动进行特征选择； 过滤式选择(filter)ReliefRelief是一种典型的过滤式特征选择方法，其核心思路是：计算每个特征的相关统计量，然后选取相关统计量最大的前k个特征，特征A的相关统计量等于所有样本的猜错邻近与猜对邻近在该特征上的距离差之和： \delta ^j = \sum_{i}diff(x_i^j,x_{i,nm}^j)^2-diff(x_i^j,x_{i,nh})^2 $\delta ^j$:第j个特征的相关统计量； $x_i^j$:第i个样本在第j维特征上的值； $x_{i,nm}^j$:第i个样本的猜错邻近(异类样本中的最邻近)在第j维特征上的值； $x_{i,nh}$:第i个样本的猜中邻近(同类样本中的最邻近)在第j维特征上的值； $diff(a,b)$：对a,b的某种距离度量，如果是分类特征，当a=b时$diff(a,b)=0$，否则等于1，如果是连续特征，$diff(a,b)=|a-b|$，a,b已规范化到[0,1]； Relief只需在数据集的采样上而不需要在完整数据集上进行；时间开销随采样次数以及原始特征数线性增长 Relief是为二分类问题设计的，其变体Relief-F可以扩展到多分类问题，只需要在每一个异类中找到一个猜错近邻即可： \delta ^j = \sum_{i}\sum_{l\neq k}p_l*diff(x_i^j,x_{i,l,nm}^j)^2-diff(x_i^j,x_{i,nh})^2 $p_l$:第l类样本在数据集D中所占比例 $x_{i,l,nm}^j$:与第i个样本在特征j上的猜错邻近(猜错为l类) 特征重要度排序首先按照重要度（特征与标签的相关性）对特征排序，之后选取前k个最重要的特征作为特征子集，对模型进行训练。 评估特征重要度的常用方法： Pearson皮尔逊相关系数； Gini-index基尼系数； IG信息增益（互信息）; 卡方检验; 包裹式选择(wrapper)包裹式选择是为给定模型量身定做的特征子集，最终效果要比过滤式选择更好，但因为需要多次训练，计算开销通常要比过滤式选择大得多。 LVWLVW(拉斯维加斯方法)是典型的包裹式选择，其基本思路是：每次随机选取一个特征子集，使用交叉验证评估模型效果，通过一定次数的迭代，最终选出效果最好的特征子集。 前向搜索和后向搜索 前向搜索： 先将每个单特征作为特征子集，通过交叉验证选出其中最优参数{$a_1$}； 再从剩下特征中选取一个新特征加入到特征子集中，通过交叉验证选出其中最有参数{$a_1,a_2$}； 重复以上过程，直至在第k+1轮时，最优的候选不如上一轮的选定集，则停止迭代，并将上一轮的子集作为最优子集； 后向搜索： 开始将所有特征纳入特征子集，进行评估； 去掉一个特征，通过交叉验证选取最优的特征子集； 重复以上过程，直至该次最优特征子集不比上一轮特征子集好，则停止迭代，并将上一轮特征子集作为最优子集； 评价： 优点：操作简单，对过拟合问题具有较高的鲁棒性； 缺点：是一种贪心策略，忽略了特征之间的相关性，可能会选择冗余特征；同时没有考虑特征的任意组合可能会带来的效果； 时间开销比较大； 嵌入式选择(embedding)L1正则L1正则因为可以得到稀疏解(只有部分参数非零)，因此可被视为一种嵌入式特征选择方法。 解释：如果做出非正则项和L1正则项损失函数等值线，整体损失会在二者相交处取到最小值，L1正则等值线为菱形形状，交点更容易出现在坐标轴上，即使得某些参数分量为零。 决策树决策树节点划分特征(信息增益、信息增益比、基尼系数、平法误差)所组成的集合就是所选出的特征子集，决策树本身即可被看做是一种嵌入式特征选择方法。 特征提取PCA时间特征变换深度学习深度学习调参过程也可看做是对特征进行选择 参考 Discover Feature Engineering, How to Engineer Features and How to Get Good at It]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习：集成学习（一）—— RF]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%9A%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%80%EF%BC%89%E2%80%94%E2%80%94%20RF%2F</url>
    <content type="text"><![CDATA[待补充。。。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习：理论基础（七）—— 泛化理论]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%9A%E7%90%86%E8%AE%BA%E5%9F%BA%E7%A1%80%EF%BC%88%E4%B8%83%EF%BC%89%E2%80%94%E2%80%94%20%E6%B3%9B%E5%8C%96%E7%90%86%E8%AE%BA%2F</url>
    <content type="text"><![CDATA[机器学习的目的可以被简单概括为“寻找泛化误差最小的模型”，学习问题可以被分解为两个基本问题： 使模型的经验误差最小，这可以通过经验风险最小化来达到； 使模型的经验误差尽可能接近泛化误差，这主要受到样本容量和模型复杂度的影响。样本容量就是数据量的多少，模型复杂度可以通过假设空间的VC维来衡量； 与两个基本问题相对应，我们可以将算法的期望泛化误差分解为偏差和方差两部分。偏差-方差分解是我们在分析机器学习系统性能的影响因素、改进机器学习系统性能时所用到的最重要工具。如果我们的机器学习系统性能欠佳，要么可归因于高偏差（欠拟合）要么可归因于高方差（过拟合）。如果把构建机器学习系统比作开车，那么偏差-方差分解就是我们的仪表盘，增加方差的措施就是在踩油门，减少方差的措施就是在踩刹车，只有将车速维持在适当水平才能安全驾驶。 学习曲线描述了经验误差和泛化误差随样本容量增加的变化趋势，通过学习曲线可以识别出增加样本容量是否能够改善机器学习系统的性能。 形式化学习问题（Why can machine learn?）监督学习监督学习：从带标签的训练集中识别出特征到标签的某种模式g，作为理想模式的近似。 目标函数(Target Function)：$y = f(x)$是特征到标记的理想模式(未知)，$(x{i},y{i})$是数据集中的观测样例，$y{i}=f(x{i})+\zeta $，$\zeta$表示噪声； 假设函数（Hypothesis Function）：$y = h(x)$是假设空间$H={h:X \to Y | h = h(x)}$中的某个模型； 损失函数（Loss Function）：$L=L(y,h(x))$，计算样本预测值和观测值的误差； 经验误差（Training error）：在训练集上损失函数的均值 E_{in}(h) = \frac{1}{m}\sum_{i=1}^{m}L(y_{i},h(x_{i})) 泛化误差（Generalization error）：在测试集上损失函数的期望/均值，在做交叉验证时将验证误差作为泛化误差的近似。 E_{out}(h) = \mathbb{E}_{(x,y)\in (x,f(x))}[L(y,h(x))] 学习问题——经验风险最小化和结构风险最小化经验风险最小化经验风险最小化(empirical risk minimization,ERM)：选取经验误差最小的模型作为最终模型。 g= \underset{h\in H}{argmin} E_{in}(h)泛化误差界如果仅追求经验风险最小化，那么使用较复杂的模型，能够在样本集上轻易达到100%的正确率，但在真实分类时却一塌糊涂。 统计学习因此而引入了泛化误差界的概念，就是指真实风险应该由两部分内容刻画: 经验风险：代表了分类器在训练样本上的误差；经验风险最小化要做的只是让这部分误差达到最小； 置信风险：代表了我们在多大程度上可以信任分类器在未知样本上分类的结果；也就是经验误差在多大程度上接近泛化误差。这部分是没有办法精确计算的，只能给出一个估计的区间，使得整个泛化误差只能计算上界，无法计算准确的值（所以叫做泛化误差界，而不叫泛化误差）。 泛化误差界的公式可以简单表示如下： E_{out} \leqslant E_{in} + \Phi (d_{vc},n,\delta ) $E_{out} $：泛化误差 $E_{in} $：经验风险 $\Phi (d{vc},n,\delta )$：置信风险，$n$表示训练集规模，$d{vc}$表示模型复杂度/假设空间VC维，$\delta$表示置信度。置信风险随训练集规模增大而指数级衰减，随模型VC维增加而多项式级增加，随置信度增加而减小。 结构风险最小化结构风险最小化(Structural Risk Minimization,SRM)：选取经验风险和置信风险之和最小的模型作为最终模型。 结构风险最小化可以转化为以下两个核心问题： 经验误差是否足够小：通过经验风险最小化来达到 g= \underset{h\in H}{argmin} E_{in}(h) 泛化误差是否足够接近经验误差：由样本容量和模型复杂度决定 E_{out}(g) \approx E_{in}(g)\approx 0VC维VC维（Vapnik-Chervonenkis Dimension，由研究人员Vapnik和Chervonenkis在1958年发现）：对于一个指示函数集H，如果存在k个样本，它的任何一种划分($2^k$)都能够被H中的函数f分开，则称H能够把k个样本打散(shatter)，函数集的VC维就是它能打散最大样本数。 VC维反映了函数集的学习能力，VC维越大模型越复杂，相应地，函数集的学习能力越强。遗憾的是，目前尚没有通用的关于任意函数集VC维计算的理论，只对一些特殊的函数集知道其VC维。 单个假设由霍夫丁（hoeffding）不等式: P[|E[X]-\overline{X}|>\epsilon ] \leqslant 2exp\left ( \frac{-2m\epsilon ^{2}}{(b-a)^2} \right )应用到泛化误差上，假设错误限定在0和1之间，则对于假设h有： P[|E_{out}(h)-E_{in}(h)|>\epsilon ]\leqslant 2exp\left ( -2m\epsilon ^{2} \right )结论：对于单个假设，经验误差与泛化误差间的差异随着样本容量的增长而指数级衰减 有限假设空间由： P(\bigcup_{i=1}^{n}x_{i})\leqslant \sum_{i=1}^{n}P(x_{i})得： \begin{align*} P[\underset{h\in H}{sup}|E_{out}(h)-E_{in}(h)|>\epsilon ]&\leqslant \sum_{h\in H}P[|E_{out}(h)-E_{in}(h)|>\epsilon ]\\ &\leqslant 2|H|exp\left ( -2m\epsilon ^{2} \right ) \end{align*}结论： 但当有多个假设时会恶化算法的泛化性能； 经验误差和泛化误差的差异，随训练集样本数的增加而指数级衰减，随假设空间容量增加而增加。 无限假设空间存在很多以实数为参数的模型，假设空间中的假设数量将是无穷的，此时根据该不等式无法给出经验误差和泛化误差的一个确定的上界。事实上，在放大不等式右侧的过程中我们假设了各个假设模型相互独立，而事实上模型之间会有很大程度的交集，也就是说它被过度放大了。我们希望能够找到一个多项式级别的函数来代替|H|用于描述假设空间的复杂度。 成长函数（Growth Function）：假设空间H中有效模型个数与样本容量m的函数关系称作该假设空间的成长函数，记做$m_{H}(m)$。H中所有将样本集划分为相同结果的模型都被看作是同一个有效模型，我们希望用假设空间中有效模型个数来代替模型总数|H|，对于样本数为m的二分类问题，其有效模型数的一个上界为$2^{m}$，但是这个指数上界仍然过于宽泛了，我们希望将其缩减到一个多项式级别的函数。 P[\underset{h\in H}{sup}|E_{out}(h)-E_{in}(h)|>\epsilon ]\leqslant 2m_{H}(m)exp\left ( -2m\epsilon ^{2} \right ) 打散（Shatter）：对于假设空间H，如果存在k个样本，它的任意划分都可以被H中的函数分开，就说假设空间H能够shatter k个样本。二维感知机模型的VC维为3。 VC维（Vapnik-Chervonenkis Dimension）：H所能shatter的最大样本数，称该假设空间的VC维； 突破点（Break Point）：如果任意k个样本都不能被H打散，则称k为H的突破点。如果k是H的突破点则任意比k大的整数也是H的突破点，H的最小突破点=VC(H)+1； 上限函数（Bounding function）：是指当假设空间H的最小突破点为k时，H的成长函数的最大值，记做B(N,k)，上限函数满足以下不等式（假设空间的成长函数以其VC维次多项式为上限）： B(N,k)\leqslant B(N-1,k)+B(N-1,k-1)\\ m_{H}(N)\leqslant B(N,k)\leqslant \sum_{i=0}^{k-1}C_{N}^{i}\leqslant N^{k-1}=N^{d_{vc}}\\ m_{H}(N)\leqslant N^{d_{vc}}举例： 输入 假设 成长函数 VC维 一维点集 正射线 N+1 1 一维点集 正区间 \binom{N+1}{2}+1 2 二维点集 凸集 2^N \infty d维点集 N维线性分类器 N^{d+1} d+1 结论：使用上限函数代替假设空间容量|H|，有以下不等式 P[\underset{h\in H}{sup}|E_{out}(h)-E_{in}(h)|>\epsilon ]\leqslant 4N^{d_{vc}}exp\left ( -\frac{1}{8}\epsilon ^{2} N \right ) 如果假设空间VC维有限，则随着样本容量的增加，模型的经验误差趋近于泛化误差； 假设空间VC维与假设空间中自由参数的个数大致相等； 泛化误差界：通过VC维重写本文开头给出的泛化误差界 E_{out}(g) \leqslant E_{in}(g) + \sqrt{\frac{8}{N}ln(\frac{4(2N)^{d_{vc}}}{\delta })}正则化正则化：通过约束模型参数向量的范数使其不要太大，来降低模型复杂度以降低过拟合风险。 L^∗(y_i,f(x_i;w))=L(y_i,f(x_i;w))+γΩ(w)\\ w=\underset{w}{argmin}L^*正则项实际上是一种惩罚机制：若增加模型复杂度所换取的在损失函数上的增益不足抵消因此导致的正则项的增加，那么增加这样的复杂度就是得不偿失的。正则化是结构最小化的等价形式。 范数范数：在赋范空间中用于度量向量大小的函数。 范数的性质： 非负性：$ \lVert\vec x\rVert\geqslant 0 $； 齐次性：$ \lVert c\cdot\vec x\rVert = \lvert c\rvert \cdot \lVert\vec x\rVert$； 三角不等式：$ \lVert \vec x + \vec y\rVert \leqslant \lVert\vec x\rVert + \lVert\vec y\rVert$; 范数的性质使得它天然地适合作为机器学习的正则项。而范数需要的向量，则是机器学习的学习目标——参数向量。 常用范数： $L{p}范数$：各元素绝对值的p次方和的p次方根，$ \lVert\vec x\rVert_p = \Bigl(\sum{i = 1}^{d}|x_i|^p\Bigr)^{1/p} $ $L_0$范数：向量中非零元素个数，$ \lVert\vec x\rVert_0 = x_i,\; \text{with }x_i\neq 0 $ $L{\infty }$：各元素中最大的绝对值，$ \lVert\vec x\rVert\infty = \lim{p\to+\infty}\Bigl(\sum{i = 1}^{d}x_i^p\Bigr)^{1/p} $ 机器学习中常用的范数是L1和L2范数: L1范数：向量中所有元素的绝对值的和，$ \lVert\vec x\rVert1 = \sum{i = 1}^{d}\lvert x_i\rvert $ L2范数：向量长度，$ \lVert\vec x\rVert2 = \Bigl(\sum{i = 1}^{d}x_i^2\Bigr)^{1/2} $ L1范数（LASSO regularizer）损失函数后面加上L1正则项就成了著名的Lasso问题（Least Absolute Shrinkage and Selection Operator）。 L1是L0的备胎：从特征选择的角度来看，适当减少特征维度可以降低模型复杂度从而降低过拟合的风险。嵌入式特征选择就是在模型训练过程中对参数施加“稀疏性”约束，即使得参数w尽可能多的分量变为0。最直接方式就是使用L0范数，但是L0范数不连续，难以进行优化求解，L1是L0的最优凸近似，因此常使用L1范数来代替0来产生稀疏解。 Ω(w)=||w||_1 L1范数能够产生稀疏解。即大多数参数被优化为0，这相当于进行了特征选择，最终只有少数特征在起作用； L2范数（Ridge Regularizer） 在损失函数（或代价函数）后面加上L2正则项就变成了岭回归（Ridge Regression），也有人叫他权重衰减。 Ω(w)=||w||^2_2 L2倾向于使参数稠密地接近于0。L2相对L1更平滑，L2会让每个参数都接近0但是不会等于0，每个特征都较小。 对比L1、L2正则项 相同点：都是通过约束模型参数向量的范数来降低模型复杂度，减少过拟合风险； 不同点： L1也称Lasso，倾向于产生稀疏解。使较多的参数变为0，相当于做了特征选择。 L2也称岭回归(Ridge Reg)，倾向于使参数稠密地接近于0。每个参数都接近0但是不会等于0，防止模型overfit到某个feature上。 各种分析曲线偏差-方差分解偏差方差分解：模型H的期望泛化误差可以分解为模型预测值的方差与期望预测值与真实值的偏差。 \begin{align*} \mathbb{E}_{D}(g_{D}-y_D)^2 &= \mathbb{E}_{D}(g_{D}-\bar{g}+\bar{g}-y_D)^2 \\ &= \mathbb{E}_{D}(g_{D}-\bar{g})^2+ \mathbb{E}_{D}(\bar{g}-y_D)^2\\ &=\mathbb{E}_{D}(g_{D}-\bar{g})^2+ \mathbb{E}_{D}(\bar{g}-y+y-y_D)^2\\ &=\mathbb{E}_{D}(g_{D}-\bar{g})^2+ \mathbb{E}_{D}(\bar{g}-y)^2+\mathbb{E}_{D}(\bar{g}-y)(y-y_{D})+\mathbb{E}_{D}(y-y_{D})^2\\ &=\mathbb{E}_{D}(g_{D}-\bar{g})^2+ (\bar{g}-y)^2+\mathbb{E}_{D}(y_D-y)^2\\ &=var(x)+bias^2(x)+\epsilon^2 \end{align*} $g$：模型 $g_D$：样本$x$在训练集D上的预测标记 $\bar{g}$：样本$x$在不同训练集D上的期望预测标记$\bar{g}=\mathbb{E}_{D}[g_D(x)]$ $y_D$：样本$x$在训练集D中的观测标记 $y$：样本$x$的真实标记 假设噪声期望为0：$\mathbb{E}_{D}[y_D-f(x)]=0$ 偏差方差分解将模型的期望泛化误差分解为三部分： 偏差：期望预测值与真实值的偏差；代表了模型本身的拟合能力； 方差：模型在不同数据集上的预测方差；代表了，即数据扰动造成的影响； 噪声：观测值与真实值的差别；代表了问题本身的误差下界； 提升泛化能力的方法：泛化性能是由算法的拟合能力、数据的充分性、任务本身的难度共同决定的（三因素：算法-数据-噪声）；给定学习任务，为获取较好的泛化能力，可以从以下三个角度来思考： 数据：保证数据数量（减少扰动）、质量（减少噪声） 算法：结构风险最小化，充分拟合数据、控制模型复杂度（减少偏差和方差） 偏差-方差窘境：学习任务的目标是要获取期望泛化误差最小的模型，也就是说让偏差和方差都尽可能小，但不幸的是偏差和方差是有冲突的： 当训练不足时，学习器的拟合能力不强，数据扰动不足以影响泛化能力，偏差主导了泛化误差，发生欠拟合； 随着训练程度的加深，学习器的拟合能力逐渐加强，数据扰动渐渐能被学习器学到 当训练程度充足后，学习器的拟合能力非常强，数据的轻微扰动都会导致学习器发生显著变化，方差主导了泛化误差。若数据自身的非全局特性被学习器学到则发生过拟合； “油门曲线”横坐标可以是训练程度、模型复杂度、VC维、自由参数个数。 如果把训练模型比作开车，模型复杂度比作车速，偏差-方差曲线就是车速仪表盘，而能够增加模型复杂度的措施相当于在“踩油门”，降低模型复杂度的措施相当于在”踩刹车“，车速过快或过慢都不能保证驾驶安全。 学习曲线学习曲线反映了经验误差和泛化误差与训练集样本数量的关系。 由经验风险最小化和VC维的相关知识可以得出学习曲线的如下结论： 当样本数较少时，如果样本数小于假设空间的VC维，模型可以将样本完全分开，此时训练误差为0，但是经验误差和泛化误差差距很大导致泛化误差很大；随着样本数的增加，模型无法将样本完全拟合，经验误差变大，同时泛化误差与经验误差的差距以接近指数级衰减；当样本数足够大时，经验误差趋于平稳，经验误差和泛化误差差距趋于0。 对于简单模型，模型的学习能力较差，经验误差和泛化误差会更早地趋于一致，但稳定后的泛化误差较高，可能发生欠拟合。此时再通过增加样本量的方法无法提高学习算法的泛化性能； 对于复杂模型，模型的学习能力很强，经验误差和泛化误差会更晚地趋于一致，稳定后的泛化误差较小，在样本数不足的时候，会发生过拟合。但是如果样本数充足，复杂模型的性能要优于简单模型； 可以把训练集规模看做是路的宽度，训练集中噪声看作是路况。路窄就要慢车速，路宽才可以飙车速；车速较低时增加路宽意义不大，车速较高时增加路宽使驾驶更安全； 正则项影响曲线对误差: 当 λ 较小时，训练集误差较小(过拟合)而交叉验证集误差较大 随着λ的增加，经验误差不断增加（欠拟合)，泛化误差先减小后增大 VC维和样本容量: 如果给定误差阈值$\epsilon$和精度/置信区间$\delta$（置信区间越大泛化误差可浮动的范围就越小），对$\delta$进行简化，得到$N^{d_{vc}}e^{-N}$，将其画出图像观察其变化趋势： 因此我们可以看到在横线所在位置，N 的增加和 dvc 几乎是一种线性的关系。经过工业界足够的现象表明，我们通常只要使得 N ≥ 10 dvc 就足够了。 提高泛化性能的方法总体上从数据和模型两方面入手（待完善）： 数据: 增加数据量 增加数据质量，减少噪声 模型: 使用和数据相称的模型 参数惩罚 参考 我对VC维的理解 经验风险最小化和泛化误差最小化 机器学习理论三部曲 谈谈 L1 与 L2-正则项 L0、L1、L2、Elastic Net正则项]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习：理论基础（八）—— 性能评估]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%9A%E7%90%86%E8%AE%BA%E5%9F%BA%E7%A1%80%EF%BC%88%E5%85%AB%EF%BC%89%E2%80%94%E2%80%94%20%E6%80%A7%E8%83%BD%E8%AF%84%E4%BC%B0%2F</url>
    <content type="text"><![CDATA[误差和过拟合误差分类 误差（error）：样本的预测输出与真实输出之间的差异。 训练误差（training error）：也叫经验误差，是指在训练集中样本的预测输出与真实输出之间的差异。 验证误差（validation error）：是指在验证集中样本的预测输出与真实输出之间的差异 泛化误差（generalization error）：是指在预测集中样本的预测输出与真实输出之间的差异 在进行模型选择（model selection）时，我们通常希望选取泛化误差最小的模型；但在模型训练（model training）时我们实际能做的是努力使训练误差最小；在模型评估（model validation）时我们通常将验证误差作为泛化误差的近似。 过拟合由于存在过拟合的风险，训练误差越小并不能保证泛化误差也越小： 过拟合（overfitting）：因为学习器学习能力太强，误把训练集自身的一些局部特性当做了全局特性，而导致学习器泛化能力降低的现象。 欠拟合（underfitting）：因为学习器学习能力太弱，连训练集的全局特性都没学好，而导致学习器泛化能力降低的现象。 过拟合是机器学习面临的关键障碍，它只能被缓解而无法彻底避免。 模型评估方法在实验中，我们通常以模型的验证误差作为其泛化误差的近似，来对模型的泛化能力进行评估。假设现在我们有一个包含m个样本的数据集D=\{(x_{1},y_{1}),(x_{2},y_{2}),...,(x_{m},y_{m})\}，既要训练又要验证，该怎么做呢？答案是：通过对数据集进行合理划分来产生所需要的训练集S和验证集T，然后用训练集来训练我们的模型，用验证集来评估模型的性能。 验证集的划分准则： 分布一致：尽可能保持验证集和训练集数据分布的一致性，否则评估结果将由于训练/验证集分布的差异而产生较大偏差； 互斥：尽可能保持验证集和训练集互斥，否则就“泄题”了，评估结果会过于“乐观”； 较大规模的训练集：尽可能使训练集的规模接近数据集D的规模，这时的评估结果最为准确； 我们可以按照这三条准则来对数据集进行划分，常用的划分方法有留出法和交叉验证法。 留出法（hold-out） 原理：把数据集D划分为两个互斥子集，分别作为训练集S和验证集T。单次留出法的评估结果往往不够稳定可靠，通常取多次评估结果的平均值作为最终评估结果。 抽样方法：通常采用随机抽样或分层抽样 抽样比例：二八分，训练集8，验证集2 交叉验证法（cross-validation） 原理：先把数据集D划分为k个大小相近的互斥子集，然后每次取其中的k-1个子集的并集作为训练集S，取剩下的那个子集作为验证集T，经过k次训练和验证，取k次评估结果的平均值作为最终的评估结果。 抽样方式：分层采样 k折交叉验证（k-fold cross-validation）：k值越大，训练集规模就越接近D的规模，评估结果也越准确，但开销也更大。k最常用的取值是10，称10折交叉验证 留一法：当k=m时，称为留一法。训练集和D接近，评估结果最准确，但开销大。 模型性能指标性能指标（performance measure）:衡量模型泛化能力的评价标准。 回归任务回归（regression）用于预测结果为数值型的模型，与分类模型相比，回归并没有简单的指标说明预测是否正确，但可以说接近或远离真实值。 残差（residuals）：预测值和真实值之间的距离。 均方误差回归任务中最常用的性能指标是“均方误差”（Mean squared Error,MSE）：预测值与真实值之差的平方和的平均值。 对于离散分布： MSE = \frac{1}{m}\sum_{i=1}^{m}(f(x_{i})-y_{i})^{2} 对于连续分布： MSE=\int_{(x,y)\in D}(f(x)-y)^{2}p(x)dx均方根误差均方根误差（root-mean-square error,RMSE）： RMSE = \sqrt{MSE} RMES的优势：结果与参与运算的值有相同单位 RMES的劣势：RMES的值和问题规模有关，不易于在不同数据集上比较，只限于在同一个项目中进行模型比较 拟合优度 拟合优度（Goodness of Fit）是指回归直线对观测值的拟合程度。 可决系数$R^{2}$：也称确定系数，是度量拟合优度的统计量 R^{2} = 1 - \frac{\sum_{i=1}^{n}(f(x_{i})-y_{i})^{2}}{\sum_{i=1}^{n}(f(x_{i})-\overline{y})^{2}}说明： SST(total sum of squares):总平方和 SSE(error sum of squares):残差平方和 SSR(regression sum of squares):回归平方和 R^{2} = \frac{SSR}{SST}=\frac{SST-SSE}{SST}=1-\frac{SSE}{SST} 分类任务错误率和精度错误率和精度是分类任务中最常用的两种性能度量。对于数据集D， 错误率(error rate)：错误分类的样本数a与样容量m的比值 精度(accuracy)：精度=1- 错误率 混淆矩阵（confusion matrix）有时相对于预测精度，我们更关心查准率和查全率（召回率）。对二分类问题，根据样本的真实类别和预测类别的不同组合可以定义出真正例（TP）、假正例（FP）、真负例（TN）、假负例（FN），组合结果的“混淆矩阵”如下： 真实\预测 正 负 正 TP FN 负 FP TN 说明：通常混淆矩阵中的元素可以是对应类别的样本数/比例 查准率（precision）：预测为正的样本中有多少真实为正。 P = \frac{TP}{TP+FP} 查全率（recall）：真实为正的样本中有多少预测为正。 R = \frac{TP}{TP+FN} 说明：对于多分类问题，也可以使用混淆矩阵，行列依然表示各个真实类别和预测类别 P-R曲线和F1度量分类阈值：很多学习器会为每个测试样本产生一个预测值，将其与分类阈值比较，大于阈值则为正类，否则为负类。（魔法旋钮） 查准率和查全率是一对矛盾，如果把阈值设置的较高则查准率较高但查全率较低，反之查准率较低但查全率较高。我们可以通过P-R曲线来直观反映这种关系。 绘制 P-R 曲线：按照预测结果是正例的可能性大小对样本进行排序，然后通过调整阈值（从大到小）逐个将样本作为正例并计算当前的查准率P和查全率R，就得到了P-R曲线。 平衡点（Break-Even Point，BEP）:P = R 时的取值,BEP较大的模型较优 F1度量：P和R的调和平均值，越大性能越好 \frac{1}{F_{1}}=\frac{1}{2}(\frac{1}{P}+\frac{1}{R})ROC 曲线和 AUC受试者工作特性（Receiver Operating Characteristic, ROC）与P-R类似，只不过ROC是使用“假正例率”（FPR）和“真正例率”（TPR）来作为曲线的横纵坐标。 TPR：真实为正的样例中有多少预测为正（同查全率）。 TPR = \frac{TP}{TP+FN} FPR：真实为假的样例中有多少预测为正 FPR = \frac{FP}{TN+FP} ROC曲线: 先将样本按预测值进行排序（同P-R曲线），先将阈值设为最大，则所有样本都被预测为负例，TPR = FPR = 0，然后依次减小阈值并重新计算TPR和FPR，当所有样本都被预测为正例时，TPR = FPR = 1。 AUC（Area Under ROC Curve）：我们期望较高的真正率和较低的假正率。从图像上看，若一个学习器的ROC曲线被另一个学习器的ROC曲线包裹，则后者性能更好。如果两者交叉，则一般通过ROC曲线下面积AUC来比较，AUC越大性能越好。那么AUC值的含义是什么呢？在论文 Breiman, L., Friedman, J., Olshen, R., Stone, C., Classification and Regression Trees. Wadsworth International Group. 1984.中有这样一段话：”The AUC value is equivalent to the probability that a randomly chosen positive example is ranked higher than a randomly chosen negative example. This is equivalent to the Wilcoxon test of ranks (Hanley and McNeil, 1982). The AUC is also closely related to the Gini coefficient (Breiman et al., 1984), which is twice the area between the diagonal and the ROC curve. Hand and Till (2001) point out that Gini + 1 = 2 AUC.”。简单翻译下：首先AUC值是一个概率值，当你随机挑选一个正样本以及一个负样本，当前的分类算法根据计算得到的Score值将这个正样本排在负样本前面的概率就是AUC值。AUC值越大，当前的分类算法越有可能将正样本排在负样本前面，即能够更好的分类。另外，AUC与Gini分数有联系，Gini + 1 = 2AUC。AUC 体现出容忍样本倾斜的能力,只反应模型对正负样本排序能力的强弱，而其直观含以上是任意取一个正样本和负样本,正样本的得分大于负样本的概率。 说明：对于多分类问题，我们依然可以对每一种类别分别绘制ROC曲线，将此分类作为正例，将其他分类作为负例来处理。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习：基础算法（一）—— MR]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%9F%BA%E7%A1%80%E7%AE%97%E6%B3%95%EF%BC%88%E4%B8%80%EF%BC%89%E2%80%94%E2%80%94MR%2F</url>
    <content type="text"><![CDATA[多元线性回归]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习：集成学习（〇）—— 概述]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%9A%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%EF%BC%88%E3%80%87%EF%BC%89%E2%80%94%E2%80%94%20%E6%A6%82%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[集成学习（ensemble learning）通过构建并结合多个基学习器来完成学习任务，通常可获得比单一学习器显著优越的泛化性能。 集成学习的一般步骤： 产生一组“基学习器”：基学习器有时也被称为弱学习器，通常指泛化性能略优于随机猜测的学习器；虽然从理论上说使用弱学习器集成就足以获得很好的性能，但在实践中出于种种考虑，往往会使用比较强的学习器；根据基学习器生成方式的不同，集成学习大致可分为两类： 串行化方法：基学习器间存在强依赖关系，通过串行的方式生成，主要代表是boosting方法； 并行化方法：基学习器之间不存在强依赖关系，可同时生成，主要代表是bagging方法； 采用某种策略将它们结合起来； 为什么集成学习能够获得比单一学习器更好的性能？ 如果做个简化的分析，考虑二分类问题$y \in {-1,+1}$和真实函数$f$，假设基分类器的错误率为$\epsilon $，即对于每个基分类器$h_i$有： P(h_i(x)\neq f(x)) = \epsilon假设通过简单投票法组合T个基分类器，若有超过半数的基分类器分类正确，则集成分类就正确，假设各个基分类器错误率相互独立，则由Hoeffding不等式可知，集成的错误率为： \begin{align*} P(H(x)\neq f(x))&=\sum_{k=1}^{\frac{T}{2}}\binom{T}{k}(1-\epsilon )^k\epsilon ^{(T-k)}\\ &\leqslant exp(-\frac{1}{2}T(1-2\epsilon )^2) \end{align*}上式显示出，随着基学习器的增加，集成错误率将指数级下降，最终趋向于零。同时，上述结论基于基学习器的误差独立的假设，且$\epsilon &lt; 0.5$，即基学习器比随机乱猜要好。要获得好的集成，个体学习器应“好而不同”，即基学习器一方面要有一定的“准确性”，并且要有“多样性”，事实上这两点本身是相互冲突的，一般准确性很高之后，要想增加多样性就需要牺牲准确性，如何产生“好而不同”的学习器恰恰是集成学习研究的核心。 基学习器生成方式常见的基学习器生成方式有boosting和bagging，前者主要关注降低偏差，后者主要关注降低方差。 Boosting提升(Boosting)方法的工作机制：先从初始训练集中训练出一个基学习器，再根据基学习器的表现对训练样本的分布进行调整，然后基于调整后的样本分布训练下一个基学习器，如此重复直至达到事先指定的个数T，最终将这T个学习器进行加权组合。 代表： Adaboost：根据前一个基学习器的错误率来调整训练样本的权重并得到基学习器的权重，加大误判样本权重减小正确判断样本权重，然后再在新的训练集上训练新的基学习器，迭代以上过程得到T个基学习器，最终对所有基学习器进行加权结合； XgBoost：每次训练一棵决策树来拟合当前模型和真实值之间的残差，更新当前模型，迭代得到T棵决策树，最终对所有决策树进行加和； BaggingBagging方法的工作机制：基于自助采样法（bootstrap sampling）/有放回抽样产生T个训练集，基于每个采样集训练一个基学习器，再将这些基学习器进行结合。 基学习器结合方式假设集成包含T个基学习器 $(h_1,h_2,…,h_T)$ 平均法(averaging)回归问题最常见的结合策略是平均法： 简单平均：$H(x)=\frac{1}{T}\sum_{i=1}^{T}h_i(x)$，在基学习器性能差别不大时宜用简单平均法； 加权平均：$H(x)=\sum{i=1}^{T}w_ih_i(x),\ \sum{i=1}^{T}w_i=1$，在基学习器性能差别较大时宜用加权平均，通常权值从训练数据中学习而得； 投票法(voting)分类任务最常见的集合策略是投票法： 绝对多数投票：若某标记得票过半数，则预测为该标记，否则拒绝预测； 相对多数投票：预测为得票最多的标记； 加权投票：某些基学习器一票顶多票； 不同基学习器可能产生不同类型的 $h_i(x)$： 硬投票(hard voting)：$h_i(x)$为类标记； 软投票(soft voting)：$h_i(x)$为类后验概率，若基学习器类型相同，基于类概率进行结合往往比基于类标记进行结合性能更好；若基学习器类型不同，则类概率不能直接进行比较，在此情形下同行可将类概率转化为类标记输出后再投票； 学习法学习法工作机制：首先训练多个初级学习器，然后基于初级学习器的预测结果训练另一个次级学习器用于将初级学习器结合起来。Stacking是学习法典型的代表。 Stacking 算法： 输入：训练集D输出：Stacking 集成模型$h^{‘}$过程： 选取T个基本模型 由基本模型的预测值产生次级训练集$D’$： 对每个模型 $t=1,2,…,T$，通过k折交叉验证的方法，每次在k-1折中训练基学习器 $h_t$，并在余下的一折上得到预测输出，最后收集每折上的预测输出，得到该模型在所有训练集上的预测输出$z_t$ 所有基学习器的预测值作为次级训练集中新的特征，加上原始训练集的标记得到次级训练集； 在次级训练集上训练次级学习器$h’$ 有研究表明，将初级学习器的输出类概率作为次级学习器的输入属性，用多响应线性回归(MLR)作为次级学习器效果较好。 集成学习已被广泛应用于几乎所有的学习任务，数据挖掘竞赛的历年冠军几乎都使用了集成学习。但由于集成学习包含了多个学习器，即使每个学习器有较好的解释性，集成仍是黑箱模型。 更多关于集成学习的内容请参考引用文章，需要注意的是对于集成学习概念上似乎没有统一界定，不同的人在使用blending、stacking这些术语时所指不同。 有些人认为这两个术语含义相同，都指的是这里的stacking方法； 有些人认为blending是范围更广的概念，泛指aggregate after getting gt，因此包含了uniform、non-uniform、stacking（林轩田）； 有些人认为blending是在验证集上做第二层训练，如KAGGLE ENSEMBLING GUIDE； 参考 西瓜书 台大机器学习基石 一文读懂集成学习]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习：集成学习（三）—— GBDT]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%9A%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%89%EF%BC%89%E2%80%94%E2%80%94%20GBDT%2F</url>
    <content type="text"><![CDATA[GBDT（Gradient Boosted Decision Tree，梯度提升决策树，常被戏称为 “广播电台”），又叫MART（Multiple Additive Regression Tree，多重累加回归树），是通过回归树不断拟合当前模型的残差，并将所得到的残差不断累加至当前模型来得到最终模型的boosting方法。 GBDT 是老牌 ensemble 模型，在10年前那个属于ensemble的年代，它既在学术界掀起了研究热点，也在工业界甚至比赛界都有广泛的应用。 原理篇我们可以从机器学习的三个要素出发来考察GBDT方法： 模型（假设空间）：是由一组含参数的函数所组成的函数空间，其中每个函数以特征为输入，以标签为输出； 策略（损失函数）：如何在假设空间中寻找最优函数； 算法（计算方法）：参数的具体求解过程； 12GBDT = 提升树 + 梯度提升算法 = (决策树加法模型 + 经验风险最小化 + 前向分步算法) + 梯度提升算法 提升树（boosting tree）提升树 = 决策树加法模型 + 经验风险最小化 + 前向分步算法 加法模型（additive model）加法模型：是由多个基学习器通过线性组合得到最终预测值的一种集成学习方法，本质上是一种特征转换方法 f(x) = \sum_{m=1}^{M}\beta _mb(x;\gamma _m) $b(x;\gamma _m)$：基学习器； $\beta _m$：基学习器的系数； 决策树加法模型：基学习器是决策树的加法模型 \begin{align*} f_M(x) &= \sum_{m=1}^{M}T(x;\gamma _m)\\ T(x;\gamma)&= \sum_{j=1}^{J}c_j[x \in R_j] \end{align*} $f_M(x)$: 第M次迭代后的集成模型； $T(x;\gamma_m)$：第m次迭代的基模型（决策树），对分类问题是二叉分类树，对回归问题是二叉回归树，以下以回归树为例; $\gamma$：{$(R_1,c_1),(R_2,c_2),…,(R_J,c_J)$}表示树的区域划分和各区域上的预测值，J是回归树的复杂度即叶子节点数； 经验风险最小化（ERM）在给定训练数据及损失函数$L(y,f(x))$的前提下，学习加法模型$f(x)$成为经验风险最小化问题： \underset{\beta _m,\gamma _m,m=1,2,..,M}{argmin} \sum_{i=1}^{N} L(y_i, \sum_{m=1}^{M}\beta _mb(x_i;\gamma _m))如果采用平方差损失函数： L(y_i,f(x_i))=(y_i - \sum_{m=1}^{M}\beta _mb(x_i;\gamma _m))^2前向分步算法（forward stagewise algorithm）上述经验风险最小化问题通常是一个很复杂的优化问题，前向分步算法是一种简便的近似迭代求解方法： 首先初始化目标函数； 然后从前向后，每一步只学习一个基学习器，用来拟合上一步得到的目标函数与真实值的残差； 将当前基学习器与上一步得到的目标函数相加得到新的目标函数； 迭代2、3，逐步逼近最终的目标函数$f(x)$。 前向分步算法： 输入：训练数据集$ T = { (x_1,y_1),(x_2,y_2),…,(x_N,y_N) } $；损失函数$L(y,f(x)$;基学习器${b(x;\gamma)}$ 输出：加法模型$f(x)$ 步骤： 初始化目标函数：$f_0(x)=0$; 对$m=1,2,…,M$ $fm(x)=f{m-1}(x)+\beta _mb(x;\gamma _m)$ 极小化损失函数，得到$\beta _m,\gamma _m$ \begin{align*} \beta _m,\gamma _m &= arg \underset{\beta,\gamma}{min}\sum_{i=1}^{N}L(y_i,f_m(x))\\ &= arg \underset{\beta,\gamma}{min}\sum_{i=1}^{N}L(y_i,f_{m-1}(x_i) + \beta b(x_i;\gamma)) \end{align*} 得到加法模型 f(x) = f_M(x) = \sum_{m=1}^{M}\beta _mb(x;\gamma _m) 回归树：对回归问题的提升树算法来说，只需简单地拟合当前模型的残差： \begin{align*} &f_0(x)=0\\ &f_m(x)=f_{m-1}(x)+T(x;\gamma _m)\\ &\gamma _m = arg \underset{\gamma _m}{min} \sum_{i=1}^{N} L(y_i,f_{m-1}(x)+T(x;\gamma_m))= arg \underset{\gamma _m}{min} \sum_{i=1}^{N}( y_i-f_{m-1}(x)-T(x;\gamma_m))^2\\ &f_M(x)=\sum_{m=1}^{M}T(x; \gamma _m)\\ \end{align*}分类树: GBDT 最初用来解决回归问题，但只需稍加转换就可以用它来解决多分类问题，核心是将分类问题转化为回归问题。 在多分类问题中，假设有k个类别，那么每一轮迭代实质是构建了k棵树，对某个样本x的预测值为： f_1(x),f_2(x),...,f_k(x)在这里我们仿照多分类的逻辑回归，使用softmax来产生概率，则属于某个类别c的概率为: p_c = exp(f_c(x))/\sum_{k}^{K}exp(f_k(x))此时该样本的loss即可以用logitloss来表示，并对f1~fk都可以算出一个梯度，f1~fk便可以计算出当前轮的残差，供下一轮迭代学习。 最终做预测时，输入的x会得到k个输出值，然后通过softmax获得其属于各类别的概率即可。 梯度提升（gradient boosting）当损失函数是平方损失或者指数损失时，求解较简单，但对一般损失函数并不那么容易，Freidman提出了梯度提升算法，它的基本思想是：用损失函数在当前模型的负梯度(函数空间)作为残差的近似值来拟合决策树。对于平方损失函数，它就是通常所说的残差。 T(x;\gamma_m)=f_m(x)-f_{m-1}(x)=-\left [ \frac{\partial L(y,f(x_i))}{\partial f(x_i)} \right ]_{f(x)=f_{m-1}(x)}梯度提升算法 输入：训练数据集T={$(x_1,y_1),(x_2,y_2),…,(x_N,y_N)$}，损失函数$L(y,f(x))$ 输出：回归树$\hat{f}(x)$ 算法： 初始化：$f0(x)=arg \underset{c}{min}\sum{i=1}^{N}L(y_i,c)$ 对m=1,2,…,M 对i=1,2,…,N，计算 r_{mi}=-\left [ \frac{\partial L(y,f(x_i))}{\partial f(x_i)} \right ]_{f(x)=f_{m-1}(x)} 对 $r{mi}$ 拟合一个回归树，得到第m棵树的叶节点区域$R{mj},j=1,2,…,J$ 对 $j=1,2,…,J$，计算每个叶节点区域的输出值：$c{mj}=arg \underset{c}{min}\sum{x \in R{mj}}L(y_i,f{m-1}(x_i)+c)$ 更新模型 $fm(x)=f{m-1}(x)+\sum{j=1}^{J}c{mj}[x \in R_{mj}]$ 得到回归树: $\hat{f}(x)=\sum{m=1}^{M}\sum{j=1}^{J}c{mj}[x \in R{mj}]$ 应用篇在sacikit-learn中，GradientBoostingClassifier为GBDT的分类类， 而GradientBoostingRegressor为GBDT的回归类。两者的参数类型完全相同，当然有些参数比如损失函数loss的可选择项并不相同。这些参数中，类似于Adaboost，我们把重要参数分为两类，第一类是Boosting框架的重要参数，第二类是弱学习器即CART回归树的重要参数。 1234567891011121314151617181920212223242526272829303132333435363738394041# 分类树class sklearn.ensemble.GradientBoostingClassifier(loss=’deviance’, learning_rate=0.1, n_estimators=100, subsample=1.0, criterion=’friedman_mse’, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0, min_impurity_split=None, init=None, random_state=None, max_features=None, verbose=0, max_leaf_nodes=None, warm_start=False, presort=’auto’)# 回归树class sklearn.ensemble.GradientBoostingRegressor(loss=’ls’, learning_rate=0.1, n_estimators=100, subsample=1.0, criterion=’friedman_mse’, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0, min_impurity_split=None, init=None, random_state=None, max_features=None, alpha=0.9, verbose=0, max_leaf_nodes=None, warm_start=False, presort=’auto’) boosting框架参数 参数 说明 调参 loss 损失函数 对于分类树，默认为对数似然函数：”deviance”。一般来说，推荐使用默认的”deviance”。它对二元分离和多元分类各自都有比较好的优化。而指数损失函数”deviance”等于把我们带到了Adaboost算法。 learning_rate 学习率 fk(x)=fk−1(x)+νhk(x)，ν的取值范围为0&lt;ν≤1，默认为1，较小的ν意味着我们需要更多的弱学习器的迭代次数，n_estimators和learning_rate要一起调参 n_estimators 最大迭代次数 默认100，越大越容易过拟合 subsample 无放回样本采样 推荐在[0.5, 0.8]之间，默认是1.0，即不使用子采样，越小方差越小偏差越大 alpha 分位数 这个参数只有GradientBoostingRegressor有，当我们使用Huber损失”huber”和分位数损失“quantile”时，需要指定分位数的值。默认是0.9，如果噪音点较多，可以适当降低这个分位数的值。 基学习器参数由于GBDT使用了CART回归决策树，因此它的参数基本来源于决策树类，也就是说，和DecisionTreeClassifier和DecisionTreeRegressor的参数基本类似。 参数 说明 调参 max_depth 决策树最大深度 数据少或者特征少的时候可以不管这个值。如果模型样本量多，特征也多的情况下，推荐限制这个最大深度，具体的取值取决于数据的分布。常用的可以取值10-100之间 max_features 划分时考虑的最大特征数 可以使用很多种类型的值，默认是”None”,意味着划分时考虑所有的特征数；如果是”log2”意味着划分时最多考虑log2N个特征；如果是”sqrt”或者”auto”意味着划分时最多考虑N‾‾√个特征。如果是整数，代表考虑的特征绝对数。如果是浮点数，代表考虑特征百分比，即考虑（百分比xN）取整后的特征数。其中N为样本总特征数。一般来说，如果样本特征数不多，比如小于50，我们用默认的”None”就可以了，如果特征数非常多，我们可以灵活使用刚才描述的其他取值来控制划分时考虑的最大特征数，以控制决策树的生成时间。 min_samples_split 内部节点再划分所需最小样本数 这个值限制了子树继续划分的条件，如果某节点的样本数少于min_samples_split，则不会继续再尝试选择最优特征来进行划分，默认为2，如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值。 min_samples_leaf 叶子节点最少样本数 默认为1，如果某叶子节点数目小于样本数，则会和兄弟节点一起被剪枝，如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值。 min_weight_fraction_leaf 叶子节点最小的样本权重和 这个值限制了叶子节点所有样本权重和的最小值，如果小于这个值，则会和兄弟节点一起被剪枝。 默认是0，就是不考虑权重问题。一般来说，如果我们有较多样本有缺失值，或者分类树样本的分布类别偏差很大，就会引入样本权重，这时我们就要注意这个值了 max_leaf_nodes 最大叶子节点数 通过限制最大叶子节点数，可以防止过拟合，默认是”None”，即不限制最大的叶子节点数。如果加了限制，算法会建立在最大叶子节点数内最优的决策树。如果特征不多，可以不考虑这个值，但是如果特征分成多的话，可以加以限制，具体的值可以通过交叉验证得到。 min_impurity_split 节点划分最小不纯度 这个值限制了决策树的增长，如果某节点的不纯度(基于基尼系数，均方差)小于这个阈值，则该节点不再生成子节点。即为叶子节点 。一般不推荐改动默认值1e-7 总结篇优点： 不需要做过多的数据预处理，特征可以是离散的或者连续的，不需要处理缺失值和异常值； 相当于是一种非线性的特征变换，表达能力强（叶节点对应路径上的节点相当于特征选择，叶节点位置相当于特征组合）； 模型解释性好； 缺点： GBDT是一个串行过程，不容易并行化，计算复杂度高； 同时其不太适合高维稀疏特征； 参考 GBDT详解上 + 下 + 后补 《统计学习方法》 GBDT如何应用于二分类问题？具体如何使用logitloss？ scikit-learn 梯度提升树(GBDT)调参小结]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习：集成学习（二）—— Adaboost]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%9A%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%BA%8C%EF%BC%89%E2%80%94%E2%80%94%20Adaboost%2F</url>
    <content type="text"><![CDATA[待补充。。。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习：集成学习（六）—— CatBoost]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%9A%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%85%AD%EF%BC%89%E2%80%94%E2%80%94%20CatBoost%2F</url>
    <content type="text"><![CDATA[CatBoost是由Yandex发布的梯度提升库。在Yandex提供的基准测试中，CatBoost的表现超过了XGBoost和LightGBM。 安装1pip install catboost 使用CatBoost的接口基本上和大部分sklearn分类器差不多，所以，如果你用过sklearn，那你使用CatBoost不会遇到什么麻烦。CatBoost可以处理缺失的特征以及类别特征，你只需告知分类器哪些维度是类别维度。 模型模型参数下面以CatBoostClassifier为例介绍catboost模型的常用参数把手，更多参数可参见文档。但如果关注的是模型表现的话，并不需要调整更多参数。 参数 描述 iterations=500 最大决策树数目。可以使用较小的值。 learning_rate=0.03 影响训练的总时长：值越小，训练所需的迭代次数就越高。 depth=6 树的深度。可能是任何不大于32的整数。推荐值：1-10 l2_leaf_reg=3 L2正则子参数。任何正值都可以。 loss_function=’Logloss’ string，object，损失函数。二元分类问题使用LogLoss或CrossEntropy。多分类使用MultiClass。 eval_metric string，性能指标，对回归问题是RMSE，二分类可用Logloss bootstrap_type 自助采样类型，对样本采样的方式，默认Bayesian，可选Bernoulli subsample bagging的比例，仅在bootstrap_type取Bernoulli或Poisson可用 rsm 特征抽取比例，取值于(0,1]，默认为1 nan_mode string，缺失值处理方式，“Forbidden” 输入数据中不允许缺失值，“Min”或“Max” one_hot_max_size 将所有特征取值个数小于该值的特征转化为one-hot编码 border_count=32 数值特征分割数。整数1至255（含）。 ctr_border_count=50 类别特征分割数。整数1至255（含） random_seed seed，用于训练的随机种子 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768Class CatBoostClassifier(iterations=None, learning_rate=None, depth=None, l2_leaf_reg=None, model_size_reg=None, rsm=None, loss_function=&#x27;Logloss&#x27;, border_count=None, feature_border_type=None, fold_permutation_block_size=None, od_pval=None, od_wait=None, od_type=None, nan_mode=None, counter_calc_method=None, leaf_estimation_iterations=None, leaf_estimation_method=None, thread_count=None, random_seed=None, use_best_model=None, verbose=None, logging_level=None, metric_period=None, ctr_leaf_count_limit=None, store_all_simple_ctr=None, max_ctr_complexity=None, has_time=None, classes_count=None, class_weights=None, one_hot_max_size=None, random_strength=None, name=None, ignored_features=None, train_dir=None, custom_loss=None, custom_metric=None, eval_metric=None, bagging_temperature=None, save_snapshot=None, snapshot_file=None, fold_len_multiplier=None, used_ram_limit=None, gpu_ram_part=None, allow_writing_files=None, approx_on_full_history=None, boosting_type=None, simple_ctr=None, combinations_ctr=None, per_feature_ctr=None, ctr_description=None, task_type=None, device_config=None, devices=None, bootstrap_type=None, subsample=None, max_depth=None, n_estimators=None, num_boost_round=None, num_trees=None, colsample_bylevel=None, random_state=None, reg_lambda=None, objective=None, eta=None, max_bin=None, scale_pos_weight=None, gpu_cat_features_storage=None) 模型方法 训练 12345678910# cat_features指定类别特征fit(X, y=None, cat_features=None, sample_weight=None, baseline=None, use_best_model=None, eval_set=None, verbose=None, logging_level=None plot=False) 预测 123456789101112predict(data, prediction_type=&#x27;Class&#x27;, ntree_start=0, ntree_end=0, thread_count=-1, verbose=None) predict_proba(data, ntree_start=0, ntree_end=0, thread_count=-1, verbose=None) 交叉验证1234567891011121314cv(pool=None, params=None, dtrain=None, iterations=None, num_boost_round=None, fold_count=3, nfold=None, inverted=False, partition_random_seed=0, seed=None, shuffle=True, logging_level=None, stratified=False, as_pandas=True) 示例： 123456789pool = Pool(x_train, y_train)params = &#123; &#x27;iterations&#x27;: 100, &#x27;depth&#x27;: 2, &#x27;loss_function&#x27;: &#x27;MultiClass&#x27;, &#x27;classes_count&#x27;: 3, &#x27;logging_level&#x27;: &#x27;Silent&#x27; &#125;scores = cv(params, pool) 网格搜索调优参数后我们得到的最终分数实际上和调优之前一样！看起来我做的调优没能超越默认值。这体现了CatBoost分类器的质量，默认值是精心挑选的（至少就这一问题而言）。增加交叉验证的n_splits，通过多次运行分类器减少得到的噪声可能会有帮助，不过这样的话网格搜索的耗时会更长。如果你想要测试更多参数或不同的组合，那么上面的代码很容易修改。 预防过拟合CatBoost提供了预防过拟合的良好设施。如果你把iterations设得很高，分类器会使用许多树创建最终的分类器，会有过拟合的风险。如果初始化的时候设置了use_best_model=True和eval_metric=’Accuracy’，接着设置eval_set（验证集），那么CatBoost不会使用所有迭代，它将返回在验证集上达到最佳精确度的迭代。这和神经网络的及早停止（early stopping）很像。如果你碰到过拟合问题，试一下这个会是个好主意。不过我没能在这一数据集上看到任何改善，大概是因为有这么多训练数据点，很难过拟合。 CatBoost集成集成指组合某种基础分类器的多个实例（或不同类型的分类器）为一个分类器。在CatBoost中，这意味着多次运行CatBoostClassify（比如10次），然后选择10个分类器中最常见的预测作为最终分类标签。一般而言，组成集成的每个分类器需要有一些差异——每个分类器犯的错不相关时我们能得到最好的结果，也就是说，分类器尽可能地不一样。 使用不同参数的CatBoost能给我们带来一些多样性。在网格搜索过程中，我们保存了我们测试的所有参数，以及相应的分数，这意味着，得到最佳的10个参数组合是一件轻而易举的事情。一旦我们具备了10个最佳参数组合，我们直接构建分类器集成，并选取结果的众数作为结果。对这一具体问题而言，我发现组合10个糟糕的参数设定，能够改善原本糟糕的结果，但集成看起来在调优过的设定上起不了多少作用。不过，由于大多数kaggle竞赛的冠军使用集成，一般而言，使用集成明显会有好处。 参考使用网格搜索优化CatBoost参数Yandex]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习：集成学习（四）—— XGBoost]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%9A%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%9B%9B%EF%BC%89%E2%80%94%E2%80%94%20XGBoost%2F</url>
    <content type="text"><![CDATA[xgboost自从被提出来后就因其出众的效率和较高的准确度而被广泛关注，在各种比赛中大放异彩，下图即是对xgboost的完美代言： xgboost（eXtreme Gradient Boosting）是GBDT的一个C++实现，作者为华盛顿大学研究机器学习的大牛陈天奇，他在研究中深感受制于现有库的计算速度和精度，因此着手搭建xgboost项目。在陈天奇的论文中提到了xgboost的几个主要特点： 加入了正则化项的结构化损失函数(叶节点数作为L1正则，叶节点输出值的平方和作为L2范数)； 使用损失函数的二阶泰勒展开作为近似； 能够自动处理稀疏数据； 采用加权分位数算法搜索近似最优分裂点； 加入了列抽样； 加入了Shrinkage，相当于学习率缩减； 基于分块技术的大量数据高效处理()； 并行和分布式计算； 原理篇提升树、GBDT 和 Xgboost 的关系如下： 提升树 = 决策树加法模型 + 经验风险最小化 + 前向分布算法 GBDT = 决策树加法模型 + 经验风险最小化 + 前向分布算法 + 梯度提升算法 xgboost = 决策树加法模型 + 结构风险最小化 + 前向分布算法 + 损失函数二阶泰勒展开 模型（additive model） xgboost与GBDT同为决策树加法模型： \begin{align*} f_M(x) &= \sum_{m=1}^{M}T(x;\gamma _m)\\ T(x;\gamma)&= \sum_{j=1}^{J}c_j[x \in R_j] \end{align*} $f_M(x)$: 第M次迭代后的集成模型； $T(x;\gamma_m)$：第m次迭代的基模型（决策树），对分类问题是二叉分类树，对回归问题是二叉回归树，以下以回归树为例; $\gamma$：参数，{$(R_1,c_1),(R_2,c_2),…,(R_J,c_J)$}表示树的区域划分和各区域上的预测值，J是回归树的复杂度即叶子节点数； 结构风险最小化（SRM）xgboost在其优化的损失函数中引入了正则化项： \begin{align*} \gamma_1,...,\gamma_m &= arg \underset{\gamma_1,...,\gamma_m}{min} \sum_{i=1}^{N} L(y_i; f_M(x_i)) + \sum_{m=1}^{M} \Omega (T(x;\gamma_m))\\ &=arg \underset{\gamma_1,...,\gamma_m}{min} \sum_{i=1}^{N} L(y_i; \sum_{m=1}^{M} T(x;\gamma_m)) + \sum_{m=1}^{M} (\alpha J_m+ \frac{1}{2}\beta \left \| c_m \right \|^2) \end{align*} $J_m$：第m棵树叶子节点个数； $cm$：($c{m1},c{m2},…,c{mJ_m}$)，第m棵树中各叶子节点的输出值； $\alpha$：一阶正则项系数； $\beta$：二阶正则项系数； 直观上通过决策树的叶子个数和叶子输出值可以对模型进行很强的控制，引入正则项后，算法会选择简单而性能优良的模型，防止过拟合。xgboost要求其损失函数至少是二阶连续可导的凸函数。 前向分步算法xgboost与GBDT类似，采用前向分步算法来简化上述优化问题：每一步通过一个决策树来拟合当前模型预测值与真实值的残差，并将其累加至当前模型得到最新模型。 1、初始化模型： f_0(x)=02、对 $m=1,2,…,M$ （1）拟合决策树： \begin{align*} \gamma_m&= arg \underset{\gamma_m}{min} \sum_{i=1}^{N} L(y_i;f_{m-1}(x_i)+T(x;\gamma_m)) + \Omega (T(x;\gamma_m))\\ &=arg \underset{\gamma_m}{min} \sum_{i=1}^{N} L(y_i;f_{m-1}(x_i)+T(x;\gamma_m)) + \alpha J_m+ \frac{1}{2}\beta \left \| c_m \right \|^2 \end{align*}（2）更新模型： f_m(x)=f_{m-1}(x) + T(x;\gamma_m)3、最终模型： f_M(x) = \sum_{m=1}^{M}T(x;\gamma _m)损失函数的二阶泰勒展开与GBDT将损失函数在当前模型下的负梯度值作为残差的近似值来拟合决策树的梯度提升方法不同，xgboost将损失函数在当前模型下的二阶泰勒展开作为近似损失函数来训练决策树： \begin{align*} \gamma_m &= arg \underset{\gamma_m}{min} \sum_{i=1}^{N} L(y_i;f_{m-1}(x_i)+T(x;\gamma_m)) + \Omega (T(x;\gamma_m))\\ &\approx arg \underset{\gamma_m}{min} \sum_{i=1}^{N} [L(y_i;f_{m-1}(x_i))+g_i T(x;\gamma_m)+\frac{1}{2} h_i T^2(x;\gamma_m)] + \Omega (T(x;\gamma_m))\\ &=arg \underset{\gamma_m}{min} \sum_{i=1}^{N} L(y_i;f_{m-1}(x_i))+ \sum_{i=1}^{N} [g_i T(x;\gamma_m)+\frac{1}{2} h_i T^2(x;\gamma_m)] + \Omega (T(x;\gamma_m))\\ &=C + arg \underset{\gamma_m}{min} \sum_{i=1}^{N} [g_i T(x;\gamma_m)+\frac{1}{2} h_i T^2(x;\gamma_m)] + \Omega (T(x;\gamma_m)) \end{align*} $gi = \frac{\partial L(y_i,f{m-1} (xi))}{\partial f{m-1}(xi)}$:为损失函数在当前模型$f{m-1}(x_i)$处的一阶偏导； $hi = \frac{\partial^2 L(y_i,f{m-1}(xi))}{\partial f{m-1}(xi)^2}$: 为损失函数在当前模型$f{m-1}(x_i)$处的二阶偏导； 去掉常数项C不会影响优化结果，因此损失函数变成如下形式： \gamma_m = arg \underset{\gamma_m}{min} \sum_{i=1}^{N} [g_i T(x;\gamma_m)+\frac{1}{2} h_i T^2(x;\gamma_m)] + \Omega (T(x;\gamma_m))使用泰勒二阶展开的优点： xgboost使用了一阶和二阶偏导, 二阶导数有利于梯度下降的更快更准. 使用泰勒展开取得函数做自变量的二阶导数形式, 可以在不选定损失函数具体形式的情况下, 仅仅依靠输入数据的值就可以进行叶子分裂优化计算, 本质上也就把损失函数的选取和模型算法优化/参数选择分开了. 这种去耦合增加了xgboost的适用性, 使得它按需选取损失函数, 可以用于分类, 也可以用于回归。引用自：@AntZ xgboost决策树生成确定叶子节点的最优输出值进一步化简我们的损失函数： \begin{align*} \gamma_m &= arg \underset{\gamma_m}{min} \sum_{i=1}^{N} [g_i T(x;\gamma_m)+\frac{1}{2} h_i T^2(x;\gamma_m)] + \Omega (T(x;\gamma_m))\\ &= arg \underset{\gamma_m}{min} \sum_{j=1}^{J_m} [c_{mj} (\sum_{i \in R_{mj}}g_i) + \frac{1}{2}c_{mj}^2 (\beta +\sum_{i \in R_{mj}}h_i )] + \alpha J_m\\ &=arg \underset{\gamma_m}{min} \sum_{j=1}^{J_m}q(c_{mj}) + \alpha J_m \end{align*}其中： q(c_{mj}) = c_{mj} (\sum_{i \in R_{mj}}g_i) + \frac{1}{2}c_{mj}^2 (\beta +\sum_{i \in R_{mj}}h_i )因为$\alpha Jm$为常数，因此最小化损失函数就可以通过分别极小化每个叶子节点中损失值$q(c{mj})$实现。$q(c_{mj})$为凸函数，这是一个无约束的凸优化问题，求导令其等于0，得到每个叶子节点的最优输出值以及该决策树所带来的损失函数值的减少： \begin{align*} &c_{mj} = -\frac{\sum_{i \in R_{mj}} g_j}{\beta + \sum_{i \in R_{mj}}h_j}\\ &L_{mj} = -\frac{1}{2} \sum_{j=1}^{J_m}\frac{(\sum_{i \in R_{mj}} g_j)^2}{\beta + \sum_{i \in R_{mj}}h_j} + \alpha J_m \end{align*}xgboost中每棵决策树中各叶子节点的输出值只由损失函数在当前模型的一阶、二阶导数决定，在第m次迭代时，只要我们预先计算出g和h，在训练生成一棵树模型后就可以直接计算出每个叶子节点的最优输出值。那么现在的问题就是：xgboost是根据什么条件来生成一棵合适的决策树呢？ 分裂条件与一般决策树选取最大信息增益(ID3)、最大信息增益率(C4.5)、最小基尼系数(CART)或最小方差(CART)类似，xgboost按照“最大损失减小值”的原则来选择最优切分特征和切分点。 假设我们要对某个节点进行分裂，设$R$表示该节点上所有样本的集合，$R_L,R_R$表示分裂后左右子节点中样本集，则根据损失函数一阶二阶导数可以计算出分裂前后第m棵树的损失函数减小量为： \Delta L = \frac{1}{2} (\sum_{j=1}^{J_m}\frac{(\sum_{i \in R_{L}} g_j)^2}{\beta + \sum_{i \in R_{L}}h_j}- \sum_{j=1}^{J_m}\frac{(\sum_{i \in R_{R}} g_j)^2}{\beta + \sum_{i \in R_{R}}h_j}-\sum_{j=1}^{J_m}\frac{(\sum_{i \in R} g_j)^2}{\beta + \sum_{i \in R}h_j}) - \alpha J_m决策树集成shrinkage方法在对各个基决策树进行加和集成时，xgboost采用了shrinkage(缩减)的方法来降低过拟合的风险，模型集成形式如下： f_m(x)=f_{m-1}(x) + \eta T(x;\gamma_m),0]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习：集成学习（五）—— LightGBM]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%9A%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%BA%94%EF%BC%89%E2%80%94%E2%80%94%20LightGBM%2F</url>
    <content type="text"><![CDATA[LightGBM的安装LightGBM CLI 版本的构建可参考LightGBM安装指南，python版本我们只需要通过pip下载安装即可，更多python版本的安装可参见LightGBM/python-package/。 构建普通版本： 1pip install lightgbm 构建GPU版本: 1pip install lightgbm --install-option=--gpu LightGBM的使用使用LightGBM的一般流程： 特征工程：尽可能多地构造新特征，再通过特征选择筛选出有价值的特征，可以交给模型训练过程自动选择也可以通过其他方式手动选择； 模型选择：对于分类问题有lightgbm.LGBMClassifier，对于回归问题有lightgbm.LGBMRegressor； 参数选择：迭代次数和一般超参数要分开来优化 通过“早停止”确定最优的迭代次数； 通过“网格搜索”确定最优的超参数； 训练预测：使用最后的模型对全量训练集进行训练，预测测试集标签； 其中 1、2、4 步骤是一般机器学习的基本流程，步骤3才是使用LightGBM的关键所在，下面我们将详细讨论LightGBM的调参之法，并给出基于Python API的相应实例。 特征工程假设我们已经通过特征工程构建了如下的数据集； 123456789101112131415161718192021222324252627import pandas as pdimport numpy as npimport lightgbm as lgbfrom sklearn.model_selection import train_test_splitfrom sklearn.model_selection import RandomizedSearchCV# 完整训练集和测试集train = pd.read_csv(&#x27;train.csv&#x27;)test = pd.read_csv(&#x27;test.csv&#x27;)# 特征空间和标签空间features = [c for c in train.columns if c not in [&#x27;label&#x27;]]target = &#x27;label&#x27;# 划分训练集、测试集、测试集X_train, X_valid, y_train, y_valid = train_test_split(train[features],train[target],test_size=0.2,shuffle=True)# 或者 ratio = int(0.8 * train.shape[0])X_train = train[features].iloc[:ratio]y_train = train[target].iloc[:ratio]X_valid = train[features].ioc[ratio:]y_valid = train[target].iloc[ratio:]X_test = test[features]y_test = test[target]# 构造 Dataset 对象，可赋权重train_lgb = lgb.Dataset(train[features], train[target], weight=w) 更多数据接口参见这里。 模型选择以回归模型lightgbm.LGBMRegressor为例。 模型参数12345678910111213141516171819202122232425262728class lightgbm.LGBMRegressor(boosting_type=&#x27;gbdt&#x27;, # 模型类型 objective=None, num_leaves=31, # 叶节点数和最大深度 max_depth=-1, learning_rate=0.1, # 学习率和迭代次数 n_estimators=100, max_bin=255, # 直方图方块数量 subsample_for_bin=200000, # 用于构建直方图的样本数 class_weight=None, # 权重 min_split_gain=0.0, # 分裂的最小增益 min_child_weight=0.001, # 一个叶子上的最小 hessian 和 min_child_samples=20, # 叶子最小样本数 subsample=1.0, # bagging行抽样比例 subsample_freq=0, # 抽样频率 colsample_bytree=1.0, # 列抽样比例 reg_alpha=0.0, # L1正则项系数 reg_lambda=0.0, # L2正则项系数 random_state=None, # 随机种子 n_jobs=-1, # 线程数 silent=True, # 迭代过程是否打印信息 **kwargs) # 额外参数 控制类型的参数： boosting_type=’gbdt’: 模型类型，default=gbdt, type=enum, options=gbdt, rf, dart, goss； objective=None：问题类型，default=regression, type=enum, options=regression, regression_l1, huber, fair, poisson, quantile, quantile_l2, binary, multiclass, multiclassova, xentropy, xentlambda, lambdarank； 控制学习效果的参数： num_leaves=31：叶节点数，这是控制树模型复杂度的主要参数，理论上, 借鉴 depth-wise 树, 我们可以设置 num_leaves = 2^(max_depth) 但是, 这种简单的转化在实际应用中表现不佳. 这是因为, 当叶子数目相同时, leaf-wise 树要比 depth-wise 树深得多, 这就有可能导致过拟合. 因此, 当我们试着调整 num_leaves 的取值时, 应该让其小于 2^(max_depth)。越大越容易过拟合； max_depth=-1：最大深度，虽然Leaf-wise 只需要控制num_leaves就可以间接控制树的深度，但你也可以利用 max_depth 来显式地限制树的深度。越大越容易过拟合； learning_rate=0.1：学习率，一般取值范围是[0.01,0.1]，需要和n_estimators配合使用；较小的 learning_rate 和较大的 num_iterations一般会有更好的准确率； n_estimators=100：迭代次数；越大越容易过拟合； max_bin=255：特征值的脂肪bins个数，LightGBM 将根据 max_bin 自动压缩内存。 例如, 如果 maxbin=255, 那么 LightGBM 将使用 uint8t 的特性值；越大越容易过拟合； subsample_for_bin=200000：用来构建直方图的样本数量，在设置更大的数据时, 会提供更好的训练效果, 但会增加数据加载时间； class_weight=None：样本权重，可以是dict、’balanced’或 None, dict只用于多分类任务，可以通过{class_label: weight}为每种标签设置权值；对于二分类可以使用is_unbalance or scale_pos_weight参数；’balanced’会自动根据标签比例调整样本权重； min_split_gain=0.0：分裂的最小增益，越大越限制节点分裂越容易欠拟合； min_child_weight=0.001：一个叶子上的最小 hessian 和，类似于 min_child_samples，越大越容易欠拟合； min_child_samples=20：叶子中的最小样本数，越大越容易欠拟合； subsample=1.0：行抽样比例，越大越容易过拟合； subsample_freq=0：抽样频率，每k次迭代进行一次采样，等于0时不采样，越大越容易过拟合； colsample_bytree=1.0：列抽样比例，越大越容易过拟合； reg_alpha=0.0：L1正则项系数，越大越不容易过拟合； reg_lambda=0.0：L2正则项系数，越大越不容易过拟合； 控制辅助的参数： random_state=None：随机种子 n_jobs=-1：线程数 silent=True：迭代过程是否打印信息 其它控制的参数： verbose=1:int,控制日志打印级别， 0 = 信息 完整参数和参数效果参见参数和参数优化。 假设我们的回归模型初始化参数为： 123456789101112131415161718192021222324lgb_reg = lgb.LGBMRegressor(num_leaves=5, max_depth=-1, learning_rate=0.02, n_estimators=1000, max_bin=255, subsample_for_bin=50000, min_split_gain=0.0, min_child_weight=0.001, min_child_samples=5, subsample=0.8, subsample_freq=3, colsample_bytree=0.8, reg_alpha=0.0, reg_lambda=1.0, random_state=999, n_jobs=-1, silent=True, verbose=-1) 模型的属性 nfeatures:int ，特征数 classes_：array of shape = [n_classes] ，分类标签 nclasses：int，分类标签数 bestscore：dict or None，模型最好的得分 bestiteration：int or None，如果训练时指定了early_stopping_rounds，模型最优迭代次数 objective：string or callable，训练时所使用的objective evalsresult：dict or None，如果训练时指定了early_stopping_rounds，评估结果 featureimportances：array of shape = [n_features]，特征重要度 早停止——确定最优迭代次数原理：LightGBM是一种迭代的Boosting方法，我们可以在模型训练时设置验证集，通过在每轮迭代评估当前模型在验证集上的性能来监控模型训练过程，通过早停止的方式确定出在给定步长下的最优迭代次数。 早停止涉及到以下几个核心概念： 步长：也称学习率learning_rate，最优迭代次数和步长的设置息息相关，必须先固定步长，一般取值范围为[0.01,0.1]; 验证集：必须在模型fit时提供相应的验证集; 评估指标：必须使用合适的评估指标eval_metric; fit方法1234567891011121314151617fit(X, # 特征空间 y, # 标签空间 sample_weight=None, # 样本权重 init_score=None, # 训练数据的初始得分 eval_set=None, # 验证集 eval_names=None, # 验证集名称 eval_sample_weight=None, # 验证集样本权重 eval_init_score=None, # 验证集初始得分 eval_metric=&#x27;l2&#x27;, # 评估指标 early_stopping_rounds=None, # 早停止迭代次数 verbose=True, # 是否打印详情 feature_name=&#x27;auto&#x27;, # 特征名 categorical_feature=&#x27;auto&#x27;, # 类别特征 callbacks=None) # callback函数列表 X：特征空间，array-like or sparse matrix of shape = [n_samples, n_features]，一般为ndarray或者DataFrame y：标签空间，array-like of shape = [n_samples],一般为ndarray或者Series sample_weight=None：样本权重，array-like of shape = [n_samples] or None init_score=None：训练数据的初始得分，array-like of shape = [n_samples] or None eval_set=None：验证集，为了早停止而用作验证集的(X, y) 元组列表 eval_names=None：验证集名称，list of strings or None eval_sample_weight=None：验证集样本权重，list of arrays or None eval_init_score=None：验证集初始得分，list of arrays or None eval_metric=’l2’：评估指标，string, list of strings, callable or None，如果为字符串，则必须是内置的评估指标，如l1, l2, ndcg, auc, binary_logloss, binary_error …；如果是函数，必须满足func(y_true, y_pred)，返回 (指标名eval_name, 指标得分eval_result, 是否越大越好is_bigger_better)； early_stopping_rounds=None：早停止迭代次数，当评估指标停止提升early_stopping_rounds多轮时，训练会提前停止； verbose=True：是否打印详情，bool值，且至少有一个验证集 feature_name=’auto’：特征名，当取’auto’且输入的数据为DataFrame时，会自动识别特征名； categorical_feature=’auto’：类别特征，list of strings or int, or ‘auto’，如果是字符串，特征名会被使用，如果是整型，特征索引会被使用，如果是’auto’且数据为DataFrame时，会自动识别特征名，被自动识别类别特征； callbacks=None：callback函数列表，函数列表中的每个函数都会在每次迭代结束时被调用，详见python的回调函数; 自定义早停止中的评估函数自定义函数遵循以下接口协议：func(y_true, y_pred), func(y_true, y_pred, weight) or func(y_true, y_pred, weight, group)，Returns (eval_name, eval_result, is_bigger_better) or list of (eval_name, eval_result, is_bigger_better)。 y_true:真实标签，array-like of shape = [n_samples] y_pred:预测标签，array-like of shape = [n_samples] weight:样本权重 group:Group/query data, used for ranking task eval_name: 评估指标名称，str eval_result: 评估结果，float is_bigger_better: 是否越大代表模型越好，bool 一个将Gini系数作为评估指标的例子： 123456789101112131415161718192021222324252627282930def gini(actual, pred): &quot;&quot;&quot; 计算真实序列按照预测序列升序排列时的相对基尼系数，如果真实序列均为0，定义gini为1是合理的 :param actual: 真实序列&gt;=0 :param pred: 预测序列 :return: 基尼系数 &quot;&quot;&quot; n = len(pred) triple = np.c_[actual, pred, range(n)].astype(float) triple = triple[np.lexsort((triple[:, 2], triple[:, 1]))] cum_sum = triple[:, 0].cumsum() if cum_sum[-1] == 0: return 1 else: x = cum_sum.sum() / cum_sum[-1] return (n + 1 - 2 * x) / ndef gini_normalized(y_true, y_pred): &quot;&quot;&quot; 自定义用于fit的eval_metric指标函数，此处为归一化的相对基尼系数 :param y_true: 真实序列ndarray :param y_pred: 预测序列ndarray :return: eval_name, eval_result, is_bigger_better &quot;&quot;&quot; gini_true = gini(y_true, y_true) gini_pred = gini(y_true, y_pred) # 如果真实值均匀分布，gini为0，定义此时的归一化为1是合理的 res = gini_pred / gini_true if gini_true else 1 return &#x27;gini_normalized&#x27;, res, True 通过早停止确定最优迭代次数通过早停止能够得到在当前参数下的最佳迭代次数，需要重新修正模型的迭代次数参数n_estimators，一个例子： 123456789101112131415161718192021def early_stopping(lgb_reg, X_train, y_train, X_valid, y_valid, online): &quot;&quot;&quot; 通过早停确定最优迭代次数 :param lgb_reg: 回归模型 :param X_train: 部分训练集特征空间 :param y_train: 部分训练集标签空间 :param X_valid: 验证集特征空间 :param y_valid: 验证集标签空间 :param online: 是否线上 :return: 设置了最优迭代次数的模型 &quot;&quot;&quot; lgb_reg.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], eval_names=[&#x27;gini&#x27;], eval_metric=gini_normalized, early_stopping_rounds=100, verbose=[True, False][online]) best_iteration = lgb_reg.best_iteration_ lgb_reg.set_params(n_estimators=best_iteration) return lgb_reg 打印日志监控迭代过程：123456789101112131415161718192021[1] gini&#x27;s l2: 12.5645 gini&#x27;s gini_normalized: 0.709833Training until validation scores don&#x27;t improve for 100 rounds.[2] gini&#x27;s l2: 12.5671 gini&#x27;s gini_normalized: 0.606205[3] gini&#x27;s l2: 12.5561 gini&#x27;s gini_normalized: 0.753056[4] gini&#x27;s l2: 12.5592 gini&#x27;s gini_normalized: 0.579402[5] gini&#x27;s l2: 12.5623 gini&#x27;s gini_normalized: 0.575758[6] gini&#x27;s l2: 12.5653 gini&#x27;s gini_normalized: 0.520632[7] gini&#x27;s l2: 12.5675 gini&#x27;s gini_normalized: 0.476118[8] gini&#x27;s l2: 12.5572 gini&#x27;s gini_normalized: 0.578779[9] gini&#x27;s l2: 12.5601 gini&#x27;s gini_normalized: 0.525161[10] gini&#x27;s l2: 12.5634 gini&#x27;s gini_normalized: 0.515673......[97] gini&#x27;s l2: 12.5506 gini&#x27;s gini_normalized: 0.217889[98] gini&#x27;s l2: 12.5534 gini&#x27;s gini_normalized: 0.107637[99] gini&#x27;s l2: 12.5552 gini&#x27;s gini_normalized: 0.105389[100] gini&#x27;s l2: 12.5425 gini&#x27;s gini_normalized: 0.270766[101] gini&#x27;s l2: 12.5458 gini&#x27;s gini_normalized: 0.270766[102] gini&#x27;s l2: 12.5492 gini&#x27;s gini_normalized: 0.270766[103] gini&#x27;s l2: 12.5511 gini&#x27;s gini_normalized: 0.215826Early stopping, best iteration is:[3] gini&#x27;s l2: 12.5561 gini&#x27;s gini_normalized: 0.753056 网格搜索在确定了最优的迭代次数之后，需要通过网格搜索来确定其他超参数的最优取值，网格搜索包含以下几个核心： 模型：用于训练的模型 参数空间：由各个待调节参数的取值范围共同组成的参数空间 搜索机制：参数空间的搜索或抽样机制 交叉验证：通过交叉验证留出验证集来验证不同参数备选解的效果 评估函数：评估模型训练效果的函数 搜索机制常用的有两种网格搜索机制： 一般网格搜索:给出每个待调节的超参数的若干备选解，它们的任意组合构成很多组解，在所有的备选解空间中搜索最优参数；因为备选解空间刚好都是各轴交点，它们一起构成了一个解的网格，因此称为网格搜索； 随机网格搜索:给出每个待调节的超参数的若干备选解或分布，每次从各个超参数备选解中抽样出一个值组成一组解，在有限抽样次数中寻找最优解； 关于二者的比较可以参见这篇文章Smarter Parameter Sweeps (or Why Grid Search Is Plain Stupid)。 随机网格搜索更加高效，接下来将主要介绍scilit-learn提供的RandomizedSearchCV方法，GridSearchCV方法也大同小异。 12345678910111213class sklearn.model_selection.RandomizedSearchCV(estimator, param_distributions, n_iter=10, scoring=None, fit_params=None, n_jobs=1, iid=True, refit=True, cv=None, verbose=0, pre_dispatch=‘2*n_jobs’, random_state=None, error_score=’raise’, return_train_score=’warn’) 参数说明: estimator：模型对象，模型中需要指定score方法，否则必须传递scoring参数； param_distributions：dict，参数名作为key，参数分布或者列表作为value；如果所有参数都以列表作为value，那么不会出现重复抽样；否则，可能出现重复抽样，参数分布必须支持rvs方法用于抽样; n_iter=10：抽样次数 scoring=None：string, callable, list/tuple, dict or None，应用于验证集上的评估方法。如果是字符串，必须是内置的评估指标，如’f1’，’roc_auc’，参见predefined values；如果是函数，可以通过sklearn.metrics.make_scorer(score_func)方法构造一个scorer，其中score_func(y, y_pred, **kwargs)，return eval_result； fit_params=None：dict，传给fit方法的参数 n_jobs=1：线程数 iid=True：是否独立同分布 refit=True：是否使用最优参数重新在全量数据集行进行训练，refit后的模型可以使用bestestimator返回，可以直接在该模型上应用predict方法； cv=None：int,cross-validation generator or an iterable，默认采用3折交叉验证。如果为整数k，采用k折交叉验证；如果为交叉验证生成器或可迭代的训练测试集划分，具体可参考Cross-validation; verbose=0：大于等于0的整数，控制日志打印详细程度，越大越详细； pre_dispatch=‘2*n_jobs’：int, or string控制线程数，默认立刻创建所有线程； random_state=None：随机种子； error_score=’raise’：fit时出错的处理机制 returntrain_score=’warn’：boolean，是否在cv_results中包含训练集得分，默认为True； 属性: cvresults:网格搜索结果，dict of numpy (masked) ndarrays（keys作为header，values作为列，可以传给DataFrame显示）。包含以下项目： params:存储历次尝试的参数字典列表 rank_test_score：存储各次参数的得分排名 123456789101112131415161718192021222324252627# 包含两组备选解的搜索结果&#123;&#123;&#x27;mean_fit_time&#x27;: array([0.75533938, 0.66926527]), &#x27;std_fit_time&#x27;: array([0.23030835, 0.06423108]), &#x27;mean_score_time&#x27;: array([0.00308728, 0.00117135]), &#x27;std_score_time&#x27;: array([2.26265789e-03, 4.74953906e-05]), &#x27;param_subsample&#x27;: masked_array(data=[0.7, 0.8],mask=[False, False],fill_value=&#x27;?&#x27;,dtype=object), &#x27;param_reg_lambda&#x27;: masked_array(data=[1.0, 0.5],mask=[False, False],fill_value=&#x27;?&#x27;,dtype=object), &#x27;param_reg_alpha&#x27;: masked_array(data=[1.5, 0.5],mask=[False, False],fill_value=&#x27;?&#x27;,dtype=object), &#x27;param_num_leaves&#x27;: masked_array(data=[4, 3],mask=[False, False],fill_value=&#x27;?&#x27;,dtype=object), &#x27;param_min_child_samples&#x27;: masked_array(data=[9, 4],mask=[False, False],fill_value=&#x27;?&#x27;,dtype=object), &#x27;param_max_depth&#x27;: masked_array(data=[10, 8],mask=[False, False],fill_value=&#x27;?&#x27;,dtype=object), &#x27;param_colsample_bytree&#x27;: masked_array(data=[0.9, 0.7],mask=[False, False],fill_value=&#x27;?&#x27;,dtype=object), &#x27;params&#x27;: [&#123;&#x27;subsample&#x27;: 0.7, &#x27;reg_lambda&#x27;: 1.0, &#x27;reg_alpha&#x27;: 1.5, &#x27;num_leaves&#x27;: 4, &#x27;min_child_samples&#x27;: 9, &#x27;max_depth&#x27;: 10, &#x27;colsample_bytree&#x27;: 0.9&#125;, &#123;&#x27;subsample&#x27;: 0.8, &#x27;reg_lambda&#x27;: 0.5, &#x27;reg_alpha&#x27;: 0.5, &#x27;num_leaves&#x27;: 3, &#x27;min_child_samples&#x27;: 4, &#x27;max_depth&#x27;: 8, &#x27;colsample_bytree&#x27;: 0.7&#125;], &#x27;split0_test_score&#x27;: array([-0.20771241, -0.56534909]), &#x27;split1_test_score&#x27;: array([ 0.09806087, -0.00358756]), &#x27;split2_test_score&#x27;: array([0.38842062, 0.44214761]), &#x27;mean_test_score&#x27;: array([ 0.08991668, -0.04749387]), &#x27;std_test_score&#x27;: array([0.24401773, 0.41343975]), &#x27;rank_test_score&#x27;: array([1, 2], dtype=int32)&#125; bestestimator:返回最佳模型，参见refit bestscore:返回最佳模型的平均交叉验证得分 bestparams：返回最佳参数 bestindex：返回最佳参数在cvresults[params]中的索引 scorer_:返回所使用的评估函数 nsplits：int,交叉验证的折数 参数空间参数空间由字典表示，参数名为key，参数取值列表或分布(具有rvs方法的对象)作为values，一个例子： 1234567891011from scipy.stats import randint as sp_randintparams_dist = &#123;&#x27;num_leaves&#x27;: range(2, 300, 2), &#x27;max_depth&#x27;: [-1, 6, 8, 10, 15], &#x27;min_child_samples&#x27;: sp_randint(10,100), &#x27;subsample&#x27;: [i / 10. for i in range(3, 11)], &#x27;colsample_bytree&#x27;: [i / 10. for i in range(3, 11)], &#x27;reg_alpha&#x27;: [0, 0.5, 1., 1.5, 2., 5., 10.], &#x27;reg_lambda&#x27;: [0, 0.5, 1., 1.5, 2., 5., 10.]&#125; 自定义网格搜索中的评估函数网格搜索自定义评估函数(可作为scoring参数)的接口协议： 123sklearn.metrics.make_scorer(score_func, greater_is_better=True, needs_proba=False, needs_threshold=False, **kwargs)[source]¶score_func(y, y_pred, **kwargs)-&gt;eval_score 一个例子： 1234567891011# 首先定义一个score_funcdef gini_grid(y_label, y_pred): &quot;&quot;&quot; 用于grid_search的scoring :param y_label: 真实序列ndarray :param y_pred: 预测序列ndarray :return: 归一化的基尼指数 &quot;&quot;&quot; return gini_normalized(y_label, y_pred)[1]# 通过make_scorer(score_func)构建scorerscoring = make_scorer(score_func) 随机网格搜索确定最优超参数一个例子： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556def random_search(lgb_reg, params_dist, X_train, y_train, n_iter=10, nfold=3): &quot;&quot;&quot; 随机网格搜索确定最优参数，返回使用最优参数全量refit后的模型 :param lgb_reg: 模型 :param params_dist: 参数网格 :param X_train: 全部训练集的特征空间DataFrame :param y_train: 全部训练集的标记空间Series :param n_iter: 参数采样次数 :param nfold: 交叉验证折数 :return: refit后的模型 &quot;&quot;&quot; rs = RandomizedSearchCV(lgb_reg, params_dist, n_iter=n_iter, scoring=make_scorer(gini_grid), cv=nfold, refit=True, verbose=0, return_train_score=False) rs.fit(X_train.values, y_train.values) report(rs.cv_results_) return rs.best_estimator_def report(results, top=10): &quot;&quot;&quot; 打印网格搜索结果中排名前10的参数组合和评估性能 :param results: 网格搜索对象的结果属性cv_results_ :param top: 排名前几的 :return: None &quot;&quot;&quot; print(&quot;GridSearchCV took %d candidate parameter settings.&quot; % len(results[&#x27;params&#x27;])) for i in range(1, top + 1): candidates = np.flatnonzero(results[&#x27;rank_test_score&#x27;] == i) for candidate in candidates: print(&#x27;model rank:%s&#x27; % i) print(&quot;Mean validation score: &#123;0:.3f&#125; (std: &#123;1:.3f&#125;)&quot;.format( results[&#x27;mean_test_score&#x27;][candidate], results[&#x27;std_test_score&#x27;][candidate])) print(&quot;Parameters: &#123;0&#125;&quot;.format(results[&#x27;params&#x27;][candidate])) print(&quot;&quot;)def report_in_dataframe(results, top=10): &quot;&quot;&quot; 以DataFrame形式打印得分topk的搜索结果：排名-参数-得分 :param results: cv_results_ :param top: int :return: DataFrame &quot;&quot;&quot; result = pd.DataFrame(results[&#x27;params&#x27;]) result[&#x27;test_score&#x27;] = results[&#x27;mean_test_score&#x27;] result[&#x27;test_std&#x27;] = results[&#x27;std_test_score&#x27;] result[&#x27;rank&#x27;] = results[&#x27;rank_test_score&#x27;] result = result.set_index(&#x27;rank&#x27;).sort_index() print(result[:top]) return result[:top] 打印网格搜索结果： 12345678910111213141516171819202122232425262728293031323334353637383940GridSearchCV took 100 candidate parameter settings.model rank:1Mean validation score: 0.529 (std: 0.184)Parameters: &#123;&#x27;subsample&#x27;: 1.0, &#x27;reg_lambda&#x27;: 0.5, &#x27;reg_alpha&#x27;: 10.0, &#x27;num_leaves&#x27;: 2, &#x27;min_child_samples&#x27;: 9, &#x27;max_depth&#x27;: 10, &#x27;colsample_bytree&#x27;: 0.6&#125;model rank:2Mean validation score: 0.440 (std: 0.300)Parameters: &#123;&#x27;subsample&#x27;: 1.0, &#x27;reg_lambda&#x27;: 0.2, &#x27;reg_alpha&#x27;: 0.5, &#x27;num_leaves&#x27;: 3, &#x27;min_child_samples&#x27;: 9, &#x27;max_depth&#x27;: -1, &#x27;colsample_bytree&#x27;: 0.6&#125;model rank:3Mean validation score: 0.317 (std: 0.398)Parameters: &#123;&#x27;subsample&#x27;: 1.0, &#x27;reg_lambda&#x27;: 0, &#x27;reg_alpha&#x27;: 1.0, &#x27;num_leaves&#x27;: 4, &#x27;min_child_samples&#x27;: 2, &#x27;max_depth&#x27;: 10, &#x27;colsample_bytree&#x27;: 1.0&#125;model rank:4Mean validation score: 0.287 (std: 0.395)Parameters: &#123;&#x27;subsample&#x27;: 1.0, &#x27;reg_lambda&#x27;: 10.0, &#x27;reg_alpha&#x27;: 0.5, &#x27;num_leaves&#x27;: 3, &#x27;min_child_samples&#x27;: 2, &#x27;max_depth&#x27;: -1, &#x27;colsample_bytree&#x27;: 1.0&#125;model rank:5Mean validation score: 0.281 (std: 0.411)Parameters: &#123;&#x27;subsample&#x27;: 0.9, &#x27;reg_lambda&#x27;: 20.0, &#x27;reg_alpha&#x27;: 1.5, &#x27;num_leaves&#x27;: 6, &#x27;min_child_samples&#x27;: 2, &#x27;max_depth&#x27;: -1, &#x27;colsample_bytree&#x27;: 1.0&#125;model rank:5Mean validation score: 0.281 (std: 0.411)Parameters: &#123;&#x27;subsample&#x27;: 0.9, &#x27;reg_lambda&#x27;: 10.0, &#x27;reg_alpha&#x27;: 1.0, &#x27;num_leaves&#x27;: 7, &#x27;min_child_samples&#x27;: 2, &#x27;max_depth&#x27;: 8, &#x27;colsample_bytree&#x27;: 1.0&#125;model rank:7Mean validation score: 0.250 (std: 0.507)Parameters: &#123;&#x27;subsample&#x27;: 0.9, &#x27;reg_lambda&#x27;: 5.0, &#x27;reg_alpha&#x27;: 1.0, &#x27;num_leaves&#x27;: 6, &#x27;min_child_samples&#x27;: 3, &#x27;max_depth&#x27;: 10, &#x27;colsample_bytree&#x27;: 1.0&#125;model rank:8Mean validation score: 0.236 (std: 0.304)Parameters: &#123;&#x27;subsample&#x27;: 0.9, &#x27;reg_lambda&#x27;: 0.2, &#x27;reg_alpha&#x27;: 5.0, &#x27;num_leaves&#x27;: 7, &#x27;min_child_samples&#x27;: 2, &#x27;max_depth&#x27;: 8, &#x27;colsample_bytree&#x27;: 0.5&#125;model rank:9Mean validation score: 0.226 (std: 0.477)Parameters: &#123;&#x27;subsample&#x27;: 0.9, &#x27;reg_lambda&#x27;: 2.0, &#x27;reg_alpha&#x27;: 2.0, &#x27;num_leaves&#x27;: 5, &#x27;min_child_samples&#x27;: 3, &#x27;max_depth&#x27;: 8, &#x27;colsample_bytree&#x27;: 1.0&#125;model rank:10Mean validation score: 0.222 (std: 0.317)Parameters: &#123;&#x27;subsample&#x27;: 0.9, &#x27;reg_lambda&#x27;: 0, &#x27;reg_alpha&#x27;: 10.0, &#x27;num_leaves&#x27;: 2, &#x27;min_child_samples&#x27;: 4, &#x27;max_depth&#x27;: -1, &#x27;colsample_bytree&#x27;: 0.5&#125; 训练预测LightGBM训练预测过程与一般机器学习无异： 12345# lgb_reg是网格搜索refit后的最优模型y_pred = lgb_reg.predict(test[features])result = pd.DataFrame(&#123;&#x27;Id&#x27;: test.index, &#x27;Pred&#x27;: y_pred&#125;, columns=[&#x27;Id&#x27;, &#x27;Pred&#x27;])# 输出，注意列名和列序result.to_csv(path_test_out, header=True, index=False) LightGBM示例完整代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270#!/usr/bin/env python# -*- coding: utf-8 -*-# @File : process.py# @Author : Likew# @Date : 2018/5/18 16:14# @Desc : 模型选择-参数选择-预测输出# @Solution : 调参之法（早停止确定最优迭代次数+网格搜索确定最优超参数）import pandas as pdimport numpy as npimport lightgbm as lgbimport timefrom sklearn.model_selection import RandomizedSearchCVfrom sklearn.metrics import make_scorerdef gini(actual, pred): &quot;&quot;&quot; 计算真实序列按照预测序列升序排列时的相对基尼系数，如果真实序列均为0，定义gini为1是合理的 :param actual: 真实序列&gt;=0 :param pred: 预测序列 :return: 基尼系数 &quot;&quot;&quot; n = len(pred) triple = np.c_[actual, pred, range(n)].astype(float) triple = triple[np.lexsort((triple[:, 2], triple[:, 1]))] cum_sum = triple[:, 0].cumsum() if cum_sum[-1] == 0: return 1 else: x = cum_sum.sum() / cum_sum[-1] return (n + 1 - 2 * x) / ndef gini_normalized(y_true, y_pred): &quot;&quot;&quot; 自定义用于fit的eval_metric指标函数，此处为归一化的相对基尼系数 :param y_true: 真实序列ndarray :param y_pred: 预测序列ndarray :return: eval_name, eval_result, is_bigger_better &quot;&quot;&quot; gini_true = gini(y_true, y_true) gini_pred = gini(y_true, y_pred) # 如果真实值均匀分布，gini为0，定义此时的归一化为1是合理的 res = gini_pred / gini_true if gini_true else 1 return &#x27;gini_normalized&#x27;, res, Truedef gini_feval(y_pred, train_data): &quot;&quot;&quot; 用于cv的自定义的feval指标函数 :param y_pred: 预测值ndarray :param train_data: Dataset :return: (eval_name:string,eval_score,higher_is_better:bool) &quot;&quot;&quot; labels = train_data.get_label() return &#x27;gini&#x27;, gini_normalized(labels, y_pred), Truedef gini_grid(y_label, y_pred): &quot;&quot;&quot; 用于grid_search的scoring :param y_label: 真实序列ndarray :param y_pred: 预测序列ndarray :return: 归一化的基尼指数 &quot;&quot;&quot; return gini_normalized(y_label, y_pred)[1]def report(results, top=5): &quot;&quot;&quot; 打印网格搜索结果中排名前10的参数组合和评估性能 :param results: 网格搜索对象的结果属性cv_results_ :param top: 排名前几的 :return: None &quot;&quot;&quot; print(&quot;GridSearchCV took %d candidate parameter settings.&quot; % len(results[&#x27;params&#x27;])) for i in range(1, top + 1): candidates = np.flatnonzero(results[&#x27;rank_test_score&#x27;] == i) for candidate in candidates: print(&#x27;model rank:%s&#x27; % i) print(&quot;Mean validation score: &#123;0:.3f&#125; (std: &#123;1:.3f&#125;)&quot;.format( results[&#x27;mean_test_score&#x27;][candidate], results[&#x27;std_test_score&#x27;][candidate])) print(&quot;Parameters: &#123;0&#125;&quot;.format(results[&#x27;params&#x27;][candidate])) print(&quot;&quot;)def report_in_dataframe(results, top=10): &quot;&quot;&quot; 以DataFrame形式打印得分topk的搜索结果：排名-参数-得分 :param results: cv_results_ :param top: int :return: DataFrame &quot;&quot;&quot; result = pd.DataFrame(results[&#x27;params&#x27;]) result[&#x27;test_score&#x27;] = results[&#x27;mean_test_score&#x27;] result[&#x27;test_std&#x27;] = results[&#x27;std_test_score&#x27;] result[&#x27;rank&#x27;] = results[&#x27;rank_test_score&#x27;] result = result.set_index(&#x27;rank&#x27;).sort_index() print(result[:top]) return result[:top]def early_stopping(lgb_reg, X_train, y_train, X_valid, y_valid, online): &quot;&quot;&quot; 通过早停确定最优迭代次数 :param lgb_reg: 回归模型 :param X_train: 部分训练集特征空间 :param y_train: 部分训练集标签空间 :param X_valid: 验证集特征空间 :param y_valid: 验证集标签空间 :param online: 是否线上 :return: 设置了最优迭代次数的模型 &quot;&quot;&quot; lgb_reg.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], eval_names=[&#x27;gini&#x27;], eval_metric=gini_normalized, early_stopping_rounds=100, verbose=[1, 10][online]) best_iteration = lgb_reg.best_iteration_ lgb_reg.set_params(n_estimators=best_iteration) return lgb_regdef random_search(lgb_reg, params_dist, X_train, y_train, n_iter=10, nfold=3): &quot;&quot;&quot; 随机网格搜索确定最优参数，返回使用最优参数全量refit后的模型 :param lgb_reg: 模型 :param params_dist: 参数网格 :param X_train: 全部训练集的特征空间DataFrame :param y_train: 全部训练集的标记空间Series :param n_iter: 参数采样次数 :param nfold: 交叉验证折数 :return: refit后的模型 &quot;&quot;&quot; rs = RandomizedSearchCV(lgb_reg, params_dist, n_iter=n_iter, scoring=make_scorer(gini_grid), cv=nfold, refit=True, verbose=0, return_train_score=False) rs.fit(X_train.values, y_train.values) report_in_dataframe(rs.cv_results_) return rs.best_estimator_def process(train, test, start, online): &quot;&quot;&quot; 调参、训练、预测全过程 :param train: 训练集DataFrame :param test: 测试集DataFrame :param start: 开始执行时间 :param online: 是否线上 :return: 预测结果DataFrame &quot;&quot;&quot; # 特征和标签 features = [c for c in train.columns if c not in [&#x27;label&#x27;]] target = &#x27;label&#x27; # 划分训练集和验证集:3:1 ratio = int(train.shape[0] * 0.66) X_train = train[features].iloc[:ratio] y_train = train[target].iloc[:ratio] X_valid = train[features].iloc[ratio:] y_valid = train[target].iloc[ratio:] # 模型：leaf_wise生长策略，线上应该适当调大num_leaves，但是线下就跑不了 if online: lgb_reg = lgb.LGBMRegressor(num_leaves=176, max_depth=6, learning_rate=0.01, n_estimators=1000, max_bin=255, subsample_for_bin=200000, min_split_gain=0.0, min_child_weight=0.001, min_child_samples=26, subsample=0.9, subsample_freq=3, colsample_bytree=0.5, reg_alpha=0.5, reg_lambda=5.0, random_state=999, n_jobs=-1, silent=True, verbose=-1) else: lgb_reg = lgb.LGBMRegressor(num_leaves=5, max_depth=-1, learning_rate=0.02, n_estimators=1000, max_bin=255, subsample_for_bin=50000, min_split_gain=0.0, min_child_weight=0.001, min_child_samples=5, subsample=0.8, subsample_freq=3, colsample_bytree=0.8, reg_alpha=0.0, reg_lambda=1.0, random_state=999, n_jobs=-1, silent=True, verbose=[0, -1][online]) # 早停确定最优迭代次数 print(&quot;\n******************* Early Stopping ***********************&quot;) lgb_reg = early_stopping(lgb_reg, X_train, y_train, X_valid, y_valid, online) print(&#x27;Early Stopping Done! Time:%.3f s&#x27; % (time.time() - start)) print(&#x27;best n_estimators:%s&#x27; % lgb_reg.best_iteration_) # 网格搜索确定最优超参数 print(&quot;\n******************* Random Search ***********************&quot;) if online: params_dist = &#123;&#x27;num_leaves&#x27;: range(2, 300, 2), &#x27;max_depth&#x27;: [-1, 6, 8, 10, 15], &#x27;min_child_samples&#x27;: range(2, 100, 2), &#x27;subsample&#x27;: [i / 10. for i in range(3, 11)], &#x27;colsample_bytree&#x27;: [i / 10. for i in range(3, 11)], &#x27;subsample_freq&#x27;: range(1, 10), &#x27;reg_alpha&#x27;: [0, 0.5, 1., 1.5, 2., 5., 10.], &#x27;reg_lambda&#x27;: [0, 0.5, 1., 1.5, 2., 5., 10.]&#125; else: params_dist = &#123;&#x27;num_leaves&#x27;: range(2, 10), &#x27;max_depth&#x27;: [-1, 6, 8, 10], &#x27;min_child_samples&#x27;: range(2, 10), &#x27;subsample&#x27;: [i / 10. for i in range(5, 11)], &#x27;colsample_bytree&#x27;: [i / 10. for i in range(5, 11)], &#x27;reg_alpha&#x27;: [0, 0.2, 0.5, 1., 1.5, 2., 5., 10., 20.], &#x27;reg_lambda&#x27;: [0, 0.2, 0.5, 1., 1.5, 2., 5., 10., 20.]&#125; lgb_reg = random_search(lgb_reg, params_dist, train[features], train[target], n_iter=200) print(&#x27;RandomizedSearchCV Done! Time:%.3f s&#x27; % (time.time() - start)) print(&#x27;\nlast params:%s&#x27; % lgb_reg.get_params()) print(&#x27;feature importances:%s&#x27; % lgb_reg.feature_importances_) # 预测，注意DataFram中的列名和列序 y_pred = lgb_reg.predict(test[features]) result = pd.DataFrame(&#123;&#x27;Id&#x27;: test.index, &#x27;Pred&#x27;: y_pred&#125;, columns=[&#x27;Id&#x27;, &#x27;Pred&#x27;]) return resultif __name__ == &#x27;__main__&#x27;: pass 引用 LightGBM项目Github地址 LightGBM中文文档 LightGBM Python API]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习：基础算法（二）—— PLA]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%9F%BA%E7%A1%80%E7%AE%97%E6%B3%95%EF%BC%88%E4%BA%8C%EF%BC%89%E2%80%94%E2%80%94%20PLA%2F</url>
    <content type="text"><![CDATA[感知器算法（Perceptron Learning Algorithm,PLA）的最初概念可以追溯到Warren McCulloch和Walter Pitts在1943年的研究，他们将生物神经元类比成带有二值输出的简单逻辑门，输入信号在神经细胞体内聚集，当聚集的信号强度超过一定的阈值，就会产生一个输出信号，并被树突传递下去。后来，Frank Rosenblatt 1957年发表了一篇论文《The perceptron, a perceiving and recognizing automaton Project Para》，基于神经元定义了感知机算法的概念，并说明了，感知机算法的目的，就是针对多维特征的样本集合，学习一个权值向量 ，使得乘以输入特征向量之后，基于乘积可以判断一个神经元是否被激活。 感知机是一个基于判别函数的线性二分类模型，其输入为实例的特征向量，通过分离超平面将实例划分为正负类。 感知机是神经网络和支持向量机的基础。 模型感知机通过以下判别函数对输入实例进行分类： f(x) = sign(wx+b) w:权值向量 b:偏置 sign:阶跃函数 sign(x)=\left\{\begin{matrix} +1 &,x\geqslant 0 \\ -1 & ,x< 0 \end{matrix}\right.感知机的几何解释：$wx+b=0$对应于特征空间$\mathbb{R}^n$中的一个超平面S；$w$对应超平面的法向量；$b$对应超平面的截距。该超平面将特征空间划分为两个部分，与法向量同向的被分为正类，反向的分为负类，因此S又称为分离超平面。 线性可分：如果存在某个超平面可以将数据集中所有实例正确划分到两侧，则称该数据集是线性可分的。 策略函数间隔：点$(x_i,y_i)$到超平面的函数间隔定义为 \hat{\gamma}_i =y_i (wx_i+b)几何间隔：点$(x_i,y_i)$到超平面的距离 \gamma_i =\frac{y_i( wx_i+b)}{\left \| w \right \|} 几何间隔的正负：表示分类预测的正确性；为正说明点被正确分类，为负说明点被错误分类，为零说明点在超平面上； 几何间隔的绝对值：表示分类预测的确信度； 损失函数最直接的选择是误分类点数，但是这样的损失函数不是参数的可导函数，不易优化，因此提出了另外的代理损失函数：误分类点几何间隔之和最小化，这被称为感知机准则。 L(w,b) = -\frac{1}{\left \| w \right \|}\sum_{y_i(wx_i+b)\leqslant 0}y_i(wx_i+b)通过参数缩放可使得$\left | w \right |=1$: L(w,b) = -\sum_{x_i \in M}y_i(wx_i+b\\ M=\left \{ (x_i,y_i)|y_i(wx_i+b)]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习：基础算法（三）—— KNN]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%9F%BA%E7%A1%80%E7%AE%97%E6%B3%95%EF%BC%88%E4%B8%89%EF%BC%89%E2%80%94%E2%80%94%20KNN%2F</url>
    <content type="text"><![CDATA[K 邻近法（k-nearist neighbor,k-NN）是一种可用于分类和回归问题的非参数估计方法。对于分类问题，KNN 模型通过在训练集中寻找距离输入实例最近的前k个实例，将 k 个实例中数量最多的类别（多数表决）作为输出。knn 没有显式的训练过程，k 的选择、距离度量、分类决策是其三要素。 模型 y = arg\ \underset{c_j}{max}\ \sum_{x_i \in N_k}I(y_i = c_j),\ \ j=1,2,..kk值的选取会对knn算法的结果产生重大影响： k较小时，只有较小邻域内的点才会被考虑，训练误差小，但对邻域内的点很敏感，意味着模型复杂度高，容易过拟合，当$k=1$时，称为最邻近算法。通常采用交叉验证来确定最佳的k值。 策略多数表决策略等价于0-1损失下的经验风险最小化。假设涵盖$N_k(x)$的区域的类别被预测为$c_j$，则0-1平均损失可以写作： L(k,c_j) = \frac{1}{k} \sum_{x_i \in N_k(x)}I(y_i\neq c_j)=1-\frac{1}{k} \sum_{x_i \in N_k(x)}I(y_i= c_j)可见，平均损失最小等价于多数表决。 关于向量间距离的度量，常用的是$L_p$距离，即差向量$(x_i-x_j)$的p范数。 L_p(x_i,x_j)=\left ( \sum_{d=1}^{D}|x_i^d-x_j^d|^p \right )^{\frac{1}{p}} $p=1$:称曼哈顿距离 $p=2$:欧氏距离 $p=\infty$:坐标距离的最大值$max|x_i^d-x_j^d|$ 算法当特征空间维数很大及训练样本很多时，通过线性扫描来寻找k邻近十分耗时，kd树（这里的k指的是特征空间维数k dimention）是一种更高效的搜索k邻近的方法。kd树包含构建kd树、在kd树中搜索两个步骤，下面以最邻近为例来说明这两个过程。 构建kd树构建kd树的过程是一个递归地选择切分点对特征空间进行划分的过程。 输入：k维空间数据集T={$x_1,x_2,…,x_N$}，其中$x_i=(x_i^1,x_i^2,…,x_i^k)$输出：kd树（1）如果T为空则返回None（2）定义kd树初始深度h=0（2）创建新节点root，将所有实例按照第$l=h\%k+1$维坐标的中位数切分为左右两个区域$T_l,T_r$，将切分点实例保存在root节点中。然后令$h=h+1$，分别递归地以$T_l,T_r$为输入，构建root节点的左右子kd树$kd_l,kd_r$（3）返回root 构建kd树的平均时间复杂度为$O(nlgn)$。 搜索kd树输入：kd树，目标点x输出：x的最邻近（1）从根节点出发递归地自上向下找到x的叶节点：若x当前维的坐标小于切分点的坐标则移动到左节点，反之则移动到右节点，直至到达叶节点，将该叶节点作为当前最近点；（2）从叶节点出发递归地自下而上地回退至根节点： （a）如果当前节点距离目标点更近，则以该节点作为当前最近点； （b）检查当前节点的另一个子节点对应的区域是否有更近的点：检查以目标点为圆心，以目标点与当前最近点间的距离为半径的超球体是否与另一子节点区域相交，如果相交则递归的在另一个子树中进行最邻近搜索，如果不相交，继续向上回退；（3）当退回到根节点时，搜索结束，将最后的备选解作为x的最邻近； kd树搜索的平均时间复杂度为$O(lgn)$，当空间维数接近样本数时，它的效率会迅速下降，几乎接近线性扫描。 示例1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950import numpy as npimport matplotlib.pyplot as pltfrom matplotlib.colors import ListedColormapfrom sklearn import neighbors, datasetsn_neighbors = 15# import some data to play withiris = datasets.load_iris()# we only take the first two features. We could avoid this ugly# slicing by using a two-dim datasetX = iris.data[:, :2]y = iris.targeth = .02 # step size in the mesh# Create color mapscmap_light = ListedColormap([&#x27;#FFAAAA&#x27;, &#x27;#AAFFAA&#x27;, &#x27;#AAAAFF&#x27;])cmap_bold = ListedColormap([&#x27;#FF0000&#x27;, &#x27;#00FF00&#x27;, &#x27;#0000FF&#x27;])plt.figure(figsize=(16,8))i=1for n_neighbors in range(1,41,5): # we create an instance of Neighbours Classifier and fit the data. clf = neighbors.KNeighborsClassifier(n_neighbors, weights=weights) clf.fit(X, y) # Plot the decision boundary. For that, we will assign a color to each # point in the mesh [x_min, x_max]x[y_min, y_max]. x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1 y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1 xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h)) Z = clf.predict(np.c_[xx.ravel(), yy.ravel()]) plt.subplot(2,4,i) i += 1 # Put the result into a color plot Z = Z.reshape(xx.shape) plt.pcolormesh(xx, yy, Z, cmap=cmap_light) # Plot also the training points plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold, edgecolor=&#x27;k&#x27;, s=20) plt.xlim(xx.min(), xx.max()) plt.ylim(yy.min(), yy.max()) plt.title(&quot;k = %i&quot;% (n_neighbors)) plt.show() 评价 优点: 可分类可回归 实现简单 可解释性强 对异常值不敏感 缺点: 计算复杂度高 将所有特征看做同等重要，无法得到特征重要度]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习：基础算法（四）—— NB]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%9F%BA%E7%A1%80%E7%AE%97%E6%B3%95%EF%BC%88%E5%9B%9B%EF%BC%89%E2%80%94%E2%80%94%20NB%2F</url>
    <content type="text"><![CDATA[朴素贝叶斯（naive Bayes，NB）是基于贝叶斯定理与特征条件独立性假设的分类方法。其模型是通过学习先验概率和类条件概率来得到后验概率的生成式模型。其策略为后验概率最大，等价于0-1损失下的期望风险最小化策略。其算法为通过极大似然估计来求解各项概率。 模型贝叶斯通过训练数据学习联合概率分布$p(x,y)$，具体地： \begin{align*} &p(Y=c_k),\ \ k=1,2...,K\\ &p(X=x|Y=c_k)=p(X^1=x^1,X^2=x^2,...,X^D=x^D|Y=c_k),\ \ k=1,2,...,K \end{align*}后者有指数级的参数，其估计实际是不可行的，NB对类条件概率做出了条件独立性假设，将参数个数降到线性的，朴素因此得名： p(X=x|Y=c_k)=\prod_{d=1}^{D}p(X^d=x^d|Y=c_k)由贝叶斯定理和全概率公式我们可以得到贝叶斯公式： \begin{align*} p(Y=c_k|X=x) &= \frac{p(Y=c_k)p(X=x|Y=c_k)}{p(X=x)}\\ &=\frac{p(Y=c_k)\prod_{d=1}^{D}p(X^d=x^d|Y=c_k)}{\sum_{k=1}^{K}p(Y=c_k)\prod_{d=1}^{D}p(X^d=x^d|Y=c_k)} \end{align*}对不同的分类，分母都相同，因此贝叶斯模型可表示为： y = arg \ \underset{c_k}{max} \ p(y=c_k)\prod_{d=1}^{D}p(X^d=x^d|Y=c_k)策略后验概率最大（MAP）等价于0-1损失下的期望风险最小化的策略： \begin{align*} R_{exp}(f)&=E[L(y,f(x))]\\ &=E_X \sum_{k=1}^{K}L(c_k,f(X))p(c_k|X)\\ \end{align*}为了极小化期望风险，可以对每一个$X=x$进行极小化： \begin{align*} f(x)&=arg\ \underset{c_k}{min}\ \sum_{k=1}^{K}L(c_k,f(X=x))p(c_k|X=x)\\ &=arg\ \underset{c_k}{min}\ \sum_{k=1}^{K}(p(y\neq c_k|X=x))\\ &=arg\ \underset{c_k}{min}\ \sum_{k=1}^{K}(1-p(y=c_k|X=x))\\ &=arg\ \underset{c_k}{max}\ p(y=c_k|X=x) \end{align*}算法不同的朴素贝叶斯方法的区别仅在于对特征的类条件概率分布 $P(x_i \mid y)$ 的假设不同，它们都是用极大似然的方法来估计分布参数。 高斯朴素贝叶斯（Gaussian Naive Bayes）高斯朴素贝叶假设所有特征的类条件概率服从高斯分布（当特征为连续时一般选用高斯贝叶）； P(x^d \mid y) = \frac{1}{\sqrt{2\pi\sigma^2_y}} \exp\left(-\frac{(x^d - \mu_y)^2}{2\sigma^2_y}\right)采用极大似然法估计 $\sigma_y$ 和 $\mu_y$。 多项式朴素贝叶斯（Multinomial Naive Bayes）：多项式朴素贝叶斯假设所有特征的类条件概率服从多项式分布，在类别为$ck$条件下，特征$X^d$的概率分布可以用一个参数向量$\theta^d_k = (\theta^d{k1},\ldots,\theta^d_{kn}) $来表示(当特征为多类别离散变量时选用多项式NB)； 可以用极大似然法来估计相应的概率： \begin{align*} &p(Y=c_k)=\frac{N_k}{N}\\ &\theta^d_{kx^d} = p(X^d=x^d|Y=c_k)=\frac{N_{k,x^d}}{N_k} \end{align*} $N_k$：所有样本中类别为$c_k$的个数； $N_{k,x^d}$：类别为$c_k$且第d维为$x^d$的样本个数； 平滑版极大似然：极大似然估计可能会出现估计的概率为0的情况，影响侯艳艳概率的计算，使分类出现偏差，可以用平滑方法解决这一问题，具体的就是在计算每个概率时，我们在分母的范围内为分子的每种可能结果加$\lambda$： \begin{align*} &p(Y=c_k)=\frac{N_k+\lambda}{N+K\lambda}\\ &p(X^d=x^d|Y=c_k)=\frac{N_{k,x^d}+\lambda}{N_k + s_d\lambda} \end{align*} K：y类别个数； $s_d$：数据集中当类别为$c_k$时，第i维坐标可能取值个数； $\lambda$: 取1时叫拉普拉斯平滑 伯努利朴素贝叶斯（ Bernoulli Naive Bayes）伯努利朴素贝叶斯假设特征的类条件分布为伯努利分布（当特征为二值特征时使用）： \begin{align*} &p(X^d=x^d \mid Y=c_k) = p \\ &p(X^d\neq x^d \mid Y=c_k) = 1-p \end{align*}通过极大似然估计p。 实例比较朴素贝叶斯与SVM的学习曲线： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960import numpy as npimport matplotlib.pyplot as pltfrom sklearn import cross_validationfrom sklearn.naive_bayes import GaussianNBfrom sklearn.svm import SVCfrom sklearn.datasets import load_digitsfrom sklearn.learning_curve import learning_curvedef plot_learning_curve(estimator, title, X, y, ylim=None, cv=None, n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)): plt.figure() plt.title(title) if ylim is not None: plt.ylim(*ylim) plt.xlabel(&quot;Training examples&quot;) plt.ylabel(&quot;Score&quot;) train_sizes, train_scores, test_scores = learning_curve( estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes) train_scores_mean = np.mean(train_scores, axis=1) train_scores_std = np.std(train_scores, axis=1) test_scores_mean = np.mean(test_scores, axis=1) test_scores_std = np.std(test_scores, axis=1) plt.grid() plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color=&quot;r&quot;) plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color=&quot;g&quot;) plt.plot(train_sizes, train_scores_mean, &#x27;o-&#x27;, color=&quot;r&quot;, label=&quot;Training score&quot;) plt.plot(train_sizes, test_scores_mean, &#x27;o-&#x27;, color=&quot;g&quot;, label=&quot;Cross-validation score&quot;) plt.legend(loc=&quot;best&quot;) return pltdigits = load_digits()X, y = digits.data, digits.targettitle = &quot;Learning Curves (Naive Bayes)&quot;# Cross validation with 100 iterations to get smoother mean test and train# score curves, each time with 20% data randomly selected as a validation set.cv = cross_validation.ShuffleSplit(digits.data.shape[0], n_iter=100, test_size=0.2, random_state=0)estimator = GaussianNB()plot_learning_curve(estimator, title, X, y, ylim=(0.7, 1.01), cv=cv, n_jobs=4)title = &quot;Learning Curves (SVM, RBF kernel, $\gamma=0.001$)&quot;# SVC is more expensive so we do a lower number of CV iterations:cv = cross_validation.ShuffleSplit(digits.data.shape[0], n_iter=10, test_size=0.2, random_state=0)estimator = SVC(gamma=0.001)plot_learning_curve(estimator, title, X, y, (0.7, 1.01), cv=cv, n_jobs=4)plt.show() 对于SVM（复杂模型）：当数据量小的时候，训练误差为0，但是测试误差较大；随着训练数据增加，训练误差缓慢增加，验证误差逐渐下降；当数据量足够大的时候，训练误差和测试误差维持在一个较低水平； 对于朴素贝叶斯（简单模型）：当数据量小的时候，训练误差相对复杂模型更大，验证误差也比较大；随着数据量增加，训练误差迅速增加，验证误差不断下降；当数据量足够大的时候，训练误差和测试误差维持在一个较高水平； 评价 优点： 实现简单 对缺失数据不敏感 适合增量学习 缺点： 朴素贝叶斯建立在独立性假设的基础上，当特征间相关性较大时，模型效果不好； 不好的先验假设会影响模型性能 对输入数据的表达形式比较敏感]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习：基础算法（五）—— DT]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%9F%BA%E7%A1%80%E7%AE%97%E6%B3%95%EF%BC%88%E4%BA%94%EF%BC%89%E2%80%94%E2%80%94%20DT%2F</url>
    <content type="text"><![CDATA[决策树（decision tree）是一种基本的分类和回归方法。决策树模型是一个树状结构（二叉树或非二叉树），每个内部节点表示在某个特征上的测试，根据测试结果将样本划分到不同子节点，样本最终会落在某个叶子节点中，每个叶子节点的输出值代表了该样本的类别或值。一般通过模型叶子节点中样本集的加权经验熵（分类）或平方误差（回归）作为损失函数。其算法主要包括特征选择、决策树生成和决策树剪枝三个环节。 常见的决策树算法有 ID3、C4.5 和 CART，他们的模型一致，只是学习算法的三个过程有所不同： 类型 特征选择 决策树生成 决策树剪枝 特征类型 输出值 是否二叉树 问题类型 ID3 每次选择能够带来最大信息增益的特征作为最优切分特征 按切分特征的不同取值划分出多颗子树 按照叶节点上的经验熵在剪枝前后的变化决定是否剪枝 离散 类别众数 否 分类 C4.5 每次选择能够带来最大信息增益比的特征作为最优切分特征 按切分特征的不同取值划分出多颗子树 按照叶节点上的经验熵在剪枝前后的变化决定是否剪枝 离散 类别众数 否 分类 分类CART 每次选择条件基尼指数最小的特征和切分点为最优切分特征和最优切分点 按切分特征和切分点划分为两颗子树 通过交叉验证在子树序列中选最优子树 离散/连续 类别众数 是 分类 回归CART 每次选择平方误差最小的特征和切分点为最优切分特征和最优切分点 按切分特征和切分点划分为两颗子树 通过交叉验证在子树序列中选最优子树 离散/连续 子集均值 是 回归 不同种类的决策树均可形式化表示为以下统一模型： T(x,c_j,R_j) = \sum_{i=1}^{J}c_jI(x \in R_j) $J$：决策树叶节点数； $R_j$：第 j 个叶节点对应的特征空间区域； $c_j$：第 j 个叶节点的输出值； 模型的不同视角：叶节点（树） = if-then规则（逻辑） = 方块区域（空间） 决策树是if-then规则的集合：每条路径或叶节点都对应了特征集上的一组if-then规则，这些规则互斥且完备； 决策树是特征空间不同区域上的条件概率分布：每条路径相当于对特征空间进行区域划分（区域边界垂直于特征轴），每个叶节点对应了特征空间中一个互斥且完备的方块区域，叶节点中的值代表了该区域上的类别或值； 决策树学习本质上是从训练集中归纳出一组分类规则，与训练数据集不相矛盾的决策树可能有多个也可能一个也没有，我们需要的是一个与训练数据矛盾较小同时具有很好泛化能力的决策树。 从所有可能的决策树中选择最优决策树是NP完全问题，通常采用启发式的贪心算法，递归地选择当前最优的特征，根据该特征对训练集进行划分，直至子集能够被基本正确划分，则构建叶子节点，将子集中的占多数的类别或者子集均值作为叶子节点的输出，这个过程叫做决策树的生成。 决策树的生成过程每次只考虑局部最优，最终会自上而下产生一棵完全生长的树，很可能导致过拟合，我们需要对已生成的树自下而上进行剪枝，决策树剪枝需要考虑全局最优，递归地去掉过于细分的叶节点，使其回退到其父节点，直至损失函数（带正则项）不再下降。 ID3特征选择——最大信息增益信息增益（information gain）又称互信息（mutual imformation）表示在得知特征X的信息后而使类Y的不确定性减少的程度，记做 $g(D|A)$。 \left\{\begin{matrix} \begin{align*} g(D,A)&=H(D)-H(D|A)\\ H(D)=&-\sum_{i=1}^{N}p(y=c_k) \cdot lgp(y=c_k)\\ H(D|A)&=\sum_{g=1}^{G}H(D|A=A_g) \cdot p(A=A_g)\\ &=-\sum_{g=1}^{G}\sum_{k=1}^{K}p(y=c_k|A=A_g) \cdot lgp(y=c_k|A=A_g) \cdot p(A=A_g)\\ &=-\sum_{g=1}^{G}\sum_{k=1}^{K}p(y=c_k,A=A_g) \cdot lgp(y=c_k|A=A_g) \end{align*} \end{matrix}\right.其中y有K个类别 ${c_1,c_2,…,c_K}$，特征A有G个取值 ${A_1,A_2,…,A_G}$ 我们可以通过极大似然估计来计算各个概率： \left\{\begin{matrix} \begin{align*} &p(y=c_k)=\frac{N_{y=c_k}}{N}\\ &p(y=c_k|A=A_g)=\frac{N_{y=c_k,A=A_g}}{N_{A=A_g}}\\ &p(y=c_k,A=A_g)=\frac{N_{y=c_k,A=A_g}}{N} \end{align*} \end{matrix}\right.选择信息增益最大的特征作为最优切分特征： A^* = \underset{A}{max} \ g(D,A)决策树的生成输入：训练数据集$D$，特征集$A$，阈值$\epsilon $ 输出：决策树 （1）几种边界情况： 如果D为空，则T为单节点树，将父节点中实例数最大的类作为该节点的输出，返回T； 如果D中所有实例都属于同一类，则T为单节点树，将该类作为节点的输出，返回T； 如果A为空，则T为单节点树，将D中实例最数最大的类作为该节点输出，返回T； （2）否则，计算A中各特征对D的信息增益，选择信息增益最大的特征$A_g$，如果其信息增益小于阈值$\epsilon $，则T为单节点树，将D中实例最数最大的类作为该节点输出，返回T； （3）否则，构建节点，并按照$A_g$的每一种可能取值将D划分为若干子集$D_i$，以$D_i$为训练集，以$A-{A_g}$为特征集，递归地调用以上步骤构建子树$T_i$，由节点和子树构成数T，返回T； 决策树的剪枝决策树的剪枝往往通过极小化整体的损失函数（等价于带正则的极大似然估计）来实现，损失函数可以通过叶节点中的加权熵来刻画： \begin{align*} L_\lambda(T)&=\sum_{j=1}^{J}N_j H(T_j)+\lambda J\\ &=-\sum_{j=1}^{J}N_j\sum_{k=1}^{K}p(c_k|j)lgp(c_k|j)+\lambda J\\ &=-\sum_{j=1}^{J}N_j\sum_{k=1}^{K}\frac{N_{j,k}}{N_j}lg\frac{N_{j,k}}{N_j}+\lambda J\\ &=-\sum_{j=1}^{J}\sum_{k=1}^{K}N_{j,k}lg\frac{N_{j,k}}{N_j}+\lambda J \end{align*} $N_j$：第j个叶节点中样本数 $N_{j,k}$：第j个叶节点中$c_k$类的样本数 $H(T_j)$：第j个叶节点中的熵 $J$：叶子节点数，可以用来衡量模型复杂度 $\lambda$：正则化项系数 决策树剪枝算法：递归地从叶节点向上回缩，如果回缩后的损失函数下降则进行剪枝，最终得到损失函数最小的子树$T_\lambda$。 C 4.5特征选择——最大信息增益比以信息增益作为选取最优特征的标准，倾向于选择取值较多的特征，使用信息增益比(information gain ratio)可以对这一问题进行校正： \begin{align*} g_R(D,A)&=\frac{g(D,A)}{H(A)}\\ &=\frac{g(D,A)}{-\sum_{g=1}^{G}p(A=A_g)lgp(A=A_g)}\\ &=\frac{g(D,A)}{-\sum_{g=1}^{G} \frac{N_g}{N}lg\frac{N_g}{N}}\\ \end{align*} $g(D,A)$：特征$A$所带来的信息增益 $H(A)$：特征$A$本身的熵（注意不是$H(D)$） 选择信息增益比最大的特征作为最优切分特征： A^* = \underset{A}{max} \ g_R(D,A)C4.5与ID3的树生成和剪枝过程一样，只需要将信息增益换做信息增益比，不再赘述。 CART分类回归树（classification and regression tree,CART）模型由Breiman等人在1984年提出，是应用最广泛的决策树学习方法，与ID3和C4.5相比有以下几点不同: 树的类型：CART决策树是二叉树，另外两种可以是非二叉树； 适用范围: CART既可用于分类也可用于回归，而另外两种只用于分类； 特征类型：CART特征可以是离散也可以是连续的，另外两种只能处理离散特征，对于连续特征需要先进行离散化； 生成过程：CART选择具有最小基尼指数或平方误差的特征和切分点作为最优切分特征和切分点，从节点划分出两颗子树；另外两种只需要选择具有最大信息增益(比)的特征作为切分特征，从节点划分出多颗子树； 剪枝过程：CART通过不断回缩能带来最小损失下降（不带正则）的分支来得到一组子树序列$T0,T_1,…,T_n$，然后通过交叉验证选出其中最有子树$T\lambda$ 特征选择CART 分类树——最小条件基尼指数基尼指数：与熵类似，基尼指数也可以作为随机变量不纯度的度量，可定义为 Gini(D)=\sum_{k=1}^{K}p(y=c_k)(1-p(y=c_k))条件基尼指数：在特征$A$的条件下，集合$D$的基尼指数定义为 \begin{align*} Gini(D|A)&=\sum_{g=1}^{G}p(A=A_g)Gini(D|A=A_g)\\ &=\sum_{g=1}^{G}p(A=A_g)\sum_{k=1}^{K}p(y=c_k|A=A_g)(1-p(y=c_k|A=A_g))\\ &=\sum_{g=1}^{G}\sum_{k=1}^{K}p(y=c_k,A=A_g)(1-p(y=c_k|A=A_g)) \end{align*}选择条件基尼指数最小的特征和切分点作为最优切分特征和切分点：CART分类树在做节点分裂时，会在所有可能的特征A以及它们所有可能的切分点$A_g$中选择条件基尼系数最小的特征和切分点，按照每个样本在该特征上的值是否等于$A_g$来决定将其划分至左子树还是右子树。我们可以通过以下方式来计算数据集$D$关于切分特征$A$和切分点$A_g$的条件基尼指数 \begin{align*} A^*,A_g^* =& \underset{A,A_g}{min} \ Gini(D|A,A_g)\\ =&p(A=A_g)Gini(D|A=A_g)+p(A\neq A_g)Gini(D|A\neq A_g)\\ =&p(A=A_g)\sum_{k=1}^{K}p(y=c_k|A=A_g)(1-p(y=c_k|A=A_g))+\\ &p(A\neq A_g)\sum_{k=1}^{K}p(y=c_k|A\neq A_g)(1-p(y=c_k|A\neq A_g)) \end{align*}其中各概率可以通过极大似然估计来求： \left\{\begin{matrix} p(A=A_g)=\frac{N_g}{N}\\ p(A\neq A_g)=\frac{N_{\bar{g}} }{N}\\ p(y=c_k|A=A_g)=\frac{N_{g,k}}{N_g}\\ p(y=c_k|A\neq A_g)=\frac{N_{\bar{g},k}}{N_{\bar{g}}} \end{matrix}\right.基尼系数-熵之半-分类误差率的关系: 回归CART——平方误差CART回归树一般将叶节点中所有样本的均值作为输出，在做节点分裂时，会在所有可能的特征A以及它们所有可能的切分点$A_g$中选择平方误差最小的特征和切分点，按照每个样本在该特征上的值与切分点的值比较，小于等于切分点的样本被划分到左子树，大于切分点的样本被分到右子树。我们可以通过以下方式来计算数据集$D$关于切分特征$A$和切分点$A_g$的平方误差 A^*,A_g^* = \underset{A,A_g}{min} [\sum_{x_i \in R_1}(y_i-c_1)^2+\sum_{x_i \in R_2}(y_i-c_2)^2]其中: \left\{\begin{matrix} R_1(A,A_g)=\left \{ x|x^A\leqslant A_g) \right \}\\ R_2(A,A_g)=\left \{ x|x^A> A_g) \right \}\\ c_1 = average(y_i | x_i \in R_1)\\ c_2= average(y_i | x_i \in R_2) \end{matrix}\right.决策树的生成CART决策树生成过程与ID3一致，只需将特征选择部分替换即可。 决策树的剪枝回看ID3决策树的剪枝过程,是在确定了正则化系数$\lambda$后，递归地收缩叶子节点，如果收缩后损失函数下降则进行剪枝的过程。而CART决策树则是通过交叉验证的方式从子树序列中选择最优子树，相当于确定了一个最优的正则项$\lambda $来进行剪枝，具体来说，可分为两步： 得到子树序列{$T_0,T_1,…,T_n$}：计算当前决策树中所有内部节点的损失函数下降(不带正则)，选择其中损失下降最小的内部节点，进行收缩剪枝，得到子树$T_1$，递归地在子树$T_1$中执行此操作最终可获取一个子树序列{$T_0,T_1,…,T_n$}； 交叉验证选择最优子树$T_\lambda$：利用独立的验证数据集，测试子树序列中各棵子树的平方误差或基尼指数，平方误差或基尼指数最小的子树被认为是最优的决策树； 在剪枝过程中，子树的损失函数可表示为： L_\lambda(T)=L(T)+\lambda JBreiman等人证明：通过以上递归剪枝的方法所得到的子树序列{$T0,T_1,…,T_n$}对应于递增的正则化系数$\lambda_0&lt;\lambda_1&lt;…&lt;\lambda_n&lt;+\infty$所产生的一系列区间$[\lambda_i,\lambda{i+1}),i=0,1,…,n$。 具体地，从整棵树$T_0$开始剪枝，对$T_0$的任意内部节点t，以t为单节点树的损失函数为： L_\lambda(t)=L(t)+\lambda以t为根节点的子树$T_t$的损失为： L_\lambda(T_t)=L(T_t)+\lambda J_t假设通过剪枝损失函数下降了： L_\lambda(T_t)-L_\lambda(t)=L(T_t)-L(t)+\lambda (J_t-1)>0即: \frac{L(t)-L(T_t)}{J_t-1}]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习：基础算法（六）—— LR]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%9F%BA%E7%A1%80%E7%AE%97%E6%B3%95%EF%BC%88%E5%85%AD%EF%BC%89%E2%80%94%E2%80%94%20LR%2F</url>
    <content type="text"><![CDATA[逻辑回归（Logistic Regression, LR）是统计学习中经典的分类方法，被广泛应用于计算广告学、社会学、生物统计学、临床、数量心理学、计量经济学、市场营销等众多领域。逻辑回归模型是在线性模型的基础上加上sigmoid激活函数的广义线性模型，它以对数似然（等价于交叉熵）为损失函数，通过梯度下降及拟牛顿法进行求解。 模型如果将正例标签取做1，负例标签取做0，则二项逻辑回归模型的后验概率分布可表示为： \begin{align*} p(y=1|x)&=\sigma (z)\\ p(y=0|x)&=\sigma (-z) \end{align*}$p(y \mid x)$ 服从伯努利分布，可统一表示为: p(Y=y|x)=\sigma (z)^y \sigma (-z)^{1-y} $z=wx$：x的仿射变换； $\sigma(z)$：sigmoid函数$\sigma(z)=\frac{1}{1+e^{-z}}$ 关于sigmoid函数，应该记住以下常用性质(加乘除)： \begin{align*} &\sigma (z)+\sigma (-z)=1\\ &\frac{\sigma(z)}{\sigma(-z)}=e^{z}\\ &\sigma(z)\cdot \sigma(-z)=\sigma ^‘(z) \end{align*}策略学习逻辑回归模型时，可以应用极大似然策略（最小化p(y)与p(y|x)的交叉熵）来估计模型参数： 极大似然函数： MLE(w,b)=\prod_{i=1}^{N}p(y_i|x_i)=\prod_{i=1}^{N} \sigma (z_i)^{y_i} \sigma (-z_i)^{1-y_i} 负对数似然损失函数： \begin{align*} L(w,b)&=-lg\ MLE(w,b)\\ &=-\sum_{i=1}^{N}lgp(y_i|x_i)\\ &=-\sum_{i=1}^{N}y_ilg\sigma (z_i)+(1-y_i)lg\sigma (-z_i)\\ &=-\sum_{i=1}^{N}y_ilg\frac{\sigma (z_i)}{\sigma (-z_i)}+lg\sigma (-z_i)\\ &=-\sum_{i=1}^{N}[y_iz_i+lg\sigma (-z_i)]\\ \end{align*}算法逻辑回归通常采用梯度下降和拟牛顿法来求解，以下以极速梯度下降法为例： （1）令 $g(y_i,z_i,w)=y_iz_i+lg\sigma (-z_i)$ （2）求损失函数偏导： \begin{align*} \frac{\partial g_i}{\partial w^j}&=\frac{\partial g_i}{\partial z_i}\frac{\partial z_i}{\partial w^j}\\ &=(y_i-\frac{\sigma (-z_i)\sigma (z_i)}{\sigma (-z_i)})x_i^j\\ &=(y_i-\sigma (z_i))x_i^j\\ \end{align*}（3）极速下降： w\leftarrow w+\eta \sum_{i=1}^{N}(y_i-\sigma (z_i))x_i0-1 表示与 ±1 表示对比有时将正例标签记做+1，将负例标签记做-1，推导过程基本一致，只是细节有些不同： 逻辑回归应用于多分类问题逻辑回归只能用于二分类问题，要想实现k分类必须改进逻辑回归，通常有两种方式： 建立k个二分类逻辑回归：对每一分类建立一个二分类器，该类别被标记为1，其他类别被标记为0，这样我们就可以得到k个普通的逻辑回归分类器，最后选择概率最大的类别作为预测类别； softmax回归：可以看做是逻辑回归在多分类问题上的推广，其概率模型为 p(y=c_k|x)=\sigma_k(w_kx+b_k),\ k=1,2,..,K其中,Softmax函数，或称归一化指数函数，是sigmoid函数的一种推广。它能将一个含任意实数的K维的向量 $z$ 的“压缩”到另一个K维实向量 $\sigma (\mathbf {z} )$中，使得每一个元素的范围都在 $ (0,1)$之间，并且所有元素的和为1。该函数的形式通常按下面的式子给出： \begin{bmatrix} y_1\\ y_2\\ \vdots\\ y_K \end{bmatrix}=softmax(\begin{bmatrix} z_1\\ z_2\\ \vdots\\ z_K \end{bmatrix})对于每一个分量： \begin{align*} y_k=\sigma_k(z)&=\frac{e^{z_k}}{\sum_{j=1}^{K}e^{z_j}}\\ &=\frac{1}{1+\sum_{j\neq k}e^{z_j-z_k}}, \ \ k = 1,2...,K \end{align*}事实上我们可以将其中一个类别的非归一化概率定为1来减少参数的数量，此时softmax回归模型可以简化为: \begin{align*} p(y=K|x)&=\frac{1}{1+\sum_{k=1}^{K-1}e^{w_kx+b_k}}\\ p(y=k|x)&=\frac{e^{w_kx+b_k}}{1+\sum_{k=1}^{K-1}e^{w_kx+b_k}},\ \ k=1,2,...,K-1 \end{align*}为什么后验概率可以表示为仿射变换的 sigmoid 函数对二分类问题，类别$c_1$的后验概率可以通过贝叶斯公式表示为： \begin{align*} p(c_1|x)&=\frac{p(c_1)p(x|c_1)}{p(c_1)p(x|c_1)+p(c_2)p(x|c_2)}\\ &=\frac{1}{1+\frac{p(c_2)p(x|c_2)}{p(c_1)p(x|c_1)}}\\ &=\frac{1}{1+e^{-z(x)}} \end{align*}其中： z(x)=ln\frac{p(c_1)p(x|c_1)}{p(c_2)p(x|c_2)}接下来，只需要证明$z(x)$是$x$的线性函数即可。我们假设所有类条件概率为协方差相同的高斯函数： p(x|c_k)=\frac{1}{(2\pi)^{\frac{D}{2}}}\frac{1}{\left | \Sigma \right |^{\frac{1}{2}}}exp(-\frac{1}{2}(x-\mu _k)^T\Sigma\ ^{-1}(x-\mu _k))代入 $z(x)$ 化简: \begin{align*} z(x)&=ln\frac{p(c_1)p(x|c_1)}{p(c_2)p(x|c_2)}\\ &=ln\left [\frac{p(c_1)}{p(c_2)}\cdot exp(\frac{1}{2}(x-\mu_2)^T\Sigma\ ^{-1}(x-\mu_2)) -\frac{1}{2} (x-\mu_1)^T\Sigma^{-1}(x-\mu_1)) \right ]\\ &=ln\frac{p(c_1)}{p(c_2)} + \frac{1}{2} [ x^T \Sigma\ ^{-1}x -x^T\Sigma^{-1}\mu_2-\mu_2^T\Sigma^{-1}x+\mu_2^T\Sigma^{-1}\mu_2- x^T \Sigma\ ^{-1}x \\ &\ \ \ +x^T\Sigma^{-1}\mu_1+\mu_1^T\Sigma^{-1}x-\mu_1^T\Sigma^{-1}\mu_1]\\ &=ln\frac{p(c_1)}{p(c_2)} +\frac{1}{2}\left [ 2x^T\Sigma^{-1}(\mu_1-\mu_2) +\mu_2^T\Sigma^{-1}\mu_2-\mu_1^T\Sigma^{-1}\mu_1 \right ]\\ &=(\Sigma^{-1}(\mu_1-\mu_2) )^Tx+\frac{1}{2}\mu_2^T\Sigma^{-1}\mu_2-\frac{1}{2}\mu_1^T\Sigma^{-1}\mu_1+ln\frac{p(c_1)}{p(c_2)} \end{align*}得证，令： \begin{align*} w&=\Sigma^{-1}(\mu_1-\mu_2)\\ b&=\frac{1}{2}\mu_2^T\Sigma^{-1}\mu_2-\frac{1}{2}\mu_1^T\Sigma^{-1}\mu_1+ln\frac{p(c_1)}{p(c_2)} \end{align*}则: p(c_1|x)=\frac{1}{1+e^{-(wx+b)}}对softmax回归也有类似推导，可用一维高斯分布做启发式推导。 实战以下使用sklearn.linear_model.LogisticRegression()来解决一个三分类问题。 官方API： 1234567891011121314LogisticRegression(penalty=&#x27;l2&#x27;, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver=&#x27;liblinear&#x27;, max_iter=100, multi_class=&#x27;ovr&#x27;, verbose=0, warm_start=False, n_jobs=1) 参数 名称 说明 penalty 正则化选择参数 ‘l1’or ‘l2’, default: ‘l2’ dual 对偶 bool, default: False，对偶或者原始方法。Dual只适用于正则化相为l2 liblinear的情况，通常样本数大于特征数的情况下，默认为False C 正则化系数λ的倒数 默认为1 fit_intercept 是否存在截距 bool, default: True solver 优化算法 {牛顿法‘newton-cg’, 拟牛顿法‘lbfgs’, 梯度下降‘liblinear’, 随机梯度下降‘sag’}, default: ‘liblinear’ multi_class 分类方式 处理多元分类的方式，str, {‘ovr’, ‘multinomial’}, default:‘ovr’，ovr(one vs rest)会训练多个二分类的逻辑回归，multinomial会使用softmax回归 class_weight 类别权重 用于解决样本不均衡问题，dictor ‘balanced’, default: None sample_weight 采样权重 用于解决样本不均衡问题的另一种方式，如果上面两种方法都用到了，那么样本的真正权重是class_weight*sample_weight. max_iter 算法收敛最大迭代次数 仅在正则化优化算法为newton-cg, sag and lbfgs 才有用 tol 迭代终止判据的误差范围 float, default: 1e-4 123456789101112131415161718192021222324252627282930313233343536373839import numpy as npimport matplotlib.pyplot as pltfrom sklearn import linear_model, datasets# import some data to play withiris = datasets.load_iris()X = iris.data[:, :2] # we only take the first two features.Y = iris.targeth = .02 # step size in the meshlogreg = linear_model.LogisticRegression(C=1e5)# we create an instance of Neighbours Classifier and fit the data.logreg.fit(X, Y)# Plot the decision boundary. For that, we will assign a color to each# point in the mesh [x_min, x_max]x[y_min, y_max].x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))Z = logreg.predict(np.c_[xx.ravel(), yy.ravel()])# Put the result into a color plotZ = Z.reshape(xx.shape)plt.figure(1, figsize=(10, 6))plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)# Plot also the training pointsplt.scatter(X[:, 0], X[:, 1], c=Y, edgecolors=&#x27;k&#x27;, cmap=plt.cm.Paired)plt.xlabel(&#x27;Sepal length&#x27;)plt.ylabel(&#x27;Sepal width&#x27;)plt.xlim(xx.min(), xx.max())plt.ylim(yy.min(), yy.max())plt.xticks(())plt.yticks(())plt.show() 评价 优点: 实现简单 速度快，存储资源低 可解释性强 输出后验概率，方便后续分析 缺点: 容易欠拟合，一般准确率较低 对数据质量要求高，需要手动处理缺失值、非线性特征]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习：基础算法（七）—— SVM]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%9F%BA%E7%A1%80%E7%AE%97%E6%B3%95%EF%BC%88%E4%B8%83%EF%BC%89%E2%80%94%E2%80%94%20SVM%2F</url>
    <content type="text"><![CDATA[支持向量机（support vector machines,SVM）是一种二分类模型，其模型为分离超平面 $wx+b=0$ 及决策函数 $f(x)=sign(wx+b)$，其学习策略为间隔最大化，其学习算法为凸二次规划。 在二分类问题上，SVM 包含以下几种由简到繁的模型： 线性可分 SVM：基于硬间隔最大化（hard margin maximization） 近似线性可分 SVM：基于软间隔最大化（soft margin maximization） 非线性可分 SVM：基于核方法（kernel trick）和软间隔最大化 SVM也可用于回归问题和异常值检测问题，本文只涉及SVM二分类模型。 线性可分SVM——硬间隔最大化线性可分：如果存在某个超平面可以将数据集中所有实例正确划分到两侧，则称该数据集是线性可分的。 模型与感知机模型类似： f(x) = sign(wx+b) w:权值向量 b:偏置 sign:阶跃函数 sign(x)=\left\{\begin{matrix} +1 &,x\geqslant 0 \\ -1 & ,x< 0 \end{matrix}\right.策略函数间隔：点$(x_i,y_i)$到超平面的函数间隔定义为 \hat{\gamma}_i =y_i (wx_i+b)几何间隔：点$(x_i,y_i)$到超平面的距离 \gamma_i =\frac{y_i( wx_i+b)}{\left \| w \right \|} 几何间隔的正负：表示分类预测的正确性；为正说明点被正确分类，为负说明点被错误分类，为零说明点在超平面上； 几何间隔的绝对值：表示分类预测的确信度； 与感知机模型不同，SVM不仅仅满足于将数据集正确划分（这样的超平面有无数个），还要求有尽可能大的划分确信度，即间隔最大化策略：找到使得最小几何间隔（在所有点中）最大的超平面。 （1）该问题可以形式化表示为以下约束优化问题： \left\{\begin{matrix} \begin{align*} &\underset{w,b}{max}\ \frac{\hat{\gamma }}{\left \| w \right \|}\\ &y_i(wx_i+b)\geqslant \hat{\gamma } \end{align*} \end{matrix}\right.（2）对参数进行放缩$w=\frac{w}{\hat{\gamma }},b=\frac{b}{\hat{\gamma }}$，原优化问题可简化为： \left\{\begin{matrix} \begin{align*} &\underset{w,b}{min}\ \frac{1}{2}\left \| w \right \|^2\\ &1-y_i(wx_i+b)\leqslant 0,i=1,2,...,n \end{align*} \end{matrix}\right.（3）由拉格朗日乘数法可将原问题转化为拉格朗日极小极大无约束优化问题: \left\{\begin{matrix} \begin{align*} &\underset{w,b}{min}\ \underset{\alpha_i\geqslant 0}{max}\ L(w,b,\alpha)\\ &L(w,b,\alpha)=\frac{1}{2}\left \| w \right \|^2+\sum_{i=1}^{n}\alpha_i(1-y_i(wx_i+b)) \end{align*} \end{matrix}\right. $L(w,b,\alpha)$：拉格朗日函数 $\alpha_i$：拉格朗日乘子 （4）进一步地，可将拉格朗日极小极大问题转化为拉格朗日极大极小问题（对偶问题），前提是满足KKT条件，KKT条件由参数梯度条件、约束条件、乘子非负条件和对偶互补条件组成（不等式乘子和不等式左侧至少有一个为0）： \left\{\begin{matrix} \begin{align*} &\nabla_w L=0\\ &\nabla_b L=0\\ &\nabla_{\alpha} L=0\\ &\alpha_i\geqslant 0,\ i=1,2,...,n\\ &1-y_i(wx_i+b)\leqslant 0,\ i=1,2,...,n\\ &\alpha_i * (1-y_i(wx_i+b)) = 0,\ i=1,2,...,n\\ \end{align*} \end{matrix}\right.转化为对偶问题: \left\{\begin{matrix} \begin{align*} &\underset{\alpha_i\geqslant 0}{max}\ \underset{w,b}{min}\ \ L(w,b,\alpha)\\ &L(w,b,\alpha)=\frac{1}{2}\left \| w \right \|^2+\sum_{i=1}^{n}\alpha_i(1-y_i(wx_i+b)) \end{align*} \end{matrix}\right.（5）化简对偶形式： 首先求解极小问题： \left\{\begin{matrix} \begin{align*} &\nabla_w L=0\\ &\nabla_b L=0\\ \end{align*} \end{matrix}\right. \Rightarrow \left\{\begin{matrix} \begin{align*} &w=\sum_{i=1}^{n}\alpha_iy_ix_i\\ &\sum_{i=1}^{n}\alpha_iy_i = 0 \end{align*} \end{matrix}\right.化简极大极小问题: \begin{align*} &\underset{\alpha_i\geqslant 0}{max}\ \frac{1}{2} w^Tw-\sum_{i=1}^{n}\alpha_iy_i(wx_i+b)+\sum_{i=1}^{n}\alpha_i\\ =\ &\underset{\alpha_i\geqslant 0}{max}\ \frac{1}{2}(\sum_{i=1}^{n}\alpha_iy_ix_i)^T(\sum_{j=1}^{n}\alpha_jy_jx_j)-\sum_{i=1}^{n}\alpha_iy_i(\sum_{j=1}^{n}\alpha_jy_jx_jx_i+b)+\sum_{i=1}^{n}\alpha_i\\ =\ &\underset{\alpha_i\geqslant 0}{max}\ \frac{1}{2}\sum_{i=1}^{n}\sum_{j=1}^{n}\alpha_i\alpha_jy_iy_jx_ix_j - \sum_{i=1}^{n}\sum_{j=1}^{n}\alpha_i\alpha_jy_iy_jx_ix_j -b\sum_{i=1}^{n}\alpha_iy_i+\sum_{i=1}^{n}\alpha_i\\ =\ &\underset{\alpha_i\geqslant 0}{max}\ -\frac{1}{2}\sum_{i=1}^{n}\sum_{j=1}^{n}\alpha_i\alpha_jy_iy_jx_ix_j + \sum_{i=1}^{n}\alpha_i\\ =\ &\underset{\alpha_i\geqslant 0}{min}\ \frac{1}{2}\sum_{i=1}^{n}\sum_{j=1}^{n}\alpha_i\alpha_jy_iy_jx_ix_j - \sum_{i=1}^{n}\alpha_i \end{align*}最终原问题的对偶形式最终可以表示为: \left\{\begin{matrix} \begin{align*} \underset{\alpha_i\geqslant 0}{min}\ &\frac{1}{2}\sum_{i=1}^{n}\sum_{j=1}^{n}\alpha_i\alpha_jy_iy_jx_ix_j - \sum_{i=1}^{n}\alpha_i\\ s.t.&\\ & \sum_{i=1}^{n}\alpha_iy_i=0\\ & \alpha_i\geqslant 0,\ i = 1,2,...,n\\ & \alpha_i(1-y_i(wx_i+b))=0,\ i = 1,2,...,n \end{align*} \end{matrix}\right.（6）通过序列最小最优化算法（SMO）求解以上对偶问题，假设拉格朗日乘子最优解为: \alpha^*=(\alpha_1^{*},\alpha_2^*,\dots,\alpha_n^*),\ \alpha_i^*\geqslant 0因为数据集线性可分，必存在$\alpha_i^*&gt; 0$，由KKT对偶互补条件: \left\{\begin{matrix} \begin{align*} &\exists \alpha_i^*>0\\ &\alpha_i(1-y_i(w^*x_i+b^*))=0 \end{align*} \end{matrix}\right. \Rightarrow \exists (x_i,y_i),y_i=w^*x_i+b^*可得： \left\{\begin{matrix} \begin{align*} &w^*=\sum_{i=1}^{n}\alpha_i^*y_ix_i\\ &b^*=y_i-\sum_{j=1}^{n}\alpha_j^*y_jx_jx_i \end{align*} \end{matrix}\right.对于线性可分数据集，以上问题最优解存在且唯一。 算法1998年，Microsoft Research的John C. Platt在论文《Sequential Minimal Optimization：A Fast Algorithm for Training Support Vector Machines》中提出针对上述问题的解法：SMO算法，它很快便成为最快的二次规划优化算法，特别是在针对线性SVM和数据稀疏时性能更优。 SMO算法的基本思想是将Vapnik在1982年提出的Chunking方法推到极致，SMO算法每次迭代只选出两个分量ai和aj进行调整，其它分量则保持固定不变，在得到解ai和aj之后，再用ai和aj改进其它分量。与通常的分解算法比较，尽管它可能需要更多的迭代次数，但每次迭代的计算量比较小，所以该算法表现出较好的快速收敛性，且不需要存储核矩阵，也没有矩阵运算 （1）初始化 $\alpha^{(0)}=0$，$k=0$ （2）选取优化变量 $\alpha^{(k)}_1,\alpha^{(k)}_2$ 先“扫描”所有乘子，把第一个违反KKT条件的作为更新对象，令为 $\alpha^{(k)}_1$ 在所有不违反KKT条件的乘子中，选择使 |$E_1-E_2$| 最大的 $\alpha^{(k)}_2$ （3）解析求解两个变量的最优解$\alpha^{(k+1)}_1,\alpha^{(k+1)}_2$，更新$\alpha$ （4）若在精度范围内满足以下停机条件则终止，否则令$k=k+1$，转（2） \begin{align*} &\sum_{i=1}^{N}\alpha_iy_i=0\\ &0\leqslant \alpha_i\leqslant C,i=1,2...,N\\ &y_i (\sum_{j=1}^{N}\alpha_jy_jK(x_j,x_i)+b)=\left\{\begin{matrix} \geqslant 1 & , \left \{x_i|\alpha_i=0 \right \} \\ = 1 & , \left \{x_i|00\Rightarrow \xi_i^*=0\Rightarrow y_i(w^*x_i+b^*)\geqslant 1 $C&gt;\alpha^*_i&gt;0$: $(x_i,y_i)$为支持向量，位于margin上（记住这一条，其他的也就方便记忆了）； C>\alpha^*_i>0\Rightarrow \mu_i^*>0\Rightarrow \xi_i^*=0\Rightarrow y_i(w^*x_i+b^*)= 1 $\alpha^*_i=C$: $(x_i,y_i)$为支持向量，$\xi_i$衡量点向内偏离margin的距离； $1&gt;\xi^*_i&gt;0$：$(x_i,y_i)$在margin和超平面之间； $\xi^*_i=1$：$(x_i,y_i)$在超平面上； $\xi^*_i&gt;1$：$(x_i,y_i)$在被超平面误分一侧； 支持向量示意图： 合页损失函数软间隔最大支持向量机的损失函数等价于带正则的合页损失函数(hinge loss function)： L(w,b)=\sum_{i=1}^{N}[1-y_i(wx_i+b)]_+ + \lambda \left \| w \right \|^2其中合页函数： [z]_+=\left\{\begin{matrix} z, &z>0 \\ 0,& z\leqslant 0 \end{matrix}\right.只需令 $[1-yi(wx_i+b)]+=\xi_i$，$\lambda = \frac{1}{2C }$ 即可得证: L(w,b)=\sum_{i=1}^{N}[1-y_i(wx_i+b)]_+ + \lambda \left \| w \right \|^2=\frac{1}{C}(\frac{1}{2} \left \| w \right \|^2+C\sum_{i=1}^{N}\xi_i )二分类问题中常见的6种损失函数对比： 令$z=y\cdot f(x)$，其中$y$代表实例的观测标签（$+1,-1$），$f(x)$代表某个判别函数，满足当$f(x)&gt;0$时判别为+1类，当$f(x)&lt;0$时判别为-1类；z的符号代表预测正确性（正代表正确预测），z的绝对值代表预测确信度（值越大，确信度越高）。 0-1损失：$l(z)=[z&lt;0]$ 感知机损失函数：$l(z)=[-z]_+$ SVM损失函数：$l(z)=[1-z]_+$，合页损失函数不仅要分类正确，而且确信度足够高时损失才为0 平方损失：$l(z)=(z-1)^2$ sigmoid交叉熵（对数似然）：$l(z)=-\frac{1 }{\sqrt{2}} lg\sigma(z)$ sigmoid平方损失；$l(z)=(\sigma (z)-1)^2$ 非线性可分SVM——核技巧线性SVM无法解决线性不可分的分类问题，此时如果数据集是（近似）非线性可分的，那么可以通过核方法来求解。 非线性可分：如果能用$R^n$空间中的一个超曲面将数据集中的正负例正确分开，则称该数据集是非线性可分的。如下图中的数据集： 解决非线性可分问题的基本思路: 通过一个非线性变换将原始输入空间映射到新的线性可分的特征空间； 在新的特征空间中训练线性分类模型； 模型非线性可分SVM分类模型先对输入进行非线性转化再由线性模型求解： f(x)=sign(w \cdot \phi (x)+b)对应的对偶形式: f(x) = sign(\sum_{i=1}^{N}\alpha_iy_i \phi(x_i) \phi(x) + b)策略非线性可分的间隔最大化可表述为以下约束优化问题： \begin{align*} \underset{\alpha}{min} &\frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}\alpha_i \alpha_jy_i y_j \phi(x_i) \phi(x_j) - \sum_{i=1}^{N} \alpha_i\\ s.t. & \sum_{i=1}^{N} \alpha_i y_i=0\\ & \alpha_i \geqslant 0 \end{align*}核技巧: 基函数：从输入空间到特征空间的映射$\phi(x):\chi \rightarrow \Psi$； 核函数：对所有$x,z \in \chi$满足$K(x,z)=\phi(x) \cdot \phi(z)$的$K(x,z)$； 核方法：只定义核函数$K(x,z)$而不显式定义基函数$\phi(x)$，相当于“隐式”地对原始输入空间进行了非线性转化，节省了基函数计算的额外开销。 正定核：通常所说的核函数就是正定核函数，正定核的等价定义是$K(x,z)$对应的Gram矩阵$[K(xi,x_j)]{N*N}$是半正定矩阵。核正定能够确保拉格朗日函数有上界。 使用核方法来重新定义原问题: 模型： f(x) = sign(\sum_{i=1}^{N}\alpha_iy_i K(x_i,x) + b) 核技巧+间隔最大化： \begin{align*} \underset{\alpha}{min} &\frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}\alpha_i \alpha_jy_i y_j K(x_i,x_j) - \sum_{i=1}^{N} \alpha_i\\ s.t. & \sum_{i=1}^{N} \alpha_i y_i=0\\ & \alpha_i \geqslant 0 \end{align*}选择核函数常用的核函数: （1） 线性核函数(Linear Kernel)：线性核是最简单的核函数。它由内积加上一个可选的常量c给出。使用线性核的核算法通常等同于它们的非内核对应，即具有线性内核的KPCA与标准PCA相同。 k(x, y) = x^T y + c（2） 多项式核函数( Polynomial Kernel): 多项式核函数可以实现将低维的输入空间映射到高纬的特征空间，但是多项式核函数的参数多，当多项式的阶数比较高的时候，核矩阵的元素值将趋于无穷大或者无穷小（不稳定核），计算复杂度会大到无法计算，使用多项式核需要先对数据进行标准化。 k(x, y) = (\alpha x^T y + c)^p（3） sigmoid/双曲核函数(Hyperbolic Tangent (Sigmoid) Kernel): k(x, y) = \tanh (\alpha x^T y + c)（4） 高斯核函数(Gaussian Kernel): 高斯核函数是径向基函数(radial basis function,RBF)的一个实例，（另外两个比较常用的径向基函数：幂指数核，拉普拉斯核）。径向基函数是指取值仅仅依赖于特定点距离的实值函数，也就是$\Phi (x,y) = \Phi (\left | x-y \right |)$。高斯核是使用最广泛的核函数，参数少好调节；可以通过$\sigma$来调节核的表现（sk-learn中通过调节方差倒数$\gamma =\frac{1}{\sigma ^2}$），如果估计过高，则指数将几乎呈线性变化，高维投影将开始失去其非线性功效（欠拟合）。另一方面，如果低估，该函数将缺乏正则化（只有附近的点起作用），并且决策边界将对训练数据中的噪声高度敏感（过拟合）。方差越小越容易过拟合。 k(x, y) = \exp\left(-\frac{ \lVert x-y \rVert ^2}{2\sigma^2}\right)（5） 指数核函数(Exponential Kernel): k(x, y) = \exp\left(-\frac{ \lVert x-y \rVert }{2\sigma^2}\right)（6） 拉普拉斯核函数(Laplacian Kernel)： k(x, y) = \exp\left(- \frac{\lVert x-y \rVert }{\sigma}\right)选用核函数时，可以通过对数据的先验知识选择合适的核函数，或者通过交叉验证选择性能最好的核函数，也可以将多个核函数结合起来构成混合核函数，具体的有以下经验法则: 一般用线性核和高斯核，线性核可以看做是高斯核的特殊情况，sigmoid核在某些参数下和RBF很像； polynomial kernel的参数比RBF多，而参数越多模型越复杂 如果特征数很多，选用线性kernel就好了； 如果特征数较少，选用高斯核； 如果特征很少，则考虑增加特征； SMO算法同硬间隔最大化，假设最优解$\alpha^*$，则模型可表示为: f(x) = sign(\sum_{i=1}^{N}\alpha_i^* y_i K(x_i, x) + b)SVM进阶问题SVM用于多分类问题SVM 用于 K 分类问题时一般会转化为二分类问题： 一对剩余(one-Versus-rest)：使用来自$c_k$的样本作为正例，剩余样本作为负例构建K个独立的SVM。缺点：① 一个样本可能会被分配到0或多个类别；② 人为造成数据偏斜； 一对一：针对每两个类别的样本训练一个SVM，总共需要构建K(K-1)/2个分类器，然后通过多数投票确定样本类别。缺点：① K比较大时比第一种方法花费更多的训练时间； DAGSVM：仍然是一对一的方式，需要训练K(K-1)/2个分类器，但是在对新的测试点分类时，只需要K-1对分类器进行计算，根据DAG中的路径确定样本最终分类。 SVM用于回归问题类比 SVM 分类器的损失函数，可以将SVM应用于回归问题时的损失函数定义为: L(w)=\frac{1}{2}\left \| w \right \|^2+C\sum_{i=1}^{n}[\left | f(x_i)-y_i \right |- \varepsilon ]_+ 按照惯例，称C为惩罚参数，通过引入松弛变量可以重新定义最优化问题： \begin{align*} arg\ \underset{w}{min}\ &\frac{1}{2}\left \| w \right \|^2+C\sum_{i=1}^{n}(\xi _i+\hat{\xi _i})\\ s.t.\ &\xi _i\geqslant 0\\ &\hat{\xi _i}\geqslant 0\\ &\xi_i\geqslant f(x_i)-y_i- \varepsilon \\ &\hat{\xi _i} \geqslant y_i-f(x_i)- \varepsilon \end{align*} SVM 用于不平衡数据数据集偏斜（unbalanced）指的是参与分类的两个类别（也可以指多个类别）样本数量差异很大。比如说正类有10，000个样本，而负类只给了100个，这会引起的问题显而易见，可以看看下面的图： 现在由于偏斜的现象存在，使得数量多的正类可以把分类面向负类的方向“推”，因而影响了结果的准确性。真实的原因并不是父类样本少而是负类的样本分布的不够广。 对付数据集偏斜问题的方法之一就是在惩罚因子上作文章，想必大家也猜到了，那就是给样本数量少的负类更大的惩罚因子(一般通过正负例比例来调整正负惩罚因子的比例)，表示我们重视这部分样本（本来数量就少，再抛弃一些，那人家负类还活不活了），因此我们的目标函数中因松弛变量而损失的部分就变成了： \begin{align*} C_+\sum_{i=1}^{p}\xi _i+C_-\sum_{j=p+1}^{p+q}\xi _j\\ C_+*N_+=C_-*N_- \end{align*}SVM 的训练-预测时间复杂度SVM训练过程： 二次规划问题求解通常的时间复杂度为 $O(n^3)$，n为变量个数(对偶问题中样本个数等于变两个数,PRML) SMO最坏最坏时间复杂度 $O(n^3)$ (维基) 求得解析解的时间复杂度最坏可以达到$O(Nsv^3)$，其中Nsv是支持向量的个数 数值解:一个具体的算法，Bunch-Kaufman训练算法，典型的时间复杂度在O(Nsv3+LNsv2+dLNsv)和O(dL2)之间，其中Nsv是支持向量的个数，L是训练集样本的个数，d是每个样本的维数 SVM预测过程：SVM保存了支持向量，$O(Nsv)$ LR 和 SVM 的异同 相同点: 都可以作为分类模型； 都是线性判别模型； 不同点： 损失函数不同：LR是交叉熵，SVM是最大间隔，合页损失； LR可得到后验概率； SVM基于距离度量； SVM待正则项； SVM更适合小规模数据集； 评价 优点: 在小样本训练集上也能够得到很好的结果：SVM本身是基于小样本统计理论的； 可以解决高维问题：通过核函数避开高维空间的复杂性； 可以解决非线性问题：通过核技巧和软间隔最大化可以应用于非线性可分问题 泛化能力较好：间隔最大化是一种结构化风险最小化策略； 缺点: 对缺失数据比较敏感； 不提供后验概率； 对非线性问题没有太好方式确定核函数； 求解二次规划问题，需要占用大量存储空间存储Gram矩阵； 如果数据量较大，效率上不及SVM比LR和朴素贝叶斯；]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习：基础算法（八）—— PCA]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%9F%BA%E7%A1%80%E7%AE%97%E6%B3%95%EF%BC%88%E5%85%AB%EF%BC%89%E2%80%94%E2%80%94%20PCA%2F</url>
    <content type="text"><![CDATA[主成分分析（principal components analysis，PCA）是一种常用的降维方法，它将原始高维特征空间中的样本通过线性变换“投影”到低维特征空间，每个新的特征都是原始特征的线性组合且相互独立。 求解投影矩阵： 数据样本中心化$\sum_{i=1}^{n} x_i=0$ 对协方差矩阵$XX^T$进行特征值分解（或奇异值分解），求出所有特征值从大到小排序 取前k大特征值所对应的特征向量作为投影举证的列向量 PCA可以看做是一种数据压缩方法，主要有以下两方面的作用: 降维：在损失较少信息的前提下，降低了特征空间的维度，可以降低计算量、减少特征冗余和噪声，降低过拟合的风险（吴恩达说PCA不可用来作为降低过拟合）； 消除特征相关性：各主成分间相互独立，更加容易做特征选择； 基本概念 协方差(Covariance)：协方差用于衡量两个变量的总体误差，方差是协方差的特殊情况。两个变量的协方差等于他们乘积的期望减去期望的乘积 \begin{align*} cov(X,Y)&=E((X-E(x)(Y-E(Y))\\ &=E(XY)-E(X)E(Y) \end{align*} 协方差矩阵：当有多个变量时，可以使用协方差矩阵来衡量任两个变量之间的总体误差，协方差矩阵的对角线为对应变量的方差 空间转换：$\boldsymbol{\alpha}= [\alpha {1},\alpha {2},…,\alpha {n}]$ 和 $\boldsymbol{\beta} = [\beta {1},\beta {2},…,\beta {d}]$分别是阿尔法空间和贝塔空间的一组标准正交基，且$\boldsymbol{\beta} = \boldsymbol{\alpha}W$，W称为从阿尔法空间到贝塔空间的过度矩阵： $W^TW{n\times d}=I{d\times d}$ 贝塔空间向量在阿尔法空间中的投影$x{\alpha } = Wz{\beta }$ 阿尔法空间向量在贝塔空间中的投影$z{\beta }=W^Tx{\alpha }$ 迹运算： $tr(a)=a$ $tr(A)=tr(A^T)$ $\text{tr}(A\pm B) = \text{tr}(A)\pm \text{tr}(B)$ $tr(AB)=tr(BA)$ PCA理论基础PCA主要解决的问题是：如何寻找一个超平面，使得样本在该超平面内的投影能够尽可能多的保留数据集中的有用信息，PCA两种等价的推导方式： 最大投影方差：样本点在超平面的投影方差最大 最小投影距离：样本点到超平面的距离和最小 最大投影方差方差和熵都是通过描述不确定性的多少来量化信息的，PCA希望投影后尽可能保留原始数据中的信息，可以通过最大化投影后的样本方差来实现。在信号处理中认为信号具有较大的方差，噪声有较小的方差，信噪比就是信号与噪声的方差比，信噪比越大越好。 1）特征空间的转置：$X = [x{ij}]{m\times n}=[\boldsymbol{x}_1,\boldsymbol{x}_2,\dots,\boldsymbol{x}_n]$，列向量$\boldsymbol{x}_i$代表了第i个样本（因为投影矩阵是对列向量进行投影） 2) 数据集去中心化：$\boldsymbol{x}_{i}\leftarrow \boldsymbol{x}_{i}-\bar{x}$，去中心化之后$XX^T$即代表了各个特征的协方差矩阵 3) 投影矩阵：$W=[w{1},w{2},…,w{k}]\{m\times k}$，其中$w{i}$是m维空间中的标准正交基，$ W^TW=I\{k\times k} $ 4) 投影：$x{i}$在k维空间中的投影可以表示为：$W^Tx{i}$ 5) 投影方差： \begin{align*} V &= \sum_{i=1}^{n}\left \| W^Tx_{i}-\frac{1}{n}\sum_{i=1}^{m}W^Tx_{i} \right \|^2\\ &=\sum_{i=1}^{n}\left \| W^Tx_{i}\right \|^2 \\ &= \sum_{i=1}^{n} ( W^Tx_{i})^TW^Tx_{i}\\ &=\sum_{i=1}^{n}tr( ( W^Tx_{i})^TW^Tx_{i})\\ &=\sum_{i=1}^{n}tr( W^Tx_{i}( W^Tx_{i})^T)\\ &=\sum_{i=1}^{n}tr(W^Tx_{i}x_{i}^TW)\\ &=tr(W^T(\sum_{i=1}^{n}x_{i}x_{i}^T)W)\\ &=tr(W^TXX^TW)\\ &=tr(\Lambda) \end{align*}6) 投影方差最大优化问题：$XX^T$为正定矩阵，$W$为正交矩阵，只需要求出特征协方差矩阵 $XX^T$ 的前k大的特征值 $(\lambda{1},\lambda{2},…\lambda{k})$ 及其对应的特征向量 $(w{1},w{2},…w{k})$； \begin{align*} \underset{W}{max}& \text{ tr}(W^TXX^TW)\\ s.t. & W^{T} W=I \end{align*}7） k维的投影超平面:$W = (w{1},w{2},…w_{k})$ 8） 投影方差为$\sum{i=1}^{k}\lambda {i}$ 9） 重构阈值：PCA降维后的维数k通常由用户事先指定，也可以从重构角度设置一个重构阈值来代表降维后信息保留的比例（一般要求90%以上）： \frac{\sum\_{i=1}^{k}\lambda \_{i}}{\sum_{i=1}^{n}\lambda _{i}}\geqslant t10) 将新样本投影至低维空间：中心化→投影$x{i}\leftarrow W^T(x{i}-\bar{x} )$ 最小投影距离1) 投影向量：$x{i}$在超平面W中的投影向量可以表示为$WW^Tx{i}$ 2) 原样本点与基于投影重构的样本点之间的距离平方和为： \begin{align*} D&=\sum_{i=1}^{n}\left \| WW^Tx_{i}-x_{i} \right \|^2\\ &= \sum_{i=1}^{n}(WW^Tx_{i}-x_{i})^T(WW^Tx_{i}-x_{i})\\ &=\sum_{i=1}^{n}(x_{i}^TWW^TWW^Tx_{i}-x_{i}^TWW^Tx_{i}-x_{i}^TWW^Tx_{i}+x_{i}^Tx_{i})\\ &=-\sum_{i=1}^{n}(x_{i}^TWW^Tx_{i})+ \text{const}\\ &=-\sum_{i=1}^{n}tr(x_{i}^TWW^Tx_{i})+ \text{const}\\ &=-\sum_{i=1}^{n}tr(W^Tx_{i}x_{i}^TW)+ \text{const}\\ &=-tr(W^TXX^TW)+ \text{const} \end{align*}3) 求最小投影距离与求最大投影方差问题等价。 PCA计算过程 输入：样本集$D=\{\boldsymbol{x}_{1},\boldsymbol{x}_{2},…,\boldsymbol{x}_{n}\}$，低维空间维数k 输出：新的特征空间 过程: 对所有样本进行中心化：$ \boldsymbol{x}_{i}\leftarrow \boldsymbol{x}_{i}-\frac{1}{m}\sum_{i=1}^{m}\boldsymbol{x}_{i}$，$X = [\boldsymbol{x}_{1},\boldsymbol{x}_{2},…,\boldsymbol{x}_{m}]$ 计算特征的协方差矩阵：$XX^T$ 对样本协方差矩阵进行特征值分解； 取前k个最大的特征值所对应的特征向量组成投影矩阵：$W=[w{1},w{2},…,w_{k}]$ 投影：$x{i}\leftarrow W^T(x{i}-\bar{x} )$ 核主成分分析（KPCA）：普通PCA是一种线性映射，有时需要非线性映射才能找到恰当的低维嵌入，常用方法是基于“核技巧”对线性降维方法进行“核化” 参考主成分分析（Principal components analysis）-最大方差解释主成分分析（Principal components analysis）-最小平方误差解释]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习：基础算法（九）—— EM]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%9F%BA%E7%A1%80%E7%AE%97%E6%B3%95%EF%BC%88%E4%B9%9D%EF%BC%89%E2%80%94%E2%80%94%20EM%2F</url>
    <content type="text"><![CDATA[EM(expectation maximization)算法是一种迭代算法，用于含隐变量的概率模型参数的极大似然估计或极大后验概率估计。 EM算法的推导考虑一个概率模型，将所有观测变量记做X，将所有隐变量记做Z，完全变量的联合概率分布$p(X,Z\mid\theta)$由一组参数控制，记做$\theta$。我们的目标是通过观测变量的极大似然来估计最优参数$\theta^*$ \theta^*=\underset{\theta}{max}\ log\ p(X\mid\theta)=\underset{\theta}{max}\ log\ \sum_{Z}p(X,Z\mid\theta)因为对数中包含了联合概率分布的和(或积分)，直接最优化$log\ p(X\mid\theta)$比较困难，通过引入隐变量分布$q(Z)$，可以将观测变量的对数似然分解为一个下界加上$q(z)$与$p(Z|X,\theta)$的KL散度(代入可证)： L(\theta)=logp(X|\theta)=B(q,\theta)+KL(q||p)其中： \left\{\begin{matrix} &B(q,\theta)=\sum_{Z}q(Z)log\frac{p(X,Z|\theta)}{q(Z)}\\ &KL(q||p)=-\sum_{Z}q(Z)log\frac{p(Z|X,\theta)}{q(Z)} \end{matrix}\right.KL散度总是大于等于0，故$B(q,\theta)$是$L(\theta)$的一个下界： L(\theta)\geqslant B(q,\theta)EM算法是一个两阶段的迭代算法，通过不断提升对数似然函数的下界来搜索对数似然函数的极大值。假设参数当前值为$\theta_k$，可以用以上似然分解来定义这两个阶段： 1、 E步：固定当前参数$\theta_k$，关于隐变量分布$q(Z)$最大化下界函数$B(q,\theta_k)$，逼近当前的对数似然函数值。很明显下界函数的最大值出现在$q(z)$与$p(Z|X,\theta)$的KL散度为0的时候，即$p(Z|X,\theta_k)=q(Z)$。 此时下界函数可表示为： \begin{align*} B(q,\theta)&=\sum_{Z}p(Z|X,\theta_k)logp(X,Z|\theta)-\sum_{Z}p(Z|X,\theta_k)logp(Z|X,\theta_k)\\ &=Q(\theta,\theta_k) + H_{X,\theta_k}(Z) \end{align*} $H_{X,\theta_k}(Z)$：分布q(Z)的熵，为常数，与$\theta$无关； $Q(\theta,\theta_k)$：Q函数，完全数据的对数似然关于隐变量分布的期望； 此时下界函数值等于对数似然函数值： \begin{align*} B(q,\theta_k)&=\sum_{Z}p(Z|X,\theta_k)logp(X,Z|\theta_k)-\sum_{Z}p(Z|X,\theta_k)logp(Z|X,\theta_k)\\ &=\sum_{Z}p(Z|X,\theta_k)logp(X|\theta_k)\\ &=logp(X|\theta_k)\\ &=L(\theta_k) \end{align*}E步的几何解释：q分布被置为当前参数下的后验概率分布，使得KL散度为0，下界函数上移到与观测数据对数似然相等的位置。 2、M步：固定当前隐变量分布，关于参数$\theta$最大化下界函数$B(q,\thetak)$，得到某个新的$\theta{k+1}$。这会增大$B(q,\thetak)$(除非已经到达极大值)，同时因为概率分布q由$\theta_k$确定，并在M步保持不变，因此它不会等于新的后验概率分布$p(Z|X,\theta{k+1})$，从而KL散度不为0，所以对数似然的增加量大于下界函数的增加量。 \theta_{k+1}= arg\ \underset{\theta}{max}\ B(q,\theta)=arg\ \underset{\theta}{max}\ Q(\theta,\theta_k) L(\theta_k)>B(q,\theta_k)M步几何解释：下界函数关于参数最大化，得到修正的$\theta$，因为KL散度非负，使得对数似然函数增量至少与下界函数增量相等。 EM算法也可以看做是参数空间中的迭代：红色曲线表示观测数据对数似然函数，我们希望搜索它的极大值，首先选取某个初始参数值$\theta^{old}$，通过E步计算隐变量上的后验概率分布，关于隐变量上的后验概率分布最大化下界函数(蓝色曲线表示)，使得在$\theta^{old}$处下界函数值等于对数似然值；在M步，关于参数$\theta$最大化下界函数，得到新的$\theta^{new}$，在新参数值下对数似然值增长量大于下界函数增长量；接下来的E步又构建了一个新的下界函数(绿色)，直至对数似然函数达到极值(EM之只能达到局部最优)。 EM算法的基本框架： 输入：观测变量数据X，隐变量数据Z，完全数据联合分布$p(X,Z|\theta)$，条件分布$p(Z|Y,\theta)$，精度$\epsilon$输出：模型参数$\theta$（1）选择参数初值$\theta0$，置k=0（2）如果$\left | \theta{k+1}-\theta_k \right |&lt;\epsilon $ E步：记$\theta_k$为第k次迭代参数的估计值，计算Q函数： Q(\theta,\theta_k)=\mathbb{E}_Z[logp(X,Z|\theta)\mid X,\theta_k]=\sum_{Z}p(Z|X,\theta_k)logp(X,Z|\theta) M步：极大化Q函数，更新参数 \theta_{k+1}=arg\ \underset{\theta}{max}\ Q(\theta,\theta_k)EM算法的收敛性定理1：假设$\theta_k,k=1,2…$是EM算法得到的参数估计序列，则对应的对数似然函数序列是递增的。 L(\theta_{k+1})\geqslant B(q,\theta_{k+1})\geqslant B(q,\theta_k)= L(\theta_k)定理2：如果EM的对数似然函数有上界，则EM算法得到的对数似然函数序列会收敛到某个值。 定理3：在Q函数和对数似然满足一定条件下，EM算法得到的参数估计序列会收敛到对数似然函数序列的稳定点，但不能保证收敛到全局/局部极大值点。 EM算法对初值比较敏感，在实际应用中通常选取几个不同的初值进行迭代，然后对各个估计值加以比较，从中选择最好的 EM算法用于极大后验概率估计我们也可以使用EM算法来最大化模型的后验概率分布$p(\theta\mid X)$，其中我们已经引入了参数上的先验概率分布$p(\theta)$，我们的目标是极大化以下对数后验概率： \begin{align*} log\ p(\theta\mid X)&=log\ \frac{p(\theta)p(X\mid \theta)}{p(X)}\\ &=log\ p(X\mid \theta) + log\ p(\theta)-log\ p(X)\\ &=B(q,\theta)+KL(q||p)+ log\ p(\theta)-log\ p(X)\\ &\geqslant B(q,\theta)+ log\ p(\theta)-log\ p(X) \end{align*}其中$log\ p(X)$是一个常数，与之前一样，我们可以交替地关于q和$\theta$对右侧进行优化： E步：因为q只出现在下界函数中，与标准EM中的E步完全相同 M步：通过引入先验分布$p(\theta)$进行修改，通常只需要做很小改动即可 GEM算法EM算法将最大化似然函数这一困难问题分解为两个阶段，每个步骤一般都很容易解决，但对于复杂的模型来说，E步或M步仍然无法计算。推广的EM算法(generalized EM,GEM)解决的是M步骤无法计算的问题。 F函数的极大极大算法EM算法还可以解释为F函数的极大极大算法： F函数： F(\tilde{P},\theta)=E_P[log\ P(X,Z\mid \theta)]+H(\tilde{P}) $\tilde{P}(Z)$：隐变量数据的概率分布 $H(\tilde{P}$：分布$\tilde{P}$的熵 E步：对固定的$\theta^i$，求$\tilde{P}^{i+1}$使$F(\tilde{P},\theta^i)$极大化 M步：对固定的$\tilde{P}^{i+1}$，求$\theta^{i+1}$使$F(\tilde{P}^{i+1},\theta)$ 由EM算法和F函数的极大极大算法得到的参数估计序列是一致的。 GEM算法框架当参数个数d大于等于2时，可采用一种特殊的GEM算法，它将EM算法的M步分解为d次条件极大化，每次只改变参数向量中的一个分量，其余分量不变。 EM评价和应用EM算法的优缺点: 优点:简单、普适 缺点:收敛速度慢，只能收敛到稳定点，不能保证收敛到全局/局部最优解 EM算法的应用: EM用于生成模型的非监督学习：可以认为非监督学习训练数据是联合概率分布$p(X,Y)$生成的数据，X为观测变量数据，Y为隐变量数据。 K-means聚类 GMM混合高斯 HMM隐马尔科夫 填充缺失值]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习：基础算法（十）—— GMM]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%9F%BA%E7%A1%80%E7%AE%97%E6%B3%95%EF%BC%88%E5%8D%81%EF%BC%89%E2%80%94%E2%80%94%20GMM%2F</url>
    <content type="text"><![CDATA[混合模型通过将基本的概率分布进行线性组合所得到的概率模型称为混合模型(mixture distributions)。混合模型可以用观测数据的边缘概率来描述： p(x)=\sum_{k=1}^{K}p(z_k=1)p(x\mid z_k=1) $p(z_k=1)$：z为K维的二值随机变量，采用“1-of-K”表示方法，其中有且只有一个元素$z_k=1$，其余所有元素等于0，$p(z_k=1)$表示选择第k个模型分量的先验概率 $p(x\mid z_k=1)$：第k个模型分量 混合高斯模型混合高斯模型(Gaussian misture model)是由若干个高斯分布线性组合而成的混合模型： p(x)=\sum_{k=1}^{K}\pi _kN(x\mid \mu _k,\Sigma_k)其中： \left\{\begin{matrix} &p(z_k=1)=\pi _k\\ &p(x\mid z_k=1)=N(x\mid \mu _k,\Sigma_k) \end{matrix}\right.更一般的表示： \left\{\begin{matrix} &p(z)=\prod_{k=1}^{K}\pi _k^{z_k}\\ &p(x\mid z)=\prod_{k=1}^{K}N(x\mid \mu _k,\Sigma_k)^{z_k} \end{matrix}\right.给定x的条件下，z的后验概率可以被理解为第k个模型分量对观测值x的贡献度： \begin{align*} \gamma(z_k)&=p(z_k\mid x)\\ &=\frac{p(z_k=1)p(x\mid z_k=1)}{\sum_{j=1}^{K}p(z_j=1)p(x\mid z_j=1)}\\ &=\frac{\pi_kN(x\mid \mu_k,\Sigma_k)}{\sum_{j=1}^{K}\pi_kN(x\mid \mu_k,\Sigma_k)} \end{align*}通过使用足够多的高斯分布，通过调节它们的均值和方差以及线性组合的系数，可以以任意精度近似所有的连续概率密度分布。下图是3个高斯分布混合的例子，图a）表示三个分量的分布及混合系数，图b)表示混合分布的边缘概率分布轮廓线，图c)表示边缘分布的曲面图。 高斯混合模型的参数估计(EM算法)可以通过极大似然估计来确定混合高斯模型中的参数： log\ p(X\mid \pi,\mu,\Sigma)=\sum_{i=1}^{N}log\sum_{k=1}^{K}\pi_kN(x_n\mid \mu_k,\Sigma_k)因为对数中存在一个求和式，导致参数的最大似然解不再有一个封闭形式的解析解。一种最大化这种似然函数的方式是使用迭代数值优化法，另一种是使用EM算法，本文接下来主要讲解EM算法。 1、明确观测变量、隐变量和参数： 将所有观测变量数据记做$X={x_1,x_2,…,x_n}$，其中$x_i$代表第i个样本 将所有隐变量数据记做$Z=[z{ik}]{n\times k}$，其中$z_{ik}=1$代表第i个样本来自第k个分模型 参数记做$\theta=(\pi,\mu,\Sigma)$，其中$\pi,\mu,\Sigma$均为K维向量，分别表示选取第k个模型的概率，第k个模型的均值和协方差 2、E步：计算完全数据的对数似然关于隐变量的条件期望函数（Q函数） Q(\theta,\theta^i)=\mathbb{E}_Z[log\ p(X,Z\mid \theta)\mid X,\theta^i]其中，完全数据的对数似然函数为： \begin{align*} log\ p(X,Z\mid \theta)&=log\ \prod_{j=1}^{N}p(x_j,z_j\mid \theta)\\ &=log\ \prod_{j=1}^{N}p(z_j\mid \theta)p(x_j\mid z_j,\theta)\\ &=log\ \prod_{j=1}^{N}\prod_{k=1}^{K}\pi_k^{z_{jk}}N(x_j\mid \mu_k,\Sigma_k)^{z_{jk}}\\ &=\sum_{j=1}^{N}\sum_{k=1}^{K}z_{jk}(log\pi_k+logN(x_j\mid \mu_k,\Sigma_k)) \end{align*}关于隐变量的条件期望： \mathbb{E}_Z[log\ p(X,Z\mid \theta)\mid X,\theta^i]=\sum_{j=1}^{N}\sum_{k=1}^{K}\mathbb{E}_Z(z_{jk}\mid X,\theta^i)(log\pi_k+logN(x_j\mid \mu_k,\Sigma_k))其中： \mathbb{E}_Z(z_{jk}\mid X,\theta^i)=p(z_{jk}=1\mid x_j,\theta^i)=\gamma (z_{jk})综上，可以写出Q函数的最终表达式: Q(\theta,\theta^i)=\sum_{j=1}^{N}\sum_{k=1}^{K}\gamma (z_{jk})(log\pi_k+logN(x_j\mid \mu_k,\Sigma_k))3、M步：关于$\theta$最大化Q函数，求出最佳参数更新$\theta$ \theta^{i+1}=arg\ \underset{\theta}{max}\ Q(\theta,\theta^i)偏导等于0，可得： (1) 第k个模型分量的混合系数等于第k个模型对所有样本的平均贡献度($N_k$代表被分到第k个分量的样本数)： \pi_k^{i+1}=\frac{\sum_{j=1}^{N}\gamma (z_{jk})}{N}=\frac{N_k}{N},k=1,2,...,K（2）第k个模型分量的均值等于所有样本关于模型贡献度的均值： \mu_k^{i+1}=\frac{\sum_{j=1}^{N}\gamma (z_{jk})x_j}{N_k},k=1,2,...,K（3）第k个模型分量的协方差等于所有样本关于模型贡献度的协方差： \Sigma_k^{i+1}=\frac{\sum_{j=1}^{N}\gamma (z_{jk})(x_j-\mu_k^{i+1})(x_j-\mu_k^{i+1})^T}{N_k},k=1,2,...,K高斯混合模型EM算法框架: 输入：观察变量数据，高斯混合模型 输出：高斯混合模型参数 （1）选取合适的参数初值，$\pi^0,\mu^0,\Sigma^0$，置i=0（2）迭代E步和M步，直至收敛 E步：基于模型当前参数值，计算每个分模型对各个观测数据的贡献度/后验概率$\gamma (z_{jk})$，得到Q函数 M步：最大化Q函数，更新参数]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习：基础算法（十一）—— 聚类]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%9F%BA%E7%A1%80%E7%AE%97%E6%B3%95%EF%BC%88%E5%8D%81%E4%B8%80%EF%BC%89%E2%80%94%E2%80%94%20Cluster%2F</url>
    <content type="text"><![CDATA[无监督学习的目标是通过对无标记训练样本的学习来揭示数据的内在规律，应用最广的是聚类(clustering)。聚类的目标是将数据集划分为若干互不相交的子集，每个子集称为“簇”，每个簇对应于一些潜在的“类别”，这些类别事先未知，需要由使用者把握和命名。 聚类既能作为一个单独的过程，也可以作为其他学校任务的前驱过程。 聚类的性能度量一般原则：簇内相似度尽可能高，簇间相似度尽可能低 聚类性能指标一般有两大类： 1、 外部指标：将聚类结果与某个参考模型进行对比，如Rand指数(越大越好)： RI=\frac{a+d}{\binom{n}{2}} a:在聚类模型和参考模型中都属于同一簇的样本二元组数 b:在聚类模型和参考模型中都属于不同簇的样本二元组数 n:样本数 2、 内部指标：直接考察聚类结果，不参考任何外部模型，如DB指数(越小越好)： DBI=\frac{1}{k}\sum_{i=1}^{k}\underset{i\neq j}{max}\frac{avg(C_i)+avg(C_j)}{d(\mu_i,\mu_j)} k：簇的个数 $avg(C_i)$：簇$C_i$中两两样本的平均距离 $d(\mu_i,\mu_j)$：簇$C_i$与$C_j$中心的距离 距离度量对$(x_i,x_j)$的距离度量需满足以下基本性质： 非负性：$d(x_i,x_j)\geqslant 0$ 同一性：$d(x_i,x_i)=0$ 对称性：$d(x_i,x_j)=d(x_j,x_i)$ 直递性：$d(x_i,x_j)\leqslant d(x_i,x_k)+d(x_k,x_j)$ 对有序特征，通常采用闵可夫斯基距离： d_p(x_i,x_j)=(\sum_{k=1}^{d}\left | x_{ik} - x_{jk}\right |^p)^{\frac{1}{p}} p=1：曼哈顿距离，街道距离 p=2：欧氏距离，直线距离 $p=\infty $：切比雪夫距离，最大分量差 对无序特征，常采用CDM距离：衡量同一无序属性上两个属性值之间的距离，同一属性上的两个值，在相同簇中样本数比例差别越大，属性值之间的距离越大 VDM_p(a,b)=\sum_{i=1}^{k}\left | \frac{m_{uai}}{m_{ua}} - \frac{m_{ubi}}{m_{ub}}\right | $m_{uai}$：第i个样本簇中属性u上取值为a的样本数 $m_{ua}$：属性u上取值为a的样本数 如果特征中混合了有序特征和无序特征，则可以混合闵可夫斯基距离和VDM距离，假设前k个属性为有序特征，后d-k个属性为无序特征，则： d_p(x_i,x_j)=(\sum_{u=1}^{k} \left | x_{iu}-x_{ju} \right |^p+\sum_{u=k+1}^{d} VDM_p(x_{iu},x_{ju}))^{\frac{1}{p}}原型聚类所谓”原型“指的是聚类中心，将所有样本按照某组聚类中心进行划分，最常见的原型聚类是K-Means算法。 原型聚类问题描述：假设有一个数据集{$x1,x_2,…,x_N$}，我们的目标是将数据集划分为K类，并找到聚类类别和聚类中心。聚类类别可以用”1-of-K“表示法来表示，K维向量$z_i$中有且只有一个元素为1，其余元素为0，$z{ik}=1$表示第i个样本属于第k个类别；第k个类别的聚类中心可以用$\mu_k$表示 K-Means算法为了极大化簇内相似度，极小化簇间相似度，K-Means算法的目标函数被定义为以下”失真度“： L(\theta)=\sum_{i=1}^{N}\sum_{k=1}^{K}z_{ik}\left \| x_i-\mu_k \right \|^2为找到目标函数的极小值，需考察数据集中所有可能的簇划分，这是一个NP难问题，K-means采用了 EM 算法来求解： 1、 明确变量和参数： （1）观测变量：$X=[x1,x_2,…,x_N]$（2）隐变量：$Z=[z_ik]{n \times K}$（3）参数：$\mu=[\mu_1,\mu_2,…,\mu_K]$ 2、初始化参数：$\mu^0$，置j=0 3、EM迭代：迭代 E 步和 M 步直至前后两次聚类中心不变，$\mu^j$即为最终的近似最优解 （1）E步：样本分配，基于当前的聚类中心，关于类别隐变量分布，最小化目标函数，只需将样本分配到距其最近的聚类中心所在类别中即可 z_{ik}=\left\{\begin{matrix} 1 & if\ k=arg\ \underset{j}{min}\left \| x_i-\mu_j \right \|^2\\ 0 & else \end{matrix}\right.（2）M步：更新聚类中心。基于当前类别隐变量分布，关于聚类中心参数，最小化目标函数，只需取当前类别内所有样本的均值作为新的聚类中心即可 目标函数对聚类中心偏导为0： 2\sum_{i=1}^{N}z_{ik}(x_i-\mu_k)=0解出新的聚类中心：$n_k$为第k个簇中样本数 \mu_k^{j+1}=\frac{\sum_{i=1}^{n_k}z_{ik}x_i}{n_k}K-Means 过程的可视化： 直接实现 K-Means 算法非常慢，因为在每个E步必须计算每个聚类中心和每个数据点的距离，常用的加速方法： 建立K-D树：详见kd-tree加速K-means 利用距离的三角不等式，避免不必要的计算：如果$2d(x,\mu_i)\leqslant d(\mu_i,\mu_j)$，则$d(x,\mu_i)\leqslant d(x,\mu_j)$，详见运用三角不等式加速Kmeans聚类算法 高斯混合聚类高斯混合模型的EM算法和K-means算法有很强的相似性，K-means算法对样本数据点的聚类进行了”硬“分配，每个数据点只属于唯一的聚类，而EM算法基于后验概率分布，进行了一个”软“分配。事实上可以将K-means算法看做是高斯混合模型EM算法在方差趋于0的极限情况下的特例。 对于一个特定的数据点x，后验概率（分量贡献度）为： \gamma (z_k)=\frac{\pi_k e^{-\frac{\left \| x-\mu_k \right \|^2}{2\sigma }}}{\sum_{j=1}^{K}\pi_j e^{-\frac{\left \| x-\mu_j \right \|^2}{2\sigma }}}当$\sigma \rightarrow 0$时，$\left | x-\muj \right |^2$较大的项，$\gamma(z{j})$会趋于0，只有$\left | x-\muj \right |^2$最小的项$\gamma(z{j})$会趋近于1，在这种极限情况下，可以得到对数据点的一个硬分解。 从另外一个角度来看，当高斯混合分布已知时，高斯混合聚类会将样本集划分为k个簇，每个样本会被归入贡献度最高的分量： z_k=arg\ \underset{j \in {1,2,...,K}}{max}\gamma (z_{j})学习向量量化学习向量量化(Learning Vector Quantization,LVQ)假设数据样本带有类别标记，学习过程利用这些监督信息来辅助聚类。 LVQ算法： 输入:样本集D={$(x_1,y_1),(x_2,y_2),…,(x_m,y_m)$}，原型向量个数K，各原型向量预设的类别标签{$c_1,c_2,…,c_K$}（可能含有重复），学习率$\eta $输出：原型向量{$p_1,p_2,…,p_K$} 初始化一组原型向量{$p_1^0,p_2^0,…,p_K^0$}，置s=0 迭代以下过程，直至相邻两次原型向量更新很小或者到达最大迭代次数 从样本中随机选取样本$(x_i,y_i)$ 找出与样本$x_i$距离最近的原型向量$p_j^s$，判断样本标签$y_i$与该原型向量的标签$c_j$是否相同 如果相同，将原型向量$p_j^s$朝着样本$x_i$方向靠拢：$p_j^{s+1}=p_j^s+\eta(x_i-p_j^s)$ 否则，将原型向量$p_j^s$朝着远离$x_i$的方向移动：$p_j^{s+1}=p_j^s-\eta(x_i-p_j^s)$ 层次聚类层次聚类视图在不同层次上对数据进行划分，形成树形的聚类结构，数据集划分通常可采用”自底向上“的聚合策略，也可采用”自顶向下“的分拆策略。 ”自底向上“的层次聚类算法：先将数据集中每个样本看做一个初始聚类簇，然后在每次迭代时将两个距离最近的聚类簇合并，直至达到预设的聚类簇个数。 衡量聚类簇间距离的指标: 最小距离：两个簇间元素间的最短距离 最大距离：两个簇间元素间最长距离 平均距离：两个簇间两两元素间的平均距离 参考 数据科学中必须熟知的5种聚类算法]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习：概述]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%A6%82%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[机器学习的定义 A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P, improves with experience E. 如果一个计算机程序在某类任务T上的性能P通过经验E得到了提升，那么就说关于T和P，该程序学习了经验E。 —— Tom Mitchell《Machine Learning》 如果说计算机科学是研究“算法”的学科，那么可以说机器学习是研究“学习算法”(从数据中产生模型的算法)的学科。 从广义上来说，机器学习是一种能够赋予机器学习的能力以此让它完成直接编程无法完成的功能的方法。但从实践的意义上来说，机器学习是一种通过利用数据，训练出模型，然后使用模型预测的一种方法。 机器学习的范围 模式识别（Pattern Recognition）模式识别 = 机器学习 模式识别源自工业界，而机器学习来自于计算机学科。不过，它们中的活动可以被视为同一个领域的两个方面。——《Pattern Recognition And Machine Learning》 机器学习的目标是发现数据中的模式和关系并予以应用。 数据挖掘（Data mining）数据挖掘 = 机器学习 + 数据库 Data mining is the analysis step of the “knowledge discovery in databases. 统计学习（statistical learning）统计学习 = 贝叶斯学派机器学习 统计学习是关于计算机基于数据构建概率统计模型并运用模型对数据进行预测与分析的一门学科。统计学习也称为统计机器学习（statistical machine learning）。——《统计学习方法》 计算机视觉（Computer vision）计算机视觉 = 机器学习 + 图像处理 Computer vision is an interdisciplinary field that deals with how computers can be made for gaining high-level understanding from digital images or videos. 语音识别（Speech recognition）语音识别 = 机器学习 + 语音处理 Speech recognition is the inter-disciplinary sub-field of computational linguistics that develops methodologies and technologies that enables the recognition and translation of spoken language into text by computers. 自然语言处理（Natural language processing）自然语言处理 = 机器学习 + 文本处理 Natural language processing (NLP) is a field of computer science, artificial intelligence and computational linguistics concerned with the interactions between computers and human (natural) languages, and, in particular, concerned with programming computers to fruitfully process large natural language corpora. 人工智能（Artificial intelligence） Artificial intelligence(AI,also machine intelligence, MI) is intelligence exhibited by machines, rather than humans or other animals(natural intelligence,NI). 狭义的人工智能是指让机器获取认知/学习的能力，机器可以不断地通过数据来改善自身的性能。广义的人工智能包括狭义人工智能、人工情感与人工意志三个方面。—— 西部世界 生命本身就是不断处理数据的过程，生物本身就是算法。—— 尤瓦尔•赫拉利《未来简史》 人也不过是一台有灵魂的机器而已。—— 丹尼尔·丹尼特《意识的解释》 深度学习（deep learning）深度学习 = 连接学派机器学习 深度学习是机器学习拉出的分支，它试图使用包含复杂结构或由多重非线性变换构成的多个处理层对数据进行高层抽象的算法。深度学习是对神经网络的“品牌重塑”。 机器学习的工作方式 ①选择数据：将你的数据分成三组：训练数据、验证数据和测试数据②模型数据：使用训练数据来构建使用相关特征的模型③验证模型：使用你的验证数据来验证你的模型④测试模型：使用你的测试数据检查被验证的模型的表现⑤使用模型：使用完全训练好的模型在新数据上做预测⑥调优模型：使用更多数据、不同的特征或调整过的参数来提升算法的性能表现 机器学习对比传统编程 ①传统编程：软件工程师编写程序来解决问题。首先存在一些数据→为了解决一个问题，软件工程师编写一个流程来告诉机器应该怎样做→计算机遵照这一流程执行，然后得出结果②统计学：分析师比较变量之间的关系③机器学习：数据科学家使用训练数据集来教计算机应该怎么做，然后系统执行该任务。首先存在大数据→机器会学习使用训练数据集来进行分类，调节特定的算法来实现目标分类→该计算机可学习识别数据中的关系、趋势和模式④智能应用：智能应用使用人工智能所得到的结果，如图是一个精准农业的应用案例示意，该应用基于无人机所收集到的数据 机器学习的优势 精确：机器学习是数据驱动的决策系统，随着数据增多，精确性也随之提高 迅速：机器学习可以在新数据进入的几毫秒内产生结果，允许系统做出实时反应 自动化：机器学习可以自动学习新的模式，用户可以把机器学习直接嵌入到自动工作流中 规模化：机器学习可以很容易地处理数据增长问题，有些机器学习可以使用云计算处理大规模数据 可自定义：许多数据驱动的问题可用机器学习解决，机器学习模型通过自己的数据构建，并可用任何评价标准来进行优化 机器学习的困难 获取可用形式的数据：数据科学家80%的时间花费在数据准备上，当前商业捕获的数据比任何以往时候都要多，而且这些数据往往是业务处理的废弃物，从这些“废弃物”中提取有用数据是一件非常繁琐的工作 对问题进行公式化表示，以便于应用机器学习并产生可操作和度量的结果 选择算法及如何应用：将原始数据转化为可预测的特征，选择合适的机器学习算法 过拟合：如果一个模型非常适合训练数据，很可能对新数据不能很好地作出预测 机器学习的发展 机器学习中的五大流派几十年来，人工智能研究者的各个「部落」一直以来都在彼此争夺主导权，现在这些部落开始联合起来，因为合作和算法融合是实现真正通用人工智能（AGI）的唯一方式。 学派 核心理念 主算法 灵感来源 符号学派 使用符号、规则和逻辑来表征知识和进行逻辑推理 规则/决策树 逻辑学 贝叶斯学派 获取发生的可能性来进行概率推理 贝叶斯/马尔科夫 统计学 连接学派 使用概率矩阵和加权神经元来动态地识别和归纳模式 反向传播/神经网络 神经科学 进化学派 生成变化，然后为特定目标获取其中最优的 基因编程 遗传学 类推学派 根据约束条件来优化函数（尽可能走到更高，但同时不要离开道路） 支持向量机 数学最优化 机器学习的历史演化阶段 时间 1980s 1990-2000 2000-2010s 主导学派 符号学派 贝叶斯学派 联结学派 架构 服务器或大型机 小型服务器集群 云服务器 主导理论 知识工程 概率论 神经科学和概率 优势 决策支持系统 可扩展的对比 在图像和语音识别、情感分析领域更加准确 机器学习的未来发展趋势 时间 Late 2020s 2020s+ 2040s+ 主流学派 连接学派+符号学派 联结主义+符号主义+贝叶斯+…… 算法融合 架构 云计算 云计算和雾计算 无处不在的服务器 主导理论 记忆神经网络、大规模集成、基于知识的推理 通过神经网络来感知、通过规则来决策 最佳组合的元学习 场景 简单问答系统 简单的感知-推理-行动 基于通过多种学习方式获得的知识或经验采取行动或做出回答 机器学习算法你应该使用哪种机器学习算法，这很大程度上依赖于可用数据的特征和数量以及你的训练目标。不要使用最复杂的算法，除非其结果值得付出昂贵的开销和资源。这里给出了一些最常见的算法，按使用简单程度排序。 机器学习算法集锦：从贝叶斯到深度学习及各自优缺点 经验之谈：如何为你的机器学习问题选择合适的算法？ 决策树（Decision Tree） 原理：典型的决策树分析使用分层变量或决策结点，在逐步决策过程中将一个样本进行分类。 优点：擅长对样本的特征进行评估 场景：基于规则的信用评估、扫码结果预测 扩展阅读： 《教程 | 从头开始：用Python实现决策树算法》 《想了解概率图模型？你要先理解图论的基本定义与形式》 支持向量机（Support Vector Machine） 原理：基于超平面（hyperplane），支持向量机可以对数据群进行分类。 优点：支持向量机擅长在变量 X 与其它变量之间进行二元分类操作，无论其关系是否是线性的 场景举例：新闻分类、手写识别。 扩展阅读：《干货 | 详解支持向量机（附学习资源）》 回归（Regression） 原理：回归可以勾画出因变量与一个或多个因变量之间的状态关系。在这个例子中，将垃圾邮件和非垃圾邮件进行了区分。 优点：回归可用于识别变量之间的连续关系，即便这个关系不是非常明显 场景举例：路面交通流量分析、邮件过滤 朴素贝叶斯（Naive Bayes Classification） 原理：朴素贝叶斯分类器用于计算可能条件的分支概率。每个独立的特征都是「朴素」或条件独立的，因此它们不会影响别的对象。 优点：对于在小数据集上有显著特征的相关对象，朴素贝叶斯方法可对其进行快速分类 场景举例：情感分析、消费者分类 隐马尔可夫模型（Hidden Markov model） 原理：显马尔可夫过程是完全确定性的——一个给定的状态经常会伴随另一个状态。交通信号灯就是一个例子。相反，隐马尔可夫模型通过分析可见数据来计算隐藏状态的发生。随后，借助隐藏状态分析，隐马尔可夫模型可以估计可能的未来观察模式。在本例中，高或低气压的概率（这是隐藏状态）可用于预测晴天、雨天、多云天的概率。 优点：容许数据的变化性，适用于识别（recognition）和预测操作 场景举例：面部表情分析、气象预测 随机森林（Random forest） 原理：随机森林算法通过使用多个带有随机选取的数据子集的树（tree）改善了决策树的精确性。本例在基因表达层面上考察了大量与乳腺癌复发相关的基因，并计算出复发风险。 优点：随机森林方法被证明对大规模数据集和存在大量且有时不相关特征的项（item）来说很有用 场景举例：用户流失分析、风险评估 扩展阅读：《教程 | 从头开始：用 Python 实现随机森林算法》 循环神经网络（Recurrent neural network） 原理：在任意神经网络中，每个神经元都通过 1 个或多个隐藏层来将很多输入转换成单个输出。循环神经网络（RNN）会将值进一步逐层传递，让逐层学习成为可能。换句话说，RNN 存在某种形式的记忆，允许先前的输出去影响后面的输入。 优点：循环神经网络在存在大量有序信息时具有预测能力 场景举例：图像分类与字幕添加、政治情感分析 长短期记忆（Long short-term memory，LSTM） 长短期记忆（Long short-term memory，LSTM）与门控循环单元神经网络（gated recurrent unit nerual network）：早期的 RNN 形式是会存在损耗的。尽管这些早期循环神经网络只允许留存少量的早期信息，新近的长短期记忆（LSTM）与门控循环单元（GRU）神经网络都有长期与短期的记忆。换句话说，这些新近的 RNN 拥有更好的控制记忆的能力，允许保留早先的值或是当有必要处理很多系列步骤时重置这些值，这避免了「梯度衰减」或逐层传递的值的最终 degradation。LSTM 与 GRU 网络使得我们可以使用被称为「门（gate）」的记忆模块或结构来控制记忆，这种门可以在合适的时候传递或重置值。 优点：长短期记忆和门控循环单元神经网络具备与其它循环神经网络一样的优点，但因为它们有更好的记忆能力，所以更常被使用 场景举例：自然语言处理、翻译 扩展阅读： 《深度 | LSTM 和递归网络基础教程》 《干货 | 图解 LSTM 神经网络架构及其 11 种变体（附论文）》 卷积神经网络（convolutional neural network） 原理：卷积是指来自后续层的权重的融合，可用于标记输出层。 优点：当存在非常大型的数据集、大量特征和复杂的分类任务时，卷积神经网络是非常有用的 场景举例：图像识别、文本转语音、药物发现 扩展阅读： 《专栏 | 卷积神经网络简介》 《从入门到精通：卷积神经网络初学者指南》 《解析深度卷积神经网络的 14 种设计模式》]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python-赋值-浅拷贝-深拷贝图例]]></title>
    <url>%2Funcategorized%2FPython%2Fpython%20%E5%9F%BA%E7%A1%80%EF%BC%9A%E8%B5%8B%E5%80%BC-%E6%B5%85%E6%8B%B7%E8%B4%9D-%E6%B7%B1%E6%8B%B7%E8%B4%9D%2F</url>
    <content type="text"><![CDATA[1import objgraph Python的对象模型Python中一切都是对象，而变量则是对对象的引用： 对象：分配的一块内存，有足够空间去表示他们所代表的值； 变量：是命名空间（字典）中的key，指向它所引用的对象； 引用：变量到对象的连接，以指针的形式实现； 关于变量-对象-引用之间的关系： 变量只能引用对象，绝不会引用其其他变量； 一个变量同一时刻只能引用一个对象，但一个对象同时间可以被多个变量引用； 容器对象可以连接到子对象； 本文用到的辅助工具： id(object): 返回对象的内存地址； a is b: 判断两个对象是否为同一对象； a == b: 判断两个对象是否等值； objgraph.show_refs(objects): 生成从对象objects开始的对象引用图例，参见文档； 赋值Python中变量定义、函数定义、函数传参、类定义、模块导入等操作本质上都是赋值操作，遵循同样的赋值逻辑：Python中的赋值只是创建引用，不会拷贝对象。 a = b C语言的赋值：”拷贝-写入”模式（拷贝b对象的值，写入a内存） python语言的赋值：“解引用-创建引用”模式（将变量b解引用为对象，创建变量a到对象b的引用） 12345b = &quot;hello&quot;a=bprint(id(a),id(b))print(a is b)objgraph.show_refs([a,b]) 4457019016 4457019016 True 1234b =&quot;no&quot;print(id(a),id(b))print(a is b)objgraph.show_refs([a,b]) 4457019016 4417389320 False 变量赋值、对象原地修改 变量赋值：对变量赋值，只是使该变量指向了新的对象； 1234a = [1,2,3]aa = aprint(id(a),id(aa))objgraph.show_refs([a,aa]) 4457160136 4457160136 123a = &#123;&#x27;a&#x27;:22,&#x27;b&#x27;:33&#125;print(id(a),id(aa))objgraph.show_refs([a,aa]) 4457132464 4457160136 原地修改：对容器对象进行原地修改，只是改变了子对象的引用，不改变容器对象的地址 1234a = [1,2,3]b = 1print(id(a),id(b))objgraph.show_refs([a,b]) 4457143432 4414302400 123a[0] = 11print(id(a),id(b))objgraph.show_refs([a,b]) 4457143432 4414302400 按照是否支持原地修改，Python中的内置数据类型可分为两大类： 可变类型：支持原地修改，如列表、字典； 不可变类型：不支持原地修改，如元组、字符串等列表、字典以外的内置类型； 共享引用共享引用：多个变量同时引用了同一个对象； 对其中一个变量赋值，不会影响到其他变量； 123a = b = c = [1,2]print(id(a),id(b),id(c))objgraph.show_refs([a,b,c]) 4454376648 4454376648 4454376648 123c = Noneprint(id(a),id(b),id(c))objgraph.show_refs([a,b,c]) 4454376648 4454376648 4414019688 对其中一个进行原地修改则会同时改变其他变量； 123b[0]=&#x27;change&#x27;print(id(a),id(b),id(c))objgraph.show_refs([a,b,c]) 4454376648 4454376648 4414019688 Python会缓存复用小的整数(-5到256)和字符串以提高效率，不同版本缓存范围不同，缓存字符串的行为令人费解，尽量不要在应用程序中使用这个特性； 123456789101112131415# 缓存整数范围：[-5~256]begin = Falsem = n= float(&#x27;-inf&#x27;)for i in range(-10000,10000): x = 0+i y = 0+i if (not begin) and id(x) == id(y): begin = True m = i if begin and id(x) != id(y): n = i - 1 breakprint(&#x27;整数缓存范围：[&#123;&#125;,&#123;&#125;]&#x27;.format(m,n)) 整数缓存范围：[-5,256] 12345# 缓存字符串长度范围：20以内x = &#x27;a&#x27; * 20y = &#x27;a&#x27; * 20print(x is y)objgraph.show_refs([x,y]) True 12345const = 20xx = &#x27;a&#x27; * constyy = &#x27;a&#x27; * constprint(xx is yy)objgraph.show_refs([xx,yy]) False 显式拷贝赋值不会发生拷贝，如果想要生成新的副本则需要显式地进行拷贝。 浅拷贝copy.copy(obj)通过拷贝生成一个新对象，新对象只拷贝了原对象的壳，但仍共享引用原对象的内容，也就是说新对象与原对象的id不同，但是新对象中的子对象与原始对象中的子对象id相同。 1234567import copya = [[1,2],[6,5]]b = copy.copy(a)print(a,b)print(id(a),id(b))print(id(a[0]),id(b[0]))objgraph.show_refs([a,b]) [[1, 2], [6, 5]] [[1, 2], [6, 5]] 4456953352 4457062536 4456953160 4456953160 对新旧对象的原地修改不会影响到另外一个对象 123a[1]=&#x27;ni&#x27;print(a,b)objgraph.show_refs([a,b]) [[1, 2], &#39;ni&#39;] [[1, 2], [6, 5]] 对新旧对象的属性和内容进行原地修改则会影响到另一对象 123a[0][0]=&#x27;aaa&#x27;print(a,b)objgraph.show_refs([a,b]) [[&#39;aaa&#39;, 2], [6, 5]] [[&#39;aaa&#39;, 2], &#39;ni&#39;] 深拷贝copy.deepcopy(obj)，通过“递归拷贝”原对象生成新对象，新对象与原始对象除了内容相同外没有任何联系。 123456a = [[1,2],[6,5]]b = copy.deepcopy(a)print(a,b)print(id(a),id(b))print(id(a[0]),id(b[0]))objgraph.show_refs([a,b]) [[1, 2], [6, 5]] [[1, 2], [6, 5]] 4457080328 4456172872 4457082376 4457081672 新旧对象互不影响 123b[1]=&#x27;ni&#x27;print(a,b)objgraph.show_refs([a,b]) [[1, 2], [6, 5]] [[1, 2], &#39;ni&#39;] 123a[0][0]=&#x27;aaa&#x27;print(a,b)objgraph.show_refs([a,b]) [[&#39;aaa&#39;, 2], [6, 5]] [[1, 2], &#39;ni&#39;] 隐式浅拷贝除了使用显式拷贝来生成原对象的副本外，需要特别注意的是，Python内置的一些方法会“隐式”地对原始对象进行浅拷贝，比如类型转换函数、列表切片、运算符重载的+ *，这往往会导致Python中某些看起来很奇怪的现象。 类型转换函数123456L = [[1],[2],[3]]T = tuple(L)print(L,T)print(id(L),id(T))print(id(L[0]),id(T[0]))objgraph.show_refs([L,T]) [[1], [2], [3]] ([1], [2], [3]) 4456981064 4457278776 4456898440 4456898440 123L[0]=99print(L,T)objgraph.show_refs([L,T]) [99, [2], [3]] ([9], [2], [3]) 123L[0][0]=9print(L,T)objgraph.show_refs([L,T]) [[9], [2], [3]] ([9], [2], [3]) 合并重复操作+ *用于列表，相当于浅拷贝了原始对象，产生多个副本，新副本中的每个子对象都还是原始对象中的子对象，任何对这些子对象的原地修改都会自动应用到所有新的副本中。 1234L=[0,1]R = L+Lprint(L,R)objgraph.show_refs([L,R]) [0, 1] [0, 1, 0, 1] 1234L=[0,1]R = L*3print(L,R)objgraph.show_refs([L,R]) [0, 1] [0, 1, 0, 1, 0, 1] 123456L=[0,1]J=[&#x27;a&#x27;,&#x27;b&#x27;]S=[L,J]R=S*3print(L,J,S,R)objgraph.show_refs([L,J,S,R]) [0, 1] [&#39;a&#39;, &#39;b&#39;] [[0, 1], [&#39;a&#39;, &#39;b&#39;]] [[0, 1], [&#39;a&#39;, &#39;b&#39;], [0, 1], [&#39;a&#39;, &#39;b&#39;], [0, 1], [&#39;a&#39;, &#39;b&#39;]] 123L[0]=99print(L,J,S,R)objgraph.show_refs([L,J,S,R]) [99, 1] [&#39;a&#39;, &#39;b&#39;] [[99, 1], [&#39;a&#39;, &#39;b&#39;]] [[99, 1], [&#39;a&#39;, &#39;b&#39;], [99, 1], [&#39;a&#39;, &#39;b&#39;], [99, 1], [&#39;a&#39;, &#39;b&#39;]] 切片切片虽然返回了新的对象，但是新对象中的子对象还是原始对象中的子对象 1234L=[[0],[1],[2],[3],[4],[5]]Q=L[1::2]print(L,Q)objgraph.show_refs([L,Q]) [[0], [1], [2], [3], [4], [5]] [[1], [3], [5]] 对新生成的对象原地修改并不会改变原对象 123Q[0]=&#x27;first&#x27;print(L,Q)objgraph.show_refs([L,Q]) [[0], [1], [2], [3], [4], [5]] [&#39;first&#39;, [3], [5]] 对新对象的元素进行原地修改会影响到原对象 123Q[1][0]=333print(L,Q)objgraph.show_refs([L,Q]) [[0], [1], [2], [333], [4], [5]] [&#39;first&#39;, [333], [5]] 拷贝不可变对象没有必要拷贝不可变对象，因为完全不用担心会不经意改动它们。如果对不可变对象进行拷贝操作，仍然会得到原对象 1234T=(1,2,3)F=copy.copy(T)print(T,F)objgraph.show_refs([T,F]) (1, 2, 3) (1, 2, 3) 1234T=([1],[2],[3])F=copy.copy(T)print(T,F)objgraph.show_refs([T,F]) ([1], [2], [3]) ([1], [2], [3]) 123T[1][0]=&#x27;ni&#x27;print(T,F)objgraph.show_refs([T,F]) ([1], [&#39;ni&#39;], [3]) ([1], [&#39;ni&#39;], [3]) 烤全羊下面的例子融合了Python中各种常见的引用关系： 12li = [1,&#x27;string&#x27;,(&#x27;tuple&#x27;,1),[&#x27;list&#x27;,3,(&#x27;gh&#x27;,4),&#123;&#x27;i&#x27;:&#x27;j&#x27;,5:&#x27;k&#x27;&#125;] ,&#123;&#x27;di&#x27;:2,3:&#x27;l&#x27;,(8,&#x27;m&#x27;):&#x27;n&#x27;,&#x27;list&#x27;:[&#x27;l&#x27;,&#x27;i&#x27;,&#x27;s&#x27;,&#x27;t&#x27;]&#125;] 1objgraph.show_refs([li], filename=&#x27;sample-graph.png&#x27;) 下面例子反应了在Python自动缓存较小整数时的引用关系 12345678import copya = [1,[2,3,4]]b = ac = copy.copy(a)d = copy.deepcopy(a)e = a+af = list(a)objgraph.show_refs([a,b,c,d,e,f])]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构与算法：动态规划]]></title>
    <url>%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%9A%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%2F</url>
    <content type="text"><![CDATA[理论篇动态规划问题满足以下两个条件的问题被称为动态规划问题： 具有最优子结构：问题的（最优）解可以由子问题的（最优）解推导出来，本质仍是递归。为了避免概念复杂化，我仍然使用”递推“概念的代替”最优子结构“或”状态转移方程“的概念； 具有重叠子问题：不同子问题包含了重复的子子问题； 以最简单的0-1背包问题为例： LintCode 92.0-1背包问题：在n个物品中挑选若干物品装入背包，最多能装多满？假设背包的大小为m，每个物品的大小为A[i]。 样例：4个物品[2, 3, 5, 7]，如果背包的大小为12，可以选择[2, 3, 7]装入背包，最多可以装满12的空间。 分析：每个物品只有放入或不放入两个状态，考虑以下具有一般意义的子问题：用i位置之前(包含i)的物品装入大小为k的背包最多能装下多少？我们用dp[k][i]来表示该子问题的解。通过讨论要不要将第i个物品放入，可以很容易得出dp的递推式：①如果不放入第i个物品，那么问题等价于将前i-1之前的物品放入k容量背包所能得到的最大解；②如果k&gt;=A[i]，那么可以将第i个物品放入背包，再加上i-1之前的物品放入剩下容量所能得到的最大解就是在这种情况下得到的最优解；如果k&lt;A[i]那么无法将第i个物品放入容量为k的背包，问题转化为第一种情况；可以形式化地用以下状态转移方程来表示子问题的最优解间的递推关系： \left\{\begin{matrix} \begin{align*} &dp[k][i]=max(dp[k][i-1], dp[k-A[i]][i-1]+A[i]) ,& \ if\ k\geqslant A[i]\ i\geqslant 1 \\ &dp[k][i]=dp[k][i-1] ,&if\ k]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构与算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构与算法：图（三）—— DFS]]></title>
    <url>%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%9A%E5%9B%BE%EF%BC%88%E4%B8%89%EF%BC%89%E2%80%94%E2%80%94%20DFS%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构与算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构与算法：图（五）—— 欧拉路径]]></title>
    <url>%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%9A%E5%9B%BE%EF%BC%88%E4%BA%94%EF%BC%89%E2%80%94%E2%80%94%20%E6%AC%A7%E6%8B%89%E8%B7%AF%E5%BE%84%2F</url>
    <content type="text"><![CDATA[术语 欧拉路径(Eulerian trail)：图中恰好将所有边都访问一次的路径； 欧拉回路(Eulerian cycle)：开始节点和结束节点相同的欧拉路径，也称欧拉环； 欧拉图：具有欧拉回路的图； 半欧拉图：有欧拉路径但没有欧拉环的图； 存在性定理（度判定）对于无向图：奇度个数为0或2！！ 欧拉图的充要条件：奇度节点个数为0； 半欧拉图的充要条件：奇度节点个数为2； 存在欧拉路径的充要条件：奇度节点个数为0或2； 对于有向图：出入度相等或各有一个为+1和-1！！(假设出度记为正，入度记为负，出度入度和记做节点的度) 欧拉图的充要条件：所有节点出度等于入度，即所有节点度为0； 半欧拉图的充要条件：存在一个节点度为1，另一个节点度为-1，其余节点度为0； 存在欧拉路径的充要条件：所有节点度为0，或者只有两个节点不为0，且其中一个节点度为1另一个节点度为-1； 获取欧拉路径 问题：判断给定图中是否存在欧拉路径，如果存在则返回其中一条欧拉路径 分析：获取欧拉路径的一般思路： 根据存在性条件判断图中是否存在欧拉路径，并获取遍历入口； 如果是欧拉图，遍历可以从任意节点出发； 如果是半欧拉图，遍历必须从例外的两个节点出发； 调用以下算法，返回欧拉路径 Fleury算法：算法思想：$v0$为遍历起点，令$P_0=v_0$；设$P_i=v_0e_1v_1e_2…e_i$ $v_i$已经访问，按下面方法从中选取$e{i+1}$： 1）$e{i+1}$与$v_i$相关联；2）只有当无别的边可供访问时，才可以选择Gi=G-{e1,e2, …, ei}中的桥（所谓桥是一条删除后使连通图不再连通的边）作为$e{i+1}$；3）当（2）不能再进行时，算法停止； 解释：我们使用栈来存储当前行走中的路径，假设当前栈顶为[…,u,v]，如果栈顶节点v还有未访问的边，我们就选择其中一条边的顶点w进行深度遍历并将遍历到的顶点压入栈中，同时删除经过的边，如果栈顶节点v所有边都已被访问，说明v的后续分量中的边都已被访问，边(u,v)就变成了连接v的后续分量的桥（没办法回来了），直接将v出栈输出，此时还要查看顶点u是否有别的未访问的边，只有当u其余未访问的边都被输出之后（必然会再次回到u，因为v后续分量必然含有奇度节点，初始节点也是奇度节点，如果不回到u，说明u后还有其他奇度节点，矛盾），才能输出(u,v)边，这样最后输出的顶点逆序就是一个欧拉路径（欧拉回路的证明更简单，略）。 算法实现： 123456789101112131415161718192021222324252627282930def fleury(graph): start = None num = 0 for k in graph: if len(graph(k))&amp;1: num += 1 start = k if num not in [0,2]: return False def dfs(v): S.append(v) while graph[v]: # 如果有边，过河拆桥，继续扩展 u = graph[v].pop() graph[u].remove(v) dfs(u) break S = [start] res = [] while S: top = S.pop() if graph[top]: # 如果栈顶有边，则继续扩展下去 dfs(top) else: # 如果栈顶无边，说明栈顶与次栈顶之间的边是桥 res.append(top) return res[::-1] 算法分析：由于算法要经过每条边，所以时间必然是Ω(E)。在最坏情况下，在每个节点处进行一次 DFS，节点会重复走所以以边计算，算法复杂度应该是 O(E(E+V)) Hierholzer算法 算法思想（以半欧拉图为例）：以一个奇度节点出发进行深度遍历，遍历的同时将经过的边删除（过河拆桥），如果某个节点不能继续遍历，则输出该节点。当遍历到u节点时，从该节点出发的深度遍历有两种结果： 1）要么重新返回该节点（子环），如果该节点还有其他未访问的边，继续沿着该边深度遍历，仍有两种遍历结果，如果该顶点没有其他未访问的边，则可将环中的顶点输出 2）要么在某个节点w处无法继续遍历，此时其所有邻接边都已访问，假设到达w的边为(v,w),此时(v-w)必然是w后分量的桥，因为u到w遍历过程必然包含了奇度节点，所以其他的遍历过程必然回到出发点，这就允许我们先从v开始得到在其他环中的欧拉路径后，再回到v，输出(v,w)边，输出w，最终得到的序列逆序即为一条欧拉路径。 算法实现：有边过河拆桥，无边输出 12345678910111213141516171819def hierholzer(graph): start = 0 num = 0 for k in graph: if len(graph(k))&amp;1: num += 1 start = k if num not in [0,2]: return False route = [] # Hierholzer算法的核心只有三行代码：有边就过河拆桥，无边就输出 def dfs(u): while graph[u]: dfs(graph[u].pop()) route.append(u) dfs(start) return route[::-1] 算法分析：时间复杂度是 O(E)，其在 DFS 的过程中不用恢复边，靠出栈记录轨。代码简洁，效率又高！！]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构与算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构与算法：图（四）—— 最小生成树与最短路径]]></title>
    <url>%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%9A%E5%9B%BE%EF%BC%88%E5%9B%9B%EF%BC%89%E2%80%94%E2%80%94%20%E6%9C%80%E5%B0%8F%E7%94%9F%E6%88%90%E6%A0%91%E4%B8%8E%E6%9C%80%E7%9F%AD%E8%B7%AF%E5%BE%84%2F</url>
    <content type="text"><![CDATA[理论篇本文用到的实例： 对节点进行编码，定义图的数据结构： 1234V_node = [&#x27;A&#x27;,&#x27;B&#x27;,&#x27;C&#x27;,&#x27;D&#x27;,&#x27;E&#x27;,&#x27;F&#x27;]V = range(6)G = &#123;0:&#123;1:6,4:5,5:1&#125;,1:&#123;0:6,2:3,5:2&#125;,2:&#123;1:3,3:7,5:8&#125;, 3:&#123;2:7,4:2,5:4&#125;,4:&#123;3:2,0:5,5:9&#125;,5:&#123;0:1,1:2,2:8,3:4,4:9&#125;&#125; 最小生成树连通图的最小生成树：边的权值最小的生成树，最小生成树不唯一，但最小生成树中权值之和唯一。 求解最小生成树的算法依赖于如下性质：如果U是V的一个子集，且(u,v)是连接U与V-U的最小权值边，则必存在一棵包含(u,v)的最小生成树。 Prim算法——双边袋法 问题：给定连通图G，找到一棵最小生成树 思路：prim算法和Dijkstra算法非常类似，同样可以用双袋法求解，不同之处在于这里的袋子存放的是边： 树边袋：存储的是已纳入最小生成树的边 图边袋：存储的是剩余顶点到树袋中所有顶点的最小边的权值 更新规则：每次从图袋中选取一个最小边(u,v)，移至图袋，用该边的尾部节点更新图袋 1234567输入：邻接表示的图G，列表表示的顶点V输出：最小生成树1. 初始化两个袋子： 1. 树边袋：初始化为空 2. 图边袋：随机选取一个顶点，其余顶点与该顶点有边则记图边长度为权值，否则记图边长度为无穷2. 移袋-更新：每次从图袋中选取一个边长最小的边(u,v)，移至树边袋，用顶点v更新图边袋中剩余的边(x,y)，如果d[v][y] &lt; d[x][y]，则用边(v,y)代替边(x,y)3. 当图边袋为空时，结束迭代，树边袋即为所求最小生成树 代码：为优化查找更新效率，图边袋用(u,v,w)的倒查表实现{v:(u,w)} 12345678910111213141516171819202122232425def Prim(G,V): # 1. 初始化树边袋和图边袋 t_bag = &#123;&#125; g_bag = &#123;v:(V[0],G[V[0]][v] if v in G[V[0]] else float(&#x27;inf&#x27;)) for v in V if v != V[0]&#125; # 2. 移动-更新 while g_bag: min_w = float(&#x27;inf&#x27;) u,v = (None,None) for y,(x,w) in g_bag.items(): if w &lt;= min_w: min_w = w u,v = x,y t_bag[(u,v)] = min_w del g_bag[v] for vv,w in G[v].items(): if vv in g_bag and w &lt; g_bag[vv][-1]: g_bag[vv] = (v,w) return t_bag Prim(G,V)&#123;(0, 5): 1, (1, 2): 3, (3, 4): 2, (5, 1): 2, (5, 3): 4&#125; 分析： 时间复杂度：$O(V^2)$，与边无关，适用于稠密图 空间复杂度：$O(V)$ 最短路径 BFS：用于求解无权图的单源最短路径 Dijkstra：用于求解非负权图的单源最短路径(无权图可以看做是一种权值为1的特例) Floyd：求解有权图中任意两个顶点间的最短路径 最短路径的重要性质：两点之间的最短路径也包含了路径上其他顶点间的最短路径。 BFS——逐层遍历，记录层数BFS用于求解无权图的单源最短路径，详情参见BFS。 Dijkstra——双点袋法，移动更新 问题：给定一个各边权值皆为非负数的图，求源顶点到图中各个顶点的最短路径； 思路：Dijkstra算法可以简单通过“双点袋法”来实现： 12345678输入：邻接表示的图G，列表表示的顶点V，源节点s输出：源顶点s到各顶点的最短路径，各顶点在最短路径中的父节点1. 初始化两个袋子和父亲字典： 1. 树点袋：装有已求出顶点的最短路径长度。初始时只含源节点的最短路径长度为0 2. 图点袋：装有剩余顶点的当前路径长度。初始时包含除源节点外的所有顶点到源节点的当前路径长度，如果为源节点的邻接点，最短路径为对应边的权值，否则即为无穷 3. 父亲字典：初始时假设所有顶点的父节点均为源节点(和图袋中当前路径对应)2. 移袋-更新：每次从图袋中选取当前路径长度最小的顶点u移动至树袋，并用该节点更新图袋中剩余顶点的当前最短路径。更新规则为，如果该节点的邻接点v在图袋中，且源节点通过u到达v的路径更短d[u]+d[u][v]&lt;d[v]，则用这条更短的路径长度更新图袋中现有的源节点到顶点v的路径长度，同时更新v的父节点为u3. 直至图点袋为空时，结束迭代，此时树袋中保存了源节点到所有顶点的最短路径长度，同时父亲字典描述了一棵“单源最短路径生成树”，注意“单源最短路径生成树”并不是“最小生成树”(V：1，2，3，4，E：(1,2)=3，(2,4)=4，(1,3)=1，(3,4)=5，从1出发得到最短路径生成树是&#123;(1,2)，(1,3)，(3,4)&#125;，但最小生成树是&#123;(1,2)，(1,3)，(2,4)&#125;) 代码： 1234567891011121314151617181920212223242526272829303132def Dijkstra(G, V, s): &quot;&quot;&quot; @G:图的邻接表示,&#123;u:&#123;v:w,...&#125;,...&#125; @V:顶点集 @s:原顶点 @return:所有节点的单源最短路径，父亲字典 &quot;&quot;&quot; # 1. 初始化双袋和父亲字典 n = len(V) t_bag = &#123;s:0&#125; g_bag = &#123;v:(G[s][v] if v in G[s] else float(&#x27;inf&#x27;)) for v in V if v != s&#125; father = &#123;v:s for v in V&#125; # 2. 移袋-更新 while g_bag: min_v = None min_w = float(&#x27;inf&#x27;) for v, w in g_bag.items(): if w &lt;= min_w: min_v, min_w = v, w t_bag[min_v] = g_bag.pop(min_v) for vv, ww in G[min_v].items(): if vv in g_bag and min_w + ww &lt; g_bag[vv]: g_bag[vv] = min_w + ww father[vv] = min_v return t_bag,fatherDijkstra(G,V,0)(&#123;0: 0, 1: 3, 2: 6, 3: 5, 4: 5, 5: 1&#125;, &#123;0: 0, 1: 5, 2: 1, 3: 5, 4: 0, 5: 0&#125;) 分析： 适用条件：权值非负的有向图或无向图 时间复杂度：$O(V^2)$ 空间复杂度：$O(V)$ Dijkstra算法是本文最重要的算法，因为： 如果各边权值为1，则Dijkstra算法可以代替BFS求解无权图的单源最短路径问题; 只需对Dijkstra算法稍加修改，就变成了求解最小生成树的Prime算法； 对每个顶点应用Dijkstra算法，可以得到任意两个顶点间的最短距离，时间复杂度同样为$O(V^3)$; Floyd——n 次绕行，更新矩阵 问题：给定一个不包含含有负权值回路的图，求任意两个顶点间的最短路径长度 思路：矩阵绕行各个顶点 12345输入：图G，顶点V输出：各顶点间最短路径长度的矩阵M1. 初始化距离矩阵M：如果uv之间有边，M[u][v]=w，否则M[u][v]=无穷2. n次绕行更新M：迭代n次，每次M中任意两个顶点u,v通过绕行k顶点(k=0~n-1)更新M[u][v]，M[u][v]=min(M[u][v],M[u][k]+M[k][v])3. 返回最终的M 代码： 123456789101112131415161718192021222324252627282930313233def Floyd(G,V): &quot;&quot;&quot; @G:邻接表&#123;i:&#123;j:w,...&#125;,...&#125; @V:邻接点 @return:最短距离矩阵 &quot;&quot;&quot; # 初始化M n = len(V) inf = float(&#x27;inf&#x27;) M = [[inf] * n for _ in xrange(n)] for i in xrange(n): for j in xrange(n): if i == j: M[i][j] = 0 elif i in G and j in G[i]: M[i][j] = G[i][j] # 绕行更新M for k in xrange(n): for i in xrange(n): for j in xrange(n): M[i][j] = min(M[i][j], M[i][k] + M[k][j]) return MFloyd(G,V)[[0, 3, 6, 5, 5, 1], [3, 0, 3, 6, 8, 2], [6, 3, 0, 7, 9, 5], [5, 6, 7, 0, 2, 4], [5, 8, 9, 2, 0, 6], [1, 2, 5, 4, 6, 0]] # 第一行与Dijkstra求出的单源最短路径是一致的 分析： 使用条件：允许图中有负权值，但不允许含含有负权值回路 时间复杂度:$O(V^3)$ 空间复杂度：$O(V^2)$ 实战篇[LeetCode 743. 网络延迟时间] 问题： 123456789101112有 N 个网络节点，标记为 1 到 N。给定一个列表 times，表示信号经过有向边的传递时间。 times[i] = (u, v, w)，其中 u 是源节点，v 是目标节点， w 是一个信号从源节点传递到目标节点的时间。现在，我们向当前的节点 K 发送了一个信号。需要多久才能使所有节点都收到信号？如果不能使所有节点收到信号，返回 -1。注意:N 的范围在 [1, 100] 之间。K 的范围在 [1, N] 之间。times 的长度在 [1, 6000] 之间。所有的边 times[i] = (u, v, w) 都有 1 &lt;= u, v &lt;= N 且 1 &lt;= w &lt;= 100。 代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748def networkDelayTime(self, times, N, K): &quot;&quot;&quot; :type times: List[List[int]] :type N: int :type K: int :rtype: int &quot;&quot;&quot; # 将边表转化为邻接表 adj = collections.defaultdict(dict) for u,v,w in times: adj[u][v] = w # 双袋法 # 1. 初始化：初始化两个袋子，树袋装有已求出单源最短路径的顶点，图袋装有剩余顶点 t_bag = &#123;&#125; g_bag = &#123;&#125; father = &#123;&#125; count = 0 for i in xrange(1,N+1): if i == K: t_bag[i] = 0 father[i] = None count += 1 elif i in adj[K]: g_bag[i] = adj[K][i] father[i] = K else: g_bag[i] = float(&#x27;inf&#x27;) father[i] = K # 2. 移袋更新：每次从图袋中选取一个单源距离最小的顶点放入树袋，并用该顶点更新图袋中所有剩余顶点的单源距离 while count &lt; N: min_v = None min_w = float(&#x27;inf&#x27;) for v, w in g_bag.items(): if w &lt;= min_w: min_w = w min_v = v t_bag[min_v] = g_bag.pop(min_v) count += 1 for vv, ww in adj[min_v].items(): if vv in g_bag and min_w + ww &lt; g_bag[vv]: g_bag[vv] = min_w + ww father[vv] = min_v res = max(t_bag.values()) return res if res != float(&#x27;inf&#x27;) else -1]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构与算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构与算法：排序算法]]></title>
    <url>%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%9A%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[排序就是使列表元素按照关键字递增或递减排列的过程。排序算法的时间复杂度一般是由比较和移动的次数决定的。(待补充计数排序、桶排序代码、排序基本概念、外排序等) 插入排序(Insertion Sort)基本思想：每次将一个待排元素插入到前面已排序列中，直至全部插入。 直接插入排序 思路：从1到n-1依次将待排元素插入到前面已排序列中，每次插入时，从后向前遍历已排序列，如果大于待排元素则后移一位，如果不大于待排元素或达到首部则插入待排元素。 实现： 1234567def insert_sort(A): for i in xrange(1,len(A)): cur,j = A[i],i-1 while j&gt;=0 and A[j]&gt;cur: A[j+1] = A[j] j -= 1 A[j+1] = cur 分析： 时间复杂度： 最好情况，元素已有序，仅需比较n次，无需移动，O(n); 最坏情况，元素反序，O(n ^ 2) 空间复杂度：O(1) 稳定性：每次只会交换相邻元素，稳定 适用：顺序表和链表 折半插入排序 思路：折半插入是直接插入的一个变种，在每次插入新的待排元素时，先使用二分查找在前面有序序列中找到插入位置，再统一移动插入位置到旧位置间的元素，最后插入待排元素 实现： 12345678def binsert_sort(A): for i in xrange(1,len(A)): l,r = 0,i-1 while l &lt;= r: mid = (l+r)/2 if A[i] &lt; A[mid]: r = mid - 1 else: l = mid + 1 A[r+2:i+1],A[r+1] = A[r+1:i],A[i] 分析： 时间复杂度：只减少了比较次数O(lgn)但并没有改变移动次数，仍为O(n^2) 空间复杂度：O(1) 稳定性：不改变相等元素的原始相对位置，稳定 适用：顺序表 希尔(插入)排序(shell sort)直接插入排序在规模较小或序列基本有序时效率较高，希尔排序正是先将序列减小规模，基本有序来提高直接插入排序的效率。 思想：按步长从n/2到1，反复进行直接插入排序。 实现： 1234567891011def shell_sort(A): n = len(A) d = n/2 while d: for i in xrange(d,n): cur,j = A[i],i-d while j&gt;=0 and A[j]&gt;cur: A[j+d] = A[j] j -= d A[j+d] = cur d = d/2 分析： 时间复杂度：平均O(n^1.3)，最坏O(n ^ 2) 空间复杂度：O(1) 稳定性：当两个相同的元素被分到不同子组时顺序可能改变，不稳定 适用：顺序表 交换排序思想：根据序列中两个元素的比较结果来交换它们的位置。 冒泡排序(bubble sort) 思想：从前向后两两比较相邻元素，若逆序则交换；每轮冒泡都将当前最大元素排到最终位置（该元素不再参与下一轮排序），某轮冒泡不再发生交换或n轮冒泡后则排序成功。 实现： 123456789def bubble_sort(A): n = len(A) for i in xrange(n-1,0,-1): flag = False for j in xrange(i): if A[j] &gt; A[j+1]: A[j],A[j+1] = A[j+1],A[j] flag = True if not flag:return 分析 时间复杂度： 最好情况下，初始有序，只比较n-1次，交换0次，O(n)； 最坏情况下，初始逆序，O(n^2)； 空间复杂度：O(1) 稳定性：相等两元素不进行交换，稳定 快速排序（quick sort） 思想：基于分治和递归的思想，任选一个元素作为枢轴点，将所有小于等于它的元素都放在它前面，所有大于它的元素都放在后面，将枢轴点放在其最终位置，然后再递归排序枢轴点左右两侧子序列。 基本实现思路：首元素作为枢轴点 ① 分解： 选取A[0]作为枢轴值，将小于(等于)它的元素放在1~k，然后交换A[0]与A[k]，k是枢轴值最终位置 过程使用口袋双指针法，j指针指向当前遍历位置(1~n-1)，i指针指向已存储的小于等于枢轴值的最后一个元素位置(1 ~ k)，如果A[j] &lt;= A[0]就把A[j]与A[i+1]交换 ② 求解：递归求解0 ~ k-1，和k+1 ~ n-1子问题 改进思路：随机快排，事实证明如果每次选取首元素作为枢轴点，很容易达到最坏情形，时间复杂度变为O(n^2)，改进思路也很简单，只需要在每次数组划分时随机选择一个元素与首元素进行交换，其余代码即可保持不变。实际使用的都是随机快排，因为首元素快排性能实在是太差了!!！以LeetCode 215题为例，是40ms与1000ms的差距。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061import randomdef partition1(A,low,high): &quot;&quot;&quot; 选取A[low:high+1]的首元素A[low]作为枢轴，通过口袋指针法对数组进行划分 &quot;&quot;&quot; bag = low for j in xrange(low+1,high+1): if A[j] &lt;= A[low]: bag += 1 A[bag],A[j] = A[j],A[bag] A[bag],A[low] = A[low],A[bag] return bag def partition2(A,low,high): &quot;&quot;&quot; 随机选取A[low:high+1]中的元素，与A[low]交换，再进行划分 &quot;&quot;&quot; rand = random.randint(low,high) A[rand],A[low] = A[low],A[rand] bag = low for j in xrange(low+1,high+1): if A[j] &lt;= A[low]: bag += 1 A[bag],A[j] = A[j],A[bag] A[bag],A[low] = A[low],A[bag] return bag def quicksort1(A,low,high): &quot;&quot;&quot; 原地+递归实现 &quot;&quot;&quot; if low &lt; high: pivot = partition(A,low,high) quiksort(A,low,pivot-1) quiksort(A,pivot+1,high)def quicksort2(A,low,high): &quot;&quot;&quot; 原地+非递归实现：使用栈模拟递归过程 &quot;&quot;&quot; if low &gt;= high:return s = [[low,high]] while s: l,h = s.pop() pivot = partition(A,l,h) # 分解 if pivot + 1 &lt; h:s.append([pivot+1,h]) if pivot - 1 &gt; l:s.append([l,pivot-1]) def quicksort(A): &quot;&quot;&quot; 非原地实现 &quot;&quot;&quot; n = len(A) if n &lt; 2: return A rand = random.randint(0,n-1) A[0],A[rand] = A[rand], A[0] l = [x for x in A[1:] if x &lt;= A[0]] r = [x for x in A[1:] if x &gt; A[0]]return quiksort(l) + [A[0]] + quiksort(r) 分析：使用递归树来分析 时间复杂度：递归深度×O(n)。递归深度和每次划分是否对称有关，最坏情况下初始有序或逆序，递归深度为n，时间复杂度为O(n^2)；最好情况下每次都进行均分，递归深度为lgn，时间复杂度为O(nlgn)；平均情况下，快排接近于最佳情况，为O(nlgn)。快速排序是内部排序中平均性能最好的排序算法。 空间复杂度：递归深度,最坏O(n),最好O(lgn),平均O(lgn) 稳定性：存在跨距交换，不稳定 改进： 规模较小时不再继续递归调用，采取直接插入排序 合理选取枢轴点：每次随机选取枢轴点或取头、尾、中间的中间值(将其与首元素交换，这样就不用修改以上代码)，这样最坏情况在实际中就几乎不会发生了。 选择排序每次从当前待排序列中选择最小元素将其交换到首位，n-1轮完成排序。 简单选择排序(selection sort) 思路：每轮从A[i:]中选出最小元素与A[i]交换，i= 0…n-2 实现： 1234567def SelectSort(A): n = len(A) for i in xrange(n-1): min = i for j in xrange(i+1,n): if A[j] &lt; A[i]:min = j A[i],A[min] = A[min],A[i] 分析 时间复杂度：元素移动次数最好情况是0，最坏情况是O(n)，元素比较次数和序列初始状态无关为n(n-1)/2;所以时间复杂度最好最坏都是O(n^2) 空间复杂度：O(1) 稳定性：存在跨距交换，不稳定 堆排序(heap sort) 堆的定义：序列A[1,2…n]被称为堆，当且仅当该序列满足 ① $A[i] &lt;= A[2i]$ 且 $A[i] &lt;= A[2i+1]，i\in [1,n//2]$,小根堆 ② $A[i] &gt;= A[2i]$ 且 $A[i] &gt;= A[2i+1]，i\in [1,n//2]$,大根堆 堆与完全二叉树：将序列A看做是一颗完全二叉树，树的根节点是A[1]，给定一个节点的下标i，A[i/2]是它的父亲节点，A[2i]是它的左孩子，A[2i+1]是它的右孩子 向下调整算法向下调整算法:假设节点i的左右子树已是大根堆，向下调整i节点以使以i为根的子树也成为大根堆 找到左右孩子中最大的，与父亲比较 如果大于父亲，则父亲与孩子中较大的交换，然后递归向下调整较大子节点 如果不大于父亲，则说明以i为根的子树已经是大根堆了 123456789101112# 堆以下标1开始，0分量可用于存储当前堆长度heap-sizedef heapify(A,i,n): # i:将元素i向下调整 # n:当前堆中元素个数 l,r = 2*i,2*i+1 if l &lt;= n: largest = r if l&lt;n and A[r] &gt; A[l] else l else: largest = i if A[i] != A[largest]: A[i],A[largest] = A[largest],A[i] heapify(A,largest,n) 分析：时间复杂度：O(h),h为堆的高度 建堆从n/2到1，依次向下调整节点 1234def Build_max_heap(A): A[0] = len(A)-1 # 使用A[0]存储当前堆容量 for i in xrange(A[0]//2,0,-1): heapify(A,i,A[0]) 分析：建堆的时间复杂度为O(n) 堆排序堆排序:每轮交换堆顶和堆底元素，将堆长度减1，堆顶向下调整，n-1轮后堆中只剩一个元素，排序完成。 12345def heapsort(A): build_max_heap(A) # 构建堆 for i in xrange(len(A)-1,0,-1): # 堆排序：交换堆顶和堆底，堆长度-1，向下调整堆顶 A[1],A[i] = A[i],A[1] heapify(A,1,i-1) 分析： 时间复杂度：建堆过程花费O(n)，每次向下调整花费O(lgn)，排序过程需要交换、调整n-1次，所以堆排序时间复杂度为O(nlgn) 空间复杂度：O(1) 稳定性：不稳定 归并排序(merge sort) 思路：二路归并排序基于分治和递归的思想，首先将序列分解为左右两个子序列，分别对左右子序列进行归并排序，再归并左右两个已排序序列 实现： 1234567891011121314151617181920def merge_sort(A,low,high): if low &lt; high: mid = (low+high)/2 # 分解 merge_sort(A,low,mid) # 解决 merge_sort(A,mid+1,high) merge(A,low,mid,high) # 合并def merge(A,low,mid,high): i,j,k=low,mid+1,low B = A[:] # 使用辅助数组B对A进行备份 while i&lt;=mid and j&lt;=high: if B[i] &lt; B[j]: # 比较左右序列，将小的放入B A[k] = B[i] i += 1 else: A[k] = B[j] j += 1 k += 1 if i &lt;= mid:A[k:high+1]=B[i:mid+1] # 前半段还有剩余，则直接添加到k后面 if j &lt;= high:A[k:high+1]=B[j:high+1] 分析 时间复杂度：一趟归并需要O(n)，总共需要lgn趟归并，所以算法时间复杂度为O(nlgn) 空间复杂度：merge操作中，辅助空间刚好占用n个单元，O(n) 稳定性：merge操作不会改变相同关键字的顺序，稳定 基数排序(Radix sort) 概念：三围n、d、r n：待排序序列中节点个数（有多少个待排序数字） d：每个节点所包含的关键字个数（每个数最大位数） r：基数，每个关键字有多少种取值（桶个数） 低位优先排序（LSD）：从低位开始排序 高位优先排序（马上到！）：从高位开始排序 思路：低位优先排序，从低位到高位需进行d轮排序，每轮排序包括了分配和收集两个过程： 分配：按照各节点在该位上的关键字值将其分配到对应的队列中去 收集：将各队列中的节点依次首尾相连，得到新的节点序列 实现： 12345678910111213def radix_sort(A): &quot;&quot;&quot;A为正整数序列&quot;&quot;&quot; r = 10 # 基数为10 d = len(str(max(A))) # 整数最大位数 bucket = [[] for i in xrange(r)] # 初始化十个队列 for i in xrange(d): # 需要d轮分配、组合 # 分配：如果整数这一位是i，则将其分配到bucket[i] for num in A:bucket[num%(r**(i+1))/(r**i)].append(num) # 收集： del A[:] for q in bucket: A.extend(q) bucket = [[] for i in xrange(r)] 分析： 时间复杂度： 总共需d轮分配收集，每趟分配需O(n)，收集需O(r)，O(d(n+r)) 空间复杂度：需要r个队列，O(r) 稳定性：对基数排序而言很重要一点就是按位排序时必须是稳定的 桶排序 思路：分桶-桶内排序-收集 假设待排序的一组数统一的分布在一个范围中，并将这一范围划分成几个子范围，也就是桶; 将待排序的一组数，分档规入这些子桶，并将桶中的数据进行排序; 将各个桶中的数据有序的合并起来; 分析：桶排序的平均时间复杂度为线性的O(N+C)，其中C=N*(logN-logM)。如果相对于同样的N，桶数量M越大，其效率越高，最好的时间复杂度达到O(N)。 当然桶排序的空间复杂度 为O(N+M)，如果输入数据非常庞大，而桶的数量也非常多，则空间代价无疑是昂贵的 常见排序算法比较 参考 排序算法可视化 十大经典排序算法PHP实现教程 十大经典排序算法最强总结（含Java代码实现)]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构与算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构与算法：算法分析]]></title>
    <url>%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%9A%E7%AE%97%E6%B3%95%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[渐进表示法虽然有时我们能够确定一个算法的精确运行时间，但通常并不值得花力气来计算它以获得额外的精度，对于足够大的输入，精确运行时间中的倍增常量和低阶项被输入规模本身的影响所支配。 当输入规模足够大，使得只与运行时间的增长量级有关时，需要研究算法的渐进效率。通常渐进地更有效的某峰算法对除很小的输入外的所有情况将是最好的选择。 渐进记号渐进记号要求每个成员都渐进非负，以下c c1 c2 均为正数常量。 $\Theta (g(n))$是以$g(n)$为渐进确界的所有函数集合 1\Theta (g(n)) = \left \&#123; f(n) | c_&#123;1&#125;\leqslant \lim_&#123;n \to \infty &#125; \frac&#123;f(n)&#125;&#123;g(n)&#125; \leqslant c_&#123;2&#125; \right \&#125; $O (g(n))$是以$g(n)$为渐进上界的所有函数集合 1O (g(n)) = \left \&#123; f(n) | \lim_&#123;n \to \infty &#125; \frac&#123;f(n)&#125;&#123;g(n)&#125; \leqslant c \right \&#125; $\Omega (g(n))$是以$g(n)$为渐进下界的所有函数集合 1\Omega (g(n)) = \left \&#123; f(n) | c \leqslant \lim_&#123;n \to \infty &#125; \frac&#123;f(n)&#125;&#123;g(n)&#125; \right \&#125; 三者的关系：$f(n) = \Theta (g(n)))\Leftrightarrow f(n) = O(g(n))$且$f(n) = \Omega (g(n))$ 等式中的渐进记号等式中渐进记号的不同含义 $f(n) = O (g(n))$：表示$f(n)$是集合$O (g(n))$的成员，等价于$f(n) \in O (g(n))$，读作“$f(n) $以 $g(n)$为上界”。 $2n^{2} + 3n + O(n^{2}) = O(n^{2})$：表示对于等号左侧任意的匿名函数，等号右侧总存在某个匿名函数使等号成立 渐进记号运算律实数的许多性质也适用于渐进记号，通过简单的类比实数的不等式性质可以轻松推测出对应渐进记号类似的性质： 基本类比 f(n) = \Theta (g(n)) \Leftrightarrow a = b f(n) = O (g(n)) \Leftrightarrow a \leqslant b f(n) = \Omega (g(n)) \Leftrightarrow a \geqslant b 传递性 a = b, b = c \Rightarrow a = cf(n) = \Theta (g(n)),g(n) = \Theta (h(n)) \Rightarrow f(n) = \Theta (h(n))a \leqslant b, b \leqslant c \Rightarrow a \leqslant cf(n) = O (g(n))) , g(n) = O (h(n)) \Rightarrow f(n) = O (h(n))a \geqslant b, b \geqslant c \Rightarrow a \geqslant cf(n) = \Omega (g(n))), g(n) = \Omega (h(n)) \Rightarrow f(n) = \Omega (h(n)) 对称性 a =b\Leftrightarrow b=af(n) = \Theta (g(n)) \Leftrightarrow g(n) = \Theta (f(n))a \leqslant b\Leftrightarrow b\geqslant af(n) = O (g(n)) \Leftrightarrow g(n) = \Omega (f(n))a \geqslant b\Leftrightarrow b\leqslant af(n) = \Omega(g(n)) \Leftrightarrow g(n) = O (f(n)) 加法法则： T(n) = T_{1}(n) +T_{2}(n) = O(f(n)) + O(g(n)) = O(max(f(n),g(n))) 乘法法则： T(n) = T_{1}(n) \times T_{2}(n) = O(f(n)) \times O(g(n)) = O(max(f(n)\times g(n)))常见渐进函数 一般渐进函数排序 O(1) < O(lgn) < O(n) < O(nlgn) < O(n^{2}) < O(2^{n}) < O(n!) < O(n^{n}) 特殊渐进函数： lg(n!) = \Theta (nlgn) (lgn)^{b} = o(n^{a})时间复杂度和空间复杂度 计算机也许是快的，但它们不是无限快；存储器也许是廉价的，但不是免费的。所以计算时间是一种有限资源，存储器中的空间也一样，你应该明智地使用这些资源。 算法的效率通常用时间复杂度和空间复杂度来度量。 时间复杂度算法中基本运算的频度记做f(n)，取f(n)中增长最快的项并将其系数置为1作为时间复杂度的度量。 f(n) = 3n^{3} + n^{2}+ 5n + 10 = O(n^{3})算法的时间复杂度不仅与输入规模n有关，还与输入实例的具体情况有关： 最坏时间复杂度：在最坏情况下，算法的时间复杂度 平均时间复杂度：在所有情况等概率出现的情况下，算法的期望运行时间 最好时间复杂度：在最好情况下，算法的时间复杂度 空间复杂度算法中除输入和程序之外所耗费的额外空间。 原地工作：算法所需的辅助空间是常数，即$O(1)$ 同样的，算法的空间复杂度不仅与输入规模n有关，还与输入实例的具体情况有关： 最坏空间复杂度：在最坏情况下，算法的空间复杂度 平均空间复杂度：在所有情况等概率出现的情况下，算法的期望空间复杂度 最好空间复杂度：在最好情况下，算法的空间复杂度 主定理主定理为求解如下形式的递归式提供了一种“菜谱式”的求解方法（$a\geq 1$，$ b&gt; 1$）： T(n) = aT(n/b) + f(n)该递归式描述的是这样一种算法的运行时间：它将规模为n的问题分解为a个规模为n/b的同类子问题，函数f(n)包含了分解和组合子问题解的代价。 总原则：通过比较$f(n)$与$n^{log_{b}a}$的多项式大小可直接得出T(n)的增长量级。 如果$f(n）$多项式大于$n^{log_{b}a}$(相差一个$n^{\varepsilon }$)，则$T(n) = O(f(n))$ 如果$f(n）$多项式小于$n^{log{b}a}$(相差一个$n^{\varepsilon }$)，则$T(n) = O(n^{log{b}a})$ 如果$f(n) = \Theta (n^{log_{b}a}lg^{k}n)$，则$T(n) = O(f(n)lgn)$ 举例： 折半查找（满足3）：$T(n) = T(n/2) + O(1)\Rightarrow T(n) = O(1) * lgn = O(lgn)$ 快速/归并排序（满足3）：$T(n) = 2T(n/2) + O(n)\Rightarrow T(n) = O(n) * lgn = O(nlgn)$ $T(n) = 2T(n/2) + nlgn\Rightarrow T(n) = O(nlgn * lgn) = O(nlg^{2}n)$]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构与算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构与算法：趣味算法 —— Nim 游戏]]></title>
    <url>%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%9A%E8%B6%A3%E5%91%B3%E7%AE%97%E6%B3%95%20%E2%80%94%E2%80%94%20Nim%20%E6%B8%B8%E6%88%8F%2F</url>
    <content type="text"><![CDATA[Nim 游戏是博弈论中最经典的模型之一，它又有着十分简单的规则和无比优美的结论 Nim游戏是组合游戏(Combinatorial Games)的一种，准确来说，属于“Impartial Combinatorial Games”（以下简称ICG）。 满足以下条件的游戏是ICG（可能不太严谨）：1、有两名选手；2、两名选手交替对游戏进行移动(move)，每次一步，选手可以在（一般而言）有限的合法移动集合中任选一种进行移动；3、对于游戏的任何一种可能的局面，合法的移动集合只取决于这个局面本身，不取决于轮到哪名选手操作、以前的任何操作、骰子的点数或者其它什么因素； 4、如果轮到某名选手移动，且这个局面的合法的移动集合为空（也就是说此时无法进行移动），则这名选手负。根据这个定义，很多日常的游戏并非ICG。例如象棋就不满足条件3，因为红方只能移动红子，黑方只能移动黑子，合法的移动集合取决于轮到哪名选手操作。 经典的 nim 游戏问题定义一共有N堆石子，编号1..n，第i堆中有个a[i]个石子。每一次操作Alice和Bob可以从任意一堆石子中取出任意数量的石子，至少取一颗，至多取出这一堆剩下的所有石子。两个人轮流行动，取走最后一个的人胜利。Alice为先手。 我们定义Position： P：表示当前局面下先手必败 N：表示当前局面下先手必胜 N，P状态的转移满足如下性质： 合法操作集合为空的局面为P； 可以转到P的局面为N，因为只要能转换到P局面，那么先手只需要使操作后的局面变为P，那么后手面临一个必败的局面； 不能转到局面P的局面为P，因为所有转换都只能转换到N，无论如何后手都必胜，此时先手必败； 问题求解其实知道这个之后应该是可以记忆化搜索或者用sg函数求解的，但是如果范围非常大，就没法做了。就引进了nim游戏一个很神奇的结论：对于一个局面，当且仅当a[1] xor a[2] xor …xor a[n]=0时，该局面为P局面，即先手必败局面；反之为先手必胜局面。 证明如下：如果初始局面为P局面，则先手会面临的一直都是P局面，直至全0，输掉比赛；如果初始局面为N局面，则先手可以将局面转化为P局面，也就是后手一定输，先手一定赢。 全0的局面一定是P局面； 如果某个局面异或值不为0，则必能转化为异或等于0的局面；设a[1] ^ a[2] ^ a[i] ^ …^ a[n]=k，那么一定存在一个a[i]与k的最高位均为1，即a[i] ^ k &lt;= a[i]，那么令a[i]=a[i] ^ k(相当于从第i堆拿掉一部分)，则a[1] ^ a[2] ^ a[i] ^ …^ a[n] = k ^ k = 0； 如果某个局面异或值为0，则不存在任何一个移动能使局面转化为异或值为0；如果一位的异或值为0，那么这一位上一定有偶数个1，那么只改变一个数，一定无法使其保持0； Moore’sNimk问题定义n堆石子，每次从不超过k堆中取任意多个石子，最后不能取的人失败 结论为：把n堆石子的石子数用二进制表示，统计每个二进制位上1的个数，若每一位上1的个数mod(k+1)全部为0，则必败，否则必胜 证明如下： 全为0的局面一定是必败态。 任何一个P状态，经过一次操作以后必然会到达N状态：在某一次移动中，至少有一堆被改变，也就是说至少有一个二进制位被改变。由于最多只能改变k堆石子，所以对于任何一个二进制位，1的个数至多改变k。而由于原先的总数为k+1的整数倍，所以改变之后必然不可能是k+1的整数倍。故在P状态下一次操作的结果必然是N状态。 任何N状态，总有一种操作使其变化成P状态。从高位到低位考虑所有的二进制位。假设用了某种方法，改变了m堆，使i为之前的所有位都回归到k+1的整数倍。现在要证明总有一种方法让第i位也恢复到k+1的整数倍。 有一个比较显然的性质，对于那些已经改变的m堆，当前位可以自由选择1或0. 设除去已经更改的m堆，剩下堆i位上1的总和为sum 分类讨论： (1)sum&lt;=k-m,此时可以将这些堆上的1全部拿掉，然后让那m堆得i位全部置成0.(2)sum&gt;k-m 此时我们在之前改变的m堆中选择k+1-sum堆，将他们的第i位设置成1。剩下的设置成0.由于k+1-sum&lt;k+1-(k-m)&lt;m+1,也就是说k+1-sum&lt;=m，故这是可以达到的。 anti-nim反nim游戏。正常的nim游戏是取走最后一颗的人获胜，而反nim游戏是取走最后一颗的人输。 一个状态为必胜态，当且仅当： 所有堆的石子个数为1，且NIM_sum=0 至少有一堆的石子个数大于1，且 NIM_sum≠0 威佐夫博弈：两堆石子，每次可以取一堆或两堆，从两堆中取得时候个数必须相同，先取完的获胜。 这种情况下是颇为复杂的。我们用（ak，bk）（ak ≤ bk ,k=0，1，2，…,n)表示两堆物品的数量并称其为局势，如果甲面对（0，0），那么甲已经输了，这种局势我们称为奇异局势。前几个奇异局势是：（0，0）、（1，2）、（3，5）、（4，7）、（6，10）、（8，13）、（9，15）、（11，18）、（12，20）。 可以看出,a0=b0=0,ak是未在前面出现过的最小自然数,而 bk= ak + k，奇异局势有如下三条性质： 任何自然数都包含在一个且仅有一个奇异局势中。由于ak是未在前面出现过的最小自然数，所以有ak &gt; ak-1 ，而 bk= ak + k &gt; ak-1 + k-1 = bk-1 &gt; ak-1 。所以性质1。成立。 任意操作都可将奇异局势变为非奇异局势。事实上，若只改变奇异局势（ak，bk）的某一个分量，那么另一个分量不可能在其他奇异局势中，所以必然是非奇异局势。如果使（ak，bk）的两个分量同时减少，则由于其差不变，且不可能是其他奇异局势的差，因此也是非奇异局势。 采用适当的方法，可以将非奇异局势变为奇异局势。假设面对的局势是（a,b），若 b = a，则同时从两堆中取走 a 个物体，就变为了奇异局势（0，0）；如果a = ak ，b &gt; bk，那么，取走b – bk个物体，即变为奇异局势；如果 a = ak ， b &lt; bk ,则同时从两堆中拿走 ak – ab – ak个物体,变为奇异局势（ ab – ak , ab – ak+ b – ak）；如果a &gt; ak ，b= ak + k,则从第一堆中拿走多余的数量a – ak 即可；如果a &lt; ak ，b= ak + k,分两种情况，第一种，a=aj （j &lt; k）,从第二堆里面拿走 b – bj 即可；第二种，a=bj （j &lt; k）,从第二堆里面拿走 b – aj 即可。 从如上性质可知，两个人如果都采用正确操作，那么面对非奇异局势，先拿者必胜；反之，则后拿者取胜。 那么任给一个局势（a，b），怎样判断它是不是奇异局势呢？我们有如下公式： ak =[k（1+√5）/2]，bk= ak + k （k=0，1，2，…,n 方括号表示取整函数)巴什博奕:只有一堆石子共n个。每次从最少取1个，最多取m个，最后取光的人取胜。问先手是否有必胜策略，第一步该怎么取。 如果n=(m+1)*k+s (s!=0) 那么先手一定必胜，因为第一次取走s个，接下来无论对手怎么取，我们都能保证取到所有(m+1)倍数的点，那么循环下去一定能取到最后一个。 参考Nim 游戏]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构与算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构与算法：趣味算法 —— 小球称重]]></title>
    <url>%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%9A%E8%B6%A3%E5%91%B3%E7%AE%97%E6%B3%95%20%E2%80%94%E2%80%94%20%E5%B0%8F%E7%90%83%E7%A7%B0%E9%87%8D%2F</url>
    <content type="text"><![CDATA[问题描述称球问题，是指若在最多$\frac{3^n-2}{2}$个球中有一个特殊球的重量与众不同（不知道偏重还是偏轻），而其他球的重量全部相同，则用无砝码的天平称n次可以找出特殊球，并确定特殊球是偏轻还是偏重；如果有$\frac{3^n-1}{2}$个球，则同样可以保证找出特殊球，但不一定能确定特殊球是偏轻还是偏重。 示例解答以12个小球的情形为例： 求解算法算法 数学方法]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构与算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构与算法：趣味算法 —— 红包分配]]></title>
    <url>%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%9A%E8%B6%A3%E5%91%B3%E7%AE%97%E6%B3%95%20%E2%80%94%E2%80%94%20%E7%BA%A2%E5%8C%85%E5%88%86%E9%85%8D%2F</url>
    <content type="text"><![CDATA[分配的目标基于对“公平”的理解，微信红包分配算法基于以下三个目标： 先后抽到的红包金额期望尽量相等(机会相等)； 所有人抽到金额的基尼系数尽量小(差距较小)； 每个人至少有1分钱； 分配算法算法思路：假设要将金额为m元的红包发给n个人。当第k个人点开红包时，按照以下规则为其分配金额(假设当前红包剩余金额为x，剩余人数为y) 为剩余的y-1个人预留1分钱，将余下的钱平分为y份，每份金额为py = (x - 0.01 * (y-1))/y; 从[0,2*py]的均匀分布中随机选取一个金额xk分配给第k个人；如果金额不足0.01则取0.01，如果金额不是0.01的整数倍，则在百分位向下取整； 更新剩余金额x = x -xk和剩余人数y=y-1； 分配效果1、不同人抽到金额的期望相等(m/n)，和抢红包顺序无关： 2、先抽的人金额方差小，后抽的人金额方差大(后抽的人更有可能获得非常小或非常大的红包) 第一个人抽到金额从小到大排序(1000次试验)： 最后一个人抽到金额从小到大排序： 3、所有人抽到的金额基尼系数近似为0(1000个人) 机会相等的证明 为什么不用简单隔板法如果只是为了使每个人的期望金额相同，完全可以用另外一种简单算法，生成n-1个0~1之间的随机数，排序后得到一个序列 $A=[0,x1,x_2,…x{n-1},1]$，将 m*(A[i]-A[i-1])分配给第i个人。 适用这种算法会产生较大的“贫富差距”，这一点从洛伦兹曲线上可以看到：]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构与算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构与算法：趣味算法 —— 蓄水池抽样]]></title>
    <url>%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%9A%E8%B6%A3%E5%91%B3%E7%AE%97%E6%B3%95%20%E2%80%94%E2%80%94%20%E8%93%84%E6%B0%B4%E6%B1%A0%E6%8A%BD%E6%A0%B7%2F</url>
    <content type="text"><![CDATA[12# Python常用内置随机函数import random 123456789101112131415# random产生[0,1)之间的浮点数random.random()0.9866624765042807# 产生[a,b]之间的整数random.randint(1,10)4# 产生range(start,end,step)中的随机整数random.randrange(2,10,2)6# 从指定序列中随机选取一个元素random.choice([1,2,5])5 蓄水池抽样算法原理 问题：给定一个长度为n的数组，从中随机抽取k个元素。要求每个元素被选取的概率相等，且时间复杂度为O(n)。 思路：维护一个长度为k的蓄水池，首先将前k个元素放入蓄水池，然后从第k+1个元素开始向后遍历，遍历到第i个元素时，以k/i的概率选中第i个元素，随机替换掉蓄水池中任一元素，直至遍历完成，蓄水池中的k个元素就是最终抽样结果。 证明：蓄水池抽样算法使得每个元素被抽到的概率均为k/n。我们只需要证明遍历完第i个元素时，前i个元素中每个元素被抽取到概率均为k/i： 当i=k时，前i个元素每个被抽取到的概率为$1=\frac{k}{i}$ 假设遍历完第i个元素后，前i个元素每个元素被抽取到的概率为$\frac{k}{i}$，现在考虑第i+1个元素，以$\frac{k}{i+1}$的概率选中第i+1个元素，替换蓄水池中随机一个元素x： 对于第i+1个元素来说，放入蓄水池的概率为$\frac{k}{i+1}$ 对于前i个元素来说，遍历第i+1个元素前，每个元素在蓄水池中的概率为$\frac{k}{i}$，遍历完第i+1个元素后仍在蓄水池中的概率为：$\frac{k}{i}\cdot (1-\frac{k}{i+1}\cdot \frac{1}{k})=\frac{k}{i+1}$ 当i=n时，前n个元素被抽取到的概率均为$\frac{k}{n}$ 实现： 12345678910import randomdef reservoid_sample(a,k): res = [] for i,num in enumerate(a): if i &lt; k: res.append(num) else: if random.random() &lt; 1. * k / (i+1): res[random.randint(0,k-1)] = num return res if i &gt;= k - 1 else False 例题LeetCode[398] 随机抽样 问题：给定一个可能含有重复元素的整数数组，要求随机输出给定的数字的索引。 您可以假设给定的数字一定存在于数组中 思路：只对等于给定数字的元素计数 代码： 1234567891011121314151617181920212223242526import randomclass Solution(object): def __init__(self, nums): &quot;&quot;&quot; :type nums: List[int] :type numsSize: int &quot;&quot;&quot; self.nums = nums def pick(self, target): &quot;&quot;&quot; :type target: int :rtype: int &quot;&quot;&quot; count = 0 for i,num in enumerate(self.nums): if num == target: if count == 0: pre = i count += 1 else: count += 1 if random.random() &lt; 1./count: pre = i return pre [LeetCode 528] 按权重随机抽样 问题： 1234567给定一个正整数数组 w ，其中 w[i] 代表位置 i 的权重，请写一个函数 pickIndex ，它可以随机地获取位置 i，选取位置 i 的概率与 w[i] 成正比。说明:1 &lt;= w.length &lt;= 100001 &lt;= w[i] &lt;= 10^5pickIndex 将被调用不超过 10000 次 思路： 1234思路： 1. 求数组各位置之前的累加和 2. 在总和范围内产生随机数 3. 通过二分查找确定随机数所属区间(第一个大于随机数的位置索引) 代码： 1234567891011121314151617import randomimport bisectclass Solution(object): def __init__(self, w): &quot;&quot;&quot; :type w: List[int] &quot;&quot;&quot; self.w = w for i in xrange(1, len(w)): self.w[i] += self.w[i-1] def pickIndex(self): &quot;&quot;&quot; :rtype: int &quot;&quot;&quot; val = random.randint(1, self.w[-1]) return bisect.bisect_left(self.w, val)]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构与算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[推荐系统（一）—— 推荐系统概论]]></title>
    <url>%2F%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%2F%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%2F%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%EF%BC%88%E4%B8%80%EF%BC%89%E2%80%94%E2%80%94%20%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E6%A6%82%E8%AE%BA%2F</url>
    <content type="text"><![CDATA[什么是推荐系统推荐系统的定义推荐系统是通过对用户的历史行为建模，自动联系用户和信息的一种工具。它能够在信息过载的环境中帮助用户发现他们感兴趣的信息，也能将信息推送给对它们感兴趣的用户。推荐系统已被广泛应用于电子商务、视频、音乐、阅读、社交网络、基于位置的服务、个性化邮件和广告等领域。 推荐系统的主要任务： 联系用户和信息，解决信息过载问题：在信息过载的时代，信息消费者很难从大量信息中找到自己感兴趣的信息，而信息生产者也很难将自己的信息展现在对它感兴趣的用户面前。为了解决信息过载的问题，人们先后提出了很多解决方案，其中最具代表性的有以下三种： 解决方案 原理 代表公司 优点 缺点 分类目录 将著名的网站分门别类 雅虎、Hao123 方便用户对著名网站的访问 只能覆盖少量热门网站 搜索引擎 用户通过主动搜索关键词寻找自己需要的信息 谷歌、百度 方便用户按需查找信息 当用户无法准确描述自己需求时，无法进行查找 推荐系统 通过对用户的历史行为建模，自动为用户推荐满足他们兴趣的信息 亚马逊、今日头条 从用户角度：在用户在没有明确目的时也能帮助他们发现感兴趣的信息；从物品角度：能够更好地发掘物品的长尾 无法满足用户即时的有针对性的信息需求 发掘长尾物品，解决长尾效应问题：在互联网领域，”长尾效应“战胜了”二八定律“，推荐系统能够充分发掘长尾物品（二八定律：经济学中的二八定律是指 80% 的销售额来自 20% 的热门商品；长尾效应：原来不受重视的销量小但种类多的产品由于总量巨大，累加起来的总收益可能会超过主流产品的现象） 推荐系统的架构 推荐系统核心组件有三层：数据层，计算逻辑层和推送服务层 数据层：是个性化推送的基础， 通过数据，将用户与信息相关联，通过特征，将用户与用户相区分，从而达到千人千面的效果； 逻辑层：是个性化推送的核心，通过特征匹配等相关的逻辑运算，将用户和信息做关联，筛选更合适的数据，计算逻辑直接影响到个性化推送的准确度； 推送层：是个性化推送与用户交互的通道。推送服务，间接的影响到个性化推送的到达率，影响着推送的效果； 数据层可以从三个维度来把握推荐系统所依赖的数据： 用户维度：用来描述用户的特征数据，一般可以从用户属性和用户行为两个层面来了解用户，但不同行业会对用户在某一方面有更细致的特征描述； 用户属性：用户的静态特征属性，如年龄、性别、爱好等； 用户行为：用户的动态行为日志，如搜索行为、点击行为、评价行为等； 信息维度：用来描述信息的特征属性，不同种类的信息要用不同的特征指标来标识； 时空维度：与时间和位置相关的用户特征和信息特征，使用时一定要注意时效性； 按照前面数据的规模和是否需要实时存取，不同的行为数据将被存储在不同的媒介中。一般来说，需要实时存取的数据存储在数据库和缓存中，而大规模的非实时地存取数据存储在分布式文件系统（如HDFS）中。 逻辑层 基于用户数据和信息数据，推荐系统连接用户和信息的基本方式有三种： 基于用户的推荐：寻找与用户兴趣相似的用户，将相似用户感兴趣的信息推荐给该用户； 基于信息的推荐：寻找与用户之前感兴趣的信息相似的信息，将相似信息推荐给该用户； 基于特征的推荐：寻找用户感兴趣信息的特征，将具有相似特征的信息推荐给该用户；这里的特征具有不同的表现方式： 用户标签：为用户推荐他经常使用的标签下的物品； 物品属性：为用户推荐他比较感兴趣的某些类型的物品； 隐语义向量：计算用户对各种隐含类别的兴趣度，将兴趣度较高的类别中匹配度较高的物品推荐给用户； 以基于特征的推荐为例，推荐系统的核心任务可以被分解为两部分，一是如何为用户生成特征，另一个是如何根据特征找到物品。如果在一个系统中把上面提到的各种特征都统筹考虑，那么系统将会非常复杂，而且很难通过配置文件方便地配置不同特征和任何的权重。因此，推荐系统需要由多个推荐引擎组成，每个推荐引擎负责一类特征和一种任务，而推荐系统的任务只是将推荐引擎的结果按照一定权重或者优先级合并、过滤、排序然后返回。这样做的好处有： 可以方便地增加/删除引擎，控制不同引擎对推荐结果的影响。对于绝大多数需求，只需要通过不同的引擎组合实现； 可以实现推荐引擎级别的用户反馈，每一个推荐引擎其实代表了一种推荐策略，而不同的用户可能喜欢不同的推荐策略； 每个推荐引擎主要包括三个部分： 生成当前用户特征：该部分负责从数据库或者缓存中拿到用户行为数据，通过分析不同行为，生成当前用户的特征向量； 生成初始推荐列表：该部分负责将用户的特征向量通过特征-物品相关矩阵转化为初始推荐物品列表； 生成最终推荐列表：该部分负责对初始的推荐列表进行过滤、排名等处理，从而生成最终的推荐结果 过滤：主要是根据推送的历史，将之前推送过的信息过滤掉，同时，我们会根据实际情况，将不满足要求，或者质量比较差的信息过滤掉； 排名：主要是拟定推送信息的优先级，一般按照新颖性，多样性和用户反馈等规则来做排序，新颖性保证了尽量给用户推送他们不知道的、长尾的信息，多样性保证用户可以获取更广的内容，而用户反馈则通过收集用户真实的意愿(如通过用户对推送内容的打开，关闭操作反应用户的喜好)实现更优的排序； 推送层个性化推送，在推送的内容和时机上都是离散的，所以很难做到批量推送，这就对推送服务的设计和性能提出了比较高的要求。 我们推送服务做了如下改进： 数据局部聚合：将相同消息内容的推送放在一起， 这样就可以做局部的批量推送，增加服务吞吐； 数据分片：将不同消息内容的推送分割为不同的数据片，不同的数据片可以并行推送， 提高推送效率； 守护线程：每台服务实例都保留一个守护线程，用于监控推送过程，确保推送有且仅有一次送达； 推荐系统的分类按照数据分： 基于用户行为的方法：基于用户点击、购买行为的推荐； 基于用户标签的方法：基于用户为物品打标签的推荐； 基于上下文的方法：基于时间和空间的推荐； 基于社交网络的方法：基于好友的推荐； 按照算法分： 基于邻域的算法：协同过滤 基于概率模型的算法：LFM、LDA 基于图的算法：二分图随机游走 什么是好的推荐系统在推荐系统的早期研究中，很多人将好的推荐系统定义为能够作出准确预测的推荐系统。然而从实践的角度来看，准确的预测并不是好的推荐： 从用户角度看：推荐系统不仅要准确预测用户的行为，而且能够扩展用户的视野，帮助用户发现那些他们可能会感兴趣但不那么容易被发现的东西； 从商家角度看：不仅要将商家热门产品推荐给用户，还要将那些埋没在长尾的好商品介绍给用户； 好的推荐系统是能够让推荐系统参与三方(用户、商家、网站)共赢的系统： 评测方法一般来说，一个新的推荐算法最终上线，需要完成以下三个实验： 首先，需要通过离线实验证明它在很多离线指标上优于现有的算法； 然后，需要通过用户调查确定它的用户满意度不低于现有的算法； 最后，通过在线的AB测试确定它在我们关心的指标上优于现有的算法； 离散实验离线实验不需要一个实际的系统供它实验，而只要有一个从实际系统日志中提取的数据集即可，离线实验的一般步骤： 获取用户行为日志，按照一定格式生成标准数据集 将数据集划分为训练集和测试集 在训练集上训练用户兴趣模型，在测试集上进行预测 通过事先定义的离线指标评测模型在测试集上的效果 优点 缺点 不需要实际系统的控制权 无法计算商业上关心的指标 不需要用户参与 离线指标和商业指标存在差异 速度快，可以测试大量算法 - 用户调查离线指标和商业指标存在差距，比如预测准确率和用户满意度之间就存在很大差别，如果要准确评测一个算法，需要相对比较真实的环境，最好的方法就是将算法直接上线测试，但在对算法不会降低用户满意度不太有把握的情况下，上线测试具有较大风险，所以在上线测试前需要做一次用户调查。 用户调查需要一些真实用户，让他们在需要测试的推荐系统上完成一些任务，在他们完成任务时观察和记录他们的行为或让他们回答一些问题，最后通过分析他们的行为和答案了解系统的性能。 优点 缺点 可以获取体现用户主观感受的指标 代价较高，多数情况下难以设计双盲实验 - 用户行为在测试环境下和在真实环境下可能有所不同 在线实验——AB test在完成离线实验和必要的用户调查后，可以将推荐系统上线做AB测试，将它和旧的算法进行比较。通过一定的规则将用户随机分成几组，并对不同组的用户采用不同的算法，然后通过统计不同组用户的各种不同的评测指标比较不同算法，比如可以统计不同组用户的点击率，通过点击率比较不同算法的性能。 优点 缺点 可以公平获得不同算法实际在线时的性能指标，包括商业指标 设计复杂、周期长 流量切分： 一个大型网站的架构分前端和后端，从前端展示给用户的界面到最后端的算法，中间往往经过了很多层，这些层往往由不同的团队控制，而且都有可能做AB测试。如果为不同的层分别设计AB测试系统，那么不同的AB测试之间往往会互相干扰。比如，当我们进行一个后台推荐算法的AB测试，同时网页团队在做推荐页面的界面AB测试，最终的结果就是你不知道测试结果是自己算法的改变，还是推荐界面的改变造成的。因此，切分流量是AB测试中的关键，不同的层以及控制这些层的团队需要从一个统一的地方获得自己AB测试的流量，而不同层之间的流量应该是正交的。 一个简单的AB测试系统： 用户进入网站后，流量分配系统决定用户是否需要被进行AB测试，如果需要的话，流量分配系统会给用户打上在测试中属于什么分组的标签。然后用户浏览网页，而用户在浏览网页时的行为都会被通过日志系统发回后台的日志数据库。此时，如果用户有测试分组的标签，那么该标签也会被发回后台数据库。在后台，实验人员的工作首先是配置流量分配系统，决定满足什么条件的用户参加什么样的测试。其次，实验人员需要统计日志数据库中的数据，通过评测系统生成不同分组用户的实验报告，并比较和评测实验结果。 评测指标如果你需要设计一些评测指标来评价你的推荐系统，最好的方式就是假想你是推荐系统三个参与方中的其中一方，你最关注的是什么。 对于用户： 系统应该给我推荐一些我一直以来都感兴趣的物品——准确度； 系统推荐不能太单调——多样性； 系统应该给我推荐一些我还不知道但我可能感兴趣的物品——新颖度、惊喜度； 系统应该给我推荐最新的信息——实时性； 总之，系统应该推荐给我一些让我满意的内容——用户满意度； 对于商家： 系统不仅要帮我把热门产品推荐给用户，还要能够将长尾物品推荐给对它感兴趣的用户——覆盖率； 推荐系统能够将新加入系统的物品推荐给用户——实时性； 对于系统: 应该让用户信任我的平台，这样才能提高他们与系统交互的积极性——信任度； 系统应该具有抗击作弊的能力——健壮性； 系统应该能为公司创造价值——商业目标； 下面我们将介绍各种推荐系统的评测指标，这些指标可以用于评价推荐系统在不同方面的性能。这些指标中有些是定量的，有些是定性的，有些可以通过离线实验得到，有些需要通过用户调查获得，有些只能在线评测。 指标 离线实验 问卷调查 在线实验 用户满意度 × √ 〇 推荐准确度 √ √ × 覆盖率 √ √ √ 多样性 〇 √ 〇 新颖性 〇 √ 〇 惊喜度 × √ × 离线指标最常用的三率:准确率、召回率、覆盖率（除了用户满意度和商业目标，大多数指标都可以通过离线实验来得到） 用户满意度用户满意度是评测推荐系统的最重要指标，只能通过用户调查或在线实验得到： 用户调查：用户调查通常采用调查问卷的方式，好的调查问卷不是简单地询问用户对结果是否满意，而是从具体的方面询问用户对结果的感受； 推荐的论文都是我非常想看的。 推荐的论文很多我都看过了，确实是符合我兴趣的不错论文。 推荐的论文和我的研究兴趣是相关的，但我并不喜欢。 不知道为什么会推荐这些论文，它们和我的兴趣丝毫没有关系。 在线系统：通过对用户行为的统计数据得到，比如： 购买率：在电子商务网站中，如果用户购买了推荐的商品，就代表了他们在一定程度上满意； 用户反馈界面收集用户满意度：让用户点击对推荐结果满意或不满意； 点击率：用户点击次数越多代表月满意； 用户停留时间：用户在推荐商品上停留的时间越长，代表用户越满意； 转化率：用户点击后成功购买的比例越高，代表用户越满意； 准确度(precision)准确度反映了系统准确预测用户行为的能力，几乎 99% 的推荐相关的论文都在讨论这个指标，主要是因为该指标可以通过离线实验计算，方便学术界研究人员研究推荐算法，准确度可以通过离线实验在测试集上计算得到；实际使用中，TopK 推荐相比评分预测更符合实际应用需要，因为预测用户是否会看一部电影比预测用户看了电影后会给它评多少分更重要： 评分预测：如果推荐系统预测的是用户对物品的评分，一般采用均方根（RMSE）和平均绝对误差（MAE）作为评测指标；RMSE更加严格，因为它加大了对预测不准的评分的乘法（平方项）；对于测试集中的一个用户 u 和物品 i，令 $r{ui}$ 是用户 u 对物品 i 的实际评分，而 $\hat{r}{ui}$ 是推荐算法给出的预测评分； RMSE=\frac{\sqrt{\sum_{u,i \in T}(r_{ui}-\hat{r}_{ui})^2}}{\left | T \right |} MAE=\frac{\sum_{u,i \in T}\left | r_{ui}-\hat{r}_{ui} \right |}{\left | T \right |} TOPK 推荐：如果推荐系统为每个用户一个个性化推荐列表，一般通过准确率（precision）或召回率（recall）来度量；有时为了全面评测 TopK 的准确率和召回率，一般会选取不同的推荐列表长度 K，计算一组准确率和召回率，然后画出准确率/召回率曲线（precision/recall curve）；令 $R(u)$ 是根据用户在训练集上的行为给用户作出的推荐列表，而 $T(u)$ 是用户在测试集上的行为列表。那么，推荐结果的准确率和召回率可定义为： Precision=\frac{\sum_{u \in U}\left | R(u)\bigcap T(u) \right |}{\sum_{u \in U}\left | R(u) \right |} Recall=\frac{\sum_{u \in U}\left | R(u)\bigcap T(u) \right |}{\sum_{u \in U}\left | T(u) \right |}覆盖率(coverage)覆盖率描述了系统发掘长尾物品的能力，覆盖率是物品提供者会关心的指标，好的推荐系统不仅要让用户满意还要让商家满意，覆盖率可以通过离线实验获取，覆盖率有以下几种形式： 覆盖比例：推荐系统能够推荐出来的物品占总物品集合的比例，假设系统的用户集合为U，推荐系统给每个用户推荐一个长度为N的物品列表R（u）。那么推荐系统的覆盖率可以通过下面的公式计算： Coverage=\frac{\left | \bigcup _{u \in U} R(u)\right |}{\left | I \right |} 覆盖分布：覆盖比例过于粗略，为了更细致地描述推荐系统发掘长尾的能力，需要描述推荐列表中物品出现次数的分布情况，经济学中有两个著名的指标可以用来定义覆盖率：熵和基尼系数 信息熵(Entropy)：熵越大代表覆盖率越 H=-\sum_{i1}^{n}p(i)logp(i) 基尼系数(Gini index)：$i_j$是按照物品流行度p从小到大排序列表中第j个物品，在洛伦兹曲线上G=A/(A+B)，基尼系数越大代表覆盖率越低 G = \frac{1}{n-1}\sum_{j=1}^{n}(2j-n-1)p(i_j) 推荐系统的马太效应：推荐系统设计的初衷是为了消除马太效应，但很多研究表明，现在主流的推荐算法(如热门推荐、PageRank、协同过滤)是具有马太效应的，进入推荐列表的如果都是热门商品，因为有更多被曝光的机会，会变得更热门。 多样性多样性是指推荐列表要能够覆盖用户的不同兴趣领域，可以通过离线计算或用户调查获取。多样性描述了推荐列表中两个物品之间的不相似性，多样性和相似性是对应的，可以通过物品间的相似性来定义推荐的多样性。假设s（i，j）∈［0，1］定义了物品i和j之间的相似度，那么用户u的推荐列表R（u）的多样性定义如下： Diversity=1-\frac{\sum_{i,j \in R(u),i\neq j}s(i,j)}{\frac{1}{2}\left | R(u) \right (\left | R(u) \right |-1) }而推荐系统整体的多样性可以被定义为所有用户推荐列表的多样性的平均值： Diversity=\frac{1}{\left | U \right |}\sum_{u \in U}Diversity(R(u))新颖性新颖性是指给用户推荐那些他们以前没有听说过的物品，可以通过离线计算和用户调查获取。评测新颖度的最简单方法是利用推荐结果的平均流行度，因为越不热门的物品越可能让用户觉得新颖。 惊喜度惊喜度是指推荐的结果和用户的历史兴趣不相似，但却让用户觉得满意，可以通过离线计算和用户调查获取。个性化和多样性-新颖度-惊喜度是一对矛盾：ACM的推荐系统会议在2011年有一个专门的研讨会讨论推荐的多样性和新颖性。该研讨会的组织者认为，通过牺牲精度来提高多样性和新颖性是很容易的，而困难的是如何在不牺牲精度的情况下提高多样性和新颖性。 其他评测指标信任度信任度是指推荐结果要提升用户对推荐系统的信任，这样才能增加用户和系统的交互。可以通过用户调查和在线实验获取。 用户调查:通过问卷调查的方式，询问用户是否信任推荐系统的结果； 提供推荐解释：增加系统的透明度； 基于好友推荐：让用户的好友向他推荐； 对评论设置可信度：在每条评论显示作者信息，并让用户判断是否信任该条评论； 实时性因为物品（新闻、微博）具有很强的时效性，所以需要在物品还具有时效性时就将它推荐给用户。事实性可以通过在线实验获取，实时性保函两个方面： 与用户相对应的实时性：推荐系统需要实时更新推荐列表来满足用户新的行为变化； 与物品相对应的实时性：推荐系统能够将新加入系统的物品推荐给用户；这主要考验系统处理物品冷启动的能力； 衡量推荐系统实时性的指标有： 与用户相对应的实时性指标：可以通过推荐列表变化速率来评测，如果推荐列表在用户有行为后变化不大，说明推荐系统的实时性不高； 与物品相对应的实时性指标：可以利用用户推荐列表中有多大比例是当天新加进来的来进行评测 健壮性健壮性衡量了一个推荐系统抗共计作弊的能力，最著名的作弊方式是行为注入攻击： 可以注册很多账号，用这些账号同时购买A和自己的商品 雇用一批人给自己的商品非常高的评分，而评分行为是推荐系统依赖的重要用户行为 可以通过模拟攻击来测试推荐系统的健壮性，给定一个数据集和一个算法，可以用这个算法给这个数据集中的用户生成推荐列表。然后，用常用的攻击方法向数据集中注入噪声数据，然后利用算法在注入噪声后的数据集上再次给用户生成推荐列表。最后，通过比较攻击前后推荐列表的相似度评测算法的健壮性。 提高推荐系统健壮性的方法： 选择健壮性高的算法 设计推荐系统时尽可能使用代价比较高的用户行为：比如购买行为要比浏览点击行为代价更高 在使用数据前，进行攻击检测，从而对数据进行清理 商业目标很多时候，网站评测推荐系统更加注重网站的商业目标是否达成。而商业目标和网站的盈利模式是息息相关的： 电子商务：主要目标是销售额 基于展示广告盈利的网站：广告展示总数 基于点击广告盈利的网站：广告点击总数 离线指标很多，在线指标也很多，如何优化离线指标来提高在线指标是推荐系统研究的重要问题，关于这个问题目前没有定论，只是不同系统的研究人员有不同的感性认识。对于可以离线优化的指标，《推荐系统实践》的作者认为应该在给定覆盖率、多样性、新颖性等限制条件下，尽量优化预测准确度： 123456max 准确度s.t. 覆盖率&gt;A 多样性&gt;B 新颖性&gt;C A,B,C的取值视不同应用而定 推荐系统冷启动问题冷启动问题：如何在没有大量用户数据的情况下设计个性化推荐系统并让用户对推荐结果满意。 冷启动问题分类: 用户冷启动：如何给新用户做推荐； 物品冷启动：如何将新物品推荐给可能对它感兴趣的用户； 系统冷启动：如何在一个新开发的网站上（还没有用户，只有一些物品信息）设计个性化推荐系统； 冷启动问题的一般解决方案：如何获取”第一推动力“ 用户冷启动解决方案： 提供非个性推荐：可以先给用户推荐热门排行榜，等到用户数据收集到一定程度的时候再切换为个性化推荐； 利用用户注册信息推荐：获取用户的注册信息，根据用户的注册信息对用户分类，给用户推荐他所属分类中用户喜欢的物品； 利用社交账号好友推荐：当一个新用户通过社交账号登录网站时，可以从社交网站中获取用户的好友列表，然后给用户推荐好友在网站上喜欢的物品； 利用首次登陆反馈推荐：在新用户第一次登陆时，给用户提供一些物品，让用户反馈他们对这些物品的兴趣，然后根据用户反馈提供个性化推荐； 物品冷启动解决方案：利用物品信息，将新物品推荐给喜欢过相似物品的用户 系统冷启动解决方案：在系统冷启动时，通过专家进行标注 参考 《推荐系统实践》项亮 浅谈推荐系统和个性化推送]]></content>
      <categories>
        <category>推荐系统</category>
      </categories>
      <tags>
        <tag>推荐系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构与算法：Trie 树]]></title>
    <url>%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%9ATrie%20%E6%A0%91%2F</url>
    <content type="text"><![CDATA[理论篇大量字符串的存储-查找-排序问题Trie树，又称字典树、前缀树(Prefix Tree)、单词查找树或键树，是一种树形结构，主要用于对大量字符串（不仅限于字符串）的高效存储、查询、排序，经常被搜索引擎系统用于文本词频统计。 以词频统计为例，最直接的想法是使用哈希表来统计每个单词的词频，它的时间复杂度为O(n)，空间复杂度为O(dn)，其中n表示单词个数，d表示单词的平均长度。但考虑到下列事实，哈希表并不总是最好的选择： 空间：哈希表需要存储每个不同的单词，当单词量很大时会出现大量相同前缀的单词，对每一个这样的单词，哈希表都会做重复的存储；此外，为了尽可能避免键值冲突，哈希表需要额外的空间避开碰撞，还会有一部分的空间被浪费； 时间：尤其是数据体量增大之后，其查词复杂度常常难以维持在O(1)，同时，对哈希值的计算也需要额外的时间，其具体复杂度由相应的哈希实现来定； Trie树能够在保证近似O(1)的查询效率的同时，利用字符串本身的特性对数据进行一定程度的压缩。 逻辑结构Trie树定义： 根节点不含字符，每个非根节点只含一个字符； 从根节点到某一节点，路径上经过的字符连起来，即是该节点对应的字符串； 每个节点的所有子节点所包含的字符各不相同； 单词列表为[‘apps’,’apply’,’apple’,’append’,’back’,’backen’,’basic’]对应的Trie树如下： 存储结构Trie树的实现方式多种多样，常见的有以下几种： Trie树类别 说明 优点 缺点 Array Trie 用定长数组表示Trie树，数组大小为字符集大小，下标代表了字符集中的字符，值代表了子Trie树 巧妙的利用了等长数组中元素位置和值的一一对应关系，完美的实现了了寻址、存值、取值的统一 每一层都要有一个数组，每个数组都必须等长，这在实际应用中会造成大多数的数组指针空置 List Trie 用可变数组表示Trie树 避免空间浪费 无法通过下标得到对应字符，需要遍历，时间O(d) Hash Trie 用嵌套字典表示Trie树,key表示下一个字符，字典中的value是嵌套的子树 有效的减少空间浪费 对哈希值的计算也需要额外的时间，因此实际查询效率要比Array Trie实现低 Double-array Trie树 将所有节点的状态都记录到一个数组之中（Base Array），避免数组的大量空置，check array 与 base array 等长，它的作用是标识出 base array 中每个状态的前一个状态 Trie 树各种实现中性能和存储空间均达到很好效果 实现复杂 本文着重讨论方便易行且性能堪用的Array Trie 和 Hash Trie。 Array Trie定长数组Trie树：节点包含用数组表示的子节点域和用布尔值表示的词尾标识域，数组大小为字符集大小，数组下标与字符集中的字符构成一一映射，数组中的值存放子Trie树节点。 1234class Trie(object): def __init__(self, N): self.children = [None] * N self.isword = False Hash TrieHash Trie树：节点包含用字典表示的子节点域和用布尔值表示的词尾标识域，字典中的key为子节点代表的字符，字典中的value代表子节点对应的子Trie树 1234class Trie(object): def __init__(self): self.children = &#123;&#125; self.isword = False 对于Trie树的简单应用，可以直接用嵌套字典简单实现，整个字典表示一棵Trie树，字典中的key表示下一个字符，字典中的value是嵌套的字典，表示以key为根节点的子Trie树 1234trie = &#123;&#125;... ...trie = &#123;&#x27;a&#x27;: &#123;&#x27;p&#x27;: &#123;&#x27;p&#x27;: &#123;&#x27;e&#x27;: &#123;&#x27;n&#x27;: &#123;&#x27;d&#x27;: &#123;&#x27;&#x27;: &#123;&#125;&#125;&#125;&#125;,&#x27;l&#x27;: &#123;&#x27;e&#x27;: &#123;&#x27;&#x27;: &#123;&#125;&#125;, &#x27;y&#x27;: &#123;&#x27;&#x27;: &#123;&#125;&#125;&#125;,&#x27;s&#x27;: &#123;&#x27;&#x27;: &#123;&#125;&#125;&#125;&#125;&#125;, &#x27;b&#x27;: &#123;&#x27;a&#x27;: &#123;&#x27;c&#x27;: &#123;&#x27;k&#x27;: &#123;&#x27;&#x27;: &#123;&#125;, &#x27;e&#x27;: &#123;&#x27;n&#x27;: &#123;&#x27;&#x27;: &#123;&#125;&#125;&#125;&#125;&#125;,&#x27;s&#x27;: &#123;&#x27;i&#x27;: &#123;&#x27;c&#x27;: &#123;&#x27;&#x27;: &#123;&#125;&#125;&#125;&#125;&#125;&#125;&#125; 基本操作操作定义单词插入和查找是Trie树最基本的操作： insert(word):将word插入到Trie树中 find(word):在Trie树中查找word，找到则返回True，找不到则返回False 此外经常需要判断前缀存在性、统计词频、打印Trie树中所有的单词等操作： startwith(prefix)：判断Trie树中是否有以prefix作为前缀的单词 count(word)：统计单词表中某个单词word的词频 print_trie()：打印Trie树中所有不同的单词 操作实现Array Trie123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596class Trie_node(object): def __init__(self,N): &quot;&quot;&quot; N:字符表长度 &quot;&quot;&quot; # 子树数组 self.children = [None] * N # 是否为结尾节点 self.isword = False # 以该节点结尾的单词出现次数 self.count = 0class Trie(object): def __init__(self,chars): &quot;&quot;&quot; chars:组成所有单词的字符表(数组) &quot;&quot;&quot; # 下标-&gt;字符 self.chars = chars # 字符表长度 self.n = len(self.chars) # 字符-&gt;下标 self.c2i = &#123;self.chars[i]:i for i in xrange(self.n)&#125; # 根节点 self.root = Trie_node(self.n) def insert(self, word): &quot;&quot;&quot; 插入一个新的字符串 &quot;&quot;&quot; node = self.root for c in word: if not node.children[self.c2i[c]]: node.children[self.c2i[c]] = Trie_node(self.n) node = node.children[self.c2i[c]] node.isword = True node.count += 1 def find(self, word): &quot;&quot;&quot; 判断word是否在Trie树中 &quot;&quot;&quot; node = self.root for c in word: if node.children[self.c2i[c]]: node = node.children[self.c2i[c]] else: return False return node.isword def startsWith(self, prefix): &quot;&quot;&quot; 判断Trie树中是否有以prefix作为前缀的单词 &quot;&quot;&quot; node = self.root for c in prefix: if node.children[self.c2i[c]]: node = node.children[self.c2i[c]] else: return False return True def get_count(self, word): &quot;&quot;&quot; 统计单词word出现的次数 &quot;&quot;&quot; node = self.root for c in word: if node.children[self.c2i[c]]: node = node.children[self.c2i[c]] else: return 0 return node.count def printt(self): &quot;&quot;&quot; 打印Trie中所有单词及其出现次数 &quot;&quot;&quot; def dfs(node,cur,res): if node.count: res.append([cur, node.count]) for i,child in enumerate(node.children): if child: dfs(child, cur+self.chars[i], res) res = [] dfs(self.root,&#x27;&#x27;,res) return res # Your Trie object will be instantiated and called as such:chars = map(char(i) for i in xrange(97,123))obj = Trie(chars)obj.insert(word)param_2 = obj.find(word) 分析：假设字符串数量很大为n，字符集大小为m，单词平均长度为d 空间：在Trie树充分生长的情况下，节点数2^d，每个节点中数组长度为m，总体为O(m2^d)，实际情况取决于单词间前缀的重叠情况，近似O(mn) 时间：无论查找还是插入，单次O(d) Hash Trie标准实现： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657class Trie_node(object): def __init__(self): self.children = &#123;&#125; self.isword = False self.count = 0class Trie(object): def __init__(self): self.root = Trie_node() def insert(self, word): node = self.root for c in word: node = node.children.setdefault(c,Trie_node()) node.isword = True def find(self, word): node = self.root for c in word: if c in node.children: node = node.children[c] else: return False return node.isword def startsWith(self, prefix): node = self.root for c in prefix: if c in node.children: node = node.children[c] else: return False return True def get_count(self, word): node = self.root for c in word: if c in node.children: node = node[c] else: return 0 return node.count def printt(self): def dfs(node,cur,res): if node.count: res.append([cur, node.count]) for child in node.children: dfs(node.children[child],cur+child,res) res = [] dfs(self.root,&#x27;&#x27;,res) return res # Your Trie object will be instantiated and called as such:trie = Trie()trie.insert(word)trie.find(word) 简单实现： 1234567891011121314151617181920212223242526272829303132333435363738394041424344def insert(trie,word): for c in word: trie = trie.setdefault(c,&#123;&#125;) trie[&#x27;#&#x27;] = trie.get(&#x27;#&#x27;,0) + 1def find(trie,word): for c in word: if c in trie: trie = trie[c] else: return False return &#x27;#&#x27; in triedef startsWith(trie, prefix): for c in prefix: if c in trie: trie = trie[c] else: return False return Truedef get_count(trie, word): for c in word: if c in trie: trie = trie[c] else: return 0 return trie.get(&#x27;#&#x27;,0) def printt(trie): def dfs(node,cur,res): if node.get(&#x27;#&#x27;,0): res.append([cur, node[&#x27;#&#x27;]]) for child in node: dfs(node[child],cur+child,res) res = [] dfs(trie,&#x27;&#x27;,res) return res # Your Trie object will be instantiated and called as such:trie = &#123;&#125;x = [&#x27;apps&#x27;,&#x27;apply&#x27;,&#x27;apple&#x27;,&#x27;append&#x27;,&#x27;back&#x27;,&#x27;backen&#x27;,&#x27;basic&#x27;]insert(trie,word)find(trie,word) 分析：假设字符串数量很大为n，字符集大小为m，单词平均长度为d 空间：trie树充分生长的情况下，节点数2^d，每个节点中字典长度为m，总体为O(m2^d)，在实际情况中，节点数取决于单词间前缀重叠情况，边数小于O(dn)，故空间复杂度&lt;O(min(m,d)n) 时间：无论查找还是插入，只需要O(d) 实战篇实战技巧 通常Trie树与哈希表的作用类似，都是以空间换取时间来提高查询效率(出于某种原因我们需要将某些中间结果存储起来，然后以O(1)的时间去读取，如DP问题) 如果只是要提高查询效率，应首先尝试哈希表，操作简单，适用广泛，且在大多数情况下效率不输Trie树； 只有在某些特定情况下才能够或才应该尝试Trie树，① 可以转化/类比为字符串查询的问题(如二进制Trie树) ② 使用哈希表将浪费大量空间，可以通过Trie树实现空间压缩的问题 对于简单问题，可以直接采用最简单的嵌套字典方式来实现，同时我们可以在单词结尾存储有价值的信息来作为结尾标识；对于复杂的衍生问题，自定义节点提供了更多的扩展性； 如果需要对字符串排序，可以通过遍历Array Trie来实现 n个单词插入和查询的空间近似O(mn)，时间O(dn)，m为字符表长度，d为单词平均长度，可近似看做O(n)； LeetCode 经典题目单词替换为前缀 问题：[648] 单词替换，在英语中，我们有一个叫做 词根(root)的概念，它可以跟着其他一些词组成另一个较长的单词——我们称这个词为 继承词(successor) 123输入: dict(词典) = [&quot;cat&quot;, &quot;bat&quot;, &quot;rat&quot;]sentence(句子) = &quot;the cattle was rattled by the battery&quot;输出: &quot;the cat was rat by the bat&quot; 思路：构建前缀树，然后遍历查询每个单词，如果前进时都遇到单词结尾，则返回前缀 代码： 1234567891011121314151617181920212223242526def replaceWords(self, dict, sentence): &quot;&quot;&quot; :type dict: List[str] :type sentence: str :rtype: str &quot;&quot;&quot; root = &#123;&#125; for word in dict: node = root for c in word: node = node.setdefault(c,&#123;&#125;) node.setdefault(&#x27;&#x27;,&#123;&#125;) def search(word): node = root for i,c in enumerate(word): if &#x27;&#x27; in node: return word[:i] else: if c in node: node = node[c] else: return word return word return &#x27; &#x27;.join(map(search,sentence.split())) 分析：时间O(nd) 数组中两个数的最大异或值 问题：[421] 数组中两个数的最大异或值，给定一个非空数组，数组中元素为 a0, a1, a2, … , an-1，其中 0 ≤ ai &lt; 2^31 ，在O(n)时间内找到 ai 和aj 最大的异或 (XOR) 运算结果，其中0 ≤ i, j &lt; n 思路：二进制Trie树+贪心：难点在于O(n)的时间要求，将每个数转化为31位二进制串，首先构建二进制Trie树，然后在Trie树中查找每个二进制串，贪心策略，高位异或为1的肯定较大，如果子节点中有与当前字符相异的，则走相异的子节点，否则走相同的！经典！ 代码： 1234567891011121314151617181920212223242526def findMaximumXOR(self, nums): &quot;&quot;&quot; :type nums: List[int] :rtype: int &quot;&quot;&quot; root = &#123;&#125; res = -1 for num in nums: node = root bi_str = format(num,&#x27;b&#x27;).zfill(31) for bit in bi_str: node = node.setdefault(bit,&#123;&#125;) node.setdefault(&#x27;value&#x27;,num) node = root for bit in bi_str: if bit == &#x27;0&#x27; and &#x27;1&#x27; in node: node = node[&#x27;1&#x27;] elif bit == &#x27;1&#x27; and &#x27;0&#x27; in node: node = node[&#x27;0&#x27;] else: node = node[bit] res = max(node[&#x27;value&#x27;] ^ num,res) return res 前 K 个高频单词 问题：[692】 前K个高频单词，给一非空的单词列表，返回前 k 个出现次数最多的单词。返回的答案应该按单词出现频率由高到低排序。如果不同的单词有相同出现频率，按字母顺序排序。尝试以 O(n log k) 时间复杂度和 O(n) 空间复杂度解决 思路：O(n)空间可以用Trie树或者一般哈希表，O(nlgk)需要长为k的小根堆； 构建Trie树：将单词表中所有单词插入到Trie树，同时统计每个单词的词频； 构建定长小根堆：如果堆的长度小于k则直接将单词和词频压入，否则先入堆再出堆，需要注意的是，词频越小优先级越高，单词字典序越大优先级越高，这需要自定义入栈元素的有限集，可以通过重载&lt;运算符来实现 代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566import heapqclass Trie_node(object): def __init__(self): self.children = &#123;&#125; self.count = 0class Trie(object): def __init__(self): self.root = Trie_node() def insert(self, word): node = self.root for c in word: node = node.children.setdefault(c,Trie_node()) node.count += 1 def find(self, word): node = self.root for c in word: if c in node.children: node = node.children[c] else: return 0 return node.count def get_freqs(self): def dfs(node,cur,res): if node.count: res.append([cur, node.count]) for child in node.children: dfs(node.children[child],cur+child,res) res = [] dfs(self.root,&#x27;&#x27;,res) return res class Element(object): def __init__(self, word, freq): self.freq = freq self.word = word def __lt__(self, other): &quot;&quot;&quot; 重载&lt;操作符 &quot;&quot;&quot; if self.freq == other.freq: return self.word &gt; other.word return self.freq &lt; other.freq class Solution(object): def topKFrequent(self, words, k): &quot;&quot;&quot; :type words: List[str] :type k: int :rtype: List[str] &quot;&quot;&quot; counter = Counter(words) heap = [] count = 0 for word in counter: heapq.heappush(heap,Element(word,counter[word])) count += 1 if count &gt; k: heapq.heappop(heap) return [ele.word for ele in heapq.nlargest(k,heap)] 分析： 空间：构建Trie树O(n)，构建小根堆O(n) 时间：构建Trie树O(n)，小根堆维护O(nlgk) 连接词 问题：[472] 连接词，给定一个不含重复单词的列表，编写一个程序，返回给定单词列表中所有的连接词，连接词的定义为：一个字符串完全是由至少两个给定数组中的单词(非空)组成的12输入: [&quot;cat&quot;,&quot;cats&quot;,&quot;catsdogcats&quot;,&quot;dog&quot;,&quot;dogcatsdog&quot;,&quot;hippopotamuses&quot;,&quot;rat&quot;,&quot;ratcatdogcat&quot;]输出: [&quot;catsdogcats&quot;,&quot;dogcatsdog&quot;,&quot;ratcatdogcat&quot;] 思路： 判断单词是否为连接词等价于判断单词是否可以由至少两个前缀词构成 思路1：哈希表+DFS，将所有单词存放在哈希表中，遍历单词的各个可能的前缀，如果对应后缀在哈希表中或者是连接词，则返回True，否则返回False 思路2：双哈希+DFS+DP，思路1存在重叠子问题，可以用另一个哈希表存储已找到的所有可连接的词； 思路3：Trie树+哈希+DFS+DP，存储这么多的单词，耗费空间巨大，可考虑用Trie树来降低空间复杂度，与思路2基本一致，只是用Trie树代替第一个哈希表 代码： 12345678910111213141516171819# 思路2：双哈希+DFS+DPdef findAllConcatenatedWordsInADict(self, words): &quot;&quot;&quot; :type words: List[str] :rtype: List[str] &quot;&quot;&quot; def dfs(word): if word in dp: return True for i,c in enumerate(word,1): left,right = word[:i],word[i:] if left in s and (right in s or dfs(right)): dp.add(word) return True return False s = set(word for word in words if word) dp = set() return filter(dfs,words) 123456789101112131415161718192021222324252627282930313233343536# 思路3：def findAllConcatenatedWordsInADict(self, words): &quot;&quot;&quot; :type words: List[str] :rtype: List[str] &quot;&quot;&quot; def insert(trie,word): for c in word: trie = trie.setdefault(c,&#123;&#125;) trie[&#x27;#&#x27;] = True def find(trie,word): for c in word: if c in trie: trie = trie[c] else: return False return &#x27;#&#x27; in trie def dfs(word): if word in dp: return True for i,c in enumerate(word,1): left,right = word[:i], word[i:] if find(trie,left) and (find(trie,right) or dfs(right)): dp.add(word) return True return False trie = &#123;&#125; dp = set() for word in words: if word: insert(trie,word) return filter(dfs, words) 分析：思路2与思路3一个用哈希表，一个用Trie树，除此之外几乎完全一致，理论上二者时间复杂度相近，但实际测试发现前者444ms，后者2000ms；而且前者代码要简洁的多。 回文对 问题：[336]回文对，给定一组独特的单词， 找出在给定列表中不同 的索引对(i, j),使得关联的两个单词，例如：words[i] + words[j]形成回文。123给定 words = [&quot;abcd&quot;, &quot;dcba&quot;, &quot;lls&quot;, &quot;s&quot;, &quot;sssll&quot;]返回 [[0, 1], [1, 0], [3, 2], [2, 4]]回文是 [&quot;dcbaabcd&quot;, &quot;abcddcba&quot;, &quot;slls&quot;, &quot;llssssll&quot;] 思路：Trie树/哈希，如果a的前缀是回文，且对应后缀的逆序b在数组中，则b+a构成回文对，类似的，如果a的后缀是回文，且对应的前缀b在数组中，则a+b是回文对，但是后缀为空的情况与对方前缀为空的情况重复，对后缀只讨论非空的情形 代码： 123456789101112131415161718192021222324252627282930313233343536373839def palindromePairs(self, words): &quot;&quot;&quot; :type words: List[str] :rtype: List[List[int]] &quot;&quot;&quot; def insert(trie,word,i): node = trie for c in word: node = node.setdefault(c,&#123;&#125;) node[&#x27;#&#x27;] = i def find(trie,word): node = trie for c in word: if c in node: node = node[c] else: return None return node.get(&#x27;#&#x27;,None) trie = &#123;&#125; for i,word in enumerate(words): insert(trie,word,i) res = [] for i,word in enumerate(words): n = len(word) for j in xrange(n+1): left, right = word[:j], word[j:] releft, reright = left[::-1], right[::-1] if left == releft: k = find(trie,reright) if (k is not None) and k != i: res.append([k,i]) if j != n and right == reright: k = find(trie, releft) if (k is not None) and k != i: res.append([i,k]) return res 分析：空间O(n)，时间O(dn) 引用小白详解 Trie 树]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构与算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构与算法：位运算]]></title>
    <url>%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%9A%E4%BD%8D%E8%BF%90%E7%AE%97%2F</url>
    <content type="text"><![CDATA[原码-反码-补码 机器数：一个数在计算机中的二进制表示叫做这个数的机器数。机器数是带符号的，在计算机中用一个数的最高位存放符号，正数为0，负数为1。 符号位：机器数的最高位 数值位：机器数的其余位 真值：将带符号的机器数对应的真正数值称为机器数的真值。 原码、反码、补码是机器数的不同编码方式，它们都以最高位作为符号位（0代表正数，1代表负数），它们对正数的编码完全相同，不同的只是负数和0的编码。 原码规则： 正数的原码就是其自身 负数的原码就是符号位为1，数值位为真值绝对值 0的原码有两种表示，数值位全为0，符号位为0表+0，为1表-0 可表示范围$[-(2^{31}-1),2^{31}-1]$ 1234567# 以4字节int型为例[1] = 00000000 00000000 00000000 000000001[-1] = 10000000 00000000 00000000 000000001[2147483647] = 01111111 11111111 11111111 11111111[-2147483647] = 11111111 11111111 11111111 11111111[+0] = 00000000 00000000 00000000 000000000[-0] = 10000000 00000000 00000000 000000000 反码规则： 正数的反码就是其自身 负数的反码为其绝对值各位取反 0的反码有两种表示，全为0表+0，全为1表-0 可表示范围$[-(2^{31}-1),2^{31}-1]$ 1234567# 以4字节int型为例[1] = 00000000 00000000 00000000 000000001[-1] = 11111111 11111111 11111111 11111110[2147483647] = 01111111 11111111 11111111 11111111[-2147483647] = 10000000 00000000 00000000 00000000[+0] = 00000000 00000000 00000000 000000000[-0] = 11111111 11111111 11111111 11111111 补码规则： 正数的补码就是其自身 负数的补码为其绝对值各位取反后加1：-n = ~n + 1 0的补码只有一种，全为0 可表示范围$[-2^{31},2^{31}-1]$ 123456# 以4字节int型为例[1] = 00000000 00000000 00000000 000000001[-1] = 11111111 11111111 11111111 11111111[2147483647] = 01111111 11111111 11111111 11111111[-2147483648] = 10000000 00000000 00000000 00000000[0] = 00000000 00000000 00000000 000000000 计算机普遍采用补码，这是因为： 修复了0的重复编码，还能多表示一个最低位 可统一加减运算规则 补码黄金公式补码黄金公式，对所有n都成立： -n = \sim n +1 = \sim (n-1) ~(n-1) = ~n + 1是二进制数按位取补的恒等式，即是说对一个整数先减1再求反和先求反再加1得到的结果是一样的。 -n = ~n + 1 连接了相反数和取补的关系（相反数取反加一）。 123证明： -n = ~n + 1 &lt;=&gt; -(n-1) = ~(n-1) + 1 &lt;=&gt; -n = ~(n-1) -n = ~n + 1 &lt;=&gt; ~(-n) =~(~n+1)=~(~(n-1))=n-1 &lt;=&gt; n = ~(-n) + 1 常见数字的补码12345678910# 32位整型上界2*31-101111111 11111111 11111111 11111111# 100000000 00000000 00000000 00000001# 000000000 00000000 00000000 00000000# -111111111 11111111 11111111 11111111# 32位整型下界-2**3110000000 00000000 00000000 00000000 补码的加减运算和的补码等于补码的和，若符号位有进位则丢掉：[x + y]补 = [x]补 + [y]补 1234567891011[33-15]补=[33]补+[-15]补 0000 .... 0010 0001 1111 .... 1111 000110000 .... 0001 0010丢掉符号位的进位得18[3-15]补=[2]补+[-15]补 0000 .... 0000 0011 1111 .... 1111 0001 1111 .... 1111 0100&lt;=&gt;减1取反得绝对值：0000 .... 0000 1100得-12 C与Python对补码处理的比较C和python对整型数据的各种位操作都是按照对该数据的补码进行的，但是在对符号位和溢出的处理上有很大不同，这些差异可能会影响到算法移植的有效性。 C 固定符号位：C语言中有符号整型数据在不同机器上可能是32位或64位，但无论位宽多少其符号位总是固定的 可能溢出：对于unsigned整型溢出，C的规范是有定义的——“溢出后的数会以2^(8*sizeof(type))作模运算”。对于signed整型的溢出，C的规范定义是“undefined behavior”，也就是说，编译器爱怎么实现就怎么实现。 Python 浮动符号位：python中int型数据在32位机器上位宽为32位，在64位机器上位宽为64位。自从Python2.2起，如果发生溢出，Python会自动将整型数据转换为长整型，Python中的长整型，没有指定位宽，也就是说Python没有限制长整型数值的大小，只限制于机器内存。由于其位宽不定，其补码的符号位可向外无限扩展 不存在溢出：由于符号位是根据数据长度自动扩展，因此python中也不存在溢出的可能 python处理32位整数： 123456789# 截取32位二进制0xffffffff &amp; -14294967295# 补全32位二进制&#x27;&#123;:032b&#125;&#x27;.format(3)&#x27;00000000000000000000000000000011&#x27;&#x27;&#123;:032b&#125;&#x27;.format(-3)&#x27;-0000000000000000000000000000011&#x27; 原码、反码和补码的相互转换三者正数的表达是一致的，对于负数三者之间的相互转化规则如下： 位运算符C和python对整型数据的各种位操作都是按照对该数据的补码进行的 两种视角看待位运算符： 平等视角：两个操作数地位平等 主从视角：把A当做被操作数，把B当做主操作数，把A * B，看做是用B去改造A，我们只需关注B的不同构造对A的不同影响。 移位运算符 位操作符 说明 C Python 等价 x &lt;&lt; y 左移，x左移y位 右侧补零，左侧截断 右侧补零，左侧不会溢出 x*(2**y) x &gt;&gt; y 右移，x右移y位 右侧截断，左侧不定 右侧截断，左侧补符号位 x//(2**y) C语言中的右移： 算术移位：右移时，某些机器将对左边空出的部分用符号位填补 逻辑移位：另一些机器则对左边空出的部分用0填补 将某位移至指定位置 1234# 获取二进制各位x = 10for i in xrange(32): visit(x &gt;&gt; i &amp; 1) 模拟乘除运算 1234561&lt;&lt;2415&gt;&gt;31-2&gt;&gt;222-1 位逻辑运算符 位操作符 平等视角 主从视角 运算律 x &amp; y 位与，都是1才是1 1处不变、0处置零 交换结合、x&amp;x=x、x&amp;0=0、x&amp;1=x x ` ` y 位或，都是0才是0 1处置1、0处不变 交换结合、x` x=x、x 0=x、x `1=x x ^ y 位异或，一个1一个0才是1 1处取反、0处不变 交换结合、x^x=0、x^0=x、x^1=~x ~ x 按位求补，1变0，0变1 与将某些位变成零：合理安排主操作数B中的1，结果中将会保留A中对应的位，而将其他位置0，起到过滤效果，因此越与越小。 1234567891011121314151617181920212223242526272829# 保留A中最后一位，可用于判断整数奇偶45&amp;11# 保留A中第32位，可用于提取符号位-1&gt;&gt;31-1-1&gt;&gt;31 &amp; 11# 保留偶数位22&amp;0xAAAAAAAA2# 保留奇数位22&amp;0x55555555# 保留第i+1位a &amp; 1&lt;&lt;i# 保留最后一个1n&amp;~(n-1)# 去掉最后一个1n&amp;(n-1)# 保留最后一个字节257 &amp; 0xFF1 或将某些位置为1：B为1的位置会将A对应位置置为1，其他位保持不变，因此越或越大。 12# 将奇数位置122|0x55555555 二进制位合并：整体上看，或运算会融合A和B中所有的1；从单个操作数来看，结果会保留本元素中所有的1，0处则取对方的数位。1234567891011121314151617# 由一些二进制位拼出完整二进制序列res = 0 for i in xrange(32): bit = 0 for num in nums: bit += num &gt;&gt; i &amp; 1 res |= bit % 3 &lt;&lt; i if res &gt;&gt; 31 == 1: # python 32位不是符号位，所以要手动处理 res -= 2**32 return res #互换二进制数的奇偶位 # 先通过左移将奇数位移动到偶数位，再与0xAAAA的与运算，只保留偶数位，奇数位置零 # 再通过右移将偶数位移动到奇数位，再与0x5555的与运算，只保留奇数位，偶数位置零 # 对以上两个结果进行或运算(n&lt;&lt;1)&amp;(0xAAAA))|((n&gt;&gt;1)&amp;(0x5555) 异或 常用公式 1234567891011121314# 交换律和结合律A ^ B = B ^ AA ^ B ^ C = A ^ (B ^ C)# 恒等率、归零律、奇偶律X ^ 0 = XX ^ X = 0A ^ A ^...^ A ^ A = 奇数个就是A,偶数个就是0# 移项A ^ B = C &lt;=&gt; A = C ^ B &lt;=&gt; B = C ^ A# 与-1异或相当于求反A ^ -1 = ~A 翻转某些位：合理安排主操作数B中的1，结果中将会翻转A中对应的位，而将其他位不变，达到翻转被操作数A中某些位的目的。常用的有以下几种： 交换两值 1234# 不用多余空间实现两值交换a = a ^ bb = a ^ ba = a ^ b 判断序列中某元素出现次数的奇偶性 1A ^ A ^...^ A ^ A = 奇数个就是A,偶数个就是0 整型数组中寻找出现特殊次数的元素 1234567891011121314151617181920212223242526272829303132333435363738394041# 问题1：除了一个元素出现一次，所有数字都出现了偶数次，找到该元素# 解法：只需求出所有数字的异或结果就是那个只出现一次的数字A ^ B ^ C ^ B ^ C ^ D ^ A= A ^ A ^ B ^ B ^ C ^ C ^ D= 0 ^ 0 ^ 0 ^ D= 0 ^ D= D# 问题2：除了两个数字出现一次，其他数字都出现两次，找到这两个元素# 解法：① 先求所有元素的异或，得到这两个数的异或，获取该异或最右侧的1，说明这两个元素在该位上不同；# ② 筛选所有该位为1的元素，求其异或就可以得到该位为1的那个出现一次的数；# ③ 然后用该数与第一步得到的值进行异或就可得到另外一个数；xor = 0for num in nums: xor ^= numlast1 = xor &amp; ~(xor-1)xo = 0for num in nums: if last1 &amp; num: xo ^= numreturn [xo,xo^xor]# 问题3：除了一个数字出现1次，其他数字都出现了3次# 解法：把所有整数的每一位分别相加后对3取余，即得到该数字的各个二进制位，然后再把二进制位合并转化为数值# 该方法可以作为求解“只有一个数字出现1次，其他都出现了N次的问题(N&gt;1)”def singleNumber(nums): &quot;&quot;&quot; :type nums: List[int] :rtype: int &quot;&quot;&quot; res = 0 for i in xrange(32): bit = 0 for num in nums: bit += num &gt;&gt; i &amp; 1 res |= bit % 3 &lt;&lt; i if res &gt;&gt; 31 == 1: # python 32位不是符号位，所以要手动处理 res -= 2**32 return res 非 各位取反 求相反数：-n = ~n + 1 = ~(n-1) 常规操作 python处理32位整数:使用python做位运算时，谨记python的“符号位浮动”、“不存在溢出”这两大特征！ 12345678910111213141516171819202122232425262728293031# 三个有用的数mask = 0xFFFFFFFF # 掩码，用于截断、按位取反MIN_INT = 0x80000000 # 32位最小负数MIN_MAX = 0x7FFFFFFF # 32位最大正数# 32位截断a &amp; 0xFFFFFFFF# 32位取反a ^ 0xFFFFFFFF# 获取符号位a &gt;&gt; 31 &amp; 1# 32位正整数：不溢出 # 在不超出0x7FFFFFFF的范围内，python正整数和C语言signedint机器数一致 # 超出0x7FFFFFFF，python不会溢出，也不会截断 2**32 4294967296# 32位负整数：浮动符号位 # 在不超出0x80000000范围内将python负数转化为32位负数，直接截断即可。截断后的二进制在python中会被当做是一个正数，在C语言中则是与截断前真值相等的负整数 -1&amp;0xFFFFFFFF 4294967295 # 将32位负数转化为python负数，高位需要补1； # 或者从另一个角度看，因为它们的正数表示是一致的，可以先将负数转化为绝对值，再转化为python中的负数~(a^mask+1)+1 = ~(a^mask)~(4294967295^0xFFFFFFFF)-1 bin和int的用法 1234567891011# bin()将整数转化为二进制字符串,有几点需注意 # ① 返回的二进制字符串只显示第一个1后面的二进制位，不显示更高位bin(255)&#x27;0b11111111&#x27; # ② 返回的二进制字符串以原码形式表示，正数以0b开头，负数0b前面有负号,可能是因为浮动符号位的原因，无法全部显示出来bin(-255)&#x27;-0b11111111&#x27;# int(str,2)将字符串转化为对应的整数，0b可带可不带int(&#x27;-11111111&#x27;,2)-255 获取32位整型的二进制补码表示 123456789101112131415# 对应小于2**31-1的正数&#x27;&#123;:0&gt;32b&#125;&#x27;.format(255)&#x27;00000000000000000000000011111111&#x27;# 负数不行&#x27;&#123;:0&gt;32b&#125;&#x27;.format(-1)&#x27;000000000000000000000000000000-1&#x27;# 自定义方法def get32bits(num): res = &#x27;&#x27; for i in xrange(31,-1,-1): res += str(num &gt;&gt; i &amp; 1) return res# 将python的整数转化为补码形式的二进制字符串之后，可方便的利用字符串方法求解二进制的一些问题 如何将32位二进制补码转化为整数 12345678def bits_32nums(s): res = 0 for i in xrange(32): res &lt;&lt;= 1 res |= int(s[i]) if res &gt;&gt; 31 == 1: # python 32位不是符号位，所以要手动处理 res -= 2**32 return res 其他问题12345678910111213141516# 问题1：判断一个数是否为2的幂# 解法：是2的幂&lt;=&gt;首位为1其他位为0&lt;=&gt; n!=0 and not n&amp;(n-1)# 问题2：判断一个数是否为4的幂# 解法：是4的幂&lt;=&gt;是2的幂，且1在奇数位&lt;=&gt;n!=0 and not n&amp;(n-1) and n&amp;0x55555555 == n# 问题3：不用条件判断求绝对值# 解法：非负数直接返回，负数，-n = ~n + 1=n^-1 - (-1)def bit_abs(num): negative = num &gt;&gt; 31 return (num ^ negative) - negative # 问题4：按整数位求解一般问题？？？ 位操作实现加减乘除基于以下公式： -n = ~n + 1 = ~(n-1) 去掉整数n二进制中最右边一个1：n &amp; (n-1),如：n=010100，n-1=010011，n&amp;(n-1)=010000 仅保留整数n二进制中最后一个1：n &amp; ~(n-1),如：n=010100，则~(n-1)=101100，n&amp;~(n-1)=000100 运算 原理 加法 a+b = a^b + (a&amp;b)&lt;&lt;1 减法 a-b = a + ~(n-1) 乘法 a*b = a*(1&lt;&lt;i1+1&lt;&lt;i2...) 除法 a = b*(1&lt;&lt;imax + 1&lt;i2...)+mod 加减运算原理：将加法计算分解为两部分：①计算不进位的结果，②是计算各位的进位，然后再将两者相加即得到结果。a,b = a^b,(a&amp;b)&lt;&lt;1 1、不进位的结果 1234567890 + 0 = 00 + 1 = 11 + 0 = 11 + 1 = 0这个过程可以用异或运算实现：0 ^ 0 = 00 ^ 1 = 11 ^ 0 = 11 ^ 1 = 0 2、各位的进位 1234567890 + 0 = 00 + 1 = 01 + 0 = 01 + 1 = 10这个过程可以用位与和移位来实现：0 &amp; 0 = 0 (0 &amp; 0)&lt;&lt;1 = 0 0 &amp; 1 = 0 (0 &amp; 1)&lt;&lt;1 = 01 &amp; 0 = 0 (1 &amp; 0)&lt;&lt;1 = 01 &amp; 1 = 1 (1 &amp; 1)&lt;&lt;1 = 10 代码一般实现 123456789101112131415# 加法-非递归def getSum(a,b): while b: a,b = a^b, (a&amp;b)&lt;&lt;1 return a# 加法-递归def getSum(a,b): if b: return getSum(a^b,(a&amp;b)&lt;&lt;1) else: return a# 减法a-b=a+(-b)=a+(~b+1)def subtraction(a,b): return getSum(a,getSum(~b,1) Python实现因为在python中不存在溢出截断、符号位也不固定，以上减法无法得到正确结果。为了模拟C的行为，需要在每次计算完之后人为截断符号位的进位，以及将符号位固定在32位。1234567def getSum(a,b): MAX = 0x7FFFFFFF # 32位整型最大值 MIN = 0x80000000 # 32位整型最小值 mask = 0xFFFFFFFF while b != 0: a, b = (a ^ b) &amp; mask, ((a &amp; b) &lt;&lt; 1) &amp; mask # 截断32位 return a if a &lt;= MAX else ~(a ^ mask) # 如果为负数，先按补码划为正数再划为负数~(a^mask+1)+1= ~(a^mask) 乘法运算 原理：a*b = a*(2**i1+2**i2+...) = a*(1&lt;&lt;i1 + 1&lt;&lt;i2...)=a&lt;&lt;i1+a&lt;&lt;i2...，从低到高检测b的每一位，如果是1，则在结果中累加（res += a&lt;&lt;i） 12345def multiply(a,b): res =0 for i in xrange(32): if b&amp;(1&lt;&lt;i):res += a&lt;&lt;i return res 除法运算 原理：x = y*(1&lt;&lt;i1+1&lt;&lt;i2...) + mod，从高到低，减得动y&lt;&lt;i就减，同时把1&lt;&lt;i加到结果中去。1234567def divide(x,y): res = 0 for i in xrange(31,-1,-1): if x &gt;= y&lt;&lt;i: res += 1&lt;&lt;i x -= y&lt;&lt;i return res 参考 [1] 原码, 反码, 补码 详解 [2] 整型溢出 [3] 不用加减乘除做加法中Python存在的bug]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构与算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构与算法：图（一）—— 概念]]></title>
    <url>%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%9A%E5%9B%BE%EF%BC%88%E4%B8%80%EF%BC%89%E2%80%94%E2%80%94%20%E6%A6%82%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[如果一个问题可以基于实体/状态间的某种多对多的关系来求解，那么可以尝试用图的数据结构和相关算法来求解。用图解决实际问题的一般步骤： 定义图的数据结构：从实际问题中抽象出相互关联的可区分的状态 状态作为图的顶点：将状态参数作为不同顶点的唯一标识； 关联作为节点的边：通过这种关联找到顶点的邻接点； 图的搜索算法：以某种方式沿着图中的边遍历图中各个顶点，图搜索算法可以用来发现图的结构，可以借助一些辅助的数据结构来描述图中的某种结构； 原问题的求解：基于图的结构和辅助数据结果求解原问题的解； 如果对图的相关算法熟悉的话，那么解决实际问题的关键就在于第一步了，即如何从实际问题中抽象出相互关联的状态，进而定义出图中的不同顶点和边，然后通过合适的搜索算法发现图中的结构，求解原问题的解。本文相关算法实例将按照以上步骤来整理解题思路。 逻辑结构图G由有限非空的顶点集V和边集E组成，记做G=(V,E)。 1、有向图：图中的边是有向的，u称为边头，v称为边尾 123G = (V,E)V = &#123;A,B,C,D&#125;E = &#123;&lt;A,D&gt;,&lt;B,A&gt;,&lt;B,C&gt;,&lt;C,A&gt;&#125; 2、无向图：图中的边是无向的 123G = (V,E)V = &#123;A,B,C,D&#125;E = &#123;(A,B),(A,C),(A,D),(B,C),(C,D)&#125; 3、简单图：无自边无重复边 4、完全图：任意两个节点间都存在边的简单图，对于含n个顶点的无向完全图来说，存在n(n-1)/2条边，对于含n个顶点的有向完全图来说，存在n(n-1)条边 5、子图：图g中顶点集和边集是图G中顶点集和边集的子集 6、连通图：在无向图中，如果任意两个顶点之间都有路径，则称该图为连通图。如果含n个顶点的无向图中边的个数小于n-1，则该图一定不是连通图 7、连通分量：无向图的极大连通子图（子图中包含子图节点间所有可能的边）称为该图的一个连通分量，直观的理解图中可以相互“导电”的节点和边构成图的一个连通分量 8、强连通图：在有向图中，任意两个顶点u,v间都有从u到v和从v到u的路径，则称该图为强连通图 9、强连通分量：有向图的极大强连通子图称为该图的一个强连通分量 10、生成树/生成森林：在连通图中，包含图中所有顶点的一个极小连通子图称为该图的一个生成树；在非连通图中，每个连通分量的生成树构成了非连通图的生成森林 11、顶点的度：在无向图中，顶点v的度等于依附于该顶点的边的个数；在有向图中，顶点v的度等于顶点v的出度和入度之和，顶点v的出度指所有以顶点v开始的边的个数，顶点v的入度指所有以顶点v结束的边的个数 12、边的权：图中每条边可以标上具有某种含义的数值，该数值称为该边的权值 13、路径：u到v的一条路径是指u,p1,p2,…,v，路径上边权值之和称为路径长度，u==v的路径称为回路或环 14、距离：从顶点u到顶点v的最短路径如果存在，则此路径的长度称为从u到v的距离 15、稀疏/稠密图：边很少的图称为稀疏图，反之称为稠密图，一般将E&lt;VlogV的图称为稀疏图 1.2 存储结构图的存储结构通常包含两个部分： 顶点存储V：使用某种数据结构存储图中所有顶点，如果用一个列表存储图中所有顶点，那么后面我们就可以用顶点下标来指代该顶点 关系存储E：存储顶点间的所有邻接关系，关系存储是图的存储结构的核心，根据关系存储形式的不同，可以将图的存储结构划分为邻接表、邻接矩阵、边表等 对于存储结构的选择需要综合考虑空间和时间上的开销，时间上的开销可以用图上最常用的基本操作来衡量，以下简单列举了图中最常用的操作API: adj(G,u):获取图G中顶点u的所有邻接点 edge(G,u,v):获取图G中的边(u,v) insert_v(G,u):在图G中添加顶点u delete_v(G,u):删除图G中的顶点u insert_e(G,u,v):在图G中添加边(u,v) delete_e(G,u,v):删除图G中的的边(u,v) 比较常用的几种图的存储结构对于空间存储以及以上各种操作的时间效率： 存储结构 空间 获取邻接点 获取边 插入顶点 删除顶点 插入边 删除边 邻接表 $O(V+E)$ $O(1)$ $O(1)$ $O(V)$ $O(V)$ $O(1)$ $O(1)$ 邻接矩阵 $O(V^2)$ $O(V)$ $O(1)$ $O(V)$ $O(V)$ $O(1)$ $O(1)$ 边表 $O(V+E)$ $O(E)$ $O(E)$ $O(V)$ $O(E)$ $O(1)$ $O(1)$ 假设以上邻接表通过hash实现，可见在大多数情形邻接表的哈希实现效率更高 1.2.1 邻接表1.2.1.1 邻接表实现邻接表存储了各个顶点到其邻接点集的映射关系： \left \{ v:\left \{ adjv1:weight1,... \right \},...\right \} v1：顶点 {$adjv1:weight1,… $}：v1的邻接表 adjv1：顶点v1的一个邻接点 weight1：边(v1,adjv1)的权重 邻接表存储结构通常包含两部分结构： 存储图中每个顶点的结构:可以用字典、列表等数据结构 存储每个顶点的邻接点集：可以用字典、集合、列表、元组、链表等数据结构 123456# 字典-字典(适用范围最广，可以直接以状态作为键值)G = &#123;0:&#123;1:1,2:5,3:2&#125;,1:&#123;2:3,4:7&#125;,2:&#123;5:6&#125;,3:&#123;5:8&#125;,4:&#123;5:4&#125;,5:&#123;&#125;&#125;# 字典-列表G = &#123;0:[(1,1),(2,5),(3,2)],1:[(2,3),(4,7)],2:[(5,6)],3:[(5,8)],4:[(5,4)],5:[]&#125;# 列表-列表(如果顶点不是0~n-1范围，可以定义另一个数组建立下标与顶点的映射)G = [[(1,1),(2,5),(3,2)],[(2,3),(4,7)],[(5,6)],[(5,8)],[(5,4)],[]] 邻接表基本操作实现123456789G = &#123;0:&#123;1:1,2:5,3:2&#125;,1:&#123;2:3,4:7&#125;,2:&#123;5:6&#125;,3:&#123;5:8&#125;,4:&#123;5:4&#125;,5:&#123;&#125;&#125;def adj(G,u): return G[u]def edge(G,u,v): return G[u][v]print adj(G,1)print edge(G,1,2) 邻接表转化为邻接矩阵和边表1234567891011121314151617181920212223242526272829303132333435363738G = &#123;0:&#123;1:1,2:5,3:2&#125;,1:&#123;2:3,4:7&#125;,2:&#123;5:6&#125;,3:&#123;5:8&#125;,4:&#123;5:4&#125;,5:&#123;&#125;&#125;# 邻接表转化为邻接矩阵def adj2matrix(G): V = G.keys() n = len(V) matrix = [[float(&#x27;inf&#x27;)] * n for _ in xrange(n)] for u,adj in G.items(): for v,w in adj.items(): matrix[u][v] = w return matrixadj2matrix(G) [[inf, 1, 5, 2, inf, inf], [inf, inf, 3, inf, 7, inf], [inf, inf, inf, inf, inf, 6], [inf, inf, inf, inf, inf, 8], [inf, inf, inf, inf, inf, 4], [inf, inf, inf, inf, inf, inf]] # 邻接表转化为边表def adj2edge(G): V = G.keys() n = len(V) edge = [] for u,adj in G.items(): for v,w in adj.items(): edge.append((u,v,w)) return edgeadj2edge(G) [(0, 1, 1), (0, 2, 5), (0, 3, 2), (1, 2, 3), (1, 4, 7), (2, 5, 6), (3, 5, 8), (4, 5, 4)] 邻接矩阵邻接矩阵实现邻接矩阵：用一个二维数组存储图中边的信息 A[i][j]=\left\{\begin{matrix} w_{ij}& (v_i,v_j)\ or\ \in \ E\\ \infty & else \end{matrix}\right. 邻接矩阵基本操作实现1234567def adj(G,u): return [v for v in G[u] if v != float(&#x27;inf&#x27;)]def edge(G,u,v): return G[u][v]print adj(G,1)print edge(G,1,2) 邻接矩阵转化为邻接表和边表12345678910111213141516171819202122232425262728293031323334353637383940414243inf = float(&#x27;inf&#x27;)G = [[inf, 1, 5, 2, inf, inf], [inf, inf, 3, inf, 7, inf], [inf, inf, inf, inf, inf, 6 ], [inf, inf, inf, inf, inf, 8 ], [inf, inf, inf, inf, inf, 4 ], [inf, inf, inf, inf, inf, inf]] # 将邻接矩阵转化为邻接表def matrix2adj(G): n = len(G) adj = collections.defaultdict(dict) for u in xrange(n): for v in xrange(n): if G[u][v] != float(&#x27;inf&#x27;): adj[u][v] = G[u][v] return adjmatrix2adj(G) defaultdict(dict, &#123;0: &#123;1: 1, 2: 5, 3: 2&#125;, 1: &#123;2: 3, 4: 7&#125;, 2: &#123;5: 6&#125;, 3: &#123;5: 8&#125;, 4: &#123;5: 4&#125;&#125;) # 将邻接矩阵转化为边表def matrix2edge(G): n = len(G) edge = [] for u in xrange(n): for v in xrange(n): if G[u][v] != float(&#x27;inf&#x27;): edge.append((u,v,G[u][v])) return edgematrix2edge(G)[(0, 1, 1), (0, 2, 5), (0, 3, 2), (1, 2, 3), (1, 4, 7), (2, 5, 6), (3, 5, 8), (4, 5, 4)] 边表边表实现 起点 0 0 0 1 1 2 3 4 终点 1 2 3 2 4 5 5 5 权 1 5 2 3 7 6 8 4 边表中不能包含所有顶点，需与顶点存储相结合使用。 边表基本操作实现12345678910111213def adj(G,u): return &#123;y:w for x,y,w in G if x == u&#125;def edge(G,u,v): for x,y,w in G: if (x,y) == (u,v): return x,y,w return Noneprint adj(G,1)print edge(G,1,2)&#123;2: 3, 4: 7&#125;(1, 2, 3) 边表转化为邻接表和邻接矩阵12345678910111213141516171819202122def edge2adj(G): adj = collections.defaultdict(dict) for u,v,w in G: adj[u][v] = w return adjprint edge2adj(G)defaultdict(&lt;type &#x27;dict&#x27;&gt;, &#123;0: &#123;1: 1, 2: 5, 3: 2&#125;, 1: &#123;2: 3, 4: 7&#125;, 2: &#123;5: 6&#125;, 3: &#123;5: 8&#125;, 4: &#123;5: 4&#125;&#125;)def edge2matrix(G,V): n = len(V) matrix = [[float(&#x27;inf&#x27;)] * n for _ in xrange(n)] for u,v,w in G: matrix[u][v] = w return matrixprint edge2matrix(G,xrange(6))[[inf, 1, 5, 2, inf, inf], [inf, inf, 3, inf, 7, inf], [inf, inf, inf, inf, inf, 6], [inf, inf, inf, inf, inf, 8], [inf, inf, inf, inf, inf, 4], [inf, inf, inf, inf, inf, inf]] 实战篇python的collections库提供了众多容器，方便对图的操作，包括: defaultdict：常用于存储图的数据结构 deque：相比于内置的list，双端队列deque可以实现在O(1)的时间复杂度下在两端进行append和pop操作 set：常用于visited DIJ单源最短路径：488祖玛游戏 应用： 有环、无环、环中元素； 连通子图 强连通图； 生成树； 最小生成树； 最短路径； 802拓扑排序；前提：有向无环图 并查集： 括号匹配： DFS进行拓扑排序 BFS出度入度进行拓扑排序：每次取出一个入度/出度为0的节点，并更新相关节点的出度/入度；一般由邻接表得到，零度队列、入度字典，出度邻接点； 初始化零度队列、出度入度字典 出队收集、更新相关节点的度，如果更新后度为0放入队列！！ 割韭菜，有环的话，会把环剩下！！！ 从BFS视角看，拓扑排序类似于层序遍历，首先将所有度为0的节点入队，然后每一次出队，修改与出队元素相关的节点的度，将修改后度为0的入队 邻接法 邻接矩阵 邻接表 邻接字典 边表法 Dijkstra 前提：如果权值非负，最短路径上的子路径也必定是最短路径 分治思路尝试用分治思想解决树相关的问题：首先将原问题转化为关于左右子树的子问题(同性质的或不同性质的)，然后通过对子问题进行综合得到原问题的解，可以形式化表示为： f(root)=h(g(root.left),g(root.right)) 这类问题有： 树是否对称 树的最大深度 树中节点值之和 叶节点最短路径 到叶节点的路径：最短-最长-和， 相同树、镜像树 图算法可以看做是树算法的扩展(树可以看做是无环连通图)，所不同的是： 图可能多源 图可能包含环 每个节点由多个邻接点 相应的处理方法： 遍历图中所有节点，以未访问过的节点为地点进行遍历 标记每个节点是否访问过，递归调用只发生在未访问过的节点 找到每个节点的邻接点表示法，遍历每个邻接点进行递归调用 标准的DFS 规范的DFS包括两个步骤：调用时保证合法性，通过参数向下传递数据，通过返回值向上传递数据 整体遍历dfs(G)： 初始化标记：将所有节点初始化为未访问 遍历所有节点：对未访问过的节点调用单源dfs 单源遍历dfs_visit(v)： 标记节点：将当前节点标记为已访问 遍历所有邻接点：对未访问过的邻接点递归调用单源dfs 如何与问题结合：在深度遍历过程中解决实际应用问题 判断题 最优题 列出所有题 DP的两种实现方式： 自下向上的毯式填表法：先求解边界的较小子问题并填表，再依据状态转移方程依次求解较大子问题并填表，当表填满时，即可方便查到最终问题的解。 求解次数：所有子问题都将被求解一次； 查询次数：查询次数等于依赖的边数； 自上向下递归填表法：如果解已在表中，直接从表中读取，否则通过递归求解，并填表。 求解次数：只会求解相关的子问题的解，每个子问题只会被求解一次； 查表次数：等于依赖边数； 评价：自上向下的递归填表法，更加容易实现，且求解次数较少； 表的形式： 如果问题规模可以通过连续的整数来表示，则一般可以使用矩阵来表示； 如果问题规模不方便用连续整数表示，则可以用字典表示，{(问题规模的参数):对应的解}； 评价：字典更通用，但会用掉更多的空间； 树宽度层序遍历：12345Q = [root]while Q: left = Q[0].val Q = [y for x in Q for y in [x.left,x.right] if y]return left 变形： 逐层处理：每次用新的一层节点替换掉队列中的节点，方便统计总共有多少层、计算每层的统计量；如最左侧、最右侧元素、每层元素个数、最大值、平均值 利用左右子树做递归时边界往往有四种情形： 1234567891011if not root: passelif root.left==None and root.right==None: passelif root.left==None: passelif root.right==None: passelse: pass 保证边界正确、递归正确，整个过程就是正确的。 树的遍历用的最多的是先序遍历和层序遍历，这可以被看做是图的BFS和DFS，在写法上有些不同，因为单源、无环、邻接点固定，所以写起来更简单： 12345def dfs(root): if root: visit(root) for child in [root.left,root.right]: dfs(child) edge = [(i,j) for i in range(m) for j in [0,n-1]] edge += [(i,j) for j in range(n) for i in [0,m-1]] 关于树的算法：第一反应必须是能否用递归解！！左子树、右子树的解和整体解有什么关系？？分情况讨论后能否和子树的解建立关系？？ 如果跟层有关用层序遍历！！ 矩阵中的DFS DFS和DP在求解DFS最优值时，如果出现了重叠子问题，可以通过记录中间结果来加快求解速度。 123456789101112131415161718def dfs(i,j): if dp[i][j]: return dp[i][j] else: res = 0 for ii,jj in adj[i,j]: if 合法邻接点: res = f(res,dfs(ii,jj)) dp[i][j] = res return resdp = [[]]for i in range(m): for j in range(n): if ...: dfs(i,j)解在dp中 解决问题的一般思路： 问题的结构化描述→设计相关数据结构→设计算法 先写思路，貌似思维更清晰，代码书写速度也更快 连通图、强连通图连通图：在外层遍历统计进入多少次574朋友圈 图的表示法https://blog.csdn.net/woaidapaopao/article/details/51732947 邻接法：“节点-邻接点”的形式方便直接用于遍历 自定义 邻接矩阵 邻接表 邻接字典：{节点:邻接点} 边表/集合/元组：节点编号+边表[(i,j)]210,边表中有可能某些节点未出现 4, [[1,0],[2,0],[3,1],[3,2]] 递归字典 拓扑排序-有环无环210 节点颜色：白-灰-黑 边的分类：u→v时v的颜色、判断有无环、两节点在生成树中的关系，白边-灰边-黑边 先判断有环无环：灰边理论 变色时间：拓扑排序、合法括号 在无环状态下，按u.f逆序排列 多源层序遍历： 初始Q放多个就行了 DFS如果需要自己构造图，可以通过构造一个哑结点来简化代码，但是也没必要引入逻辑的例外，反而复杂.不如统一为按图原来最直观的样子不做额外处理！！这样的好处是外层循环调用和内层循环调用形式一样，不用特殊处理了！ 判断一个矩阵是否为空：if not any(A): 494一类题：满足条件的路径，中间路径作为DFS的参数，每次维护路径，满足条件时处理 DFS超时的处理思路：DP或者剪枝 待整理： 理论：实践：索引形式、每种题型的思路 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061import collectionsT = input()def solve(adj,n): color = [0] * n base = set(xrange(n)) def helper(i): si = base - set(adj[i]) si.add(i) for j in si: if j != i and color[j] == 0: color[j] = 1 ssi = base - set(adj[j]) ssi.add(j) if ssi != si: return False return True for i in xrange(n): if color[i] == 0: color[i] = 1 if not helper(i): return False return Truefor _ in xrange(T): N,M = map(int,raw_input().split()) adj = collections.defaultdict(list) for _ in xrange(M): x,y = map(int,raw_input().split()) adj[x-1].append(y-1) adj[y-1].append(x-1) def solve(adj,n): color = [0] * n base = set(xrange(n)) def helper(i): si = base - set(adj[i]) si.add(i) for j in si: if j != i and color[j] == 0: color[j] = 1 ssi = base - set(adj[j]) ssi.add(j) if ssi != si: return False return True for i in xrange(n): if color[i] == 0: color[i] = 1 if not helper(i): return False return True if solve(adj,N): print &#x27;Yes&#x27; else: print &#x27;No&#x27;]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构与算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构与算法：图（二）—— BFS]]></title>
    <url>%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%9A%E5%9B%BE%EF%BC%88%E4%BA%8C%EF%BC%89%E2%80%94%E2%80%94%20BFS%2F</url>
    <content type="text"><![CDATA[广度优先搜索(Breadth-First-Search,BFS)：类似于二叉树的层序遍历，首先访问起始顶点v，接着依次访问v的所有未被访问的邻接点w1,w2,…，再依次访问w1,w2,…所有未被访问的邻接点…，直至所有顶点都已被访问。类似的思想还应用于Prime最小生成树算法和Dijkstra单源最短路径算法。 BFS能够发现图中关于源节点的“层次结构”，BFS算法只有在发现所有距离源节点s为k的所有节点之后，才会发现距离源节点s为k+1的其他节点。因此BFS常被用来求解多对多关系中的层次依赖问题，如单源最短距离、凸台积水等问题。 理论篇BFS算法模板为了深入理解BFS的搜索过程，我们通过一些辅助数据结构来追踪算法进展： 顶点颜色：灰色是已发现和未发现的边界 白色顶点：未被发现的顶点 灰色顶点：顶点已被发现，但是顶点的邻接点尚未被完全发现 黑色顶点：顶点以及它所有的邻接点都已被发现 顶点的父节点：记录每个顶点在当前搜索过程中的父亲节点，通过各个节点的父亲节点可以方便地构造出一棵广度优先生成树 顶点的层数：各个顶点距离源顶点的距离，对于同一个源节点来说，每层节点的遍历顺序不同，对应的广度优先生成树可能不同，但是每个顶点到源节点的距离是唯一的 BFS标准模板 问题：按照广度优先的顺序依次遍历图中每个顶点，记录每个顶点的单源距离和父节点 思路：使用队列，循环父出子进，直至队列为空 123456789101. 初始化辅助数据结构： 1. 节点属性：如颜色、父亲、距离等，通常有三种方式存储节点属性 1. 字典：key为标识节点的状态参数，value为对应属性，在顶点状态参数和属性间建立直接联系，通用、简单； 2. 列表：列表下标用于标识节点，列表中的值代表对应节点的属性值，顶点通过属性列表的下标和顶点列表中的顶点参数建立映射关系； 3. 定义顶点类：简单情形，字典可以起到同样效果 2. 队列：将初始节点放入队列中，如有必要的数据需要在父子之间传递，也可随顶点一起入队，标记灰色2. 循环出队入队，直至队列为空 1. 获取出队顶点u 2. 遍历出队顶点u的所有白色邻接点，入队，标记为灰色 3. 将对对顶点u标记为黑色 代码： 1234567891011121314151617181920212223242526272829import collectionsdef bfs(G, V, s): &quot;&quot;&quot; BFS逐个遍历，这里使用deque双端队列 @G：以邻接表表示的图，&#123;u:&#123;v1,v2,...&#125;&#125; @V：图中所有顶点 @s：广度优先遍历的出发点 return：每个节点在广度优先树中的父节点，深度 &quot;&quot;&quot; # 初始化指标 color = collections.defaultdict(int) father = collections.defaultdict(lambda: None) level = collections.defaultdict(int) # 广度优先遍历 Q = collections.deque() Q.append((s,0)) color[s] = 1 while Q: u, d = Q.popleft() for v in G[u]: if not color[v]: color[v] = 1 father[v] = u level[v] = d + 1 Q.append((v, d+1)) color[u] = 2 return father,level 时间复杂度分析：邻接表O(V+E)，邻接矩阵 $O(V^2)$ 对队列的操作：因为每个节点只出队入队一次，所以队列操作的时间复杂度为O(V) 搜索邻接点：每条边都会被遍历一次，如果使用邻接表，O(E)，如果使用邻接矩阵，$O(V^2)$ 空间复杂度分析：使用了队列，最坏的情形下O(V) 整层遍历 问题：有很多情形，需要对图中的节点进行整层处理： 1231. 求每层节点的统计量；2. 定制每层节点的访问顺序；3. 消除同层节点相互间的干扰； 思路：与标准写法类似，只是整层出整层入 代码： 12345678910111213141516171819202122232425262728def bfs(G, V, s): &quot;&quot;&quot; BFS逐个遍历 @G：以邻接表表示的图，&#123;u:&#123;v1,v2,...&#125;&#125; @V：图中所有顶点 @s：广度优先遍历的出发点 return：每个节点在广度优先树中的父节点，深度 &quot;&quot;&quot; # 初始化指标 color = collections.defaultdict(int) father = collections.defaultdict(lambda: None) level = collections.defaultdict(int) # 广度优先遍历 Q = [(s,0)] color[s] = 1 while Q: tmp = [] for u,d in Q: for v in G[u]: if not color[v]: color[v] = 1 father[v] = u level[v] = d + 1 tmp.append((v,d+1)) color[u] = 2 Q = tmp return father,level BFS基本应用求广度优先生成树 问题：根据BFS搜索过程得到的各节点的父亲节点，计算对应的生成树 思路：父节点为None的顶点作为根节点，以邻接表形式表示树 代码： 123456789101112def get_tree(fathers): &quot;&quot;&quot; @fathers：BFS算法所得到的所有节点的父节点 @return：用字典表示的树&#123;父节点:[孩子节点]&#125;，根节点 &quot;&quot;&quot; T = &#123;&#125; root = None for child,father in fathers.items(): T.setdefault(father,[]).append(child) if not father: root = father return T,root 求单源最小距离 问题：求图中每个顶点到源节点的最小距离 思路：单源最小距离既是边的权值为1的单源最短路径，可以用类似于证明Dijkstra的方法来证明BFS得到的各节点的层数既是节点到源节点的最短距离。 代码： 1levels = bfs(G,s)[1] 打印广度优先搜索树中的单源最小路径 问题：给定图中某个节点，打印源节点到该节点的最短路径 思路：根据BFS得到的节点的父节点关系，打印源节点s到任意节点v在广度优先搜索树中的最短路径 代码： 123456789def print_path(father, s, v): &quot;&quot;&quot; 打印s到v在广度优先生成树中的路径 &quot;&quot;&quot; res = [] while v: res.append(v) v = father[v] return res[::-1] 打印到指定节点所有的单源最短路径 问题：找到源节点s到给定节点v所有最短路径 思路：BFS+DFS。 BFS得到父节点字典：因为同层节点的不同搜索顺序会导致生成的广度优先搜索树也不同，源节点到给定节点的最短路径可能不止一条。我们需要在广度优先遍历时找到每个节点在上一层所有可能的父节点，困难在于，当我们u的未被访问的邻接点v时，只是找到了v的一个可能的父节点，在与u同层的后续节点中还可能存在v的父节点。如果上一层节点没有完全出队，那么下一层节点对上一层节点就应该是未访问的，同时在寻找上一层节点u的邻接点时，那些和u在同层或更早层的节点对于u来说应该是已访问的。解决办法是，通过逐层遍历，将新一层入队的元素标记为灰色，将旧的已入队的元素标记为黑色，灰色节点对于当前出队层节点是透明的，黑色节点对当前层出队层节点是不透明的。 DFS从父节点中找到s到v所有的最短路径 代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051def print_paths(G, V, s, v): &quot;&quot;&quot; 返回源节点s到目标节点v所有的最短路径 &quot;&quot;&quot; fathers = bfs(G, V, s) res = dfs(fathers, s, v, [], []) return res def bfs(G, V, s): &quot;&quot;&quot; 返回所有可能的生成树中节点的父亲节点 &quot;&quot;&quot; color = &#123;&#125; fathers = &#123;&#125; for v in V: color[v] = 0 fathers[v] = [] Q = [s] color[s] = 2 while Q: tmp = [] for u in Q: for v in G[u]: if color[v] &lt; 2: fathers[v].append(u) if color[v] == 0: tmp.append(v) color[v] = 1 for x in tmp: color[x] = 2 Q = tmp return fathersdef dfs(fathers, s, v, cur, res): &quot;&quot;&quot; @fathers:父节点字典 @s：源节点 @v：目标节点 @cur：存储目标节点到当前v节点之前的路径 @res：存储所有s到v的路径 &quot;&quot;&quot; if v == s: cur.append(v) res.append(cur[::-1]) else: for father in fathers[v]: dfs(fathers, s, father, cur + [v], res) return res 具体实例详见Leetcode126 单词接龙 II。 寻找最近公共祖先123456789101112131415161718192021222324252627282930313233343536373839404142434445class Solution(object): &quot;&quot;&quot; LCA思路1： 递归，对任意子树，存在以下几种情形： 1. 子树中含有p,q的LCA，返回LCA 2. 子树中不含p,q的LCA，但是含p或q，返回p或q 3. 子树中不含p或q，返回None 子树和其左右子树存在递推关系，如果左子树含p或q，右子树也含p或q，则说明当前根节点就是LCA，否则如果左子树不含p或q，那么右子树返回的肯定就是LCA，反之亦然； 思路2：BFS返回广度优先生成树，在树中找到从p和q到根节点的路径，路径上第一个重复节点就是p和q的LCA &quot;&quot;&quot; def lowestCommonAncestor(self, root, p, q): &quot;&quot;&quot; :type root: TreeNode :type p: TreeNode :type q: TreeNode :rtype: TreeNode &quot;&quot;&quot; def bfs(root): Q = collections.deque() Q.append(root) father = &#123;root:None&#125; while Q: node = Q.pop() if node.left: Q.append(node.left) father[node.left] = node if node.right: Q.append(node.right) father[node.right] = node return father fathers = bfs(root) p_path = [p] q_path = [q] while p: p_path.append(fathers[p]) p = fathers[p] while q: q_path.append(fathers[q]) q = fathers[q] for x in p_path: for y in q_path: if x == y: return x 实战篇应用场景BFS实际多用于求解“单源最短距离”和“层次依赖问题”： 单源最短距离(无权)问题： 迷宫问题：走出迷宫最少步数，关键是定义好状态参数 单词接龙：一个单词到另一个单词的最短距离，关键是定义好相连关系 求树中到指定节点距离为k的所有节点：先将树转化为图 层次依赖问题： 凸台积水问题：从边界向内更新水位 定制化 BFSBFS用于解决实际问题的一般步骤： 123456789101. 定义图的数据结构：从实际问题中抽象出相互联系的可区分的状态 1. 定义顶点：顶点就是我们需要搜索的不同状态，不同顶点可以用一组参数唯一标识，注意唯一性往往是定义状态的重要依据； 2. 定义边：找到遍历顶点的邻接点的方式，即构建邻接表adj(u);2. 设计BFS算法： 1. 队列：队列提供了父子节点数据交流的通道，可将父子需要交流的数据一同入队Q=deque(); 2. 访问记录：BFS每种状态最多被访问一次，用标识顶点的参数来记录哪些状态已被访问visited=set(); 3. 迭代过程： 1. 初始时将标识源节点的参数以及父子关联数据一同入队，将标识源节点的状态参数添加到已访问集合 2. 循环出队，将出队顶点的未访问的邻接点入队，并标记为已访问，直至队列为空；3. 根据BFS得到的有用信息求解原问题的解 解决好BFS问题的核心在于定义好合适的状态，唯一性是定义状态的重要依据！！！最典型的代表参见“迷宫找钥匙”。 LeetCode典型题目[LeetCode 863. 二叉树中所有距离为 K 的结点] 问题:给定一个二叉树（具有根结点 root）， 一个目标结点 target ，和一个整数值 K 。返回到目标结点 target 距离为 K 的所有结点的值的列表。 答案可以以任何顺序返回。输入：root = [3,5,1,6,2,0,8,null,null,7,4],target = 5, K = 2，输出：[7,4,1] 思路：先将树转化为无向图，再通过BFS求出第K层的所有顶点 12341. 构建图的数据结构：首先遍历整棵树，构建图的数据结构 1. 顶点：用原始顶点中的值作为顶点 2. 边：具有父子关系的顶点是邻接点2. BFS：从target出发寻找距离为K的所有顶点 代码: 123456789101112131415161718192021222324252627282930313233343536def distanceK(self, root, target, K): &quot;&quot;&quot; :type root: TreeNode :type target: TreeNode :type K: int :rtype: List[int] &quot;&quot;&quot; adj = collections.defaultdict(list) def dfs(root): &quot;&quot;&quot;将树转化为图的数据结构&quot;&quot;&quot; if root.left: adj[root.val].append(root.left.val) adj[root.left.val].append(root.val) dfs(root.left) if root.right: adj[root.val].append(root.right.val) adj[root.right.val].append(root.val) dfs(root.right) dfs(root) level = 0 visited = set([target.val]) Q = [target.val] while Q: if level == K: return Q tmp = [] for u in Q: for v in adj[u]: if v not in visited: tmp.append(v) visited.add(v) level += 1 Q = tmp return [] [LeetCode 126. 单词接龙 II] 问题：给定两个单词（beginWord 和 endWord）和一个字典 wordList，找出所有从 beginWord 到 endWord 的最短转换序列。转换需遵循如下规则：每次转换只能改变一个字母。转换过程中的中间单词必须是字典中的单词。 12345678910输入:beginWord = &quot;hit&quot;,endWord = &quot;cog&quot;,wordList = [&quot;hot&quot;,&quot;dot&quot;,&quot;dog&quot;,&quot;lot&quot;,&quot;log&quot;,&quot;cog&quot;]输出:[ [&quot;hit&quot;,&quot;hot&quot;,&quot;dot&quot;,&quot;dog&quot;,&quot;cog&quot;], [&quot;hit&quot;,&quot;hot&quot;,&quot;lot&quot;,&quot;log&quot;,&quot;cog&quot;]] 思路：BFS+DFS 数据结构：每个单词作为一个节点，单词间相差一个字母代表单词间有边；众多单词中判断单词间是否有边，普通方法需要遍历两次，时间复杂度为O(n2)，如果将每个单词的每个字母用_掩码代替，如果代替后掩码相同说明两个单词有边，只需用一个字典维护掩码:[单词]的映射，寻找一个单词的邻接点时，该单词的所有掩码对应的单词都是它的邻接点；考虑单词长度不大，构建数据结构只需要O(n)的时间复杂度； BFS算法：通过BFS可以找到每个节点在上一层中的所有父节点，技巧在于将之前层的节点标位黑色，将当前层已发现的节点标记为灰色，将未访问节点标记为白色，在统计父节点的孩子时，遍历其邻接点中非黑节点，将邻接点中白色节点入队标灰，最后得到父节点字典 DFS算法： 代码：从结束单词除法，向上遍历其所有可能的父节点，如果遇到初始节点则说明找到一组解，进行收集，返回最后收集到的所有最短路径 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960from collections import dequefrom collections import defaultdictclass Solution(object): def findLadders(self, beginWord, endWord, wordList): &quot;&quot;&quot; :type beginWord: str :type endWord: str :type wordList: List[str] :rtype: List[List[str]] &quot;&quot;&quot; # 图的数据结构：单词-掩码-邻接单词 adj = defaultdict(list) for word in wordList: for i,c in enumerate(word): mask = word[:i] + &#x27;_&#x27; + word[i+1:] adj[mask].append(word) # 求解 self.res = [] self.fathers = self.bfs(beginWord, endWord, adj) self.dfs(beginWord, endWord, []) return self.res def bfs(self, beginWord, endWord, adj): # 存放节点的父节点 fathers = defaultdict(set) # 广度遍历，color=0表示未访问，color=1表示当前层已访问，color=2表示在之前层已访问 color = defaultdict(int) Q = [beginWord] color[beginWord] = 2 ok = True while ok and Q: next_Q = [] for pre_w in Q: for j,c in enumerate(pre_w): mask = pre_w[:j] + &#x27;_&#x27; + pre_w[j+1:] for w in adj[mask]: if color[w] &lt; 2: fathers[w].add(pre_w) if not color[w]: if w == endWord: ok = False else: next_Q.append(w) color[w] = 1 Q = [] for q in next_Q: Q.append(q) color[q] = 2 return fathers # 存放最终结果 def dfs(self, beginWord, endWord, cur): if endWord == beginWord: cur.append(endWord) self.res.append(cur[::-1]) return for father in self.fathers[endWord]: self.dfs(beginWord, father, cur + [endWord]) [LeetCode 407. 平台接雨水 II] 问题:给定一个m x n的矩阵，其中的值均为正整数，代表二维高度图每个单元的高度，请计算图中形状最多能接多少体积的雨水 思路：BFS确定平台水面最大深度 数据结构：平台位置作为一个顶点，位置相邻说明有边 BFS： 边界最大水深即为边界平台高度，加入队列 出队一个平台i，更新邻接点j水面深度，如果更新成功则将邻接点入队，更新规则：limit=max(heightMap[j],h[i]),h[j] &gt; limit 统计最终平台水面高度-平台高度之和 代码：每个节点最多被更新四次，故时间复杂度为O(m*n) 12345678910111213141516171819202122232425262728def trapRainWater(self, heightMap): &quot;&quot;&quot; :type heightMap: List[List[int]] :rtype: int &quot;&quot;&quot; if not any(heightMap): return 0 else: m,n = len(heightMap),len(heightMap[0]) inf = float(&#x27;inf&#x27;) h = [[inf] * n for _ in xrange(m)] Q = collections.deque() for i in xrange(m): for j in xrange(n): if i == 0 or i == m-1 or j == 0 or j == n-1: h[i][j] = heightMap[i][j] Q.append((i,j)) while Q: x,y = Q.popleft() for dx,dy in [(0,-1),(0,1),(-1,0),(1,0)]: if 0 &lt; x+dx &lt; m and 0 &lt; y+dy &lt; n: limit = max(heightMap[x+dx][y+dy], h[x][y]) if h[x+dx][y+dy] &gt; limit: h[x+dx][y+dy] = limit Q.append((x+dx,y+dy)) return sum(h[i][j] - heightMap[i][j] for i in xrange(m) for j in xrange(n)) 以上方法对于一维平台接雨水同样适用，参见42. 接雨水。 [LeetCode 864. 获取所有钥匙的最短路径] 问题：给定一个二维网格 grid。 “.” 代表一个空房间， “#” 代表一堵墙， “@” 是起点，（”a”, “b”, …）代表钥匙，（”A”, “B”, …）代表锁。我们从起点开始出发，一次移动是指向四个基本方向之一行走一个单位空间。我们不能在网格外面行走，也无法穿过一堵墙。如果途经一个钥匙，我们就把它捡起来。除非我们手里有对应的钥匙，否则无法通过锁。假设 K 为钥匙/锁的个数，且满足 1 &lt;= K &lt;= 6，字母表中的前 K 个字母在网格中都有自己对应的一个小写和一个大写字母。换言之，每个锁有唯一对应的钥匙，每个钥匙也有唯一对应的锁。另外，代表钥匙和锁的字母互为大小写并按字母顺序排列。返回获取所有钥匙所需要的移动的最少次数。如果无法获取所有钥匙，返回 -1 。 示例: 1234输入：[ &quot;@ . a . #&quot;, &quot;# # # . #&quot;, &quot;b . A . B&quot;]输出：8 思路：BFS 12345678思路：单源最短距离问题，找到所有钥匙返回到源节点的距离 1. 构建图的数据结构： 1. 定义顶点：将(位置,持有的钥匙集)作为顶点 2. 定义边：位置相邻且不为墙，说明两个节点为邻接点 2. BFS算法：从源节点开始进行广度优先遍历，标记已访问节点，遇到非墙邻接点 1. 如果是.或@入队(新位置，钥匙集) 2. 如果是小写字母，检查加入该钥匙是否完成收集，是则返回当前距离，否则继续入队(新位置，新钥匙集) 3. 如果是大写字母，检查钥匙集中是否有对应钥匙，有则开门入队，无则不是邻接点不用处理 代码: 1234567891011121314151617181920212223242526272829303132333435363738394041def shortestPathAllKeys(self, grid): &quot;&quot;&quot; :type grid: List[str] :rtype: int &quot;&quot;&quot; m,n = len(grid),len(grid[0]) # 邻接点函数 def adj(i,j): for x,y in [(i,j-1),(i,j+1),(i-1,j),(i+1,j)]: if 0 &lt;= x &lt; m and 0 &lt;= y &lt; n: yield x,y # 寻找起点和钥匙总数 n_keys = 0 start = None for i in xrange(m): for j in xrange(n): if grid[i][j] == &#x27;@&#x27;: start = [i,j] elif &#x27;a&#x27; &lt;= grid[i][j] &lt;= &#x27;z&#x27;: n_keys += 1 # BFS Q = collections.deque() Q.append([start[0],start[1],0,set()]) visited = set([(start[0],start[1],frozenset())]) while Q: i,j,d,keys = Q.popleft() for x,y in adj(i,j): if grid[x][y] != &#x27;#&#x27; and (x,y,frozenset(keys)) not in visited: visited.add((x,y,frozenset(keys))) if &#x27;a&#x27; &lt;= grid[x][y] &lt;= &#x27;z&#x27;: new_keys = keys | &#123;grid[x][y]&#125; if len(new_keys) == n_keys: return d + 1 Q.append([x,y,d+1,new_keys]) elif &#x27;A&#x27; &lt;= grid[x][y] &lt;= &#x27;Z&#x27;: if grid[x][y].lower() in keys: Q.append([x,y,d+1,keys]) else: Q.append([x,y,d+1,keys]) return -1 反思：复杂问题没什么可怕的，只要抽象出合适的数据结构，就会发现还是常规问题！本题的核心在于使用位置和钥匙集来标识一个状态 [LeetCode 815. 公交路线] 问题： 123456789101112我们有一系列公交路线。每一条路线 routes[i] 上都有一辆公交车在上面循环行驶。例如，有一条路线 routes[0] = [1, 5, 7]，表示第一辆 (下标为0) 公交车会一直按照 1-&gt;5-&gt;7-&gt;1-&gt;5-&gt;7-&gt;1-&gt;... 的车站路线行驶。假设我们从 S 车站开始（初始时不在公交车上），要去往 T 站。 期间仅可乘坐公交车，求出最少乘坐的公交车数量。返回 -1 表示不可能到达终点车站。示例:输入: routes = [[1, 2, 7], [3, 6, 7]]S = 1T = 6输出: 2解释: 最优策略是先乘坐第一辆公交车到达车站 7, 然后换乘第二辆公交车到车站 6。 思路： 1234567891011思路：仍然是单源最短距离问题，BFS求解1. 数据结构： 1. 顶点：每辆公交作为一个顶点，可用下标唯一标识 2. 边：如果一个公交路线与该公交路线交集不为空，则说明两辆公交之间有边2. BFS： 1. 队列，压入公交下标和距离 2. 已访问：下标标识 3. 迭代： 1. 初始时不在车上，找到含初始栈的公交，距离为1，入队，标记已访问， 2. 出队时判断目的地在不在该公交上，如果在直接返回当前距离 3. 如果不在，继续遍历邻接点，存入队列标记已访问 代码： 1234567891011121314151617181920212223242526272829303132333435363738394041def numBusesToDestination(self, routes, S, T): &quot;&quot;&quot; :type routes: List[List[int]] :type S: int :type T: int :rtype: int &quot;&quot;&quot; # 先将站转化为字典，加速查询 routes = [set(rout) for rout in routes] n = len(routes) # 按照站将公交分组，公交A有a站-&gt;有a站的所有公交都是A的邻接点，类似于成单词接龙 adj = collections.defaultdict(list) for i in xrange(n): for c in routes[i]: adj[c].append(i) # 如果S或T不在adj中，返回-1 if S == T: return 0 elif S not in adj or T not in adj: return -1 # BFS搜索 Q = collections.deque() visited = set() for x in adj[S]: Q.append([x,1]) visited.add(x) while Q: j, d = Q.popleft() if T in routes[j]: return d for x in routes[j]: for v in adj[x]: if v not in visited: visited.add(v) Q.append((v,d+1)) return -1 [LeetCode 847. 访问所有节点的最短路径] 问题： 1234567给出 graph 为有 N 个节点（编号为 0, 1, 2, ..., N-1）的无向连通图。 graph.length = N，且只有节点 i 和 j 连通时，j != i 在列表 graph[i] 中恰好出现一次。返回能够访问所有节点的最短路径的长度。你可以在任一节点开始和停止，也可以多次重访节点，并且可以重用边输入：[[1,2,3],[0],[0],[0]]输出：4解释：一个可能的路径为 [1,0,2,0,3] 思路： 12345最短距离问题：BFS1. 数据结构： 1. 顶点：不可以重复访问的状态是(顶点,已访问顶点集) 2. 边：图中的邻接点2. BFS: 代码： 123456789101112131415161718192021222324def shortestPathLength(self, graph): &quot;&quot;&quot; :type graph: List[List[int]] :rtype: int &quot;&quot;&quot; n = len(graph) res = float(&#x27;inf&#x27;) Q = collections.deque() visited = set() for i in xrange(n): Q.append([i, 0, &#123;i&#125;]) visited.add((i,frozenset(&#123;i&#125;))) while Q: j,d,path = Q.popleft() if len(path) == n: res = min(res,d) elif d &lt; res: for x in graph[j]: tmp = path | &#123;x&#125; if (x,frozenset(tmp)) not in visited: Q.append([x, d + 1, tmp]) visited.add((x,frozenset(tmp))) return res [LeetCode 854. 相似度为 K 的字符串] 问题:如果可以通过将 A 中的两个小写字母精确地交换位置 K 次得到与 B 相等的字符串，我们称字符串 A 和 B 的相似度为 K（K 为非负整数）。给定两个字母异位词 A 和 B ，返回 A 和 B 的相似度 K 的最小值。 12345678输入：A = &quot;abc&quot;, B = &quot;bca&quot;输出：2输入：A = &quot;abac&quot;, B = &quot;baca&quot;输出：21 &lt;= A.length == B.length &lt;= 20A 和 B 只包含集合 &#123;&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;, &#x27;d&#x27;, &#x27;e&#x27;, &#x27;f&#x27;&#125; 中的小写字母。 思路： 123456789101112131415思路1：最短距离问题，考虑BFS，超时 1. 构建数据结构： 1. 定义顶点：单词 2. 定义边：任意两个不同位置的不同元素交换 2. BFS算法：找最短路径思路2：记忆化搜索DFS找到所有可能的解，然后求最优解936ms 1. AB中相同的部分可以排除，只对不同的位置进行交换 2. 如果AB首部位置不同，在该位置至少要进行一次交换，遍历后续合法交换位置，问题就转化为了A[1:]和B[1:]的交换了思路3：借鉴2的思路，重回BFS 1. 构建数据结构： 1. 定义顶点：(A,B)作为状态 2. 定义边：寻找AB第一个不同元素A[i],B[i]，在A[i]的后续元素中找到一个等于B[i]的元素与A[i]交换，得到(A[i+1:],B[i+1:])作为A的邻接点 2. BFS算法： 1. 初始(A,B,0)入队，标记已访问 2. 循环出队，依次将未访问过的邻接点入队，标记已访问，直至找不到不相同的元素(A==B)，返回d 代码： 12345678910111213141516171819202122232425262728def kSimilarity(self, A, B): &quot;&quot;&quot; :type A: str :type B: str :rtype: int &quot;&quot;&quot; if A == B: return 0 Q = collections.deque() visited = set() Q.append((A,B,0)) visited.add((A,B)) while Q: a,b,d = Q.popleft() w = -1 for i,c in enumerate(a): if w == -1 and a[i] != b[i]: w = i elif w != -1 and a[i] == b[w]: cur_a = a[w+1:i] + a[w] + a[i+1:] cur_b = b[w+1:] pair = (cur_a,cur_b,d+1) if (cur_a,cur_b) not in visited: Q.append(pair) visited.add((cur_a,cur_b)) if w == -1: return d [LeetCode 199. 二叉树的右视图] 问题：给定一棵二叉树，想象自己站在它的右侧，按照从顶部到底部的顺序，返回从右侧所能看到的节点值。 123456789输入: [1,2,3,null,5,null,4]输出: [1, 3, 4]解释: 1 &lt;--- / \2 3 &lt;--- \ \ 5 4 &lt;--- 思路：整行BFS，每行返回最后一个节点值 代码： 123456789101112def rightSideView(self, root): &quot;&quot;&quot; :type root: TreeNode :rtype: List[int] &quot;&quot;&quot; res = [] if root: Q = [root] while Q: res.append(Q[-1].val) Q = [child for node in Q for child in [node.left,node.right] if child] return res [LeetCode 279. 完全平方数] 问题：给定正整数 n，找到若干个完全平方数（比如 1, 4, 9, 16, …）使得它们的和等于 n。你需要让组成和的完全平方数的个数最少。如：12= 4 + 4 + 4。 思路：实质也是单源最短路径问题，最坏时间复杂度O(dn) 1234561. 数据结构： 1. 顶点：当前累加的平方数和作为顶点 2. 边：累加平方数和与下一次&lt;=n的累加平方数和为邻接点2. BFS： 1. 队列：存放当前累加和和当前距离 2. 已访问：存放累加和，已访问的累加和就不用再访问 代码: 123456789101112131415161718192021222324def numSquares(self, n): &quot;&quot;&quot; :type n: int :rtype: int &quot;&quot;&quot; srt = int(n**0.5) if srt * srt == n: return 1 Q = collections.deque() visited = set() for i in xrange(1,srt+1): Q.append((i*i,1)) visited.add(i*i) while Q: q,d = Q.popleft() for j in xrange(1,srt+1): tmp = q + j * j if tmp not in visited: visited.add(tmp) if tmp == n: return d + 1 elif tmp &lt; n: Q.append((tmp,d+1)) 另一种思路：本体最快的解法是利用数论中的四数平方和定理与三数平方和定理 1234567思路2： 四数平方和定理：任何自然数都可以用最多四个平方和数之和表示， 三数平方和定理：只有当n=4^a*(8b+7)时才需要用四个平方和树之和表示 1. 如果n==int(n**0.5) ** 2，返回1 2. 如果n可以被分解为任意两个数的平方和之和，返回2 3. 如果n==4^a*(8b+7)，返回4 4. 否则返回3 代码： 1234567891011121314def numSquares(self, n): &quot;&quot;&quot; :type n: int :rtype: int &quot;&quot;&quot; while n &amp; 3 == 0: n &gt;&gt;= 2 if n &amp; 7 == 7: return 4 for a in xrange(int(n**0.5)+1): b = int((n - a * a)**0.5) if a * a + b * b == n: return (a != 0) + (b!=0) return 3]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构与算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构与算法：并查集]]></title>
    <url>%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%9A%E5%B9%B6%E6%9F%A5%E9%9B%86%2F</url>
    <content type="text"><![CDATA[理论篇动态连通性问题假设用 0~N-1 的整数来表示一组对象，整数对(p,q)可以理解为“对象p和对象q是相连的”，相连是一种对等关系，意味着： 自反性：p和p是相连的； 对称性：p和q相连那么q和p也相连； 传递性：p和q相连，q和r相连，则p和r也是相连的； 对等关系能够将对象分为多个等价类，当且仅当两个对象相连时它们才属于同一个等价类。动态连通性问题或说等价类划分问题需要设计一种数据结构来保存所有对象间的连通/等价关系，并用它来判断一对新的对象是否是连通/等价的。 逻辑结构并查集（Union Find Set）是解决动态连通性问题的一种非常高效的树型数据结构，并查集是由一组树构成的森林，每棵树都描述了一个等价类，树中所有节点都是相连的。 称谓 整数 整数对 相连的整数 标识符 集合观点 元素 是否属于同一集合 等价类 集合名 树的观点 节点 是否连通 连通分量/树 根节点 存储结构通常用树的双亲表示法作为并查集的存储结构，这种存储方式采用一组连续的空间来存储每个节点，同时在每个节点中增设一个伪指针，指示其双亲节点在数组中的下标。通常用数组元素的下标代表元素名，根节点的下标代表集合名，根节点的双亲节点为其本身或负数。 基本运算运算定义Union-Find算法API定义（图论语言描述）: uf(int n)：用整数标识(0~N-1)初始化N个节点，每个节点单独构成一个连通分量； int find(int p)：查找节点p所在分量的标识符； void union(int p,int q)：如果p和q在不同分量，union会融合两个分量； bool connected(int p,int q)：如果p和q存在于同一分量则返回True； int count()：连通分量的数量； 运算实现1）quick-find 实现find 的时间复杂度取决于树的高度，因此为了加快find的速度需要在合并两个分量时尽量减小合并后树的高度，一种方式是在每次合并不同分量时，统一分量的标识符，这种方式虽然使find时间复杂度变为O(1)，但union时需要表里整个数组，对于大量数据并不适用： 12345678910111213141516171819202122232425class Uf(object): def __init__(self, n): &quot;&quot;&quot;用一个数组来存储并查集，初始化时每个节点的分量标签是其自身id&quot;&quot;&quot; self.id = [for i in range(n)] self.count = n def find(self, p): &quot;&quot;&quot;查找p节点的分量标签，即根节点id&quot;&quot;&quot; while p != self.id[p]: p = self.id[p] return p def union(self,p,q): &quot;&quot;&quot;如果p和q不在同一分量，则合并这两个分量，同时统一分量的标识符&quot;&quot;&quot; p_id = self.find(p) q_id = self.find(q) if p_id != q_id: for i in range(len(self.id)): if self.id[i] == q_id: self.id[i] = p_id self.count -= 1 def connectd(self,p,q): &quot;&quot;&quot;判断p和q是否在同一分量&quot;&quot;&quot; return self.find(p) == self.find(q) def count(): &quot;&quot;&quot;统计连通分量个数&quot;&quot;&quot; return self.count 分析: find:所有节点的父节点就是根节点，$O(1)$ union:$O(n)$ 处理N对整数，$O(n^2)$ 2）quick-union 实现quick-union不去统一union的两个分量的标识符，通过链式追溯找到节点的标识符。 12345678910111213141516171819202122232425class Uf(object): def __init__(self, n): &quot;&quot;&quot;用一个数组来存储并查集，初始化时每个节点的分量标签是其自身&quot;&quot;&quot; self.id = [for i in range(n)] self.count = n def find(self, p): &quot;&quot;&quot;查找p节点的分量标签，即根节点id&quot;&quot;&quot; while p != self.id[p]: p = self.id[p] return p def union(self,p,q): &quot;&quot;&quot;如果p和q不在同一分量，则合并这两个分量&quot;&quot;&quot; p_id = self.find(p) q_id = self.find(q) if p_id != q_id: self.id[q_id] = p_id self.count -= 1 def connectd(self,p,q): &quot;&quot;&quot;判断p和q是否在同一分量&quot;&quot;&quot; return self.find(p) == self.find(q) def count(): &quot;&quot;&quot;统计连通分量个数&quot;&quot;&quot; return self.count 分析： find：比较次数等于树的高度，最坏情况下O(n)； union：两次find，树的高度O(n) 处理N对整数所需的所有find()操作访问数组的总次数在最坏情况下是平方级别的 3）终极优化——带路径压缩的加权 quick-unionfind操作是整个并查集操作的核心，要加快find操作效率需尽量减小树的高度（扁平化），我们可以在find或union操作时实现这一点： 路径压缩：在find的同时将节点直接链接到根节点； 加权quick-union：在union时总是将较小子树链接到较大子树； 1234567891011121314151617181920212223242526272829303132333435class Uf(object): def __init__(self, n): &quot;&quot;&quot;用一个数组来存储并查集，初始化时每个节点的分量标签是其自身&quot;&quot;&quot; self.id = [for i in range(n)] # 统计连通分量个数 self.count = n # 统计分量中节点个数 self.rank = [1] * n def find(self, p): &quot;&quot;&quot;查找p节点的分量标签，即根节点id，同时将路径上的所有节点的父节点设置为根节点&quot;&quot;&quot; while p != self.id[p]: self.id[p] = find(self.id[p]) p = self.id[p] return p def union(self,p,q): &quot;&quot;&quot;如果p和q不在同一分量，则将小分量合并到大分量&quot;&quot;&quot; x, y = self.find(p), self.find(q) if x != y: if self.rank[x] &gt; self.rank[y]: self.id[y] = x self.rank[x] += self.rank[y] else: self.id[x] = y self.rank[y] += self.rank[x] self.count -= 1 def connectd(self,p,q): &quot;&quot;&quot;判断p和q是否在同一分量&quot;&quot;&quot; return self.find(p) == self.find(q) def component_count(self): &quot;&quot;&quot;统计连通分量个数&quot;&quot;&quot; return self.count def node_count(self): &quot;&quot;&quot;统计每个分量中节点个数&quot;&quot;&quot; return &#123;self.find(i):self.rank[i] for i in self.id&#125; 示例：依次添加[(1,2),(3,4),(0,9),(4,7),(6,5),(5,8),(3,9),(1,8)]的过程 分析：同时使用路径压缩、按秩（rank）合并优化的程序每个操作的平均时间仅为 $O(\alpha (n))$，其中 $ \alpha (n)$ 是 $ {\displaystyle n=f(x)=A(x,x)}$ 的反函数， ${\displaystyle A}$ 是急速增加的阿克曼函数。因为 $\alpha (n)$ 是其反函数，故 $ \alpha (n) $在 n 十分巨大时还是小于 5。因此，平均运行时间是一个极小的常数。实际上，这是渐近最优算法：Fredman 和 Saks 在 1989 年解释了 $ \Omega (\alpha (n))$ 的平均时间内可以获得任何并查集。 find:均摊成本接近O(1)； union:均摊成本接近O(1)； 保存所有连接关系:O(n)，n为边的个数; 空间复杂度O(n)； 带路径压缩的加权quick-union算法是最优的算法，但并非所有操作都能在常数时间内完成； 值得注意的是，路径压缩并不能保证最终所有节点都能直接作为根节点的孩子节点，除非最后对每个节点都find一遍。find过程确实将路径上的所有节点都放在了根节点下，但是Union过程会破坏这一点，导致最终的树高度变得未知。因此加权quick-union效率会更高，而且统计了每个分量中节点个数。 加权quick-union已经使得树变得很低，进一步的路径压缩很难再对其进一步改进了。 quick-union和加权quick-union算法对比： 三种算法性能对比： 三种算法均摊成本对比（灰色点代表累积成本，红色点代表均摊成本）： 关于并查集更详细内容，Algorithms一书的Section 1.5有非常精彩的讨论。 典型应用划分等价类 问题：给定一组对象的对等关系，对该组对象进行划分，使得具有对等关系的对象归为一类； 思路：典型的等价类划分问题，遍历所有对等关系，union两个节点，即完成了等价类的划分； 1234567891011def eq_class(vertices,edges): n = len(vertices) uf = Uf(len(vertices)) for p,q in uf: if uf.find(p) != uf.find(q): uf.union(p,q) res = &#123;&#125; for i in range(n): res[uf.find(i)] = res.get(uf.find(i),[]) + [i] return res Kruskal算法 问题：寻找带权图的最小生成树 思路：贪心策略，从小到大依次选取不构成环的边，直至所有顶点都被纳入 代码： 12345678910111213141516171819202122232425def kruskal(graph): &quot;&quot;&quot; graph = &#123;node:[(node1,weight1),...]&#125; &quot;&quot;&quot; # 构建并查集 vertices = graph.keys() n = len(vertices) uf = Uf(n) # 维护边的优先队列 edges = [] for key in graph: edge = [graph[key][1],(key,graph[key][0])] heappush(edges,edge) # kruskal算法 T = [] while n &gt; 1: i,j = heappop(edges)[1] if uf.find(i) != uf.find(j): uf.union(i,j) T.append([i,j]) else: continue return T 分析：采用优先队列存放边的集合，每次选取最小权值的边需$O(lg|E|)$，每次使用并查集判断是否构成环需$O(1)$，最坏情形需要遍历所有边，所以时间复杂度为$O(|E|lg|E|)$。 实战篇实战技巧 实战的难点在于问题的定义和转化，是否可以转化为对象间连通性的讨论 抽象出节点：节点代表了某种相互联系的对象 抽象出节点之间的边：代表了节点之间的某种等价性 “动态连通问题”的衍生问题包括： 判断两个节点是否连通：判断两个节点是否属于同一类 添加一组新的连通关系 统计等价类、连通分量个数，每个等价类中的节点数 并查集的实现： 节点用整数来指示，我们可以将对象存储在一个数组中，通过其下标来指示该对象 建议使用带路径压缩的quick-union，代码简单，性能最优 在节点合并时，需要遍历所有边，输入可以是边表或邻接表，时间复杂度为边的个数 并查集能解决的问题通过DFS也都能解决2.2 经典LeetCode题目[684.medium] 识别无向图中的冗余边 问题：给定含n个节点的无向图（用边表表示），该图由一棵树和一条多余边构成，返回一条可以删去的边，如果有多个答案则返回最后出现的那条边。 123456输入: [[1,2], [2,3], [3,4], [1,4], [1,5]]输出: [1,4]解释: 给定的无向图为:5 - 1 - 2 | | 4 - 3 思路：无向图中冗余边必定构成环，只需要识别出无向图的环，从环中去掉最后一条边即可。构建n个节点的并查集，依次合并边表中的边的两个顶点，如果在合并时发现这两个顶点在并查集中连通，则说明该边是多余的，返回该边； 代码： 1234567891011121314151617181920212223242526272829class Solution(object): &quot;&quot;&quot; 思路：典型的并查集应用，从一系列边中找出生成树的冗余边 &quot;&quot;&quot; def findRedundantConnection(self, edges): &quot;&quot;&quot; :type edges: List[List[int]] :rtype: List[int] &quot;&quot;&quot; def init_uf(n): return [i for i in range(n)] def find(a,i): while a[i] != i: a[i] = find(a,a[i]) i = a[i] return i def union(a,i,j): a[find(a,j)] = a[find(a,i)] return a n = len(edges) uf = init_uf(n+1) for p,q in edges: if find(uf,p) == find(uf,q): return [p,q] else: union(uf,p,q) [685.hard] 识别有向图中的冗余边 问题：将问题684中的无向图换做有向图，问题不变 123456789101112131415输入: [[1,2], [2,3], [3,4], [4,1], [1,5]]输出: [4,1]解释: 给定的有向图如下:5 &lt;- 1 -&gt; 2 ^ | | v 4 &lt;- 3 输入: [[1,2], [1,3], [2,3]]输出: [2,3]解释: 给定的有向图如下: 1 / \v v2--&gt;3 思路：有向图的冗余边情况稍复杂一些，需分三种情况讨论。 1）如果所有节点最多都只有一个父亲：则必在根节点处形成环，只需返回最后一个成环的边即可（类似于无环题）2）如果某节点由两个父亲，但该节点未形成环，则返回两个父亲最后一条边；3）如果某节点由两个父亲，且该节点在环中，则需返回在环中的那条父亲边； 代码： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849class Solution(object): def findRedundantDirectedConnection(self, edges): &quot;&quot;&quot; :type edges: List[List[int]] :rtype: List[int] &quot;&quot;&quot; def find(a): # 返回a的根节点，即a的类 while uf[a] != a: a = uf[a] return a def union(a,b): # a祖先作为b祖先的父亲 uf[find(b)] = find(a) def cycle_edge(x): # 返回环中边，无环返回None visited[x] = True for i in adj[x]: if visited[i]: return x,i else: tmp = cycle_edge(i) if tmp:return tmp return None n = len(edges) uf = range(n+1) adj = [[] for i in range(n+1)] has_father = [False] * (n+1) edge_d = None visited = [0] * (n+1) for u,v in edges: adj[u] += [v] if has_father[v]: edge_d = (u,v) has_father[v] = True if find(u) == find(v): edge_c = (u,v) union(u,v) if not edge_d: return edge_c else: res = cycle_edge(edge_d[1]) return res if res else edge_d [547.medium] 朋友圈 问题：班上有 N 名学生。其中有些人是朋友，有些则不是。他们的友谊具有是传递性。如果已知 A 是 B 的朋友，B 是 C 的朋友，那么我们可以认为 A 也是 C 的朋友。所谓的朋友圈，是指所有朋友的集合。给定一个 N * N 的矩阵 M，表示班级中学生之间的朋友关系。如果M[i][j] = 1，表示已知第 i 个和 j 个学生互为朋友关系，否则为不知道。你必须输出所有学生中的已知的朋友圈总数。 1234567输入: [[1,1,0], [1,1,0], [0,0,1]]输出: 2 说明：已知学生0和学生1互为朋友，他们在一个朋友圈。第2个学生自己在一个朋友圈。所以返回2。 思路:每名学生作为一个节点，朋友关系作为边（传递性-对称性），构建并查集，统计并查集中联通分量的个数 代码： 123456789101112131415161718192021222324252627class Solution(object): def findCircleNum(self, M): &quot;&quot;&quot; :type M: List[List[int]] :rtype: int &quot;&quot;&quot; n = len(M) edge = [(i,j) for i in range(n) for j in range(i+1,n) if M[i][j]] self.count = n def find(a,i): while i != a[i]: a[i] = find(a,a[i]) i = a[i] return i def union(a,i,j): s,t = find(a,j),find(a,i) if s != t: a[s] = a[t] self.count -= 1 uf = [i for i in range(n)] for i,j in edge: union(uf,i,j) return self.count [200.medium] 岛屿的个数 问题：给定一个由 ‘1’（陆地）和 ‘0’（水）组成的的二维网格，计算岛屿的数量。一个岛被水包围，并且它是通过水平方向或垂直方向上相邻的陆地连接而成的。你可以假设网格的四个边均被水包围 1234567输入:11000110000010000011输出: 3 思路：并查集求连通分量个数；增加哑结点存放所有无关节点’0’，相连关系具有传递性，因此只需要向上和向左合并节点即可； 代码： 12345678910111213141516171819202122232425262728293031323334353637383940414243class Solution(object): def numIslands(self, grid): &quot;&quot;&quot; :type grid: List[List[str]] :rtype: int &quot;&quot;&quot; # 定义并查集操作 def find(uf,p): while p != uf[p]: uf[p] = find(uf,uf[p]) p = uf[p] return p def union(uf,p,q): x,y = find(uf,p),find(uf,q) if x != y: if self.rank[x] &gt; self.rank[y]: uf[y] = uf[x] self.rank[x] += self.rank[y] else: uf[x] = uf[y] self.rank[y] += self.rank[x] self.count -= 1 if not any(grid): return 0 # 创建并查集，使用m*n哑元存放0分量 m,n = len(grid), len(grid[0]) total = m * n self.count = m*n self.rank = [1] * (total+1) uf = [i for i in range(total+1)] # 保存所有相连关系 for i in range(m): for j in range(n): if grid[i][j] == &#x27;0&#x27;: union(uf,total,i*n+j) else: if i &gt; 0 and grid[i-1][j]==&#x27;1&#x27;: union(uf,(i-1)*n+j,i*n+j) if j &gt; 0 and grid[i][j-1]==&#x27;1&#x27;: union(uf,i*n+j-1,i*n+j) return self.count 分析： 时间复杂度：保存所有相连关系花费O(m*n) 空间复杂度：O(m*n) [765.hard] 情侣牵手 问题：N 对情侣坐在连续排列的 2N 个座位上，想要牵到对方的手，计算最少交换座位的次数，以便每对情侣可以并肩坐在一起。人和座位用 0 到 2N-1 的整数表示，情侣们按顺序编号，第一对是 (0, 1)，第二对是 (2, 3)，以此类推 123输入: row = [0, 2, 1, 3]输出: 1解释: 我们只需要交换row[1]和row[2]的位置即可。 思路:将每对情侣看做一个节点并编号0~N-1，其中(2i,2i+1)是编号为i的两位情侣，如果初始时2j和2j+1位置坐着来自不同情侣u=row[2j]/2和v=row[2j+1]/2的两个人，则认为u节点和v节点是相连的，保存所有连通关系后，每个连通分量必定构成一个交换环，含有n个节点的交换环最少需要n-1次交换可将交换环拆解为n个自环，即所有情侣相遇，故总共需要的交换次数为sum(n-1)=N-k，k代表并查集中的连通分量个数。 代码： 1234567891011121314151617181920212223class Solution(object): def minSwapsCouples(self, row): &quot;&quot;&quot; :type row: List[int] :rtype: int &quot;&quot;&quot; def find(uf,i): while i != uf[i]: uf[i] = find(uf,uf[i]) i = uf[i] return i def union(uf,i,j): x,y = find(uf,j),find(uf,i) if x != y: uf[y] = uf[x] self.count -= 1 n = len(row)/2 uf = [i for i in range(n)] self.count = n for i in range(n): union(uf,row[2*i]/2,row[2*i+1]/2) return n - self.count 分析：建立并查集保存所有连接关系需要O(n) [130.medium] 被围绕的区域 问题：给定一个二维的矩阵，包含 ‘X’ 和 ‘O’（字母 O），找到所有被 ‘X’ 围绕的区域，并将这些区域里所有的 ‘O’ 用 ‘X’ 填充 12345678910示例：X X X XX O O XX X O XX O X X运行程序后，矩阵变为:X X X XX X X XX X X XX O X X 思路：只需要判断哪些为O的元素不与边界的O连通。节点(i，j)可用整数in+j来指示，遍历所有O顶点，可以将边界的所有O节点合并到一个哑结点mn下，将O节点与左侧和上侧的O节点连通；最后判断不与哑结点相连的节点，改为X。 代码： 12345678910111213141516171819202122232425262728293031class Solution(object): def solve(self, board): &quot;&quot;&quot; :type board: List[List[str]] :rtype: void Do not return anything, modify board in-place instead. &quot;&quot;&quot; def find(uf,i): while i != uf[i]: uf[i] = find(uf,uf[i]) i = uf[i] return i def union(uf,i,j): uf[find(uf,j)] = uf[find(uf,i)] def connected(uf,i,j): return find(uf,i) == find(uf,j) if not any(board):return m,n = len(board),len(board[0]) uf = [i for i in range(m*n+1)] for i in range(m): for j in range(n): if board[i][j] == &#x27;O&#x27;: index = i * n + j if i == 0 or j == 0 or i == m-1 or j == n-1: union(uf,index, m*n) if i &gt; 0 and board[i-1][j] == &#x27;O&#x27;: union(uf,index,index-n) if j &gt; 0 and board[i][j-1] == &#x27;O&#x27;: union(uf,index,index-1) for i in range(m): for j in range(n): if board[i][j] == &#x27;O&#x27; and not connected(uf,i*n+j,m*n): board[i][j] = &#x27;X&#x27; [721.medium] 账户合并 问题：给定一个列表 accounts，每个元素 accounts[i] 是一个字符串列表，其中第一个元素 accounts[i][0] 是 名称 (name)，其余元素是 emails 表示该帐户的邮箱地址。如果两个帐户都有一些共同的邮件地址，则两个帐户必定属于同一个人，即使两个帐户具有相同的名称，它们也可能属于不同的人，因为人们可能具有相同的名称。一个人最初可以拥有任意数量的帐户，但其所有帐户都具有相同的名称。现在，我们想合并这些帐户，按以下格式返回帐户：每个帐户的第一个元素是名称，其余元素是按顺序排列的邮箱地址。accounts 本身可以以任意顺序返回。 12345678Input: accounts = [[&quot;John&quot;, &quot;johnsmith@mail.com&quot;, &quot;john00@mail.com&quot;], [&quot;John&quot;, &quot;johnnybravo@mail.com&quot;], [&quot;John&quot;, &quot;johnsmith@mail.com&quot;, &quot;john_newyork@mail.com&quot;], [&quot;Mary&quot;, &quot;mary@mail.com&quot;]]Output: [[&quot;John&quot;, &#x27;john00@mail.com&#x27;, &#x27;john_newyork@mail.com&#x27;, &#x27;johnsmith@mail.com&#x27;], [&quot;John&quot;, &quot;johnnybravo@mail.com&quot;], [&quot;Mary&quot;, &quot;mary@mail.com&quot;]]Explanation: 第一个和第三个 John 是同一个人，因为他们有共同的电子邮件 &quot;johnsmith@mail.com&quot;。 第二个 John 和 Mary 是不同的人，因为他们的电子邮件地址没有被其他帐户使用。 我们可以以任何顺序返回这些列表，例如答案[[&#x27;Mary&#x27;，&#x27;mary@mail.com&#x27;]，[&#x27;John&#x27;，&#x27;johnnybravo@mail.com&#x27;]， [&#x27;John&#x27;，&#x27;john00@mail.com&#x27;，&#x27;john_newyork@mail.com&#x27;，&#x27;johnsmith@mail.com&#x27;]]仍然会被接受 思路：同一个account内的email具有连通关系（属于同一个人），只需要找到所有email的连通分量，绑定到对应的人输出即可。将email作为节点，同时存在于同一个acount的email具有连通关系，数组收集所有的email，使用其数组下标作为email的指示，构建并查集，可以得到属于不同人的连通分量，再由连通分量找到对应的email和人。在此过程，我们需要email到id，id到email，email到人的三种映射，我们使用三个hash表来建立它们的映射关系。 代码: 12345678910111213141516171819202122232425262728293031323334353637383940class Solution(object): def accountsMerge(self, accounts): &quot;&quot;&quot; :type accounts: List[List[str]] :rtype: List[List[str]] &quot;&quot;&quot; # 由email到客户 em = &#123;email:account[0] for account in accounts for email in account[1:]&#125; # 由id到email emails = list(em.keys()) n = len(emails) # 由email到id em_id = &#123;emails[i]:i for i in range(n)&#125; def find(uf,i): while i != uf[i]: uf[i] = find(uf,uf[i]) i = uf[i] return i def union(uf,i,j): uf[find(uf,j)] = uf[find(uf,i)] # 保存所有连通关系 uf = [i for i in range(n)] for account in accounts: for j in range(2,len(account)): union(uf,em_id[account[j-1]],em_id[account[j]]) # 每个连通分量的节点 res = &#123;&#125; for i in range(n): res[find(uf,i)] = res.get(find(uf,i),[]) + [i] # 输出每个分量 r = [] for key in res: cur = [emails[i] for i in res[key]] cur = [em[cur[0]]] +sorted(cur) r.append(cur) return r 分析：建立哈希表需要O(n)，n代表email个数，建立并查集需要O(n)，故总的时间复杂度为O(n) [128.hard] 最长连续序列 问题：给定一个未排序的整数数组，找出最长连续序列的长度，要求算法的时间复杂度为 O(n)。 123输入: [100, 4, 200, 1, 3, 2]输出: 4解释: 最长连续序列是 [1, 2, 3, 4]。它的长度为 4。 思路：节点表示nums数组下标，整数相邻代表他们之间相连，统计并查集中节点最多的连通分量，用rank记录以每个节点为根节点的子树节点树，求最大值 代码： 1234567891011121314151617181920212223242526272829303132333435class Solution(object): def longestConsecutive(self, nums): &quot;&quot;&quot; :type nums: List[int] :rtype: int &quot;&quot;&quot; def find(uf,i): while i != uf[i]: uf[i] = uf[uf[i]] i = uf[i] return i def union(uf,i,j): x,y = find(uf,i),find(uf,j) if x != y: if self.rank[x] &gt; self.rank[j]: uf[y] = uf[x] self.rank[y] += self.rank[x] else: uf[x] = uf[y] self.rank[y] += self.rank[x] if not nums: return 0 n = len(nums) self.uf = [i for i in range(n)] self.rank = [1] * n dic = &#123;&#125; for i in xrange(n): # 忽略重复值 if nums[i] in dic:continue dic[nums[i]] = i # 如果前面出现了与nums[i]相邻的元素则与其合并 if nums[i] - 1 in dic:union(self.uf,i,dic[nums[i]-1]) if nums[i] + 1 in dic:union(self.uf,i,dic[nums[i]+1]) return max(self.rank) [778.hard] Swim in Rising Water 问题：给定长宽为N的二维方阵grid，记grid[x][y] = z表示当时刻t &gt;= z时，x, y可达。在grid上的移动可以瞬间完成，求从0, 0出发，到达N - 1, N - 1的最短时刻 1234567891011Input: [[0,1,2,3,4],[24,23,22,21,5],[12,13,14,15,16],[11,17,18,19,20],[10,9,8,7,6]]Output: 16Explanation: 0 1 2 3 424 23 22 21 512 13 14 15 1611 17 18 19 2010 9 8 7 6The final route is marked in bold.We need to wait until time 16 so that (0, 0) and (4, 4) are connected 思路：并查集+二分查找，判断连通性。本题是要找到最小限高使得(0,0)与(n-1,n-1)能够连通，连通性关于限高具有单调性，可以采用二分查找的思路，每次将问题规模减半； 代码: 12345678910111213141516171819202122232425262728293031323334353637383940414243class Solution(object): def swimInWater(self, grid): &quot;&quot;&quot; :type grid: List[List[int]] :rtype: int &quot;&quot;&quot; def find(uf,i): while i != uf[i]: uf[i] = find(uf,uf[i]) i = uf[i] return i def union(uf,i,j): x,y = find(uf,i), find(uf,j) if x != y: if rank[x] &gt; rank[y]: uf[y] = x rank[x] += rank[y] else: uf[x] = y rank[y] += rank[x] def connected(uf,i,j): return find(uf,i) == find(uf,j) def is_connected(uf, limit): for i in xrange(n): for j in xrange(n): if grid[i][j] &lt;= limit: cur = i*n+j if i &gt; 0 and grid[i-1][j] &lt;= limit: union(uf,cur-n,cur) if j &gt; 0 and grid[i][j-1] &lt;= limit: union(uf,cur-1,cur) return connected(uf,0,n*n-1) n = len(grid) low,high = max(grid[0][0],grid[n-1][n-1]), n*n-1 while low &lt;= high: uf = [i for i in xrange(n*n)] rank = [1] * n * n mid = (low+high)/2 if is_connected(uf,mid): high = mid - 1 else: low = mid + 1 return low 分析:时间复杂度 $O(nlgn)$ 参考[1] Algorithms 一书的Section 1.5[2] LeetCode 并查集]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构与算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构与算法：线性表（三）—— 双指针]]></title>
    <url>%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%9A%E7%BA%BF%E6%80%A7%E8%A1%A8%EF%BC%88%E4%B8%89%EF%BC%89%E2%80%94%E2%80%94%20%E5%8F%8C%E6%8C%87%E9%92%88%2F</url>
    <content type="text"><![CDATA[线性表中的双指针法是指通过两个指针（游标）来指示线性表中的元素的方法。双指针的使用本身并没有什么神奇之处，但是通过合适地操纵指针的移动，在某些特定问题中却能化腐朽为神奇，大大降低算法的时间复杂度。 根据对双指针不同的移动规则，整理了常用的双指针方法及其所用来解决的问题： 双指针法 操作说明 经典问题 首尾指针 使用两个指针指示线性表的首尾元素 二分查找、NSum 口袋指针 一个指针指示口袋口，另一个指针用于遍历 数组划分 窗口指针 两个指针指示窗口范围 窗口蠕动法求解连续子序列问题 快慢指针 使用两个移动速度不同的指针 环的检测和交点确定 早晚指针 使用出发时间不同的两个指针 消除距离差额 双表指针 两个指针分别指示两个线性表当前位置 数组归并 首尾指针 适用问题：需要同时操作首尾元素，或者需要指示变化中的线性表范围，使用首尾指针经常需要先对数组进行排序； 使用思路： 指针含义：l指示线性表首部元素，r指示线性表尾部元素 移动规则：依条件向内移动，直至相遇 反转线性表 问题：[344] 反转字符串，请编写一个函数，其功能是将输入的字符串反转过来； 思路：使用首尾指针交换元素，然后首尾指针向内移动，直至首指针不再小于尾指针； 代码： 12345678910111213class Solution(object): def reverseString(self, s): &quot;&quot;&quot; :type s: str :rtype: str &quot;&quot;&quot; l, r = 0, len(s) - 1 res = list(s) while l &lt; r: res[l],res[r] = res[r],res[l] l += 1 r -= 1 return &#x27;&#x27;.join(res) 分析：空间O(n)，时间O(n) 盛最多水的容器 问题：[11]盛最多水的容器，给定 n 个非负整数 a1，a2，…，an，每个数代表坐标中的一个点 (i, ai) 。画 n 条垂直线，使得垂直线 i 的两个端点分别为 (i, ai) 和 (i, 0)。找出其中的两条线，使得它们与 x 轴共同构成的容器可以容纳最多的水。 思路：分治+首尾指针，问题首先划分为两个子问题，使用两侧较小边和不使用两侧较小边，然后取二者中的最大值；使用较小边时，容器两侧容纳最多的水，不使用较小边的子问题可以是原始问题的同类子问题，可以用递归求解，也可以用首尾指针来求，如果使用了较小边l，收集当前最大后l++，反之较小边为r则r—，直至l == r。 代码： 12345678910111213141516class Solution(object): def maxArea(self, height): &quot;&quot;&quot; :type height: List[int] :rtype: int &quot;&quot;&quot; l,r = 0,len(height) - 1 res = 0 while l &lt; r: if height[l] &lt; height[r]: res = max(res,height[l]*(r-l)) l += 1 else: res = max(res,height[r]*(r-l)) r -= 1 return res 分析：空间O(1)，时间O(n) 二分查找1234567891011121314151617181920# 闭区间写法:用[l,r]代表当前查找范围def Binary_search(L,key): &#x27;&#x27;&#x27; @L:有序数组 @key:目标值 @return:如果查找成功则返回目标元素索引，如果查找失败则返回目标值应插入的位置 &#x27;&#x27;&#x27; # 闭区间-初始化窗口 l,r = 0,len(L)-1 # 闭区间-退出条件：窗口内不含元素，l=r+1，目标值在l,r之间 while l &lt;= r: mid = (l + r)//2 if key == L[mid]: return mid elif key &gt; L[mid]: l = mid + 1 else: r = mid - 1 return l 2 Sum无序数组的 2 Sum 问题：[1] 两数之和，给定一个整数数组和一个目标值，找出数组中和为目标值的两个数。你可以假设每个输入只对应一种答案，且同样的元素不能被重复利用 思路：使用哈希表存储元素残差，如果元素在哈希表则说明和为target，返回 代码： 12345678910111213class Solution(object): def twoSum(self, nums, target): &quot;&quot;&quot; :type nums: List[int] :type target: int :rtype: List[int] &quot;&quot;&quot; d = &#123;&#125; for i,num in enumerate(nums): if num not in d: d[target-num] = i else: return [d[num],i] 分析:空间O(n)，时间O(n) 有序数组的2Sum 问题：[167] 两数之和 II，给定一个已按照升序排列 的有序数组，找到两个数使得它们相加之和等于目标数。函数应该返回这两个下标值 index1 和 index2，其中 index1 必须小于 index2 思路：首尾指针法，如果量元素之和小于target则移动右移左指针，如果大于则左移右指针，如果相等，则返回两下标 代码： 1234567891011121314151617class Solution(object): def twoSum(self, numbers, target): &quot;&quot;&quot; :type numbers: List[int] :type target: int :rtype: List[int] &quot;&quot;&quot; low,high = 0,len(numbers)-1 while low &lt;= high: total = numbers[low] + numbers[high] if total == target: return [low+1,high+1] elif total &lt; target: low += 1 else: high -= 1 分析：空间O(1)，时间O(n) 3 Sum 问题：[15]三数之和，给定一个包含 n 个整数的数组 nums，判断 nums 中是否存在三个元素 a，b，c ，使得 a + b + c = 0 ？找出所有满足条件且不重复的三元组。 思路：先排序+2Sum，先对数组进行排序，从前向后每次选取一个不同的元素x作为基准，然后在后面的数组中找到所有不重复的2Sum等于target-x的组合，再合并收集；保证解完备且不重复的核心有两点（选择下一个不同元素中的第一个）：①基准选择：每次选择下一个不同元素中的第一个，不同基准不会有重复解，第一个元素作为基准不会漏解②2Sum时：每次移动指针到第一个不重复的位置，也保证了2Sum的完备且不重复 代码：12345678910111213141516171819202122232425class Solution(object): def threeSum(self, nums): &quot;&quot;&quot; :type nums: List[int] :rtype: List[List[int]] &quot;&quot;&quot; nums.sort() n = len(nums) target = 0 res = [] for i in xrange(n-2): if i == 0 or nums[i] != nums[i-1]: base = nums[i] l,r = i + 1, n - 1 while l &lt; r: if nums[l] + nums[r] == target - base: res.append([base,nums[l],nums[r]]) l,r = l+1,r-1 while l &lt; r and nums[l] == nums[l-1]: l += 1 while l &lt; r and nums[r] == nums[r+1]: r -= 1 elif nums[l] + nums[r] &lt; target - base: l += 1 else: r -= 1 return res 分析：空间$O(1)$，时间$O(n^2)$ N sum 问题：[18] 四数之和，给定一个包含 n 个整数的数组 nums 和一个目标值 target，判断 nums 中是否存在四个元素 a，b，c 和 d ，使得 a + b + c + d 的值与 target 相等？找出所有满足条件且不重复的四元组 思路：排序+递归+2Sum，NSum可以转化为N-1Sum，逐步转化为2Sum，类似的只要每次选取下一个不同元素中的第一个元素，就能保证最终解的完备且不重复 代码： 1234567891011121314151617181920212223242526272829303132333435class Solution(object): def fourSum(self, nums, target): &quot;&quot;&quot; :type nums: List[int] :type target: int :rtype: List[List[int]] &quot;&quot;&quot; def NSum(nums, target, N): n = len(nums) if n &lt; N or nums[0] * N &gt; target or nums[-1] * N &lt; target: return [] res = [] if N == 2: l,r = 0, n-1 while l &lt; r: if nums[l] + nums[r] &lt; target: l += 1 elif nums[l] + nums[r] &gt; target: r -= 1 else: res.append([nums[l],nums[r]]) l += 1 r -= 1 while l &lt; r and nums[l] == nums[l-1]:l += 1 while l &lt; r and nums[r] == nums[r+1]:r -= 1 return res else: for i in xrange(n-N+1): # 核心，保证了两轮解中不包含重复解 if i == 0 or nums[i] != nums[i-1]: for pre in NSum(nums[i+1:],target-nums[i],N-1): res.append([nums[i]] + pre) return res return NSum(sorted(nums),target,4) 分析：空间$O(1)$，时间$O(n^{N-1})$ 最接近的三数之和 问题：[16. 最接近的三数之和]给定一个包括 n 个整数的数组 nums 和 一个目标值 target。找出 nums 中的三个整数，使得它们的和与 target 最接近。返回这三个数的和。假定每组输入只存在唯一答案。 思路：排序后转化为2Sum，收集绝对差值最小的组合 代码： 123456789101112131415161718192021222324252627282930class Solution(object): def threeSumClosest(self, nums, target): &quot;&quot;&quot; :type nums: List[int] :type target: int :rtype: int &quot;&quot;&quot; n = len(nums) if n &lt; 3: return None nums.sort() res = None delta = float(&#x27;inf&#x27;) for i in xrange(n-2): if i == 0 or nums[i] != nums[i-1]: l, r = i + 1, n-1 while l &lt; r: cur = nums[i] + nums[l] + nums[r] - target if abs(cur) &lt; delta: res = [nums[i],nums[l],nums[r]] delta = abs(cur) if cur == 0: return target elif cur &gt; 0: r -= 1 else: l += 1 return sum(res) 口袋指针 适用问题：将数组中满足条件的元素移至一端，要求原地修改且满足条件的元素相互位置保持稳定； 使用思路：假想用一只“袋子”盛放满足条件的元素，初始时袋子为空，用l=-1指示袋子口的位置，使用另一个指针从0开始遍历整个数组，如果当前元素满足条件，则袋口+1，将当前元素通过交换放入袋中，当遍历结束时，所有满足条件的元素就都被放入到袋子中了。 移动 0 问题：[283] 移动零，给定一个数组 nums，编写一个函数将所有 0 移动到数组的末尾，同时保持非零元素的相对顺序 思路：口袋指针法，初始口袋口-1，遍历整个数组，如果元素不等于0，袋口扩充，并将该元素和袋口元素交换 代码： 1234567891011class Solution(object): def moveZeroes(self, nums): &quot;&quot;&quot; :type nums: List[int] :rtype: void Do not return anything, modify nums in-place instead. &quot;&quot;&quot; i = -1 for j in range(len(nums)): if nums[j]: i += 1 nums[i],nums[j] = nums[j], nums[i] 分析：空间O(1)，时间O(n) 红白蓝划分 问题：[75] 分类颜色，给定一个包含红色、白色和蓝色，一共 n 个元素的数组，原地对它们进行排序，使得相同颜色的元素相邻，并按照红色、白色、蓝色顺序排列，我们使用整数 0、 1 和 2 分别表示红色、白色和蓝色 思路：两次口袋指针法，先把0放头部，再把1放0后面数组的头部；或者使用首尾两个口袋，一次遍历将0放头口袋，将2放尾口袋 代码： 123456789101112131415161718192021222324class Solution(object): &quot;&quot;&quot; 思路1：使用前后指针，口袋交换法，将等于0的交换至前面，将等于2的交换至右边 开区间表示两次装袋 思路2：双边装袋，略显复杂，还不如遍历两次 &quot;&quot;&quot; def sortColors(self, nums): &quot;&quot;&quot; :type nums: List[int] :rtype: void Do not return anything, modify nums in-place instead. &quot;&quot;&quot; n = len(nums) i,j = -1,n k = 0 while k &lt; j: if nums[k] == 0: i += 1 nums[k],nums[i] = nums[i],nums[k] k += 1 elif nums[k] == 2: j -= 1 nums[k],nums[j] = nums[j],nums[k] else: k += 1 分析：空间O(1)，时间O(n) 快排划分 问题：选取首元素作为枢轴值，将数组中小于等于该值的元素交换至头部，大于该值的交换置数组尾部，返回枢轴值最终的位置 思路：口袋指针法 代码： 1234567891011def partition(A,low,high): &quot;&quot;&quot; 在A[low:high+1]中选取枢轴点，并按首轴值排序 &quot;&quot;&quot; i = low for j in xrange(low+1,high+1): if A[j] &lt;= A[low]: i += 1 A[i],A[j] = A[j],A[i] A[i],A[low] = A[low],A[i] return i 分析:空间O(1)，时间O(n) 寻找第k大 问题：[215] 数组中的第K个最大元素,在未排序的数组中找到第 k 个最大的元素 思路：有很多方法可以求解此问题，但最高效的方法是口袋指针法，类似于快排划分，将首元素(先做随机交换)作为枢轴值，将所有大于等于它的元素交换至数组头部，返回数轴值最终位置pivot，如果枢轴值位置刚好是k-1，则说明该位置的值即为第k大的值，如果i k-1，在左半边nums[:i]寻找第k大 代码：如果枢轴值不进行随机选择，效率还不如先排序！！ 1234567891011121314151617181920212223242526272829303132333435363738394041import randomclass Solution(object): &quot;&quot;&quot; 思路1：先排序，时间O(nlgn) 思路2：建立大根堆，弹出第k个O(n+klgn) 思路3：口袋指针法，平均O(n)，以首元素为枢轴值，进行划分 1. 如果数轴位置i==k-1则直接返回i对应的值 2. 如果i &lt;k-1,在有半部分nums[i+1:]寻找k-i-1大 3. 如果i &gt; k-1，在左半边nums[:i]寻找第k大 &quot;&quot;&quot; def findKthLargest(self, nums, k): &quot;&quot;&quot; :type nums: List[int] :type k: int :rtype: int &quot;&quot;&quot; n = len(nums) if n &lt; 1 or n &lt; k: return None if n == 1: return nums[0] pivot = self.partition(nums,0,n-1) if pivot == k - 1: return nums[pivot] elif pivot &lt; k - 1: return self.findKthLargest(nums[pivot+1:],k-pivot-1) else: return self.findKthLargest(nums[:pivot],k) def partition(self,nums,low,high): rand = random.randint(low,high) nums[rand],nums[low] = nums[low],nums[rand] bag = low for i in range(low+1,high+1): if nums[i] &gt;= nums[low]: bag += 1 nums[bag],nums[i] = nums[i], nums[bag] nums[bag], nums[low] = nums[low],nums[bag] return bag 分析：最好情形$T(n)=T(n/2)+O(n)$，空间复杂度O(lgn)，时间复杂度为O(n) 窗口指针包含剪枝条件的连续子序列问题： 连续子序列问题：求解满足条件f的连续子序列； 包含剪枝条件：如果在找到了一个首次满足特定条件g的连续子序列之后，发现可以排除掉(l&lt;=x]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构与算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构与算法：线性表（二）—— 栈]]></title>
    <url>%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%9A%E7%BA%BF%E6%80%A7%E8%A1%A8%EF%BC%88%E4%BA%8C%EF%BC%89%E2%80%94%E2%80%94%20%E6%A0%88%2F</url>
    <content type="text"><![CDATA[理论篇逻辑结构栈(Stack)：只允许在一端进行插入或删除操作的线性表。 栈顶：允许插入和删除的那一端 栈底：不允许插入和删除的那一端 存储结构栈多以顺序存储的方式进行存储，同时附设一个top指针指示当前栈顶位置。 C语言实现： 12345# define Maxsize 50typedef strucet&#123; Elemtype data[Maxsize]; int top;&#125; Python中的列表可直接作为栈的实现： 1stack = [] 栈也可以采用链式存储，将头结点作为栈顶，方便节点的插入和删除。 基本操作 创建空栈：InitStack(S) 入栈：push(S) 出栈：pop(S) 读取栈顶：gettop(S) 判断栈是否为空：is_empty(S) python实现： 123456789101112# 创建空栈stack = []# 入栈stack.append(obj)# 出栈p = stack.pop()# 获取栈顶/栈底对象stack[-1]stack[0]# 判断栈是否为空if stack: pass 应用篇栈的使用核心是解决以下三个问题(类似于递归)： 入栈和出栈条件(调用前的处理及返回条件)； 入栈元素(函数参数)； 出栈处理(返回后的处理) 1234567# 将序列中的元素依次通过栈的处理s = []for i,value in enumerate(a): while s and condition(s[-1],value): top = s.pop() visit(top) s.append(value) 括号匹配 问题：[LeetCode 20] 有效的括号 12345给定一个只包括 &#x27;(&#x27;，&#x27;)&#x27;，&#x27;&#123;&#x27;，&#x27;&#125;&#x27;，&#x27;[&#x27;，&#x27;]&#x27; 的字符串，判断字符串是否有效，有效字符串需满足：1. 空字符串，或者满足以下两条2. 左括号必须用相同类型的右括号闭合3. 左括号必须以正确的顺序闭合 思路：左空入栈，右出栈相消，不能消或者最终栈非空返回False 代码: 123456789101112131415def isValid(self, s): &quot;&quot;&quot; :type s: str :rtype: bool &quot;&quot;&quot; d = &#123;&#x27;(&#x27;:&#x27;)&#x27;,&#x27;[&#x27;:&#x27;]&#x27;,&#x27;&#123;&#x27;:&#x27;&#125;&#x27;,&#x27;)&#x27;:&#x27;(&#x27;,&#x27;]&#x27;:&#x27;[&#x27;,&#x27;&#125;&#x27;:&#x27;&#123;&#x27;&#125; stack = [] for c in s: if not stack or c in &#x27;(&#123;[&#x27;: stack.append(c) elif stack[-1] == d[c]: stack.pop() else: return False return not stack 表达式计算前/中/后缀表达式常用的表达式： 中缀表达式：运算符位于操作数中间，运算符按照优先级依次运算。A+B×(C-D)-E/F； 前缀表达式：运算符位于操作数前面，运算符符自右向左依次运算。-+A×B-CD/EF； 后缀表达式：运算符位与操作数后面，运算符自左向右依次运算。ABCD-×+EF/-； 表达式是递归定义的，前一个运算符的运算结果会作为下一个运算符的操作数，下一个运算符的运算结果又会作为下下一个运算符的操作数，最终返回整个运算的结果。前缀表达式又称为波兰式，后缀表达式称为逆波兰式(Reverse Polish notation，RPN)。 表达式可以表示为一颗表达式树：运算符作为内部节点，操作数作为叶节点，子表达式作为内部节点的左右子树。 中缀表达式：对应表达式树的中序遍历 前缀表达式：对应表达式树的先序遍历 后缀表达式：对应表达式树的后序遍历 表达式转换转换方法：虽然中缀表达式、前缀表达式、后缀表达式表现方式不一样，但是运算符的运算顺序却是是统一的。因此按照表达式的统一的运算顺序，依次将旧表达式的字表达式转化为新表达式的子表达式，最终得到整个新表达式。 中缀表达式转后缀表达式 思路： 1234567891. 设置栈内、栈外操作符优先级，栈内1356栈外6241；操作符栈和操作数栈；2. 遍历中缀表达式 1. 如果是数字，入数字栈 2. 如果是操作符 1. 如果操作符栈非空，且操作符优先级低于操作符栈顶，则操作符出栈、操作数栈入栈； 2. 如果操作符栈栈非空，且操作符优先级等于操作符栈顶，则操作符出栈； 3. 否则，操作符入操作符栈；3. 如果操作符栈非空，则不断出栈移入操作数栈；4. 顺序返回操作数栈中的元素； 代码： 123456789101112131415161718192021222324mid = &#x27;A+B*(C-D)-E/F&#x27;def mid_pos(str): dic_in = &#123;&#x27;(&#x27;:1,&#x27;+&#x27;:3,&#x27;-&#x27;:3,&#x27;*&#x27;:5,&#x27;/&#x27;:5,&#x27;)&#x27;:6&#125; dic_out = &#123;&#x27;(&#x27;:6,&#x27;+&#x27;:2,&#x27;-&#x27;:2,&#x27;*&#x27;:4,&#x27;/&#x27;:4,&#x27;)&#x27;:1&#125; ope = [] res = [] for c in mid: if c.isalpha(): res.append(c) else: while ope and dic_out[c] &lt; dic_in[ope[-1]]: res.append(ope.pop()) if ope and dic_out[c] == dic_in[ope[-1]]: ope.pop() else: ope.append(c) while ope: res.append(ope.pop()) return &#x27;&#x27;.join(res)mid_pos(mid)&#x27;ABCD-*+EF/-&#x27; 中缀表达式转前缀表达式 思路：与中缀转后缀类似，只需做三个切换即可：反向遍历、反向输出、栈内站外优先级互换； 代码： 123456789101112131415161718192021222324252627mid = &#x27;A+B*(C-D)-E/F&#x27;def mid_pos(str): # 1. 栈内栈外优先级互换 dic_out = &#123;&#x27;(&#x27;:1,&#x27;*&#x27;:5,&#x27;/&#x27;:5,&#x27;+&#x27;:3,&#x27;-&#x27;:3,&#x27;)&#x27;:6&#125; dic_in = &#123;&#x27;(&#x27;:6,&#x27;*&#x27;:4,&#x27;/&#x27;:4,&#x27;+&#x27;:2,&#x27;-&#x27;:2,&#x27;)&#x27;:1&#125; ope = [] res = [] # 2.反向遍历 for c in mid[::-1]: if c.isalpha(): res.append(c) else: while ope and dic_out[c] &lt; dic_in[ope[-1]]: res.append(ope.pop()) if ope and dic_out[c] == dic_in[ope[-1]]: ope.pop() else: ope.append(c) while ope: res.append(ope.pop()) # 3. 反向输出 return &#x27;&#x27;.join(res[::-1]) mid_pos(mid)&#x27;-+A*B-CD/EF&#x27; 计算表达式计算中缀表达式虽然中缀表达式是人们所习惯的表达方式，但是对于计算机来说它却很复杂，而后缀和前缀表达式则很容易求值。在计算中缀表达式的值时，通常需要先将中缀表达式转化为后缀或前缀表达式，然后再求值。 问题：将中缀表达式”A+B*(C-D)-E/F”转化为后缀表达式，并求值。其中每个字母表示一个正整数。 思路： 1234567891. 设定栈内(1356)栈外优先级，操作符函数，操作符栈和操作数栈；2. 遍历中缀表达式： 1. 如果是数字，收集多位数数值，并入数字栈； 2. 如果是操作符 1. 如果栈非空，且栈外操作符优先级小于栈顶优先级，则不断出栈操作符，并从操作数栈出栈两个元素，计算出结果后再次入操作数栈； 2. 如果栈非空，且栈外优先级等于栈顶优先级，则出栈； 3. 否则，操作数直接入操作符栈；3. 如果操作符栈不为空，则操作符出栈、操作数出栈两个操作数，计算结果后再入栈操作数4. 最后返回操作数栈中的数值； 代码: 1234567891011121314151617181920212223242526272829303132333435363738def mid_pos(mid): dic_in = &#123;&#x27;(&#x27;:1,&#x27;+&#x27;:3,&#x27;-&#x27;:3,&#x27;*&#x27;:5,&#x27;/&#x27;:5,&#x27;)&#x27;:6&#125; dic_out = &#123;&#x27;(&#x27;:6,&#x27;+&#x27;:2,&#x27;-&#x27;:2,&#x27;*&#x27;:4,&#x27;/&#x27;:4,&#x27;)&#x27;:1&#125; dic_fun = &#123;&#x27;+&#x27;:lambda x,y:x+y,&#x27;-&#x27;:lambda x,y:x-y,&#x27;*&#x27;:lambda x,y:x*y,&#x27;/&#x27;:lambda x,y:x/y,&#125; ope = [] res = [] n = len(mid) num = 0 for i,c in enumerate(mid): if c.isdigit(): num = num * 10 +int(c) if i == n - 1 or not mid[i+1].isdigit(): res.append(num) num = 0 else: while ope and dic_out[c] &lt; dic_in[ope[-1]]: y = res.pop() res[-1] = dic_fun[ope.pop()](res[-1],y) if ope and dic_out[c] == dic_in[ope[-1]]: ope.pop() else: ope.append(c) while ope: y = res.pop() res[-1] = dic_fun[ope.pop()](res[-1],y) return res[-1]N = input()for _ in xrange(N): line = raw_input().strip() if line: # 防止首字符为负号 if line[0] == &#x27;-&#x27;: line = &#x27;0&#x27; + line print mid_pos(line) else: print 0 计算后缀表达式 思路： 123451. 初始化中间结果栈res2. 从左向右扫描表达式3. 遇到多位数字时，先计算数字真实值，num = num *10 + int(c),如果当前字符是数字且是最后一个字符或当前为运算符，则将刚刚保存的数值压入res4. 遇到操作符时，出栈两个元素运算后将结果压入栈5. 扫描结束，栈中的唯一元素就是表达式的结果 算法：略，很容易实现，但后缀前缀表达式不好区分多位数，在后面计算中缀表达式时也会涉及这个过程。 计算前缀表达式 思路：只需改变扫描方向，还有多位数字的处理 123451. 初始化中间结果栈res2. 从右向左扫描表达式3. 遇到多位数字时，先计算数字真实值，num = num + int(c) * 10,如果当前字符是数字且是最后一个字符或当前为运算符，则将刚刚保存的数值压入res4. 遇到操作符时，出栈两个元素运算后将结果压入栈5. 扫描结束，栈中的唯一元素就是表达式的结 卡特兰数卡特兰数(Dyck word)：满足以下条件的序列的个数称为n阶卡特兰数： 序列由n个X和n个Y组成； 所有的前缀序列中X的个数大于等于Y的个数； 1XXXYYY XYXXYY XYXYXY XXYYXY XXYXYY 卡特兰通项： C_n = \binom{2n}{n}-\binom{2n}{n+1}=\frac{\binom{2n}{n}}{n+1},\ n\geqslant 1证明：镜像法。令1表示进栈，0表示出栈，则可转化为求一个2n位、含n个1、n个0的二进制数，满足从左往右扫描到任意一位时，经过的0数不多于1数。显然含n个1、n个0的2n位二进制数共有 $\binom{2n}{n}$个。下面考虑不满足要求的数目，考虑一个含n个1、n个0的2n位二进制数，扫描到第2m+1位上时有m+1个0和m个1（容易证明一定存在这样的情况），则后面的0-1排列中必有n-m个1和n-m-1个0。将2m+2及其以后的部分0变成1、1变成0，则对应一个n+1个0和n-1个1的二进制数$\binom{2n}{n+1}$。 卡特兰数可用于分析“同等数量的两种状态交替出现”的问题。 合法的进出栈序列个数n个不同元素的出栈序列个数=卡特兰数：如果将入栈作为X，出栈作为Y，则每个进出栈过程可表示为01序列，且出入栈次数相等，任何前缀中进栈次数大于等于出栈次数。每个合法的进出序列都唯一对应于一种出栈序列。 1进出进进出进出出 合法的括号表达式个数n对括号组成的合法序列数=卡特兰数：左括号作为X，右括号作为Y，合法括号等价于括号序列的所有前缀中左括号个数大于等于右括号个数，且左右总括号数相等。 1()((()())) 合法的二叉树形态个数n个节点组成的二叉树的形态数=卡特兰数：节点作为X，空节点作为Y，空节点数比节点树多一，且二叉树的先序遍历序列尾节点一定是空节点，去掉最后一个空节点，那么每一个先序遍历序列都满足卡特兰定理。 推广： n个内部节点组成的满二叉树个数=卡特兰数：将内部节点作为X，叶节点作为Y 合法的不超过对角线的网格路径个数n×n 网格不越过对角线的路径数=卡特兰数：X代表“向右”，Y代表“向上”。 [LeetCode 331] 验证二叉树的先序序列化 问题：序列化二叉树的一种方法是使用前序遍历。当我们遇到一个非空节点时，我们可以记录下这个节点的值。如果它是一个空节点，我们可以使用一个标记值记录，例如 #。例如，上面的二叉树可以被序列化为字符串 “9,3,4,#,#,1,#,#,2,#,6,#,#”，其中 # 代表一个空节点。给定一串以逗号分隔的序列，验证它是否是正确的二叉树的前序序列化。 1234567 _9_ / \ 3 2 / \ / \ 4 1 # 6/ \ / \ / \# # # # # # 思路：任意前缀中虚拟叶子节点(#)数不超过内部节点数，且最终虚拟叶子节点(#)数等于内部节点数，等价于该序列是合法先序序列 代码： 12345678910111213141516171819202122class Solution(object): &quot;&quot;&quot; 思路： 1. 末尾节点为虚节点，去掉末尾节点后 2. 任何前缀中节点数大于等于虚节点数 3. 前缀节点总个数=虚节点总个数 &quot;&quot;&quot; def isValidSerialization(self, preorder): &quot;&quot;&quot; :type preorder: str :rtype: bool &quot;&quot;&quot; if preorder[-1] != &#x27;#&#x27;: return False preorder = preorder.split(&#x27;,&#x27;)[:-1] count = 0 for i in xrange(len(preorder)): count += [1,-1][preorder[i] == &#x27;#&#x27;] if count &lt; 0: return False return count == 0 递归转非递归 递归调用：相当于入栈，函数参数相当于入栈元素； 递归返回：相当于出栈后做处理的结果； 递归深度：栈的深度； 树的先中后序非递归遍历是最好的实例，详见树的遍历。 递增递减栈“递增/递减栈”用于查找线性表中每个元素左右两侧第一个更小/更大(小于等于/大于等于)的元素。有以下几种变形： 前向递增栈：寻找每个元素右侧第一个小于/小于等于它的元素； 后向递增栈：寻找每个元素左侧第一个小于/小于等于它的元素； 前向递减栈：寻找每个元素右侧第一个大于/大于等于它的元素； 后向递减栈：寻找每个元素左侧第一个大于/大于等于它的元素； 以前向递增栈为例，维护递增栈的一般过程： 如果栈非空且入栈元素小于栈顶元素，则不断出栈直至条件不满足；出栈时，待入栈元素是栈顶元素右侧第一个小于它的元素； 将入栈元素下标入栈；入栈时，栈顶元素是待入栈元素左侧第一个小于等于它的元素； 说明：递增递减栈的灵活性很强，递增栈找邻近更小，递减栈找邻近更大；前向后向，等号视情况而定。 [LeetCode 496] 下一个更大元素 问题：给定两个没有重复元素的数组 nums1 和 nums2 ，其中nums1 是 nums2 的子集。找到 nums1 中每个元素在 nums2 中的下一个比其大的值。nums1 中数字 x 的下一个更大元素是指 x 在 nums2 中对应位置的右边的第一个比 x 大的元素。如果不存在，对应位置输出-1。 思路：递减栈，寻找a两边最近的比它大的值，可以通过递减栈，入栈元素比栈顶小则入栈，否则一直出栈，直至比栈顶小，每次出栈元素右侧第一个最大就是当前入栈元素，O(n+m) 代码： 12345678910111213141516171819202122def nextGreaterElement(self, findNums, nums): &quot;&quot;&quot; :type findNums: List[int] :type nums: List[int] :rtype: List[int] &quot;&quot;&quot; stack = [] d = &#123;&#125; for i,num in enumerate(nums): while stack and num &gt; nums[stack[-1]]: d[nums[stack[-1]]] = num stack.pop() stack.append(i) print d,stack res = [] for num in findNums: if num in d: res.append(d[num]) else: res.append(-1) return res [LeetCode 456] 132模式 问题：给定一个整数序列：a1, a2, …, an，一个132模式的子序列 ai, aj, ak 被定义为：当 i &lt; j &lt; k 时，ai &lt; ak &lt; aj。设计一个算法，当给定有 n 个数字的序列时，验证这个序列中是否含有132模式的子序列。注意：n 的值小于15000。 思路：后向递减栈；从后向前遍历数组，如果入栈元素比上一个出栈元素小则返回True，如果入栈元素比栈顶大则出栈直至栈空或不大于栈顶，入栈；否则最后返回False。 代码： 1234567891011def find132pattern(self, nums): n = len(nums) s = [] pre = float(&#x27;-inf&#x27;) for num in nums[::-1]: if num &lt; pre: return True while s and num &gt; s[-1]: pre = s.pop() s.append(num) return False [LeetCode 402] 移掉K位数字 问题：给定一个以字符串表示的非负整数 num，移除这个数中的 k 位数字，使得剩下的数字最小。注意:num 的长度小于 10002 且 ≥ k。num 不会包含任何前导零。 思路： 12345思路：前向递增栈+拼接； 1. 如果待入栈元素比栈顶元素小，则出栈，去掉出栈元素，直至大于等于栈顶或栈为空 2. 入栈 出栈个数等于k则结束，去掉出栈元素后的字符串，去掉前导0 出栈个数不足k则将末尾的差额元素去掉，去掉前导零 代码： 12345678910111213141516def removeKdigits(self, num, k): n = len(num) s = [] ns = set() for i,x in enumerate(num): while s and x &lt; num[s[-1]] and k: ns.add(s.pop()) k -= 1 s.append(i) if k == 0: break tmp = [num[i] for i in xrange(n) if i not in ns and i &lt; n - k] res = &#x27;&#x27;.join(tmp).lstrip(&#x27;0&#x27;) return res or &#x27;0&#x27; [LeetCode 42] 接雨水 问题：给定 n 个非负整数表示每个宽度为 1 的柱子的高度图，计算按此排列的柱子，下雨之后能接多少雨水。 思路：维护递减栈，如果入栈元素小于栈顶则入栈，如果入栈元素大于栈顶则出栈，如果相等则pass，此时次栈顶元素a&lt;栈顶元素b&lt;入栈元素c，且a,c是b两侧最近的较大元素，abc可以理解为以b为底，ab为两边所能装下的水，将元素全部装入栈的过程统计水和。 代码: 123456789101112131415def trap(self, height): &quot;&quot;&quot; :type height: List[int] :rtype: int &quot;&quot;&quot; res = 0 s = [] for i,w in enumerate(height): while s and w &gt;= height[s[-1]]: top = s.pop() if s: res += (min(w,height[s[-1]]) - height[top]) * (i - s[-1] - 1) if not s or w &lt; height[s[-1]]: s.append(i) return res 栈实现队列 问题：使用栈实现队列的下列操作： push(x) — 将一个元素放入队列的尾部。 pop() — 从队列首部移除元素。 peek() — 返回队列首部的元素。 empty() — 返回队列是否为空。 思路： 12345使用两个栈instack, outstack来模拟队列，push入instack，pop出outstack，只有在outstack中没有元素时才会向instack借 1. 初始化，两个空栈 2. push，入A 3. pop从B出，B无则将A中所有元素放入B中 3. peek，返回队首元素，返回B中栈顶，如果没有则将A中所有元素放入B中，A也没有的话返回空 代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344class MyQueue(object): def __init__(self): &quot;&quot;&quot; Initialize your data structure here. &quot;&quot;&quot; self.instack = [] self.outstack = [] def push(self, x): &quot;&quot;&quot; Push element x to the back of queue. :type x: int :rtype: void &quot;&quot;&quot; self.instack.append(x) def pop(self): &quot;&quot;&quot; Removes the element from in front of queue and returns that element. :rtype: int &quot;&quot;&quot; if not self.outstack: while self.instack: self.outstack.append(self.instack.pop()) return self.outstack.pop() def peek(self): &quot;&quot;&quot; Get the front element. :rtype: int &quot;&quot;&quot; if not self.outstack: while self.instack: self.outstack.append(self.instack.pop()) return self.outstack[-1] def empty(self): &quot;&quot;&quot; Returns whether the queue is empty. :rtype: bool &quot;&quot;&quot; return not (self.instack or self.outstack)]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构与算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构与算法：二分查找]]></title>
    <url>%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%9A%E4%BA%8C%E5%88%86%E6%9F%A5%E6%89%BE%2F</url>
    <content type="text"><![CDATA[理论篇二分查找（binary search）又称折半查找（half-interval search），用于在有序数组中以 $O(lgn)$ 的时间复杂度快速查找元素。 问题定义二分查找算法所适用问题的形式化定义：在递增函数$f(x), x \in [l,r]$中, 由函数值y反向求取x。 递增函数 $f(x)$：$f(x)$ 表示由x计算y的一种函数映射，可以是非严格递增函数； x的定义域：x 的定义域标识初始化查找窗口； 问题变形：该问题常常演化为寻找第一个 $f(x)\geqslant y$ 的x，或者寻找第一个 $f(x)&gt; y$ 的x； 1234567891011121314# 可将bisect统一理解为返回插入后的位置；# bisect_left(a,x)返回a中第一个&gt;=x的位置；# bisect_right(a,x)返回a中第一个&gt;x的位置；# 如果没有查找到，bisect_left和bisect_right效果相同；# 如果查找到，bisect_left返回a中第一个等于x的位置，bisect_right返回a中第一个大于x的位置；bisect.bisect_left([1,2,2,3,3,4,4], 3)3bisect.bisect_right([1,2,2,3,3,4,4], 3)5bisect.bisect_left([1,2,2,3,3,4,4], 2.5)3bisect.bisect_right([1,2,2,3,3,4,4], 2.5)3 一般原理二分查找算法的一般框架： 初始化查找窗口：使用位置变量来表示当前查找窗口范围，可以使用闭区间表示$[l,r]$或开区间表示$[l,r)$ 如果查找窗口不为空则循环以下过程： 在查找窗口内选取一个元素作为枢轴点，与目标值进行比较 如果目标值等于枢轴值，则返回枢轴点下标 如果目标值大于数轴值，则在枢轴点右侧窗口继续查找 如果目标值小于枢轴值，则在枢轴点左侧窗口继续查找 如果窗口为空仍未找到目标值，则返回目标值应插入位置的下标 说明： 查找区间表示：既可以使用闭区间也可以使用开区间，在代码实现上注意保证循环不变性以及最终返回值的含义； 枢轴值的选择：枢轴点原则上可以选取窗口内的任意元素，对枢轴值的不同选择可能会影响最终二分查找的效率； 代码实现普通二分查找闭区间二分查找闭区间+下中位数是折半查找最清晰简便的实现，建议作为一般模板： 使用闭区间来初始化查找窗口：l=0,r=n-1，[l,r]表示初始化窗口； 如果窗口不为空则循环以下过程：窗口不为空即$l\leqslant r$ 下中位数作为枢轴点：下中位数对应$ \frac{l+r}{2} $位置，上中位数对应$\frac{l+r+1}{2}$位置 如果目标值等于枢轴值，则返回数轴值下标 如果目标值大于数轴值，则在枢轴点右侧窗口继续查找 如果目标值小于枢轴值，则在枢轴点左侧窗口继续查找 如果未找到目标值，循环退出时l=r+1，目标值一定处于l和r之间； 123456789101112# 闭区间写法:用[l,r]代表当前查找范围def Binary_search(nums,key): l,r = 0,len(nums)-1 while l &lt;= r: mid = (l + r) &gt;&gt; 1 if key == nums[mid]: return mid elif key &gt; nums[mid]: l = mid + 1 else: r = mid - 1 return l 半开区间二分查找对窗口的描述方式变了，窗口初始化、退出条件都要做出相应修改，建议只作为参考。 123456789101112# 开区间写法:用[l,r)代表当前查找范围def Binary_search(nums,key): l,r = 0,len(L) while l &lt; r: mid = (l + r) &gt;&gt; 1 if key == nums[mid]: return mid elif key &gt; nums[mid]: l = mid + 1 else: r = mid return l 结论：考虑思路的清晰性和结果的统一性，建议使用闭区间写法，本文以下讨论都是基于二分查找的闭区间写法。 含重复元素的二分查找当数组中含有重复元素时，如果只需要返回任意一个与目标值相同的元素的下标，则上述代码无需变动，但如果需要查找目标值在数组中第一次/最后一次出现的位置，则需要对以上代码稍加改变即可。 接下来以“LeetCode 34. 在排序数组中查找元素的第一个和最后一个位置”为例，讨论两种方式。 等于作为小于/大于1、如果要寻找target第一次出现的位置，可以假装寻找$target-\varepsilon$，$\varepsilon$是一个大于0的很小的值，实际操作时可以把target==nums[mid]看做是小于，那么最终得到的l,r有以下关系，l=r+1 如果数组中存在目标值，nums[l]就是第一个等于target的元素； 如果数组中不存在目标值，nums[l]就是第一个大于target的元素； 2、如果要寻找target最后一次出现的位置，可以假装寻找$target+\varepsilon$，$\varepsilon$是一个大于0的很小的值，实际操作时可以把target==nums[mid]看做是大于，那么最终得到的l,r有以下关系，l=r+1 如果数组中存在目标值，nums[r]就是最后一个等于target的元素； 如果数组中不存在目标值，nums[r]就是最后一个小于target的元素； 12345678910111213141516171819202122232425262728def searchRange(self, nums, target): &quot;&quot;&quot; :type nums: List[int] :type target: int :rtype: List[int] &quot;&quot;&quot; # 寻找第一次出现的位置 l,r = 0, len(nums) - 1 L = R = -1 while l &lt;= r: mid = l + (r-l)/2 if target &lt;= nums[mid]: r = mid - 1 else: l = mid + 1 L = l # 寻找最后一次出现的位置 l,r = 0, len(nums) - 1 while l &lt;= r: mid = l + (r-l)/2 if target &lt; nums[mid]: r = mid - 1 else: l = mid + 1 R = r # 如果L&lt;=R说明存在目标值 return [L,R] if L &lt;= R else [-1,-1] 记录上一次成功位置另一种方法对普通的二分查找的修改更少，只需要记录上次找到的位置，然后如果要找第一个就继续在左半部分找，如果要找最后一个就继续在右半部分继续找。 1234567891011121314151617181920212223242526272829303132# 返回key在nums中第一次和最后一次出现的位置[L,R]，如果不存在则返回[-1,-1]def searchRange(self, nums, target): &quot;&quot;&quot; :type nums: List[int] :type target: int :rtype: List[int] &quot;&quot;&quot; l,r = 0, len(nums) - 1 L = -1 while l &lt;= r: mid = l + (r-l)/2 if target == nums[mid]: L = mid r = mid - 1 elif target &lt; nums[mid]: r = mid - 1 else: l = mid + 1 l,r = 0, len(nums) - 1 R = -1 while l &lt;= r: mid = l + (r-l)/2 if target == nums[mid]: R = mid l = mid + 1 elif target &lt; nums[mid]: r = mid - 1 else: l = mid + 1 return [L,R] 分析： 最坏时间复杂度：O(lgn) ASL ≈ lg(n+1)-1 二分查找的改进折半查找每次选取中间元素作为枢轴点，插值查找和斐波那契查找通过改变枢轴点的选取方式对折半查找进行了改进。插值查找每次以插值的方式确定枢轴点，适合于元素关键字分布比较均匀的情况，斐波那契查找则是每次选取黄金分割点作为枢轴点。 插值查找插值查找每次通过插值的方式来选取枢轴点： 插值查找：mid = l + (r-l)*(key-L[l])/(L[r]-L[l])，其中L[r] != L[l],L[l] &lt;= key &lt;= L[r] 12345678910111213def insertion_search(L,key): l,r = 0,len(L)-1 while l &lt;= r: if key &gt; L[r] or key &lt; L[l]:return -1 if L[r] == L[l]:return r mid = int(l + (r-l)*(key-L[l])/float((L[r]-L[l]))) if key == L[mid]: return mid elif key &gt; L[mid]: l = mid + 1 else: r = mid - 1 return -1 分析：平均时间复杂度O(loglogN)，当关键字在数据集中分布较均匀时性能最好，当分布不均时可能比折半要差 斐波那契查找 找到大于F[k]-1 &gt;= n的第一个k，在列表后面补充最后一个元素，使列表长度恰好为F[k]-1 选取mid = l+F[k-2]-1，恰好将列表分为左右F[k-2]-1和F[k-1]-1个元素 接下来和折半查找过程一致，只需判断相等时mid是否大于n-1 12345678910111213141516171819202122232425262728293031F = [0,1]def Fibonacci(k): if k &lt; len(F): return F[k] else: p = Fibonacci(k-1) + Fibonacci(k-2) F.append(p) return pdef Fibonacci_Search(L,key): n = len(L) k = 0 while n &gt; Fibonacci(k) - 1: k += 1 L = L + L[-1:] * (F[k]-1-n) l,r = 0,n-1 while l &lt;= r: mid = l+ F[k-2] - 1 if key == L[mid]: if mid &gt;=n: return n-1 else: return mid elif key &gt; L[mid]: l = mid + 1 k -= 1 else: r = mid - 1 k -= 2 return -1 分析：同样，斐波那契查找的时间复杂度还是O(log2n)，但是与折半查找相比，斐波那契查找的优点是它只涉及加法和减法运算，而不用除法，而除法比加减法要占用更多的时间，因此，斐波那契查找的运行时间理论上比折半查找小，但是还是得视具体情况而定 应用篇应用二分查找算法解决实际问题的关键在于将问题抽象为“递增函数由y求x的问题”。具体地，应用二分查找算法分三步： 定义递增函数$f(x)$：明确递增函数的含义，x一般表示问题待求的值(如下标)，f(x)一般表示条件取值(如数组元素)；实现$f(x)$的计算过程(如数组索引取值)；值得说明的是，在定义出$f(x)$之后，并不需要将x所有定义域上的函数值全部求出，只需要计算每次选取的数轴点处的函数值即可(现取现用)； 确定$f(x)=k$时的定义域：一般可以通过函数值确定x的一个大致取值范围，该范围不必精确，只需保证包含x的确切解即可； 问题转化：将原始问题转化为在$f(x)$序列中寻找第一个大于(第一个大于等于、最后一个小于、最后一个小于等于)y的x的问题； 问题转化后即可套用二分查找的一般框架进行求解。 二分查找一般用于求解以下问题： 递增函数中查找某个值的问题：值作为y，下标作为x 递增函数中查找第k小问题：名次作为y，值作为x 数学问题[LeetCode 69] 求x的平方根 问题：实现 int sqrt(int x) 函数 思路：f(x)=x**2，$x \in [1,x]$，在f(x)序列中查找最后一个&lt;=target的x 实现： 12345678910111213141516def mySqrt(self, x): &quot;&quot;&quot; :type x: int :rtype: int &quot;&quot;&quot; l, r = 0, x while l &lt;= r: mid = (l+r) &gt;&gt; 1 cur = mid * mid if x == cur: return mid elif x &lt; cur: r = mid - 1 else: l = mid + 1 return r [LeetCode 367] 有效的完全平方数 问题：给定一个正整数 num，编写一个函数，如果 num 是一个完全平方数，则返回 True，否则返回 False 思路：单调函数f(i) = i*i，f(x)=val，x in [0,num]，是否能找到 代码： 12345678910111213141516def isPerfectSquare(self, num): &quot;&quot;&quot; :type num: int :rtype: bool &quot;&quot;&quot; l,r = 0, num while l &lt;= r: mid = (l+r)/2 cur = mid * mid if cur == num: return True elif cur &lt; num: l = mid + 1 else: r = mid - 1 return False [LeetCode 668] 乘法表中第k小的数 问题：给定高度m 、宽度n 的一张 m * n的乘法表，以及正整数k，你需要返回表中第k 小的数字 123456789输入: m = 3, n = 3, k = 5输出: 3解释: 乘法表:1 2 32 4 63 6 9第5小的数字是 3 (1, 2, 2, 3, 3). 思路：二分查找。令f(x)表示乘法表中小于等于x的数字个数 单调增函数f(x)：sum(min(m, x / i) for i in range(1, n + 1)) f(x)=k时，x取值范围[1,k]问题转化为在f(x)中查找第一个大于等于k的x 代码： 12345678910def findKthNumber(self, m, n, k): f = lambda x:sum(min(m, x/i) for i in xrange(1,n+1)) l,r = 1, k while l &lt;= r: mid = (l + r) &gt;&gt; 1 if k &lt;= f(mid): r = mid - 1 else: l = mid + 1 return l [LeetCode 275] H指数 II 问题：给定一位研究者论文被引用次数的数组（被引用次数是非负整数），数组已经按照升序排列。编写一个方法，计算出研究者的 h 指数。h 指数的定义: “一位有 h 指数的学者，代表他（她）的 N 篇论文中至多有 h 篇论文，分别被引用了至少 h 次，其余的 N - h 篇论文每篇被引用次数不多于 h 次。” 12输入: citations = [0,1,3,5,6]输出: 3 思路：定义设f(h)表示”h-引用次数第h大的文章的引用次数”，h取值范围为[1,n]，f(h)是非降函数，问题转化为寻找最后一个h使得f(h)&lt;=0，如果没有则返回0 代码： 1234567891011def hIndex(self, citations): n = len(citations) f = lambda i:i-citations[n-i] l, r = 1, n while l &lt;= r: mid = (l+r)&gt;&gt;1 if f(mid) &lt;= 0: l = mid + 1 else: r = mid - 1 return r [LeetCode 483] 最小好进制 问题：对于给定的整数 n, 如果n的k（k&gt;=2）进制数的所有数位全为1，则称 k（k&gt;=2）是 n 的一个好进制。以字符串的形式给出 n, 以字符串的形式返回 n 的最小好进制。 思路： 12341+k+k^2+...k^r=(1-k^(r+1))/(1-k)=n 1. r有最大值：因为k最小为2，r和k负相关，可以得到r的最大值r&lt;=log(n+1,2)-1 2. 给定r，k有唯一备选解：又(k+1)^r&gt;k^r+k^(r-1)+...+1 = n &gt;= k^r，得k=int(n^(1./r))r越大k越小，从大到小遍历r，第一次全1二进制和等于n的返回k 代码： 123456789101112def smallestGoodBase(self, n): &quot;&quot;&quot; :type n: str :rtype: str &quot;&quot;&quot; n = int(n) max_r = int(math.log(n+1, 2) - 1) for r in xrange(max_r,1,-1): k = int(n ** (1./r)) if (k ** (r+1)-1)/(k-1) == n: return str(k) return str(n - 1) [LeetCode 878] 第 N 个神奇数字 问题：如果正整数可以被 A 或 B 整除，那么它是神奇的。返回第 N 个神奇数字。由于答案可能非常大，返回它模 10^9 + 7 的结果 12输入：N = 1, A = 2, B = 3输出：2 思路： 1234二分查找，设f(x)表示&lt;=x的正整数中有多少个神奇数字 1. f(x) = x/A + x/B - x/lcm(A,B) 2. x的取值范围为：[1,min(A,B)*N] 问题转化为，求第一个f(x)=N的x 代码: 123456789101112131415161718192021222324def nthMagicalNumber(self, N, A, B): &quot;&quot;&quot; :type N: int :type A: int :type B: int :rtype: int &quot;&quot;&quot; def gcd(m,n): while m: m,n = n%m, m return n def lcm(m,n): return m * n / gcd(m,n) def f(x): return x/A + x/B - x/lcm(A,B) l,r = min(A,B), min(A,B) * N while l &lt;= r: mid = (l+r) &gt;&gt; 1 if N &lt;= f(mid): r = mid - 1 else: l = mid + 1 return l % (10**9 + 7) [LeetCode 793] 阶乘函数后K个零 问题:f(x) 是 x! 末尾是0的数量。（回想一下 x! = 1 2 3 … x，且0! = 1）找出多少个非负整数x ，有 f(x) = K 的性质。 123456789示例 1:输入:K = 0输出:5解释: 0!, 1!, 2!, 3!, and 4! 均符合 K = 0 的条件。示例 2:输入:K = 5输出:0解释:没有匹配到这样的 x!，符合K = 5 的条件 思路： 1234n!后缀0的个数K，等于不大于n的所有乘数中，因子5的个数： 1. f(x)=x/5+x/25+x/125+...+=x/4*(1-(1/5)**n)&lt;x/4，单调增； 2. f(x) &gt;= x/5;有 5k=&gt;n&gt;4k 3. 只要存在f(x)==k，那么第一个满足该等式的x必是5的倍数，x+1,x+2,x+3,x+4也都满足有k个0结尾，即只要存在就只有5个，如果不存在有0个； 代码： 1234567891011121314151617181920212223def preimageSizeFZF(self, K): &quot;&quot;&quot; :type K: int :rtype: int &quot;&quot;&quot; def count(n): res = 0 while n: res += n/5 n /= 5 return res l,r = 4 * K, 5 * K while l &lt;= r: mid = (l + r) &gt;&gt; 1 tmp = count(mid) if tmp == K: return 5 elif tmp &lt; K: l = mid + 1 else: r = mid - 1 return 0 [LeetCode 4] 两个排序数组的中位数 问题：给定两个大小为 m 和 n 的有序数组 nums1 和 nums2 。请找出这两个有序数组的中位数。要求算法的时间复杂度为 O(log (m+n)) 。你可以假设 nums1 和 nums2 不同时为空。 1234示例 1:nums1 = [1, 3]nums2 = [2]中位数是 2.0 思路：对齐+二分查找 123456789将A、B数组(m&lt;=n)在i、j位置对齐，如果对齐位置满足以下两个条件： 1. i + j = (m+n)/2 2. 对齐位置左侧(&lt;i,&lt;j)元素总是小于等于右侧元素注意到，对整个有序数组来说，左侧有(m+n)/2个元素的位置为上中位数，那么： 1. 如果m+n为奇数，对齐位置右侧元素中的最小元素即为最终的中位数； 2. 如果m+n为偶数，对齐位置右侧元素中的最小元素为最终的上中位数，左侧最大元素为最终的下中位数；问题就转化为寻找这样的i,j，如果设f(i)表示左下大于右上个数-左上大于右下个数，那么f(i)是递增函数(非降)，可以通过二分查找在[0,m]范围内找到这样的i： 1. 如果i &lt; m and A[i] &lt; B[j-1]，在i的右半部分继续查找； 2. 如果i &gt; 0 and B[j] &lt; A[i-1]，在i的左半部分继续查找； 代码： 12345678910111213141516def findMedianSortedArrays(self, nums1, nums2): A, m, B, n = nums1, len(nums1), nums2, len(nums2) if m &gt; n: A, m, B, n = B, n, A, m l, r = 0, m while l &lt;= r: i = (l + r) &gt;&gt; 1 j = (m+n) / 2 - i if i &gt; 0 and A[i-1] &gt; B[j]: r = i - 1 elif i &lt; m and B[j-1] &gt; A[i]: l = i + 1 else: left_max = max(A[i-1] if i else float(&#x27;-inf&#x27;), B[j-1] if j else float(&#x27;-inf&#x27;)) right_min = min(A[i] if i &lt; m else float(&#x27;inf&#x27;), B[j] if j &lt; n else float(&#x27;inf&#x27;)) return right_min if (m+n) &amp; 1 else (left_max + right_min) / 2. 分析： 时间复杂度：我们对较短的数组进行了二分查找，所以时间复杂度是 O（log（min（m，n））） 空间复杂度：只有一些固定的变量，和数组长度无关，所以空间复杂度是 O ( 1 ) 树[LeetCode 230] 二叉搜索树中第K小的元素 问题：如题 思路：统计左子树节点数，如果==k-1则返回根节点，如果&gt;k-1则在左子树中差汇总啊第K小，否则再右子树中查找k-left-1小； 代码： 123456789101112131415161718def kthSmallest(self, root, k): &quot;&quot;&quot; :type root: TreeNode :type k: int :rtype: int &quot;&quot;&quot; def getNodes(root): return getNodes(root.left) + getNodes(root.right) + 1 if root else 0 if not root: return 0 left = getNodes(root.left) if left + 1 == k: return root.val elif left &gt;= k: return self.kthSmallest(root.left, k) else: return self.kthSmallest(root.right, k-left -1) [LeetCode 222] 完全二叉树的节点个数 问题：给出一个完全二叉树，求出该树的节点个数 思路： 如果左右子树高度相同，那么左子树节点数为2^h-1，h为左子树高度，右子树为一个完全二叉树 如果左右子树高度不同，那么右子树节点数为2^h-1，h为右子树高度，左子树为一个完全二叉树 代码: 1234567891011121314151617def countNodes(self, root): &quot;&quot;&quot; :type root: TreeNode :rtype: int &quot;&quot;&quot; if not root: return 0 l, r = self.get_depth(root.left), self.get_depth(root.right) if l == r: return 2 ** l + self.countNodes(root.right) else: return 2 ** r + self.countNodes(root.left) def get_depth(self, root): if not root: return 0 return self.get_depth(root.left) + 1 二维矩阵[LeetCode 74] 二维矩阵中的二分查找 问题：编写一个高效的算法来判断 m x n 矩阵中，是否存在一个目标值。该矩阵具有如下特性：每行中的整数从左到右按升序排列；每行的第一个整数大于前一行的最后一个整数。 12345678输入:matrix = [ [1, 3, 5, 7], [10, 11, 16, 20], [23, 30, 34, 50]]target = 3输出: true 思路：两次二分查找，现在首列进行二分查找，找到则返回，找不到就返回较小的行号，再在所在行进行二分查找，找到就返回，找不到就返回False； 代码： 12345678910111213141516171819202122232425def searchMatrix(self, matrix, target): &quot;&quot;&quot; :type matrix: List[List[int]] :type target: int :rtype: bool &quot;&quot;&quot; def bisect(A, key): l,r = 0, len(A) - 1 while l &lt;= r: mid = l + (r-l)/2 if A[mid] == key: return mid elif A[mid] &lt; key: l = mid + 1 else: r = mid - 1 return r if not any(matrix): return False tmp = [r[0] for r in matrix] x = bisect(tmp, target) if x &lt; 0: return False y = bisect(matrix[x], target) return matrix[x][y] == target [LeetCode 240] 二维矩阵中的二分查找II 问题： 123456789101112131415161718编写一个高效的算法来搜索 m x n 矩阵 matrix 中的一个目标值 target。该矩阵具有以下特性：每行的元素从左到右升序排列。每列的元素从上到下升序排列。示例:现有矩阵 matrix 如下：[ [1, 4, 7, 11, 15], [2, 5, 8, 12, 19], [3, 6, 9, 16, 22], [10, 13, 14, 17, 24], [18, 21, 23, 26, 30]]给定 target = 5，返回 true。给定 target = 20，返回 false。 思路：取左下角元素为枢轴值，比它大就把规模缩减到右侧，比它小就把规模缩减到上面。O(m+n) 1234567891011121314def searchMatrix(self, matrix, target): if not matrix: return False m,n = len(matrix), len(matrix[0]) r,c = m - 1, 0 while r &gt;= 0 and c &lt; n: mid = matrix[r][c] if mid == target: return True elif mid &lt; target: c += 1 else: r -= 1 return False [LeetCode 378] 有序矩阵中第K小的元素 问题：给定一个 n x n 矩阵，其中每行和每列元素均按升序排序，找到矩阵中第k小的元素 思路： 123456思路：二分查找是用于“在递增函数中由y求x的算法”，二分查找用于实际问题的精髓在于将原始问题转化为这在递增函数中由y求x的问题，然后定义递增函数的三要素：定义域、值域、映射；具体的，需要定义函数f(x)，定义域[l,r]，目标值target 在本问题中，寻找第k小的数，f(x)=k， 1. 定义域：所有可能的数作为下标，范围为左上角到右下角的整数 2. 值域：第几小作为y 3. 映射：计算x是第几小，可以通过在每行进行二分查找求和得到 因为f(x)存在重复元素，应返回第一个数 代码： 123456789101112131415def kthSmallest(self, matrix, k): &quot;&quot;&quot; :type matrix: List[List[int]] :type k: int :rtype: int &quot;&quot;&quot; l, r = matrix[0][0], matrix[-1][-1] while l &lt;= r: mid = (l+r) &gt;&gt; 1 loc = sum(bisect.bisect(m, mid) for m in matrix) if loc &gt;= k: r = mid - 1 else: l = mid + 1 return l 旋转数组[LeetCode 153]查找旋转数组中的最小值 问题：假设按照升序排序的数组在预先未知的某个点上进行了旋转，假设不含重复元素，请找出其中最小的元素。 12输入: [3,4,5,1,2]输出: 1 思路：中值和右边界比较，l==r时退出循环；如果中间的值大于右侧的值，说明最小值肯定在mid右侧，否则最小值肯定在mid或mid左侧。 代码： 123456789def findMin(self, nums): l, r = 0, len(nums) - 1 while l != r: mid = (l+r) &gt;&gt; 1 if nums[mid] &gt; nums[r]: l = mid + 1 else: r = mid return nums[l] 分析：时间复杂度为O(lgn) [LeetCode 154]查找含重复元素的旋转数组中的最小值 问题：同153，但是数组中可能含有重复元素 思路：中值与右边界比较，l==r时退出循环(l&lt;r时mid&lt;r)；如果中值大于右边界，说明最小值在mid右侧，如果中值小于右边界说明最小值在mid或mid左侧，如果中值等于右边界说明最小值在r左侧； 代码： 1234567891011def findMin(self, nums): l,r = 0, len(nums)-1 while l != r: mid = (l+r) &gt;&gt; 1 if nums[mid] &gt; nums[r]: l = mid + 1 elif nums[mid] &lt; nums[r]: r = mid else: r -= 1 return nums[l] 分析：最坏时间复杂度为O(n)，平均O(lgn) 动态规划[LeetCode 887] 鸡蛋掉落 问题: 12345678910111213141516171819你将获得 K 个鸡蛋，并可以使用一栋从 1 到 N 共有 N 层楼的建筑。每个蛋的功能都是一样的，如果一个蛋碎了，你就不能再把它掉下去。你知道存在楼层 F ，满足 0 &lt;= F &lt;= N 任何从高于 F 的楼层落下的鸡蛋都会碎，从 F 楼层或比它低的楼层落下的鸡蛋都不会破。每次移动，你可以取一个鸡蛋（如果你有完整的鸡蛋）并把它从任一楼层 X 扔下（满足 1 &lt;= X &lt;= N）。你的目标是确切地知道 F 的值是多少。无论 F 的初始值如何，你确定 F 的值的最小移动次数是多少？输入：K = 1, N = 2输出：2解释：鸡蛋从 1 楼掉落。如果它碎了，我们肯定知道 F = 0 。否则，鸡蛋从 2 楼掉落。如果它碎了，我们肯定知道 F = 1 。如果它没碎，那么我们肯定知道 F = 2 。因此，在最坏的情况下我们需要移动 2 次以确定 F 是多少 思路： 1234567理解题意：用[0,0,...,1,1,1]表示每层楼的结果，0表示不碎，1表示碎，任务是在与1的比较次数&lt;=K的前提下，最少比较几次才能找到最后一个0的位置。 思路：动态规划，用dp[m][k]表示使用k个鸡蛋移动m次所能检验的最大层数 1. dp[m][k] = dp[m-1][k-1] + dp[m-1][k] + 1，蛋碎+蛋不碎+1 2. 边界：dp[m][0]=0,dp[0][k]=0 问题转化为求最小的m使得dp[m][K]&gt;=N, 令f(m)=dp[m][K]，f(m)单调增，求第一个大于等于N的m（bisect_left），二分查找 1. f(m)=dp[m][K] 2. m 取值范围为 [1,N] 代码: 1234567891011def superEggDrop(self, K, N): &quot;&quot;&quot; :type K: int :type N: int :rtype: int &quot;&quot;&quot; dp = [[0] * (K + 1) for i in range(N + 1)] for m in range(1, N + 1): for k in range(1, K + 1): dp[m][k] = dp[m - 1][k - 1] + dp[m - 1][k] + 1 if dp[m][K] &gt;= N: return m [LeetCode 354] 俄罗斯套娃信封问题 问题:给定一些标记了宽度和高度的信封，宽度和高度以整数对形式 (w, h) 出现。当另一个信封的宽度和高度都比这个信封大的时候，这个信封就可以放进另一个信封里，如同俄罗斯套娃一样。请计算最多能有多少个信封能组成一组“俄罗斯套娃”信封（即可以把一个信封放到另一个信封里面）。 123输入: envelopes = [[5,4],[6,4],[6,7],[2,3]]输出: 3 解释: 最多信封的个数为 3, 组合为: [2,3] =&gt; [5,4] =&gt; [6,7]。 思路： 123排序后变成求最长递增子序列问题 1. 排序：首先按照宽度排序，宽度相同则高度小的在前面；这一点至关重要，可防止宽度相同时，大的包裹小的 2. 最长递增子序列：维护一个辅助数组，每个元素的下标代表以该元素结尾的最长递增子序列的长度-1 代码: 12345678910111213141516def maxEnvelopes(self, envelopes): &quot;&quot;&quot; :type envelopes: List[List[int]] :rtype: int &quot;&quot;&quot; envelopes.sort(key=lambda x:(x[0],-x[1])) dp = [] count = 0 for _, h in envelopes: idx = bisect.bisect_left(dp, h) if idx &gt; len(dp) - 1: dp.append(h) else: dp[idx] = h return len(dp) [LeetCode 410] 分割数组的最大值 问题:给定一个非负整数数组和一个整数 m，你需要将这个数组分成 m 个非空的连续子数组。设计一个算法使得这 m 个子数组各自和的最大值最小。 1234567891011输入:nums = [7,2,5,10,8]m = 2输出:18解释:一共有四种方法将nums分割为2个子数组。其中最好的方式是将其分为[7,2,5] 和 [10,8]，因为此时这两个子数组各自的和的最大值为18，在所有情况中最小。 思路: 1234567思路1：动态规划，超时。令dp[i][m]表示以i为结尾(不含i)，分为m段，各段最大值的最小值；有： 1. dp[i][m] = min(max(dp[j][m-1], sum(nums[j:i]))),j=1,2,...,i-1 2. 边界：i &lt; m时，取无穷，i==m时，取所有元素的最大值 思路2：二分查找。设f(x)表示使得子数组各自和的最大值的最小值为x时，所需的最小子数组数是否&lt;=m，则： 1. x取值范围为[sum(nums)/m,sum(nums)] 2. f(x)取值[0,0,...,1,1] 问题转化为求f(x)第一次取1时的x 代码： 1234567891011121314151617181920212223242526def splitArray(self, nums, m): &quot;&quot;&quot; :type nums: List[int] :type m: int :rtype: int &quot;&quot;&quot; def valid(x): count = 1 cur = 0 for num in nums: cur += num if cur &gt; x: count += 1 if count &gt; m: return False cur = num return True l,r = max(nums), sum(nums) while l &lt;= r: mid = (l+r) &gt;&gt; 1 if valid(mid): r = mid - 1 else: l = mid + 1 return l [LeetCode 719] 找出第 k 小的距离对(至尊二分查找) 问题： 123456789101112131415给定一个整数数组，返回所有数对之间的第 k 个最小距离。一对 (A, B) 的距离被定义为 A 和 B 之间的绝对差值。示例 1:输入：nums = [1,3,1]k = 1输出：0 解释：所有数对如下：(1,3) -&gt; 2(1,1) -&gt; 0(3,1) -&gt; 2因此第 1 个最小距离的数对是 (1,1)，它们之间的距离为 0。 思路：二分查找+含重复元素的二分查找。令f(x)表示距离小于等于x的数对个数，观察到f(x)是单调增(含重复元素)的函数，寻找数对之间第k小的距离，即是寻找x使得f(x-1)&lt;k&lt;=f(x)，这样的问题可以通过二分查找求解；计算f(x)时可以先对nums进行排序，f(x)=sum(i - bisect.bisect_left(nums, num - mid, 0, i) for i, num in enumerate(nums)) 代码： 1234567891011def smallestDistancePair(self, nums, k): nums.sort() l, r = 0, nums[-1] - nums[0] while l &lt;= r: mid = (l+r) &gt;&gt; 1 cur = sum(i - bisect.bisect_left(nums, num - mid, 0, i) for i, num in enumerate(nums)) if k &lt;= cur: r = mid - 1 else: l = mid + 1 return l 其他二分查找问题[LeetCode 852] 山脉数组的峰顶索引 问题： 12345我们把符合下列属性的数组 A 称作山脉：A.length &gt;= 3存在 0 &lt; i &lt; A.length - 1 使得A[0] &lt; A[1] &lt; ... A[i-1] &lt; A[i] &gt; A[i+1] &gt; ... &gt; A[A.length - 1]给定一个确定为山脉的数组，返回任何满足 A[0] &lt; A[1] &lt; ... A[i-1] &lt; A[i] &gt; A[i+1] &gt; ... &gt; A[A.length - 1] 的 i 的值。 思路：如果A[mid] &lt; A[mid+1]，说明山峰在mid右侧，否则说明山峰在mid或mid左侧； 代码： 12345678910111213def peakIndexInMountainArray(self, A): &quot;&quot;&quot; :type A: List[int] :rtype: int &quot;&quot;&quot; l, r = 0, len(A)-1 while l != r: mid = (l + r) &gt;&gt; 1 if A[mid] &lt; A[mid+1]: l = mid + 1 else: r = mid return l [LeetCode 875] 爱吃香蕉的珂珂 问题： 1234567珂珂喜欢吃香蕉。这里有 N 堆香蕉，第 i 堆中有 piles[i] 根香蕉。警卫已经离开了，将在 H 小时后回来。珂珂可以决定她吃香蕉的速度 K （单位：根/小时）。每个小时，她将会选择一堆香蕉，从中吃掉 K 根。如果这堆香蕉少于 K 根，她将吃掉这堆的所有香蕉，然后这一小时内不会再吃更多的香蕉。 珂珂喜欢慢慢吃，但仍然想在警卫回来前吃掉所有的香蕉。返回她可以在 H 小时内吃掉所有香蕉的最小速度 K（K 为整数） 思路： 12341. f(x)表示速度为x时需要多少个小时吃完，f(x)=sum(math.ceil(p*1./k) for p in piles)，为单调减函数2. x取值范围：l,r = 1, max(piles)3. 问题转化:求f(x)&lt;=H的最小x 代码： 123456789101112131415161718def minEatingSpeed(self, piles, H): &quot;&quot;&quot; :type piles: List[int] :type H: int :rtype: int &quot;&quot;&quot; def helper(k): return sum(math.ceil(p*1./k) for p in piles) l,r = 1, max(piles) while l &lt;= r: mid = (l+r) &gt;&gt; 1 if helper(mid) &lt;= H: r = mid - 1 else: l = mid + 1 return l [360 笔试] 问题： 思路： 123思路1: 直接对a[l-1:r]进行集合运算，返回长度，时间O(qn)，空间O(n)；超时思路2：存储count[n][m]，表示&lt;=n时每种花的数目，时间O(mq)，空间O(mn)；超内存思路3：二分查找，存储每种花出现的下标列表，给定[l,r]，每次在每种花中进行二分查找，对l返回第一个大于等于l的下标x，对r返回第一个大于r的下标y，如果y&gt;l则说明该花在[l,r]内； 代码： n,m = map(int, raw_input().split()) a = map(int, raw_input().split()) q = input() d = collections.defaultdict(list) for i,x in enumerate(a): d[x].append(i) def solve(l,r): l,r = l-1, r-1 res = 0 for key in d: x = bisect.bisect_left(d[key], l) y = bisect.bisect_right(d[key], r) res += (y &gt; x) return res for _ in xrange(q): l,r = map(int, raw_input().split()) print solve(l,r)]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构与算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构与算法：数论]]></title>
    <url>%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%9A%E6%95%B0%E8%AE%BA%2F</url>
    <content type="text"><![CDATA[数论的理论部分详见 math 专题，本部分仅讨论数论相关的核心算法，及其在实际问题中的应用。 整除性与素数辗转相除法 欧几里得算法（Euclidean algorithm）：也称为辗转相除法，通常用于计算两个整数的最大公约数，对$0\leqslant m&lt;n$，EA用到以下递推式： \begin{align*} &gcd(0,n)=n;\\ &gcd(m,n)=gcd(n\%m,m),\ m>0 \end{align*} 分析：时间复杂度为$O(lgn)$，具体证明参见拉梅定理 计算最大公约数123456789def gcd(m,n): &quot;&quot;&quot;返回m,n的最大公约数。无所谓大小，m为除数，n为被除数&quot;&quot;&quot; while m: m,n = n%m,m return ndef gcd(m,n): &quot;&quot;&quot;更加简洁的递归式&quot;&quot;&quot; return gcd(n%m,m) if m else n 计算最小公倍数最小公倍数和最大公约数有以下关系： gcd(m,n)*lcm(m,n)=m*n123def lcm(m,n): &quot;&quot;&quot;返回m,n的最小公倍数&quot;&quot;&quot; return m * n /gcd(m,n) 求解贝祖方程扩展欧几里得算法（Extended Euclidean algorithm）：在求得gcd(m,n)的同时，一定能找到整数x,y（其中一个可能是负数），使得它们满足贝祖等式： mx + ny = gcd(m,n)由欧几里得算法递推过程可得： \left\{\begin{matrix} \begin{align*} &gcd(0,n)=n\\ &gcd(m,n)=gcd(n\%m,m)\\ &mx+ny=gcd(m,n)\\ &(n\%m)x_1+my_1=gcd(n\%m,m)\\ &n\%m=n-\left \lfloor \frac{n}{m} \right \rfloor m \end{align*} \end{matrix}\right.整理化简得 a,b 的递推式 $xm+yn=(y_1-x_1\left \lfloor \frac{n}{m} \right \rfloor)m+x_1n$，即： \left\{\begin{matrix} \begin{align*} &x=y_1-x_1\left \lfloor \frac{n}{m} \right \rfloor\\ &y=x_1\\ &x_{end}=0\\ &y_{end}=1 \end{align*} \end{matrix}\right.123456def bz(m,n): &quot;&quot;&quot;求解贝祖方程x*m+y*n=gcd(m,n)，返回一组(x,y)&quot;&quot;&quot; if m == 0: return 0,1 x,y = bz(n%m,m) return y-x*(n/m),x 贝祖定理(Bézout’s identity)：整数不定方程 $mx+ny=c$ 有解（无穷多个解）的充要条件是$gcd(m,n)\mid c$。 如果贝祖方程有解，则一定有无穷解，设$(x_0,y_0)$是$mx+ny=gcd(m,n)$由辗转相除法得到的一个解，$d=gcd(m,n)$，则$mx+ny=c$的通解可表示为： \left\{\left({\frac {c}{d}}x_{0}+{\frac {kn}{d}},\ {\frac {c}{d}}y_{0}-{\frac {km}{d}}\right)\mid k\in \mathbb {Z} \right\}推论：$mx+ny=1$有解的充要条件为gcd(m,n)=1，即m,n互素。 [592] 分数加减运算 问题：给定一个表示分数加减运算表达式的字符串，你需要返回一个字符串形式的计算结果。 这个结果应该是不可约分的分数，即最简分数。 如果最终结果是一个整数，例如 2，你需要将它转换成分数形式，其分母为 1。所以在上述例子中, 2 应该被转换为 2/1。 12输入:&quot;-1/2+1/2&quot;输出: &quot;0/1&quot; 思路：通过最小公倍数通分，通过最大公约数约分 找到所有单元+-x/y，进而得到所有分子分母， 求分母最小公倍数 通分x*lcm/y求和，得到最终的分子、分母 最后再求最终分子分母的最大公约数 分子分母除以最大公约得到最简分数 代码： 123456789101112131415161718192021222324252627282930313233343536def fractionAddition(self, expression): &quot;&quot;&quot; :type expression: str :rtype: str &quot;&quot;&quot; def gcd(a,b): if a &lt; b: a,b = b,a while b: a,b = b,a%b return a def lcm(a,b): return a * b /gcd(a,b) parts = [] part = &#x27;&#x27; for c in expression: if c in &#x27;+-&#x27;: if part: parts.append(part) part = &#x27;&#x27; part += c if part: parts.append(part) x = [] y = [] for p in parts: cur = p.split(&#x27;/&#x27;) x.append(int(cur[0])) y.append(int(cur[1])) # print x,y,parts lcm_y = reduce(lcm,y) total_x = sum(x[i]*lcm_y/y[i] for i in range(len(x))) gcd_xy = abs(gcd(lcm_y,total_x)) return &#x27;%d/%d&#x27;%(total_x/gcd_xy,lcm_y/gcd_xy) [365] 水壶问题 问题：有两个容量分别为 x升 和 y升 的水壶以及无限多的水。请判断能否通过使用这两个水壶，从而可以得到恰好 z升 的水？如果可以，最后请用以上水壶中的一或两个来盛放取得的 z升 水。 思路：等价于ax+by=z有整数解(a,b)，且z &lt;= x+y 代码： 1234567def canMeasureWater(self, x, y, z): def gcd(m,n): while m: m,n = n%m,m return n g = gcd(x,y) return z % g == 0 and z &lt;= x + y if g else not z 寻找 n 以内的素数一般有”试除法“和”筛法“两种思路来寻找n以内的素数，这里介绍三种常用的方法： 方法 思路 空间 时间 优化试除法 对[2,n]中的每一个数k，试除[2,sqrt(k)]中所有素数，如果能整除说明k不是素数 O(1) O(nlgn) 埃拉托斯特尼筛法 假设所有数都是素数，然后[2,sqrt(n)]间的素数p，将[p*p::p]的整数标记为非素数 O(n) O(nlglgn) 线性筛法 每个合数都能唯一分解为最小素因子和一个小于自己的数之积，只用合数的最小素因子筛将其筛去，避免重复的筛选 O(n) O(n) [204] 计算质数12345678910111213141516171819202122232425262728293031323334353637383940414243444546# 1. 试除法：4684 msdef countPrimes(self, n): if n &lt; 3: return 0 res = [2] for k in xrange(3,n): i = 0 tmp = int(k**0.5) while res[i] &lt;= tmp: if k % res[i]: i += 1 else: break else: res.append(k) return len(res)# 2. 埃氏筛法：176ms，接近线性时间复杂度，实际效果竟然比线性筛还好def countPrimes(self, n): if n &lt; 3: return 0 filt = [1] * n filt[0] = filt[1] = 0 for i in xrange(2,int(n**0.5)+1): if filt[i]: filt[i*i::i] = [0] * ((n-1-i*i)/i + 1) return sum(filt)# 3. 线性筛: 924ms def countPrimes(self, n): if n &lt; 3: return 0 prime = [] filt = [1] * n filt[0] = filt[1] = 0 for i in xrange(2,n): if filt[i]: prime.append(i) for p in prime: if i * p &gt; n - 1: break filt[i*p] = 0 # 如果i能被p整除，则i*后续素数的合数只能被p筛去 if i % p == 0: break return sum(filt) 扩展：以上思路很容易用来求解一个整数是否为素数、整数因式分解等问题。 算术基本定理每一个正整数都能唯一地表示为素数幂积的形式，p代表所有可能的素数： n=\prod_{p}p^{n_p},\ n_p\geqslant 0[313] 超级丑数 问题：编写一段程序来查找第 n 个超级丑数。超级丑数是指其所有质因数都是长度为 k 的质数列表 primes 中的正整数。 123输入: n = 12, primes = [2,7,13,19]输出: 32 解释: 给定长度为 4 的质数列表 primes = [2,7,13,19]，前 12 个超级丑数序列为：[1,2,4,7,8,13,14,16,19,26,28,32] 。 思路： 12345后续丑数必然是由已有丑数与质数列表中的质数乘积的结果，为了实现有序输出，可进行k路归并，丑数排序可以看做是k个有序表的归并排序2*1，2*2，2*4，2*7...7*1，7*2，7*4，7*7......可以用k个指针指示每个有序表当前元素的下标，将较小的值放入总表，同时如果有序表中的值等于总表的值，下标要加1 代码： 123456789101112131415161718def nthSuperUglyNumber(self, n, primes): &quot;&quot;&quot; :type n: int :type primes: List[int] :rtype: int &quot;&quot;&quot; k = len(primes) index = [0] * k ugly = [1] count = 1 while count &lt; n: cur = min(ugly[index[i]] * primes[i] for i in xrange(k)) ugly.append(cur) count += 1 for i in xrange(k): if ugly[index[i]] * primes[i] == cur: index[i] += 1 return ugly[-1] 分析：时间复杂度O(kn) 模运算定义模运算：n模m表示n除以m所得余数，记做$n\ mod\ m$或$n\%m$ n\ mod\ m = n - \left \lfloor \frac{n}{m} \right \rfloor m 模：mod后面的数称为模，至今还没有人给mod前面的数取名 余数：取模的结果，余数总是处于0和模之间 模运算可以推广到任意实数 模运算性质mod运算的性质：以下性质都可以通过取模运算的定义来证明 四则运算法则： $(a+b)\%c=(a\%c+b\%c)\%c$ $(a-b)\%c=(a\%c-b\%c)\%c$ $(ab)\%c=(a\%cb)\%c=(a\%c*b\%c)\%c$ $(a^b)\%c=((a\%c)^b)\%c$ $\frac{a}{b}\%c = a\bar{b}\%c,\ if\ b \mid a, b\perp c$ 分配律：$c(a\%b)=(ca)\%(cb)$，是mod运算最重要的代数性质 模模律：$a\%c\%c=a\%c$ [523] 连续子数组和为k的倍数 问题：给定一个包含非负数的数组和一个目标整数 k，编写一个函数来判断该数组是否含有连续的子数组，其大小至少为 2，总和为 k 的倍数，即总和为 n*k，其中 n 也是一个整数。 123输入: [23,2,6,4,7], k = 6输出: True解释: [23,2,6,4,7]是大小为 5 的子数组，并且和为 42。 思路：(a-b)%m=a%m-b%m，连续数组的和是k的倍数在k!=0时等价于能被k整除，等价于余数为0，等价于前i项和前j项和对k的模相等。记录模m第一次出现的位置，下次出现与首次出现隔了1个元素以上则返回True，初始没有元素设模为0，下标为-1. 代码: 1234567891011121314151617def checkSubarraySum(self, nums, k): &quot;&quot;&quot; :type nums: List[int] :type k: int :rtype: bool &quot;&quot;&quot; d = &#123;0:-1&#125; total = 0 for i,num in enumerate(nums): total += num tmp = total % (k or total+1) if tmp in d: if i &gt; d[tmp] + 1: return True else: d[tmp] = i return False 大数取模一般大数取模对于某个大数，我们可以用字符串$s=a_1a_2…a_n$来存储，设f(n)代表前n个字符所组成的数值，则有: \left\{\begin{matrix} \begin{align*} &f(n)=10f(n-1)+a_n\\ &f(1)=a_1 \end{align*} \end{matrix}\right.利用取模四则运算法则来模拟手算竖式的方法: \left\{\begin{matrix} \begin{align*} &f(n)\% m=(f(n-1)\% m \times 10+a_n)\%m\\ &f(1)\%m=a_1 \%m \end{align*} \end{matrix}\right.代码: 12345def get_mod(s,m): res = 0 for c in s: res = (res%m * 10 + int(c))%m return res 快速幂模 蒙哥马利(Montgomery)幂模运算是快速计算a^b%k的一种算法，是RSA加密算法的核心之一。设$f(c,e,n)=c^e\%n$，则有以下递推式： \left\{\begin{matrix} \begin{align*} &f(c,e,n)=f(c,e-1,n)*c\%n=f(c^2\%n,e>>1,n)*c\%n,\ if\ e\&1=1\\ &f(c,e,n)=f(c^2\%n,e>>1,n),\ else \end{align*} \end{matrix}\right.代码： 1234567891011121314# 时间复杂度为lgedef quick_mod(base,exp,mod): res = 1 while exp: if exp &amp; 1: res = res * base % mod base, exp = base * base % mod, exp &gt;&gt; 1 return res # 递归更简洁def quick_mod(base,exp,mod): if exp == 0: return 1%mod return (Montgomery1(base*base%mod,exp&gt;&gt;1,mod)*[1,base%mod][exp&amp;1])%mod [372] 超级次方——幂模 问题：你的任务是计算 a^b 对 1337 取模，a 是一个正整数，b 是一个非常大的正整数且会以数组形式给出。 思路：快速取模,取模的乘法法则(ab)%m=(a%mb%m)%m，f(a,b0+b110+…) = (a^b0%m f(a^10,b1+b2*10+…))%m 代码: 123456789101112131415161718192021def superPow(self, a, b): &quot;&quot;&quot; :type a: int :type b: List[int] :rtype: int &quot;&quot;&quot; res = 1 base = a for n in b[::-1]: res = (res * self.quick_mod(base,n,1337)) % 1337 base = self.quick_mod(base,10,1337) return resdef quick_mod(self,base,exp,mod): res = 1 while exp: if exp &amp; 1: res = res * base % mod base = base * base % mod exp &gt;&gt;= 1 return res 其他数论问题勾股素数 问题：找到&lt;=n的素勾股数 和 不能构成勾股数的边的长度个数。如果 (a, b, c) 是勾股数，它们的正整数倍数，也是勾股数，即 (na, nb, nc) 也是勾股数。若果 a, b, c 三者互质（它们的最大公因数是 1），它们就称为素勾股数。 思路： 12345678以下的方法可用来找出勾股数。设 m &gt; n 、 m 和 n 均是正整数，a = m2 − n2,b = 2mn,c = m2 + n2若 m 和 n 是互质，而且 m 和 n 其中有一个是偶数，计算出来的 a, b, c 就是素勾股数。（若 m 和 n 都是奇数， a, b, c 就会全是偶数，不符合互质。）所有素勾股数可用上述列式当中找出，这亦可推论到数学上存在无穷多的素勾股数。 代码: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354#include&lt;cstdio&gt;#include&lt;iostream&gt;#include&lt;cstring&gt;#include&lt;cmath&gt;using namespace std;bool vis[1000010];int gcd(int x,int y)&#123; return y==0?x:gcd(y,x%y);&#125;int main()&#123; int n; while(scanf(&quot;%d&quot;,&amp;n)!=EOF) &#123; int cnt = 0,num = 0; memset(vis,0,sizeof(vis)); for(int i=1;i&lt;=sqrt(n);i++) &#123; for(int j=i+1;j&lt;=sqrt(n);j++) &#123; int x = j*j-i*i; int y = 2*i*j; int z = i*i+j*j; if(x&lt;=n &amp;&amp; y&lt;=n &amp;&amp; z&lt;=n) &#123; if(!vis[x]) vis[x] = true,cnt++; if(!vis[y]) vis[y] = true,cnt++; if(!vis[z]) vis[z] = true,cnt++; int f = n/z; for(int k=1;k&lt;=f;k++) &#123; int nx = x*k,ny = y*k,nz = z*k; if(!vis[nx]) vis[nx] = true,cnt++; if(!vis[ny]) vis[ny] = true,cnt++; if(!vis[nz]) vis[nz] = true,cnt++; &#125; if(gcd(i,j)==1) &#123; if(((i&amp;1) &amp;&amp; (j&amp;1)==0) || ((j&amp;1) &amp;&amp; (i&amp;1)==0)) num++; &#125; &#125; &#125; &#125; printf(&quot;%d %d\n&quot;,num,n-cnt); &#125; return 0;&#125; 整数-序列转化这可能是最常用的操作了 1234567891011121314151617def n2s(n): &quot;&quot;&quot;将整数n转化为字符串&quot;&quot;&quot; if n == 0: return &#x27;0&#x27; res = &#x27;&#x27; while n: div,mod = divmod(n,10) n = n/10 res = str(mod) + res return res def s2n(s): &quot;&quot;&quot;将字符串转化为整数&quot;&quot;&quot; res = 0 for c in s: res = res * 10 + int(c) return res [171] Excel列名转列号 问题:给定一个Excel表格中的列名称，返回其相应的列序号 12345678A -&gt; 1B -&gt; 2C -&gt; 3...Z -&gt; 26AA -&gt; 27AB -&gt; 28 ... 思路：26进制转化为10进制 代码： 123456def titleToNumber(self, s): c2i = lambda x:ord(x)-ord(&#x27;A&#x27;)+1 res = 0 for i,c in enumerate(s): res = res * 26 + c2i(c) return res [168] Excel列号转列名 问题：给定一个正整数，返回它在 Excel 表中相对应的列名称 思路：难点在于A表示1而不是0，Ai26**i = ai26^i+26^i，a026^0+26^0 + a126^1+26^1 + … + =n，(n-1)/26 = a1*26^0 + 26^0+…转化为子问题，因此n = (n-1)/26，a0 = (n-1)%26 0~25对应A~Z 代码： 123456def convertToTitle(self, n): ans = &#x27;&#x27; while n: ans = chr(ord(&#x27;A&#x27;) + (n - 1) % 26) + ans n = (n - 1) / 26 return ans [812] 三点构成最大面积 问题:给定包含多个点的集合，从其中取三个点组成三角形，返回能组成的最大三角形的面积 思路：遍历所有可能的三点组合，求出最大面积，核心在于已知三点如何求三点构成的三角形的面积，设A(x1,y1),B(x2,y2),C(x3,y3)，则三角形面积可以表示为AB与AC叉乘绝对值的一半： 12345 i j x2-x1, y2-y1AB×AC = = (x2-x1)*(y3-y1)-(x3-x1)*(y2-y1) x3-x1, y3-y1S(ABC) = 0.5*abs(AB×AC) 代码： 1234567891011121314151617def largestTriangleArea(self, points): &quot;&quot;&quot; :type points: List[List[int]] :rtype: float &quot;&quot;&quot; def area(a,b,c): x1,y1,x2,y2,x3,y3 = a[0],a[1],b[0],b[1],c[0],c[1] return 0.5 * abs((x2-x1)*(y3-y1)-(x3-x1)*(y2-y1)) n = len(points) res = 0 for i in xrange(n-2): for j in xrange(i+1,n-1): for k in xrange(j+1,n): res = max(res, area(points[i], points[j], points[k])) return res [593] 四点是否构成正方形 问题：给定二维空间中四点的坐标，返回四点是否可以构造一个正方形 思路:正方形充要条件：四条边相等，对角线相等&gt;0;识别对角点的方法：如果是正方形，则排序后，第一个点代表左下点，最后一个点代表右上点，它们必是对角顶点 代码: 123456789101112131415def validSquare(self, p1, p2, p3, p4): &quot;&quot;&quot; :type p1: List[int] :type p2: List[int] :type p3: List[int] :type p4: List[int] :rtype: bool &quot;&quot;&quot; dis = lambda x,y:(x[0]-y[0])**2 + (x[1]-y[1])**2 p = [p1,p2,p3,p4] p.sort() if dis(p[0],p[1]) == dis(p[0],p[2]) == dis(p[3],p[1]) == dis(p[3],p[2]) &gt; 0: if dis(p[0],p[3]) == dis(p[1],p[2]): return True return False [223] 矩形面积 问题:在二维平面上计算出两个由直线构成的矩形重叠后形成的总面积。每个矩形由其左下顶点和右上顶点坐标表示，如图所示。 思路:转化为求交集的面积sum() = max(0,min(D,H)-max(B,F)) * max(0,min(C,G)-max(A,E)) 代码: 1234567891011121314def computeArea(self, A, B, C, D, E, F, G, H): &quot;&quot;&quot; :type A: int :type B: int :type C: int :type D: int :type E: int :type F: int :type G: int :type H: int :rtype: int &quot;&quot;&quot; total = (D-B) * (C-A) + (G-E) * (H-F) return total - max(0,min(D,H)-max(B,F)) * max(0,min(C,G)-max(A,E)) [453] 最小移动次数使数组元素相等 问题:给定一个长度为 n 的非空整数数组，找到让数组所有元素相等的最小移动次数。每次移动可以使 n - 1 个元素增加 1 12345678910输入:[1,2,3]输出:3解释:只需要3次移动（注意每次移动会增加两个元素的值）：[1,2,3] =&gt; [2,3,3] =&gt; [3,4,3] =&gt; [4,4,4] 思路：：一次移动将n - 1个元素加1，等价于将剩下的1个元素减1。因此累加数组中各元素与最小值之差即可 代码: 123456def minMoves(self, nums): &quot;&quot;&quot; :type nums: List[int] :rtype: int &quot;&quot;&quot; return sum(nums) - min(nums) * len(nums) [462] 最少移动次数使数组元素相等 II 问题：给定一个非空整数数组，找到使所有数组元素相等所需的最小移动数，其中每次移动可将选定的一个元素加1或减1 1234567输入:[1,2,3]输出:2说明：只有两个动作是必要的（记得每一步仅可使其中一个元素加1或减1）： [1,2,3] =&gt; [2,2,3] =&gt; [2,2,2] 思路：中位数定理：当a取x中的中位数时，绝对残差和sum(|x-a|)最小 代码: 123456789101112131415161718192021222324252627282930313233343536373839import randomclass Solution(object): def minMoves2(self, nums): &quot;&quot;&quot; :type nums: List[int] :rtype: int &quot;&quot;&quot; n = len(nums) median = self.findKthLargest(nums,0,n-1,(n-1)/2+1) return sum(abs(num-median) for num in nums) def findKthLargest(self, nums, l, r, k): &quot;&quot;&quot; :type nums: List[int] :type k: int :rtype: int &quot;&quot;&quot; pivot = self.partition(nums,l,r) if pivot == k - 1: return nums[pivot] elif pivot &lt; k - 1: return self.findKthLargest(nums, pivot+1, r, k) else: return self.findKthLargest(nums, l, pivot-1, k) def partition(self,nums,low,high): rand = random.randint(low,high) nums[rand],nums[low] = nums[low],nums[rand] bag = low for i in range(low+1,high+1): if nums[i] &lt;= nums[low]: bag += 1 nums[bag],nums[i] = nums[i], nums[bag] nums[bag], nums[low] = nums[low],nums[bag] return bag [517] 超级洗衣机——均衡 问题:假设有 n 台超级洗衣机放在同一排上。开始的时候，每台洗衣机内可能有一定量的衣服，也可能是空的。在每一步操作中，你可以选择任意 m （1 ≤ m ≤ n） 台洗衣机，与此同时将每台洗衣机的一件衣服送到相邻的一台洗衣机。给定一个非负整数数组代表从左至右每台洗衣机中的衣物数量，请给出能让所有洗衣机中剩下的衣物的数量相等的最少的操作步数。如果不能使每台洗衣机中衣物的数量相等，则返回 -1。 思路: 1234讨论每个元素处的情形，记l,r代表i元素左右两侧和超出平均水平的值，讨论三种情形： 1. l &gt;=0,r&gt;=0，两边一定会流向i，可同时进行，需要次数max(l,r) 2. l &lt; 0,r &lt; 0,i一定会流向两边，不可同时进行，需要次数-l-r 3. 其他情形流经i的元素需要有max(abs(l),abs(r))次流动 代码： 12345678910111213141516171819202122def findMinMoves(self, machines): &quot;&quot;&quot; :type machines: List[int] :rtype: int &quot;&quot;&quot; n = len(machines) total = sum(machines) if total % n: return -1 avg = total/n l,r = 0,0 res = 0 for i, num in enumerate(machines): r += avg - num if l &gt;= 0 and r &gt;= 0: res = max(res,max(l,r)) elif l &lt; 0 and r &lt; 0: res = max(res,abs(l)+abs(r)) else: res = max(res,abs(l),abs(r)) l += num - avg return res [319] 灯泡开关 问题：初始时有 n 个灯泡关闭。 第 1 轮，你打开所有的灯泡。 第 2 轮，每两个灯泡你关闭一次。 第 3 轮，每三个灯泡切换一次开关（如果关闭则开启，如果开启则关闭）。第 i 轮，每 i 个灯泡切换一次开关。 对于第 n 轮，你只切换最后一个灯泡的开关。 找出 n 轮后有多少个亮着的灯泡。 思路：对于第i个灯泡，当i的因子个数为奇数时，最终会保持点亮状态，例如9的因子为1，3，9，当且仅当i为完全平方数时，其因子个数为奇数，另外1~n中完全平方数的个数为n**0.5 代码: 12def bulbSwitch(self, n): return int(n**0.5) [172] 阶乘后的零 问题：给定一个整数 n，返回 n! 结果尾数中零的数量 思路：n!后缀0的个数 = n!质因子中5的个数 = floor(n/5) + floor(n/25) + floor(n/125) + …. 代码： 123456def trailingZeroes(self, n): res = 0 while n: res += n/5 n = n/5 return res [343] 整数拆分 问题:给定一个正整数 n，将其拆分为至少两个正整数的和，并使这些整数的乘积最大化。 返回你可以获得的最大乘积。例如，给定 n = 2，返回1（2 = 1 + 1）；给定 n = 10，返回36（10 = 3 + 3 + 4）。注意：你可以假设 n 不小于2且不大于58。 思路：向下分解到3就不能再分了 如果刚好，返回3**(n/2) 如果余1，就把1加到3上构成4 如果余2，乘2 代码： 12345678def integerBreak(self, n): mod = n % 3 if mod == 0: return 3 ** (n/3) if n &gt; 3 else 2 elif mod == 1: return 3 ** (n/3-1) * 4 else: return 3 ** (n/3) * 2 if n &gt; 2 else 1 [279] 完全平方数——四平方和定理 问题：给定正整数 n，找到若干个完全平方数（比如 1, 4, 9, 16, …）使得它们的和等于 n。你需要让组成和的完全平方数的个数最少。 123输入: n = 13输出: 2解释: 13 = 4 + 9. 思路1：动态规划，状态转移方程dp[n] = min(dp[n-i^2]) + 1,if n不是完全平方数，如果是完全平方数返回1，时间复杂度O(n * sqrt n)； 思路2：四平方和定理：任何自然数都可以用最多四个平方和数之和，且只有当$n=4^a(8b+7)$时才需要最少四个平方数之和； 代码： 123456789101112131415161718192021222324# 思路1def numSquares(self, n): if int(n**0.5)**2 == n: return 1 res = 1 lst = [i*i for i in xrange(1,int(n**0.5)+1)] Q = set(lst) while n not in Q: res += 1 Q = set(q+x for q in Q for x in lst if q+x&lt;=n) return res# 思路2:def numSquares(self, n): while n &amp; 3 == 0: n &gt;&gt;= 2 if n &amp; 7 == 7: return 4 for a in xrange(int(n**0.5)+1): b = int((n - a * a)**0.5) if a * a + b * b == n: return (a != 0) + (b!=0) return 3 [368] 最大整除子集 问题：给出一个由无重复的正整数组成的集合, 找出其中最大的整除子集, 子集中任意一对 (Si, Sj) 都要满足: Si % Sj = 0 或 Sj % Si = 0。如果有多个目标子集，返回其中任何一个均可 12集合: [1,2,3]结果: [1,2] (当然, [1,3] 也正确) 思路：动态规划，dp[x] = max(dp[x], dp[y] + 1) 其中： 0 &lt;= y &lt; x 且 nums[x] % nums[y] == 0记录最大路径，只需每次转移时，记录x的父节点，最后即可从最大DP处追溯到整条最大的路径 代码： 1234567891011121314151617181920212223def largestDivisibleSubset(self, nums): &quot;&quot;&quot; :type nums: List[int] :rtype: List[int] &quot;&quot;&quot; if not nums: return [] nums.sort() n = len(nums) dp =[1] * n pre = [None] * n for i in xrange(1,n): for j in xrange(i): if nums[i] % nums[j] == 0 and dp[j] + 1 &gt; dp[i]: dp[i] = dp[j] + 1 pre[i] = j start = dp.index(max(dp)) res = [] while start is not None: res.append(nums[start]) start = pre[start] return res[::-1] [670] 最大交换 问题：给定一个非负整数，你至多可以交换一次数字中的任意两位。返回你能得到的最大值。 123输入: 2736输出: 7236解释: 交换数字2和数字7。 思路：从后向前，记录当前后缀中最大元素的位置，相等算后面的；从前向后，如果当前位置元素小于后续最大元素，则交换二者； 代码: 12345678910111213141516171819def maximumSwap(self, num): &quot;&quot;&quot; :type num: int :rtype: int &quot;&quot;&quot; num = map(int,str(num)) n = len(num) max_id = range(n) for i in xrange(n-2,-1,-1): if num[i] &lt;= num[max_id[i+1]]: max_id[i] = max_id[i+1] for j in xrange(n): if num[j] &lt; num[max_id[j]]: num[j],num[max_id[j]] = num[max_id[j]],num[j] break return int(&#x27;&#x27;.join(map(str,num))) [754] 到达终点数字 问题:在一根无限长的数轴上，你站在0的位置。终点在target的位置。每次你可以选择向左或向右移动。第 n 次移动（从 1 开始），可以走 n 步。返回到达终点需要的最小移动次数。 123456输入: target = 2输出: 3解释:第一次移动，从 0 到 1 。第二次移动，从 1 到 -1 。第三次移动，从 -1 到 2 。 思路: 12345分情况讨论： 1. 1+2+。。。+k=target，则返回k 2. 1+2+...+k刚好大于target，如果多出d为偶数，则只需将d/2反向即可，返回k 3. 如果d为奇数，任何数的反向都只会改变偶数次，k次达不到，这时+k+1差距如果是偶数，返回k+1， 4. 如果是奇数,返回k+2，因为必能在k次达到target-1，再经过两次+k-(k+1) 代码： 12345678910111213import mathdef reachNumber(self, target): n = abs(target) # 解二次方程 k = int(math.ceil(((8*n+1)**0.5-1)/2.)) total = k*(k+1)/2 d = total - n if d % 2 == 0: return k elif (d+k+1) % 2 == 0: return k + 1 else: return k + 2 分析：时间复杂度O(1) [50] 快速幂 问题：实现 pow(x, n) ，即计算 x 的 n 次幂函数 思路: 1234快速计算幂：时间复杂度lgn 1. x^n = (x^2)^(n&gt;&gt;2),if n &amp; 1 == 0 2. x^n = (x^2)^(n&gt;&gt;2) * x,if n &amp; 1 3. 边界如果是正数则n=0，负数则n=-1 代码: 1234567891011def myPow(self, x, n): &quot;&quot;&quot; :type x: float :type n: int :rtype: float &quot;&quot;&quot; if n == 0: return 1. if n == -1: return 1./x return self.myPow(x*x, n&gt;&gt;1)*[1,x][n&amp;1] [829] 连续整数求和 问题:给定一个正整数 N，试求有多少组连续正整数满足所有数字之和为 N? 123输入: 9输出: 3解释: 9 = 9 = 4 + 5 = 2 + 3 + 4 思路: 1234思路2：假设连续数组长度为c，那么和为N的数组满足： 1. 如果c为奇数，N/c为整数 2. 如果c为偶数，(N/c+0.5)*c=N 同时，要求为正整数，如果c(c+1)/2&gt;N，break 代码： 12345678910111213141516def consecutiveNumbersSum(self, N): &quot;&quot;&quot; :type N: int :rtype: int &quot;&quot;&quot; c = 1 res = 0 while c &lt;= N: if c*(c+1)/2 &gt; N: break if c &amp; 1: res += N % c == 0 else: res += N / c * c + c / 2 == N c += 1 return res [400] 第N个数字 问题：在无限的整数序列 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, …中找到第 n 个数字。 123456输入:11输出:0说明:第11个数字在序列 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, ... 里是0，它是10的一部分。 思路: 123456789101 1-92 10-993 100-9994 1000-99995 10000-999996 100000-9999997 1000000-99999998 10000000-999999999 100000000-99999999先确定所属段，再确定所属整数，再确定对应字符 代码: 12345678910def findNthDigit(self, n): for i in xrange(10): delta = 9 * 10 ** i if n &lt;= delta * (i+1): break n -= delta * (i+1) number = str((n - 1) / (i + 1) + 10 ** i) index = (n - 1) % (i + 1) return int(number[index]) [233] 数字1的个数 问题：给定一个整数 n，计算所有小于等于 n 的非负整数中数字 1 出现的个数。 思路: 12345分类讨论1在各个位上出现的次数：if n = xyzdabc，现讨论千位上的1出现的次数 (1) xyz * 1000 if d == 0 xy(z-1)1000~xy(z-1)1999:1000~1999 (2) xyz * 1000 + abc + 1 if d == 1 xyz1000~xyz1abc (3) xyz * 1000 + 1000 if d &gt; 1 xyz1000~xyz1999对每一位上1出现的次数加和即可 代码: 123456789101112131415161718def countDigitOne(self, n): if n &lt; 1: return 0 res = 0 n = str(n) k = len(n) for i,c in enumerate(n): left = int(n[:i] or 0) mid = int(n[i]) right = int(n[i+1:] or 0) cur = 10 ** (k-i-1) if mid == 0: res += left * cur elif mid == 1: res += left * cur + right + 1 else: res += left * cur + cur return res [43] 字符串相乘 问题: 给定两个以字符串形式表示的非负整数 num1 和 num2，返回 num1 和 num2 的乘积，它们的乘积也表示为字符串形式。 12输入: num1 = &quot;123&quot;, num2 = &quot;456&quot;输出: &quot;56088&quot; 思路: 模拟手动乘法，从后向前，div,mod=divmod(xi*yj,10)，mod存放在数组res的i+j位，div存放在i+j+1位，最后从后向前遍历res，div,mod=divmod(res[i],10)，mod留到i位，div加到i+1位，结束时如果div不为0则创建新的位来存 代码: 1234567891011121314151617181920212223242526def multiply(self, num1, num2): &quot;&quot;&quot; :type num1: str :type num2: str :rtype: str &quot;&quot;&quot; m,n = len(num1),len(num2) res =[0] * (m + n) num1,num2 = num1[::-1],num2[::-1] for i in xrange(m): for j in xrange(n): div,mod = divmod(int(num1[i])*int(num2[j]),10) res[i+j] += mod res[i+j+1] += div for k in xrange(m+n-1): div,mod = divmod(res[k], 10) res[k] = mod res[k+1] += div count = m + n while count &gt; 1 and res[-1] == 0: res.pop() count -= 1 return &#x27;&#x27;.join(map(str,res[::-1])) 不使用乘除的除法 问题：给定两个整数，被除数 dividend 和除数 divisor。将两数相除，要求不使用乘法、除法和 mod 运算符。 12输入: dividend = 7, divisor = -3输出: -2 思路：a/b=c，则：a = bc + m=(b2^k1 + b 2^k2 + …b2^0 + m)，尝试用被除数减y的231…20倍数，如果减得动则把相应的倍数加到商中，如果减不动则尝试更小的倍数 代码: 1234567891011121314def divide(self, dividend, divisor): &quot;&quot;&quot; :type dividend: int :type divisor: int :rtype: int &quot;&quot;&quot; mark = (dividend &gt;= 0) == (divisor &gt;= 0) x, y = map(abs,[dividend, divisor]) res = 0 for i in xrange(31,-1,-1): if x &gt;&gt; i &gt;= y: res += 1 &lt;&lt; i x -= y &lt;&lt; i return min(2**31-1,res) if mark else max(-2**31,-res) [166] 分数到小数 问题：给定两个整数，分别表示分数的分子 numerator 和分母 denominator，以字符串形式返回小数。如果小数部分为循环小数，则将循环的部分括在括号内。 12输入: numerator = 2, denominator = 3输出: &quot;0.(6)&quot; 思路: 12345思路：模拟手除法 1. 首先算出整数部分n/m和余数部分n%m，如果n和m有负号，将符号单独拿出 2. 余数不为零，或者不在余数字典&#123;余数:最后出现的位置&#125;中，则将余数放在余数字典，同时余数*10再除被除数，记录值和新的余数 3. 如果余数为0，则将当前的整数和小数部分组合 4. 如果余数不为0，则将当前整数和小数部分组合，同时找到当前余数上次出现的位置，加上括号 代码: 12345678910111213141516171819202122232425262728293031def fractionToDecimal(self, numerator, denominator): &quot;&quot;&quot; :type numerator: int :type denominator: int :rtype: str &quot;&quot;&quot; flag = &#x27;-&#x27; * (numerator ^ denominator &lt; 0 and numerator != 0) n,m = abs(numerator), abs(denominator) inter = n/m n = n% m mod = &#123;&#125; dec = [] index = 0 while n and n not in mod: mod[n] = index n *= 10 dec.append(n/m) index += 1 n %= m if n == 0: if dec: decimal = &#x27;.%s&#x27;%(&#x27;&#x27;.join(map(str,dec))) else: decimal = &#x27;&#x27; else: decimal = &#x27;.%s(%s)&#x27;%(&#x27;&#x27;.join(map(str,dec[:mod[n]])),&#x27;&#x27;.join(map(str,dec[mod[n]:index]))) return &#x27;%s%d%s&#x27;%(flag, inter, decimal) [149] 直线上最多的点数——按起点分类 问题：给定一个二维平面，平面上有 n 个点，求最多有多少个点在同一条直线上。 1234567891011输入: [[1,1],[3,2],[5,3],[4,1],[2,3],[1,4]]输出: 4解释:^|| o| o o| o| o o+-------------------&gt;0 1 2 3 4 5 6 思路1：对于每个点统计其他点到它的斜率出现的次数，次数最大的斜率，对应共线点数最多的直线；需注意如果横坐标相同时记斜率为None；可能含有重复的点，重复的点可以算作任何斜率 思路2：RANSAC (Random sample consensus)算法，每次随机抽出一对点，找到所有和他们共线的点数，只需要重复几十次抽样就可以得到正确结果（但不保证每次都能成功） 代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960# 思路1:def maxPoints(self, points): &quot;&quot;&quot; :type points: List[Point] :rtype: int &quot;&quot;&quot; def slope(point1,point2): if point1.x == point2.x: return None else: return 100.*(point2.y - point1.y)/(point2.x-point1.x) n = len(points) res = 0 for i in xrange(n): d = collections.Counter() same = 1 for j in xrange(i+1,n): if points[i].x == points[j].x and points[i].y == points[j].y: same += 1 else: d[slope(points[i],points[j])] += 1 if d: res = max(res, d.most_common(1)[0][1] + same) else: res = max(res, same) return res# 思路2def maxPoints(self, points): &quot;&quot;&quot; :type points: List[Point] :rtype: int &quot;&quot;&quot; def get_pairs(): while True: index1, index2 = random.sample(xrange(n), 2) p1,p2 = points[index1],points[index2] if p1.x != p2.x or p1.y != p2.y: return p1, p2 n = len(points) s = &#123;(p.x, p.y) for p in points&#125; if n &lt; 2 or len(s) == 1: return n res = 0 for i in xrange (0,50): p1, p2 = get_pairs() cur = 0 for j in xrange(n): p3 = points[j] if (p2.y-p1.y) * (p3.x-p1.x) == (p2.x-p1.x) * (p3.y-p1.y): cur += 1 res = max(res, cur) return res [810] 黑板异或游戏（位运算） 问题：一个黑板上写着一个非负整数数组 nums[i] 。小红和小明轮流从黑板上擦掉一个数字，小红先手。如果擦除一个数字后，剩余的所有数字按位异或运算得出的结果等于 0 的话，当前玩家游戏失败。 (另外，如果只剩一个数字，按位异或运算得到它本身；如果无数字剩余，按位异或运算结果为 0。）换种说法就是，轮到某个玩家时，如果当前黑板上所有数字按位异或运算结果等于 0，这个玩家获胜。假设两个玩家每步都使用最优解，当且仅当小红获胜时返回 true。 思路： 1234567891011121314 bit : 3 2 1 0---------------------num1 : 10 | 1 0 1 0 num2 : 11 | 1 0 1 1num3 : 1 | 0 0 0 1num4 : 2 | 0 0 1 0num5 : 2 | 0 0 1 0num6 : 8 | 1 0 0 0--------------------XOR : 8 | 1 0 0 0 如果开始时xor(nums) = 0，则A赢 如果开始时xor(nums) !=0，如果len(nums)是偶数，则A赢，反之B赢。 1. A只需要先找到xor(nums)中为1的二进制位，数组中必存在该位为0的元素（否则偶数个元素的异或就变成0了），A选择移去该元素，对该位的结果无影响，xor(nums) !=0 2. B做选择，B随便做什么选择，A要么赢，要么回到上一轮同样的状态，偶数个元素异或不为0，选择不能无限持续下去，最终必然无论B选什么，异或结果都会变成0 代码: 123456def xorGame(self, nums): &quot;&quot;&quot; :type nums: List[int] :rtype: bool &quot;&quot;&quot; return reduce(lambda x,y:x^y, nums) == 0 or len(nums) &amp; 1 == 0]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构与算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[推荐系统（二）—— 基于用户行为推荐]]></title>
    <url>%2F%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%2F%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%2F%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%EF%BC%88%E4%BA%8C%EF%BC%89%E2%80%94%E2%80%94%20%E5%9F%BA%E4%BA%8E%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E6%8E%A8%E8%8D%90%2F</url>
    <content type="text"><![CDATA[用户行为不是随机的，而是蕴含着很多模式，这里面最著名的例子就是啤酒和尿布的例子。基于用户行为分析的推荐算法是个性化推荐系统的重要算法，学术界一般将这种类型的算法称为协同过滤算法(Collaborative Filtering, CF)。顾名思义，协同过滤就是指通过用户和网站的不断交互，齐心协力过滤掉用户不感兴趣的物品。 基于邻域的方法(neighborhood-based)基于邻域的推荐方法是最著名的、在业界得到最广泛应用的算法。基于邻域的算法分为两大类： 基于用户的协同过滤：给用户推荐和他相似的其他用户喜欢的物品 基于物品的协同过滤：给用户推荐和他之前喜欢的物品相似的物品 基于用户的协同过滤(UserCF)基于用户的协同过滤是推荐系统中最古老的算法，这个算法的诞生标志了推荐系统的诞生。该算法在1992年被提出，并应用于邮件过滤系统，1994年被GroupLens用于新闻过滤。在此之后直到2000年，该算法都是推荐系统领域最著名的算法。 基于用户的协同过滤主要包含两个步骤： 找到和目标用户兴趣相似的用户集合； 找到这个集合中的用户喜欢的，但目标用户没有听说过的物品推荐给目标用户； 计算用户间相似度常用的计算两个用户的兴趣相似度的方法有以下几种： （1）Jaccard公式： w_{uv}=\frac{\left | N(u)\bigcap N(v) \right |}{\left | N(u)\bigcup N(v) \right |}（2）余弦相似度： w_{uv}=\frac{\left | N(u)\bigcap N(v) \right |}{\sqrt{\left | N(u) \right |\left | N(v) \right |}}（3）改进后的余弦相似度：两个用户都买过某个热门物品，并不能代表他们兴趣相似，因此需要惩罚热门物品对相似度的影响 w_{uv}=\frac{\sum_{i \in N(u)\bigcap N(v)}\frac{1}{log(1+\left | N(i) \right |)}}{\sqrt{\left | N(u) \right |\left | N(v) \right |}} $N(u)$：用户u曾经有过正反馈的物品集合； $N(i)$：对物品i有过正反馈的用户集合； 计算余弦相似度的Python代码： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455train = &#123;&#x27;A&#x27;:set(&#x27;abcd&#x27;),&#x27;B&#x27;:set(&#x27;bcdw&#x27;),&#x27;C&#x27;:set(&#x27;cdeft&#x27;),&#x27;D&#x27;:set(&#x27;abef&#x27;)&#125;# 计算余弦相似度，时间复杂度O(m^2*n)，m表用户数，n表每个用户平均喜欢物品数def similarity(train): &quot;&quot;&quot; 计算用户间兴趣的余弦相似度 @train:&#123;用户:喜欢的物品集合&#125; @return:&#123;u:&#123;v:w,...]&#125;，用户作为节点，用户间如果有相同物品，代表有边，物品数代表边的权值 &quot;&quot;&quot; W = collections.defaultdict(dict) for u in train.keys(): for v in train.keys(): if u != v: a = len(train[u] &amp; train[v]) b = (len(train[u]) * len(train[v])) ** 0.5 cur = round(a / b if b else 0,3) W[u][v] = cur return Wsimilarity(train)defaultdict(dict, &#123;&#x27;A&#x27;: &#123;&#x27;B&#x27;: 0.75, &#x27;C&#x27;: 0.447, &#x27;D&#x27;: 0.5&#125;, &#x27;B&#x27;: &#123;&#x27;A&#x27;: 0.75, &#x27;C&#x27;: 0.447, &#x27;D&#x27;: 0.25&#125;, &#x27;C&#x27;: &#123;&#x27;A&#x27;: 0.447, &#x27;B&#x27;: 0.447, &#x27;D&#x27;: 0.447&#125;, &#x27;D&#x27;: &#123;&#x27;A&#x27;: 0.5, &#x27;B&#x27;: 0.25, &#x27;C&#x27;: 0.447&#125;&#125;)# 如果用户很多，而且大多数用户之间没有共同喜欢的物品时，可以通过建立物品到用户的倒查表，将代码优化到O(m*n)def similarity(train): # 建立物品-用户倒查表 item_users = collections.defaultdict(set) for u,items in train.items(): for i in items: item_users[i].add(u) # 计算用户喜欢物品数、用户间公共物品数 W = collections.defaultdict(dict) N = &#123;&#125; for i,users in item_users.items(): for u in users: N[u] = N.get(u,0) + 1 for v in users: if u != v: W[u][v] = W[u].get(v,0) + 1 # 计算余弦相似度 for u,vs in W.items(): for v,cuv in vs.items(): cur = round(W[u][v] /(N[u] * N[v]) ** 0.5,3) W[u][v] = cur return Wsimilarity(train)defaultdict(dict, &#123;&#x27;A&#x27;: &#123;&#x27;B&#x27;: 0.75, &#x27;C&#x27;: 0.447, &#x27;D&#x27;: 0.5&#125;, &#x27;B&#x27;: &#123;&#x27;A&#x27;: 0.75, &#x27;C&#x27;: 0.447, &#x27;D&#x27;: 0.25&#125;, &#x27;C&#x27;: &#123;&#x27;A&#x27;: 0.447, &#x27;B&#x27;: 0.447, &#x27;D&#x27;: 0.447&#125;, &#x27;D&#x27;: &#123;&#x27;A&#x27;: 0.5, &#x27;B&#x27;: 0.25, &#x27;C&#x27;: 0.447&#125;&#125;) 计算用户对物品兴趣度得到用户间的兴趣相似度后，就可以根据和目标用户u最相似的前K个用户对目标物品i的兴趣度来预测目标用户u对目标物品i的兴趣度了： p(u,i)=\sum_{v \in S(u,K)}w_{uv}r_{vi} $S(u,K)$：包含和用户u兴趣最接近的K个用户 $r_{vi}$：代表用户v对物品i的兴趣度，因为使用单一行为的隐反馈数据，所以取1 基于用户相似度为用户生成推荐列表的Python代码： 12345678910111213def recommend(train, user, W, k): rank = dict() u_items = train[user] for v,wuv in sorted(W[user].items(), key=lambda x:-x[1])[:k]: for i in train[v]: if i not in u_items: rank[i] = rank.get(i,0) + round(wuv, 3) return rankW = similarity(train)recommend(train, &#x27;B&#x27;, W, 3)&#123;&#x27;a&#x27;: 1.0, &#x27;e&#x27;: 0.697, &#x27;f&#x27;: 0.697, &#x27;t&#x27;: 0.447&#125; 基于物品的协同过滤算法(ItemCF)ItemCF是目前业界应用最多的算法，无论是亚马逊网，还是Netflix、Hulu、YouTube，其推荐算法的基础都是该算法。有以下两个原因使得ItemCF比UserCF更加常用： 随着网站的用户数目越来越大，计算用户兴趣相似度矩阵将越来越困难，其运算时间复杂度和空间复杂度的增长和用户数的增长近似于平方关系。 基于用户的协同过滤很难对推荐结果作出解释。 ItemCF算法主要分两步： 计算物品之间的相似度：ItemCF并不是通过物品属性而是通过用户历史行为来计算物品相似度，两个物品的相似度可以被定义为有多少个人同时喜欢这两个物品 根据物品间的相似度和用户的历史行为为用户生成推荐列表 计算物品间的相似度（1）亚马逊显示相关物品推荐时的标题是“Customers Who Bought This Item Also Bought”（购买了该商品的用户也经常购买的其他商品）。从这句话的定义出发，我们可以用下面的公式定义物品的相似度： w_{ij}=\frac{\left | N(i) \bigcap N(j) \right |}{\left | N(i) \right |}（2）余弦相似度：类似的，为了消除热门产品对产品相似度的影响，我们需要惩罚热门物品品对目标物品相似度的影响： w_{ij}=\frac{\left | N(i) \bigcap N(j) \right |}{\sqrt{\left | N(i) \right | \left | N(j) \right |}} $N(i)$：对物品i有过正反馈的用户集合 和UserCF类似，我们通过建立用户到物品的倒查表，然后对每个用户，将他物品列表中的物品两两在共现矩阵中加1，python代码如下： 12345678910111213141516171819202122def similarity(train): &quot;&quot;&quot; 返回物品两两间的相似度 @train:&#123;用户:喜欢的物品集合&#125; @return:&#123;i:&#123;j:w,...&#125;物品作为节点，物品间如果有共同用户，代表有边，共同用户数代表权值 &quot;&quot;&quot; W = collections.defaultdict(dict) N = &#123;&#125; # 计算共现矩阵(用字典表示稀疏矩阵) for u, items in train.items(): for i in items: N[i] = N.get(i,0) + 1 for j in items: if i != j: W[i][j] = W[i].get(j,0) + 1 # 计算物品间的相似度 for i in W: for j in W[i]: cur = round(W[i][j] / (N(i) * N(j)) ** 0.5, 3) W[i][j] = cur return W 计算用户对物品的兴趣度在得到物品之间的相似度后，ItemCF 通过如下公式计算用户 u 对一个物品 i 的兴趣： p(u,i)=\sum_{j \in S(i,K)\bigcap N(u)}r_{uj}w_{ji} $S(i,K)$:和物品x最相似的K个物品集合 基于物品相似度为用户生成推荐列表的Python代码： 1234567891011121314def reconmendation(train, user, W, k): &quot;&quot;&quot; 生成用户user的推荐列表 @train:&#123;用户:喜欢的物品集合&#125; @user:用户id @W:&#123;i:&#123;j:w,...&#125; @k:推荐与用户喜欢过的每个物品最相似的前k个物品 &quot;&quot;&quot; rank = &#123;&#125; ru = train[user] for i in ru: for j,wj in sorted(W[i].items(), key=lambda x:-x[1])[:k]: rank[j] = rank.get(j,0) + wj return rank 一个简单的基于物品推荐的例子： Karypis在研究中发现如果将ItemCF的相似度矩阵按最大值归一化，可以提高推荐的准确率和覆盖率（因为几乎所有物品与热门物品的相似度都比较高，所以归一化之前更倾向于推荐热门物品），其研究表明，如果已经得到了物品相似度矩阵w，那么可以用如下公式得到归一化之后的相似度矩阵w’： w_{ij} = \frac{w_{ij}}{\underset{j}{max}\ w_{ij}}对比UserCF和ItemCF 原理 特点 适用 UserCF 给用户推荐那些和他有共同爱好的用户喜欢的物品 社会化 物品个性化程度较低，物品更多、更新更快的领域，如新闻 ItemCF 给用户推荐那些和他之前喜欢的物品类似的物品 个性化 物品个性化程度较高，用户更多、更新更快的领域，如图书、电子商务 UserCF和ItemCF算法在不同K值下的召回率曲线： UserCF和ItemCF算法在不同K值下的覆盖率曲线： UserCF和ItemCF算法在不同K值下的流行度曲线： 隐语义模型(latent factor model)隐语义模型LFM（latent factor model）最早在文本挖掘领域被提出，用于找到文本的隐含语义，相关名词有LSI、pLSA、LDA和Topic Model，这些技术和方法本质上是相通的，其中很多方法都可以用于个性化推荐系统。 LFM基于隐含特征/类别联系用户和信息，通过如下公式计算用户u对物品i的兴趣： p(u,i)=r_{ui}=p_{u}^Tq_{i}=\sum_{f=1}^{F}p_{uk}q_{ik} $p_{uk}$：可以看做用户u对隐含类别k的兴趣度； $q_{ik}$：可以看做物品i属于隐含类别k的概率； 可以通过机器学习方法训练得到这两个参数： 数据集：可以用一个用户-物品二元组(u,i)代表训练集和测试集中的样本，用u是否对i有正反馈作为标签$r{ui}$，如果用户u对i有正反馈操作，则记$r{ui}=1$，否则记$r_{ui}=0$（负样本可以通过抽样得到）。 损失函数：我们使用均方误差作为损失函数 L = \sum_{(u,i) \in K}(r_{ui}- \hat{r}_{ui})^2=\sum_{(u,i) \in K}(r_{ui}-\sum_{f=1}^{F}p_{uk}q_{ik})^2+\lambda \left \| p_u \right \|^2+\lambda \left \| q_i \right \|^2 $\lambda \left | p_u \right |^2+\lambda \left | q_i \right |^2$：用来防止过拟合的正则化项 算法：可以通过梯度下降或你牛顿法训练出最优参数 评价： LFM具有比较好的理论基础：它是一种学习方法，基于邻域的方法是一种基于统计的方法，并没有学习过程 离线空间复杂度：基于邻域的方法需要维护一张离线的相关表，假设是用户相关表，那么需要的存储空间是O(MM)，假设是物品相关表，那么需要的存储空间是O(NN)；而LFM在建模过程中，如果是F个隐类，那么需要的存储空间是O(F*(M+N))，在M和N都很大时能够很好地节省离线计算内存 离线时间复杂度：LFM需要多次迭代，总体上和基于邻域的方法没有本质区别 推荐解释：LFM计算出的隐类在语义上确实代表了一类兴趣和物品，但很难用自然语言描述并生成解释展示给用户 基于图的随机游走(random wolk on graph)用户-物品模型很容易用二分图表示，因此很多图的算法都可以用到推荐系统中。 二分图令G(V,E)表示用户-物品二分图，$V=VU\bigcup V_I$由用户顶点集合和物品顶点集合组成，如果用户u对物品i有正反馈，对应顶点u和顶点i之间的一条边$e{ui}$。下图是一个简单的用户-物品二分图，其中圆形节点代表用户，方形节点代表物品，圆形节点和方形节点之间的边代表用户对物品的行为： PersonalRank算法用户-用户之间的相似性、用户-物品之间的兴趣度、物品-物品之间的相似性可以转化为度量顶点之间的相关性。研究人员设计了很多计算图中顶点之间相关度的方法，下面介绍一种基于随机游走的PersonalRank算法来计算用户u对物品i的相似度： 初始化顶点u的概率为1，其余所有顶点的概率为0 重复以下过程，直至不满足迭代条件： 从用户u节点出发进行随机游走，游走到任意节点时，按概率p决定是否继续向下游走： 如果决定向下游走，则从当前节点指向的节点中随机选择一个节点前进； 否则停止这次游走，重新从u节点出发 经过很多次迭代游走之后，每个物品节点被访问的概率会收敛到一个数，将这个数作为用户u对物品i的兴趣度 通过以上过程得到用户u对每个物品i的兴趣度p(i)可以用以下公式表示： 1234p(i) = \left\&#123;\begin&#123;matrix&#125;p\sum_&#123;j \in in(i)&#125; \frac&#123;p(j)&#125;&#123;\left | out(j) \right |&#125;&amp; j\neq i \\ (1-p)+p\sum_&#123;j \in in(i)&#125; \frac&#123;p(j)&#125;&#123;\left | out(j) \right |&#125; &amp; j=i\end&#123;matrix&#125;\right. 1234567891011121314151617def personal_rank(G, p, u): &quot;&quot;&quot; @G:图的邻接表 @p:选择继续游走的概率 @u:用户 &quot;&quot;&quot; rank = &#123;x:0. for x in G.keys()&#125; rank[u] = 1. for k in xrange(1000): pre = rank.copy() for i in G: for j in G[i]: rank[j] = p * pre[i] / len(G[i]) if j == u: rank[j] += 1- p rank = pre return rank 以上算法在为每个用户进行推荐时，都需要在整个用户物品二分图上进行迭代，直至收敛，这一过程时间复杂度非常高。可以将PersonalRank算法转化为矩阵运算，令M表示二分图的转移概率矩阵，即: M(i,j)=\frac{1}{\left | out(i) \right |}迭代公式可以写作： r=(1-\alpha )r_0+\alpha M^Tr得： r=(1-\alpha )(1-\alpha M^T)^{-1}r_0只需计算一次稀疏矩阵的逆$(1-\alpha M^T)^{-1}$。 总结1、推荐算法的核心是计算用户u对物品i的兴趣度： UserCF: p(u,i)=\sum_{v \in S(u,K)}w_{uv}r_{vi} ItemCF: p(u,i)=\sum_{j \in S(x,K),x \in N(u)}w_{ji}r_{ui} LFM: p(u,i)=r_{ui}=p_{u}^Tq_{i}=\sum_{f=1}^{F}p_{uk}q_{ik} 二分图: r=(1-\alpha )r_0+\alpha M^Tr2、计算A,B相似度的一般思路：一般通过A,B的某项集合属性的重叠度来度量A,B的相似度。如通过用户购买过的物品集合的重合度来度量用户间的相似度；通过物品的购买者集合的重叠度来度量物品间的相似度；通过标签下物品集合的重叠度来度量标签之间的相似度； 3、通常需要对热门物品进行惩罚，可以直接过滤掉热门物品或者除以物品的流行度；]]></content>
      <categories>
        <category>推荐系统</category>
      </categories>
      <tags>
        <tag>推荐系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[推荐系统（三）—— 基于用户标签推荐]]></title>
    <url>%2F%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%2F%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%2F%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%EF%BC%88%E4%B8%89%EF%BC%89%E2%80%94%E2%80%94%20%E5%9F%BA%E4%BA%8E%E7%94%A8%E6%88%B7%E6%A0%87%E7%AD%BE%E6%8E%A8%E8%8D%90%2F</url>
    <content type="text"><![CDATA[UGC(User Generated Content, 用户生成的内容)标签：当一个用户对一个物品打上一个标签，这个标签一方面描述了用户的兴趣，另一方面表示了物品的语义，从而将用户和物品联系起来。下图为豆瓣读书中《推荐系统实践》一书的常用标签。 本文主要探讨如何利用用户打标签的行为为其推荐物品。一个用户标签行为的数据集可以由一个三元组集合表示，其中每条样本(u,i,b)表示用户u为物品i打上了标签b。 标签作为特征/类别可以将标签看做是LFM中的隐类(只不过这里是显式的)，用户u对物品i的兴趣度可表示为： p(u,i)=\sum_{b}n_{ub}n_{bi} $n_{ub}$：用户u打过标签b的次数，可以看做是用户u对b类物品的喜好程度； $n_{bi}$：物品i被打过b标签的次数，可以看做物品i属于b类的”概率“； 仔细研究以上公式可以发现以下缺点： 热门标签给$n_{ub}$的权重很大； 热门物品给$n_{bi}$的权重很大； 借鉴TF-IDF的思路，可以通过惩罚用户或物品中热门标签的方式来优化以上公式： p(u,i)=\sum_{b}\frac{n_{ub}}{log(1+n_b^u)}\frac{n_{bi}}{log(1+n_i^u)} $1+n_b^u$:标签b被多少个用户使用过 $n_i^u$:物品i被多少个用户打过标签 标签的稀疏性：对于新用户或新物品，集合$B(u)\bigcap B(i)$中的标签数量很少。 标签扩展：为提高推荐准确率，需要对标签集合进行扩展，如果用户使用过某个标签，我们可以将与这个标签相似的标签也加入到用户标签集合中去。扩展标签的方式有很多，常用的有话题模型，这里介绍一种基于邻域的方法，核心是计算标签之间的相似度。 标签相似度：可以用两个标签下物品的重合度来度量它们的相似度 基于图的推荐定义用户-物品-标签三分图：图中有三种顶点，即用户顶点、物品顶点、标签顶点，如果一个用户u给物品i打了标签b，则在图中添加三条边(u,i)、(u,b)、(i,b)。 然后使用类似于二分图中随机游走的方式计算出所有物品相对于当前用户节点的相关性，排序后为用户推荐排名最高的K个物品。]]></content>
      <categories>
        <category>推荐系统</category>
      </categories>
      <tags>
        <tag>推荐系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[推荐系统（四）—— 基于上下文推荐]]></title>
    <url>%2F%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%2F%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%2F%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%EF%BC%88%E5%9B%9B%EF%BC%89%E2%80%94%E2%80%94%20%E5%9F%BA%E4%BA%8E%E4%B8%8A%E4%B8%8B%E6%96%87%E6%8E%A8%E8%8D%90%2F</url>
    <content type="text"><![CDATA[上下文包括用户访问推荐系统的时间、地点、心情等，对于提高推荐系统的推荐系统是非常重要的。比如： 一个卖衣服的推荐系统在冬天和夏天应该给用户推荐不同种类的服装 当用户在中关村打开一个美食推荐系统时，如果这个推荐系统推荐的餐馆都是中关村附近的，显然推荐结果更加能够令用户满意 用户上班时和下班后的兴趣会有区别 用户在平时和周末的兴趣会有区别 用户和父母在一起与和同学在一起时的兴趣有区别 基于时间的推荐时间信息对用户兴趣的影响表现在以下几个方面： 用户兴趣是随时间变化的 物品也是有生命周期的 季节效应 系统的时间特性分析： 用户每天独立用户数增长情况：用来度量系统是处于增长期、稳定期还是衰落期 系统物品变化情况：新闻网站变化很快，但图书网站变化很慢 用户访问情况：统计用户平均活跃天数，也可以统计相隔T天用户的重合度 推荐系统中的时间效应： 推荐系统的实时性：实时推荐系统需要在每个用户访问推荐系统时，都根据这个时间点前的行为实时计算推荐列表；推荐系统需要平衡考虑用户的近期行为和长期行为 推荐系统的时间多样性：推荐系统每天推荐结果的变化程度。有以下几种方式来实现推荐系统的时间多样性： 在生成推荐结果时加入一定的随机性； 记录用户每天看到的推荐结果，然后再每天的推荐中对重复出现的推荐结果适当降权； 每天使用不同的推荐算法进行推荐 最近热门推荐包含时间信息的用户行为数据集可以由三元组集合表示，其中每条样本(u,i,t)代表用户u在时刻t对物品i产生过行为。 物品i最近的流行度可以定义为： n_i(T)=\sum_{u,i,t \in Train,t]]></content>
      <categories>
        <category>推荐系统</category>
      </categories>
      <tags>
        <tag>推荐系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[推荐系统（五）—— 基于社交网络推荐]]></title>
    <url>%2F%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%2F%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%2F%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%EF%BC%88%E4%BA%94%EF%BC%89%E2%80%94%E2%80%94%20%E5%9F%BA%E4%BA%8E%E7%A4%BE%E4%BA%A4%E7%BD%91%E7%BB%9C%E6%8E%A8%E8%8D%90%2F</url>
    <content type="text"><![CDATA[基于社交网络的推荐社会化推荐有以下优点： 好友推荐可以增加推荐的信任度 社交网络可以解决冷启动问题：当一个新用户通过社交账号登录网站时，可以从社交网站中获取用户的好友列表，然后给用户推荐好友在网站上喜欢的物品； 社会化推荐有以下缺点：好友关系并不是基于共同兴趣产生的，用户好友的兴趣和用户的兴趣往往差别较大，所以社会化推荐并不一定能够提高推荐算法的离线精度（准确率和召回率）。 实验表明，综合考虑用户的社会兴趣和个人兴趣对于提高用户满意度是有帮助的。 基于邻域的社会化推荐算法基本的思路是给用户推荐好友喜欢的物品集合： p(u,i)=\sum_{v \in out(u)}w_{uv}r_{vi} $w_{uv}$：用户u和好友v的熟悉度及兴趣相似度 $r_{vi}$：用户v是否喜欢物品i 用户熟悉度：可以通过两个用户共同好友比例来衡量它们的熟悉度 familiarity(u,v)=\frac{\left | out(u)\bigcap out(v) \right |}{\left | out(u)\bigcup out(v) \right |} $out(u)$：用户的好友集合 用户相似度：可以通过两个用户共同喜欢的物品比例来衡量它们的兴趣相似度 familiarity(u,v)=\frac{\left | N(u)\bigcap N(v) \right |}{\left | N(u)\bigcup N(v) \right |} $N(u)$：用户喜欢的物品集合 基于图的推荐图的定义：将用户社交网络图与用户-物品二分图结合成一张图 通过调整α和β，可以调节好友行为和用户历史行为所占的不同权重： 用户和用户之间边的权值：用户和用户之间的相似度的α倍(熟悉度和兴趣相似度) 用户和物品之间边的权值：用户和物品之间的兴趣度的β倍 随机游走算法：PersonalRank 在社交网络中，除了用户之间直接的社交网络关系(friendship)之外，还存在一种社群网络(membership)，加入社群节点后的图： 好友推荐好友推荐系统的目的是根据用户现有的好友、用户的行为记录给用户推荐新的好友，从而增加整个社交网络的稠密程度和社交网站用户的活跃度 基于属性的推荐可以给用户推荐和他们具有相似属性的用户作为好友： 用户人口统计学属性，包括年龄、性别、职业、毕业学校和工作单位等。 用户的兴趣，包括用户喜欢的物品和发布过的言论等。 用户的位置信息，包括用户的住址、IP地址和邮编等。 计算方式和前面介绍过的计算物品相似度类似。 基于社交网络的好友推荐基于社交网络进行好友推荐，最简单的思路就是给用户推荐好友的好友。 用户熟悉度：基于用户共同好友比例计算用户间的数系度： w_{out}(u,v)=\frac{\left | out(u)\bigcap out(v) \right |}{\left | out(u)\bigcup out(v) \right |}out(u)代表u指向的好友集合，在无向社交网络(如QQ好友)中，out(u)=in(u)，但在有向社交网络(如微博关注)中，它们是不同的集合，可以定义另一种相似度。在有向社交网络中$w{out}(u,v)$代表关注相似度，$w{in}(u,v)$代表被关注相似度。 w_{in}(u,v)=\frac{\left | in(u)\bigcap in(v) \right |}{\sqrt{\left | in(u) \right |\left | in(v) \right |}}另一种有向图中的相似度定义：u关注的人中有多少关注了v w_{out,in}(u,v)=\frac{\left | out(u)\bigcap in(v) \right |}{\left | out(u) \right |}如果v是名人的话，几乎所有用户都和名人有较高相似度，因此可以用以下公式改进： w_{in}(u,v)=\frac{\left | out(u)\bigcap in(v) \right |}{\sqrt{\left | out(u) \right |\left | in(v) \right |}}]]></content>
      <categories>
        <category>推荐系统</category>
      </categories>
      <tags>
        <tag>推荐系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[推荐系统（六）—— 评分预测]]></title>
    <url>%2F%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%2F%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%2F%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%EF%BC%88%E5%85%AD%EF%BC%89%E2%80%94%E2%80%94%20%E8%AF%84%E5%88%86%E9%A2%84%E6%B5%8B%2F</url>
    <content type="text"><![CDATA[以上所讲皆为TopK推荐，因为TopK更加接近于满足实际系统的需求，本文再来讲讲评分预测。 评分预测问题就是如何通过已知的用户历史评分记录预测未知的用户评分，如何提高分数的预测精度是要解决的主要问题。评分预测一般使用离线指标RMSE进行评测： RMSE=\frac{\sqrt{\sum_{(u,i) \in T}(r_{ui}-\hat{r}_{ui})^2}}{Test} $r_{ui}$：用户u对物品i的真实评分 $\hat{r}_{ui}$：用户u对物品i的预测评分 评分预测的目的就是找到最好的模型最小化测试集的RMSE。 平均值最简单的评分预测算法是利用平均值预测用户对物品的评分 全局平均值训练集中所有评分记录的评分平均值： \mu = \frac{\sum_{(u,i) \in Train}r_{ui}}{\sum_{(u,i) \in Train}1}用户评分平均值用户u在训练集中所有评分的平均值： \overline{r_u}=\frac{\sum_{i \in N(u)}r_{ui}}{\sum_{i \in N(u)}1}物品评分平均值物品i在训练集中接受的所有评分的平均值： \overline{r_i}=\frac{\sum_{i \in N(i)}r_{ui}}{\sum_{i \in N(i)}1}同类用户对同类物品的平均值训练集中同类用户对同类物品评分的平均值： \hat{r_{ui}}=\frac{\sum_{(v,j) \in Train,\phi (u)=\phi(v),\varphi (i)=\varphi (j)}r_{vj}}{\sum_{(v,j) \in Train,\phi (u)=\phi(v),\varphi (i)=\varphi (j)}1} $\phi (u)$:表示用户u所属的类 $\varphi (i)$:表示物品i所属的类 值得说明的是，前面的各种求平均评分的方法都可以看做是同类用户对同类物品的平均值的特殊情况。 基于邻域的方法基于用户的邻域算法基于用户的邻域算法认为，预测一个用户对一个物品的评分，需要参考和这个用户兴趣相似的用户对该物品的评分： \hat{r_{ui}}=\hat{r_u}+\frac{\sum_{v \in S(u,K)\bigcap N(i)}w_{uv}(r_{vi}-\overline{r_v})}{\sum_{v \in S(u,K)\bigcap N(i)}\left | w_{uv} \right |} $S(u,K)$：与用户u兴趣最相似的K个用户集合 $\hat{r_u}$：用户u对他评过分的物品的平均评分 用户之间的相似度可以通过皮尔逊系数计算： w_{uv}=\frac{\sum_{i \in I}r_{ui}-\overline{r_u}r_{vi}-\overline{r_v}}{\sqrt{\sum_{i \in I}(r_{ui}-\overline{r_u})^2}\sum_{i \in I}(r_{vi}-\overline{r_v})^2}基于物品的邻域算法基于物品的邻域算法在预测用户u对物品i的评分时，会参考用户u对和物品i相似的其他物品的评分： \hat{r_{ui}}=\hat{r_i}+\frac{\sum_{j \in S(i,K)\bigcap N(u)}w_{ij}(r_{uj}-\overline{r_i})}{\sum_{j \in S(i,K)\bigcap N(u)}\left | w_{ij} \right |} $S(i,K)$：和物品i最相似的K个物品的集合 $\hat{r_i}$:物品i的平均评分 关于如何计算物品的相似度，Badrul Sarwar等在论文里做了详细的研究，文章比较了3种主要的相似度： 普通余弦相似度： 皮尔逊系数： 被修正的余弦相似度： Sarwar利用MovieLens最小的数据集对3种相似度进行了对比，并将MAE作为评测指标。实验结果表明利用修正后的余弦相似度进行评分预测可以获得最优的MAE。不过需要说明的是，在一个数据集上的实验并不意味着在其他数据集上也能获得相同的结果。 隐语义模型与矩阵分解模型用户的评分行为可以表示成一个评分矩阵R，其中R[u][i]就是用户u对物品i的评分，但是用户不会对所有物品都评分，所以矩阵中很多元素都是空的，评分预测在某种意义上说就是填空。 SVD补全一个矩阵的方法有很多，而我们要找的是一种对矩阵扰动最小的补全方法，一般认为如果补全后矩阵的特征值和补全之前矩阵的特征值相差不大，就算是扰动比较小。 给定m个用户和n个物品，和用户对物品的评分矩阵R，SVD方法的步骤： 补全缺失值：通过简单方式补全，如用全局平均值或用户/物品平均值 SVD分解：计算补全后的评分矩阵的奇异值和左右奇异向量 R_f=U_f^TS_fV_f $S_f$：取最大的f个奇异值组成的“对角矩阵” SVD分解是早起推荐系统研究中常用的矩阵分解方法，不过该方法具有以下缺点，因此很难在实际系统中应用： 巨大的空间需求:推荐系统中的评分矩阵是非常稀疏的，一旦补全，评分矩阵会变成一个稠密矩阵，从而使评分矩阵的存储需要非常大的空间，这种空间的需求在实际系统中是不可能接受的。 SVD分解计算复杂度很高：特别是在稠密的大规模矩阵上更慢 LFM2006年Netflix Prize开始后，Simon Funk在博客上公布了一个算法（称为Funk-SVD），一下子引爆了学术界对矩阵分解类方法的关注。而且，Simon Funk的博客也成为了很多学术论文经常引用的对象。Simon Funk提出的矩阵分解方法后来被Netflix Prize的冠军Koren称为Latent Factor Model（简称为LFM）。 LFM在之前介绍过，从矩阵分解的角度说，如果将评分矩阵分解为两个低维矩阵相乘： R = P^TQ $R$：m×n，R[u][i]表示用户u对物品i的评分 $P$：k×m，P[k][u]表示用户u对隐类k的兴趣度 $Q$：k×n，Q[k][i]表示物品i对隐类k的关联度 用户u对物品i的评分预测值，可以表示为： \hat{r_{ui}}=\sum_{f}p_{uf}q_{if}以上公式通过隐类将用户和物品联系在了一起，但是在实际情况下，系统中包含了某些和用户物品无关的因素，用户中也包含了某些和物品无关的因素，物品也包含了某些和用户无关的因素，因此可以通过三者的偏置来修正以上预测函数： \hat{r_{ui}}=\mu +b_u+b_i+p_u^Tq_i $\mu$：训练集中所有记录的评分的全局平均值，不同网站定位不同，网站的整体评分分布也会有所差异 $b_u$：用户偏置，代表了用户的评分习惯中和物品没有关系的那种因素，初始化为0 $b_i$：物品偏置，代表了物品接收的评分中和物品没有关系的那种因素，初始化为0 损失函数： L = \sum_{(u,i) \in K}(r_{ui}- \hat{r}_{ui})^2=\sum_{(u,i) \in K}(r_{ui}-\sum_{f=1}^{F}p_{uk}q_{ik})^2+\lambda \left \| p_u \right \|^2+\lambda \left \| q_i \right \|^2然后用机器学习的方法求解。 SVD++Koren在Netflix Prize比赛中提出了一个模型，将用户历史评分的物品加入到了LFM模型中，Koren将该模型称为SVD++。 首先将ItemCF设计成一个像LFM那样可以学习的模型： \hat{r_{ui}}=\frac{1}{\sqrt{\left | N(u) \right |}}\sum_{j \in N(u)}w_{ij}损失函数： C(w)=\sum_{(u,i) \in Train}(r_{ui}-\sum_{j \in N(u)}w_{ij}r_{uj})^2+\lambda w_{ij}^2该模型参数过多，容易造成过拟合，Koren提出应该对w矩阵也进行分解，将参数个数降低到2nF个，模型如下： \hat{r_{ui}}=\frac{1}{\sqrt{\left | N(u) \right |}}\sum_{j \in N(u)}x_i^Ty_j进一步，可以将该模型与前面LFM模型相加，同时为了进一步减少参数，可以令x=q： \hat{r_{ui}}=\mu +b_u+b_i+q_i^T(p_u+\frac{1}{\sqrt{\left | N(u) \right |}}\sum_{j \in N(u)}y_j)通过梯度下降法训练以上参数。 加入时间信息将时间信息应用到基于邻域的模型Netflix Prize的参赛队伍BigChaos在技术报告中提到了一种融入时间信息的基于邻域的模型，本节将这个模型称为TItemCF。 \hat{r_{uit}}=\frac{\sum_{j \in N(u)\bigcap S(i,k)}f(w_{ij},\Delta t)r_{uj}}{\sum_{j \in N(u)\bigcap S(i,k)}f(w_{ij},\Delta t)} $\Delta t=t_{ui}-t{uj}$：用户u对物品i和物品 j评分的时间差 $f(w_{ij},\Delta t)$：考虑了时间衰减后的相似度函数，随着\Delta t的绝对值增大，f减小，也就是说用户很久以前的行为对预测用户当前评分的影响越来越小 f(w_{ij},\Delta t)=\sigma (\delta w_{ij}exp(\frac{-\left | \Delta t \right |}{\beta })+\gamma ) $\sigma(x)$：sigmoid函数，目的是将相似度压缩到(0,1)区间 将时间信息应用到矩阵分解模型在SVD++的基础上融入时间信息： 这里，$t_u$ 是用户所有评分的平均时间。period（t）考虑了季节效应，可以定义为时刻t所在的月份。该模型同样可以通过随机梯度下降法进行优化。]]></content>
      <categories>
        <category>推荐系统</category>
      </categories>
      <tags>
        <tag>推荐系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构与算法：线性表（一）—— 链表]]></title>
    <url>%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%9A%E7%BA%BF%E6%80%A7%E8%A1%A8%EF%BC%88%E4%B8%80%EF%BC%89%E2%80%94%E2%80%94%20%E9%93%BE%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[理论篇链表是线性表的链式存储，它不需要使用地址连续的存储单元，而是通过指针建立数据元素之间的逻辑关系。 优点：插入、删除操作不需要移动元素，只需要修改指针； 缺点：不支持随机存取，不能直接找到表中某个特定的节点，需要从头开始遍历，依次查找； 类别 顺序表 链表 存取方式 顺序存取、随机存取 不支持随机存取 存储结构 逻辑相邻位置也相邻 逻辑相邻位置不一定相邻 查找 按序查找O(1) 按序查找O(n) 插入、删除 需要移动众多元素 只需修改指针O(1) 空间分配 预先分配足够大的空间 需要时分配 单链表单链表：即线性表的链式存储，通过一组任意的存储单元来存储链表节点，每个节点包含数据域和后继指针域。 单链表的存储结构python实现：1234class ListNode(object): def __init__(self,val): self.val = val self.next = None 头结点：在单链表第一个节点之前附加一个节点，称为头结点，头结点值域可以不设任何信息，也可记录表长等信息。头指针能够统一第一个位置和其他位置上的很多操作(特别是插入、删除)，使得代码逻辑更加清晰。 单链表的基本操作创建可通过头插法和尾插法两种方式创建单链表： 头插法：每次将新节点插入到第一个位置，链表节点顺序和原始输入顺序逆序 1234567891011121314151617def create_link(nums): &quot;&quot;&quot; 头插法创建单链表 &quot;&quot;&quot; dummy = ListNode(0) for num in nums: node = ListNode(num) dummy.next,node.next = node,dummy.next return dummy.nextlink = create_link([1,2,3])while link: print(link.val) link = link.next321 尾插法：每次将新节点插入到链表尾部，与输入顺序相同 1234567891011121314151617def create_link(nums): &quot;&quot;&quot; 尾插法创建单链表 &quot;&quot;&quot; tail = dummy = ListNode(0) for num in nums: tail.next = tail = ListNode(num) return dummy.nextlink = create_link([1,2,3])while link: print(link.val) link = link.next123 查找 按值查找：从前向后依次比较，找到则返回 12345678def get_element_by_val(head,val): &quot;&quot;&quot; 在链表head中查找值为val的第一个节点，找到该节点则返回，找不到则返回None &quot;&quot;&quot; p = head while p and p.val != val: p = p.next return p 按序号查找：查找第i个节点，链表计数时，注意初始对齐，即初始count的值对应初始节点，条件不满足时的coun的值对应最终节点； 123456789101112def get_element_by_id(head,i): &quot;&quot;&quot; 在链表head中查找第i个节点，并返回，未找到则返回None &quot;&quot;&quot; if i &lt; 1: return None count = 1 p = head while p and count &lt; i: p = p.next count += 1 return p 遍历 获取链表长度：遍历的同时统计计数12345678910def get_length(head): &quot;&quot;&quot; 获取链表head的长度 &quot;&quot;&quot; p = head count = 0 while p: p = p.next count += 1 return count 获取尾指针：根据尾指针的next域为None来查找 12345678910def get_tail(head): &quot;&quot;&quot; 获取链表head的尾节点 &quot;&quot;&quot; if not head: return None p = head while p.next: p = p.next return p 插入 在第i-1个节点之后插入：要插入节点，需要找到其对应的前驱节点 123456789101112131415161718def insert_node(head,i,val): &quot;&quot;&quot; 在第i-1个节点之后插入新节点，并返回插入后的链表，如果插入位置不合法返回False &quot;&quot;&quot; p = dummy = ListNode(0) dummy.next = head if i &lt; 1: return False count = 0 while p and count &lt; i - 1: p = p.next count += 1 if not p: return Fasle else: new_node = ListNode(val) p.next,new_node.next = new_node,p.next return dummy.next 在第i个节点之前插入:如果无法获取其前驱节点，可通过换值实现插入1234567891011121314151617181920def insert_node(head,i,val): &quot;&quot;&quot; 在第i个节点之前插入新节点 &quot;&quot;&quot; p = dummy = ListNode(0) dummy.next = head if i &lt; 1: return False count = 0 while p and count &lt; i: p = p.next count += 1 if not p: return Fasle else: new_node = ListNode(val) # 插入第i个节点之后，再换值 p.next,new_node.next = new_node,p.next p.val,new_node.val = new_node.val,p.val return dummy.next 删除 删除第i-1个节点的后继节点:同样的要删除一个节点需要先找到其对应的前驱节点 1234567891011121314def del_node(head,i): p = dummy = ListNode(0) dummy.next = head if i &lt; 1: return False count = 0 while p and count &lt; i-1: p = p.next count += 1 if p is None or p.next is None: return Fasle else: p.next = p.next.next return dummy.next 删除第i个节点：如果无法找到前驱节点，可删除第i个节点的后继节点，然后再将第i个节点中的值替换为其后继节点中的值 12345678910111213141516def del_node(head,i): p = dummy = ListNode(0) dummy.next = head if i &lt; 1: return False count = 0 while p and count &lt; i: p = p.next count += 1 if p is None or p.next is None: return Fasle else: val = p.next.val p.next = p.next.next p.val = val return dummy.next 双链表单链表无法直接获取前驱节点，需要从头重新遍历，时间复杂度为O(n)，双链表通过引入前驱指针解决了这个问题。 双链表的存储结构python实现： 12345class Doublelink(object): def __init__(val): self.val = val self.prior = None self.next = None 双链表的基本操作双链表的查找和单链表基本一致，只不过多了一个反向查找的方向；双链表的插入和删除操作前后需要保持双链表的基本结构，加入需要在p节点后插入一个新节点node： 1p.next.prior, node.next, p.next, node.prior = node,p.next.next, node, p 循环单链表 循环单链表在单链表的基础上使得尾节点的next指针指向头结点，构成一个环；这样做的好处是可以在任意位置遍历整个链表。 如果对单链表常做的操作是在表头和表尾，此时可使用带尾指针的循环链表； 循环双链表 循环双链表是在双链表的基础上使得尾节点的next指向头结点，使头节点的prior指向尾节点； ### 静态链表 在不支持指针的高级语言中可以使用二元组数组来实现链表，每个节点用一个二元组(val,next_id)来表示，其中val是节点的数据域，next_id是节点的指针域，表示后继节点的下标，也叫游标。 实战篇链表中常用技巧 绘图：链表操作常常涉及到很复杂的指针变换，建议先画出操作图示，按图示理清算法思路后再进行代码编写； 序列赋值：python的序列赋值给多个指针赋值带来了巨大便利，牢记序列赋值的两个关键点： 右侧生成元组：右侧多个对象会生成一个临时元组，指向这些变量所对应的原始对象，只要这些对象在赋值期间没有被原地修改，就能保持原始的值； 左侧依序赋值：依次将元组中的对象按左侧变量的顺序进行赋值；如果前面的赋值会改变后面的变量含义，则应将后面的变量放在前面先进行赋值； 头结点：引入头结点能够统一所有位置上的操作，简化代码逻辑； 分身哨兵：在移动指针时，如果后续还需要用到该位置的指针，可以复制一个分身哨兵代替原来的指针进行移动； 递归：链表的递归式结构决定了很多问题都可以通过递归的思路来求解； 快慢指针：通过快慢指针可以找到链表的（上/下）中位数、还可以判断链表是否有环、以及环的入口； 分类重组：如果需要将链表按照某种条件进行重排，可以将满足条件的单独作为一个链表，不满足条件的单独作为一个链表，最后再将它们拼接； LeetCode经典题目反转链表 问题：反转单链表 思路：三指针法，pre代表新链表头指针，head代表旧链表头指针，head.next代表旧链表头指针的下一个节点；head加入到pre，head移动到head.next，pre移动到head。 代码：核心head.next,pre,head = pre,head,head.next 12345678910111213def reverseList(self, head): &quot;&quot;&quot; :type head: ListNode :rtype: ListNode &quot;&quot;&quot; pre = None while head: # tmp = head # head = head.next # tmp.next = pre # pre = tmp head.next,pre,head = pre,head,head.next return pre 反转链表II 问题：反转从位置 m 到 n 的链表。请使用一趟扫描完成反转 思路:找到m-1位置的节点作为pre_start，第m位置的节点start，第n位置节点end，第n+1位置节点end_next，对m-n范围内的链表进行逆转，再和其他段拼接 代码： 123456789101112131415161718192021222324252627282930class Solution(object): def reverseBetween(self, head, m, n): &quot;&quot;&quot; :type head: ListNode :type m: int :type n: int :rtype: ListNode &quot;&quot;&quot; p = dummy = ListNode(0) dummy.next = head count = 0 while p: if count == m-1: start_pre = p break else: p = p.next count += 1 start = p = p.next pre = None while count &lt; n: p.next, pre, p = pre, p, p.next count += 1 end = pre end_next = p start_pre.next, start.next = end, end_next return dummy.next 合并两个有序链表 问题：将两个有序链表合并为一个新的有序链表并返回，新链表是通过拼接给定的两个链表的所有节点组成的； 思路：创建一个个新的头节点，比较两个链表的节点，依次将较小者插入到新链表尾部，直至某个链表为空，再将另一个链表连接至新链表末尾； 代码：核心p.next = l1 or l2 123456789101112131415161718def mergeTwoLists(self, l1, l2): &quot;&quot;&quot; :type l1: ListNode :type l2: ListNode :rtype: ListNode &quot;&quot;&quot; p = dummy = ListNode(0) while l1 and l2: if l1.val &lt; l2.val: p.next = p = l1 l1 = l1.next else: p.next = p = l2 l2 = l2.next p.next = l1 or l2 return dummy.next 合并K个有序链表 问题:合并 k 个排序链表，返回合并后的排序链表 思路：使用二路归并排序算法 （1）先将k个链表划分为两组，递归地对这两组链表进行合并，如果某组链表只有一条就直接返回这条链表； （2）然后对合并后的两条有序链表进行合并； 代码：核心是分组、合并2条有序链表 123456789101112131415161718192021222324252627def mergeKLists(self, lists): &quot;&quot;&quot; :type lists: List[ListNode] :rtype: ListNode &quot;&quot;&quot; n = len(lists) if n == 0: return None elif n == 1: return lists[0] mid = (n-1)/2 l1 = self.mergeKLists(lists[:mid+1]) l2 = self.mergeKLists(lists[mid+1:]) p = dummy = ListNode(0) while l1 and l2: if l1.val &lt; l2.val: p.next = p = l1 l1 = l1.next else: p.next = p = l2 l2 = l2.next p.next = l1 or l2 return dummy.next 回文链表 问题:判断一个链表是否为回文链表 思路：找到中位数(下中位数)-前半部分逆序-对比前后部分，全相同才是回文链表 代码：核心是寻找中位数 1234567891011121314151617181920212223242526272829303132def isPalindrome(self, head): &quot;&quot;&quot; :type head: ListNode :rtype: bool &quot;&quot;&quot; def get_mid(head): # 获取上中位元素 slow = fast = head while fast and fast.next: slow = slow.next fast = fast.next.next return slow def reverse_link(head): p,pre = head,None while p: p.next, pre, p = pre,p,p.next return pre if not head or not head.next: return True mid = get_mid(head) re_mid = reverse_link(mid) p,q = head,re_mid while q: if q.val != p.val: return False else: p,q = p.next,q.next return True 相交链表 问题：找到两个单链表相交的起始节点 思路：跑完自己跑对方，首遇非空即为交点，为空则无交点 代码： 12345678910111213def getIntersectionNode(self, headA, headB): &quot;&quot;&quot; :type head1, head1: ListNode :rtype: ListNode &quot;&quot;&quot; pa = headA pb = headB while pa != pb: pa = pa.next if pa else headB pb = pb.next if pb else headA return pa 环形链表 问题：给定一个链表，判断链表中是否有环 思路：快慢指针，快完之前相遇则有环，否则无环 代码： 1234567891011def hasCycle(self, head): &quot;&quot;&quot; :type head: ListNode :rtype: bool &quot;&quot;&quot; slow = fast = head while fast and fast.next: slow,fast = slow.next,fast.next.next if slow == fast: return True return False 环形链表II 问题：给定一个链表，返回链表开始入环的第一个节点。 如果链表无环，则返回 null。 思路：快慢指针-判断环，相遇说明有环，相遇后派一个新的慢指针从头开始移动，与原始慢指针的下次相遇点既是环的入口。证明，y = (n-m)c+x，其中x是第一次相遇点到入口的距离，y是起点到入口的距离。 代码： 1234567891011121314151617181920def detectCycle(self, head): &quot;&quot;&quot; :type head: ListNode :rtype: ListNode &quot;&quot;&quot; slow1 = fast = head while fast and fast.next: slow1 = slow1.next fast = fast.next.next if slow1 == fast: slow2 = head break else: return None while slow1 != slow2: slow1 = slow1.next slow2 = slow2.next else: return slow1 奇偶链表 问题：给定一个单链表，把所有的奇数节点和偶数节点分别排在一起。请注意，这里的奇数节点和偶数节点指的是节点编号的奇偶性，而不是节点的值的奇偶性。 思路：分类重组，使用两个哑结点，一个向后插入奇数结点，一个向后插入偶数结点，最后再将二者合并 代码： 1234567891011121314151617181920212223def oddEvenList(self, head): &quot;&quot;&quot; :type head: ListNode :rtype: ListNode &quot;&quot;&quot; tail1 = link1 = ListNode(0) tail2 = link2 = ListNode(0) count = 1 p = head while p: if count%2: tail1.next = p tail1 = tail1.next else: tail2.next = p tail2 = tail2.next p = p.next count += 1 tail2.next = None tail1.next = link2.next return link1.next 两两交换链表中的节点 问题：给定一个链表，两两交换其中相邻的节点，并返回交换后的链表，给定 1-&gt;2-&gt;3-&gt;4, 你应该返回 2-&gt;1-&gt;4-&gt;3 思路：交换前两个，递归求出后面的，再连接 代码： 12345678910def swapPairs(self, head): &quot;&quot;&quot; :type head: ListNode :rtype: ListNode &quot;&quot;&quot; if head is None or head.next is None: return head p = head.next p.next, head.next = head, self.swapPairs(p.next) return p k 个一组翻转链表 问题：给出一个链表，每 k 个节点一组进行翻转，并返回翻转后的链表。k 是一个正整数，它的值小于或等于链表的长度。如果节点总数不是 k 的整数倍，那么将最后剩余节点保持原有顺序。给定这个链表：1-&gt;2-&gt;3-&gt;4-&gt;5，当 k = 3 时，应当返回: 3-&gt;2-&gt;1-&gt;4-&gt;5。 思路：先统计链表长度n，总共有n/k份长度为k的需要逆序，循环逆序即可 代码： 12345678910111213141516171819202122232425262728293031def reverseKGroup(self, head, k): &quot;&quot;&quot; :type head: ListNode :type k: int :rtype: ListNode &quot;&quot;&quot; if not head or k == 1: return head dummy = ListNode(0) p = dummy.next = head n = 0 while p: p = p.next n += 1 t = n/k pre_start = dummy p = head for i in range(t): count = 0 start = p pre = None while count &lt; k: count += 1 p.next,pre,p = pre,p,p.next end = pre start.next = p pre_start.next = end pre_start = start return dummy.next 排序链表 问题：对链表进行排序 思路：可以用插入排序、快排或归并排序 插入排序：使用两个指针，p.next代表当前待插入的元素，q.next代表已排序的节点，每次q.next从头开始遍历，直至找到第一个不小于p.next.val的节点，将其插入； 快排：分类重组，将节点划分为小于less、等于eq、大于more某个值的三个链表-&gt;递归地快排less和more-&gt;合并三个链表； 归并排序：寻找中位数-两侧归并排序(空或单节点直接返回)-合并两侧归并结果； 代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115# 插入排序def insert_sort_link(self, head): &quot;&quot;&quot; :type head: ListNode :rtype: ListNode &quot;&quot;&quot; if not head: return None q = dummy = ListNode(0) p = dummy.next = head while p.next: while p.next.val &gt; q.next.val: q = q.next if p == q: p = p.next else: p.next.next, p.next, q.next = q.next, p.next.next, p.next q = dummy return dummy.next # 快排def quick_sort_link(self, head): &quot;&quot;&quot; :type head: ListNode :rtype: ListNode &quot;&quot;&quot; def patition(head,val): less = u = ListNode(0) eq = v = ListNode(0) more = w = ListNode(0) while head: if head.val &lt; val: u.next = u = head elif head.val == val: v.next = v = head else: w.next = w = head head = head.next u.next = v.next = w.next = None return less,eq,more def get_tail(head): tail = head if tail: while tail.next: tail = tail.next return tail def quick_sort(head): if head: less,eq,more = patition(head,head.val) l1 = ListNode(0) l1.next = quick_sort(less.next) l2 = ListNode(0) l2.next = quick_sort(more.next) get_tail(l1).next = eq.next get_tail(eq).next = l2.next return l1.next else: return None return quick_sort(head) # 归并排序def merge_sort_link(self, head): &quot;&quot;&quot; :type head: ListNode :rtype: ListNode &quot;&quot;&quot; def get_mid(head): &quot;&quot;&quot; 找到下中位数，head不为None &quot;&quot;&quot; slow,fast = head,head.next while fast and fast.next: slow,fast = slow.next,fast.next.next return slow def merge(l1,l2): &quot;&quot;&quot; 归并两个有序链表 &quot;&quot;&quot; dummy = p = ListNode(0) while l1 and l2: if l1.val &lt; l2.val: p.next = l1 l1 = l1.next else: p.next = l2 l2 = l2.next p = p.next if l1:p.next = l1 if l2:p.next = l2 return dummy.next def merge_sort(head): if not head: return None if not head.next: return head mid = get_mid(head) # print mid.val l1 = head l2 = mid.next mid.next = None left = merge_sort(l1) right = merge_sort(l2) res = merge(left,right) return res return merge_sort(head) 两数相加 问题：给定两个非空链表来表示两个非负整数。位数按照逆序方式存储，它们的每个节点只存储单个数字。将两数相加返回一个新的链表。输入：(2 -&gt; 4 -&gt; 3) + (5 -&gt; 6 -&gt; 4) 输出：7 -&gt; 0 -&gt; 8 原因：342 + 465 = 807 思路：递归思路更简洁 两个都有，随便用哪个节点放结果(l1,l2,flag)，然后next指向子问题的解 其中一个没有，只需要计算有的和flag的和，next指向子问题的解 两个都没有且flag不为0，新建节点存储，否则返回None 代码： 12345678910111213141516171819202122def addTwoNumbers(self, l1, l2): &quot;&quot;&quot; :type l1: ListNode :type l2: ListNode :rtype: ListNode &quot;&quot;&quot; def add_all(flag,l1,l2): if l1 is None and l2 is None: return ListNode(flag) if flag else None elif l1 is None or l2 is None: if l1 is None: l1,l2 = l2,l1 if flag: flag, l1.val = divmod(l1.val + flag, 10) l1.next = add_all(flag,l1.next,l2) return l1 else: flag,l1.val = divmod(l1.val+l2.val+flag,10) l1.next = add_all(flag,l1.next,l2.next) return l1 return add_all(0,l1,l2) 两数相加II 问题：给定两个非空链表来代表两个非负整数。数字最高位位于链表开始位置。它们的每个节点只存储单个数字。将这两数相加会返回一个新的链表。输入: (7 -&gt; 2 -&gt; 4 -&gt; 3) + (5 -&gt; 6 -&gt; 4) 输出: 7 -&gt; 8 -&gt; 0 -&gt; 7 思路：先逆序，再相加，转化为问题2，结果再逆序 代码： 12345678910111213141516171819202122232425262728293031323334def addTwoNumbers(self, l1, l2): &quot;&quot;&quot; :type l1: ListNode :type l2: ListNode :rtype: ListNode &quot;&quot;&quot; def reverse_link(head): p, pre = head, None count = 0 while p: count += 1 p.next,pre,p = pre,p,p.next return pre,count l1_re,count_l1 = reverse_link(l1) l2_re,count_l2 = reverse_link(l2) if count_l1 &lt; count_l2: l1_re,l2_re,count_l1,count_l2 = l2_re,l1_re,count_l2,count_l1 head1 = l1_re head2 = l2_re flag = 0 while head1: if head2: flag,head1.val = divmod(head1.val + head2.val + flag,10) head2 = head2.next else: flag,head1.val = divmod(head1.val + flag,10) if head1.next is None and flag: head1.next = ListNode(flag) flag = 0 head1 = head1.next return reverse_link(l1_re)[0] 旋转链表 问题：给定一个链表，旋转链表，将链表每个节点向右移动 k 个位置，其中 k 是非负数 思路：等价于右移k%n,这相当于将最后k%n个节点插入到头结点后面，只需要找到第n-k%n个节点和尾节点（头结点-按序查找-插入） 代码： 12345678910111213141516171819202122232425262728def rotateRight(self, head, k): &quot;&quot;&quot; :type head: ListNode :type k: int :rtype: ListNode &quot;&quot;&quot; if not head: return head dummy = ListNode(0) p = dummy.next = head n = 1 while p.next: p = p.next n += 1 tail = p k = k % n if k == 0: return head p = head count = 1 while count &lt; n - k: p = p.next count += 1 dummy.next, tail.next, p.next = p.next, head, None return dummy.next 删除链表的倒数第N个节点 问题：给定一个链表，删除链表的倒数第 n 个节点，并且返回链表的头结点。给定一个链表: 1-&gt;2-&gt;3-&gt;4-&gt;5, 和 n = 2。当删除了倒数第二个节点后，链表变为 1-&gt;2-&gt;3-&gt;5。 思路:先用一个指针p移动到第n+1个节点，然后安排另一个指针q从头节点开始移动，直至p==None，q指向了要删除节点的前一个节点。 代码： 1234567891011121314151617181920def removeNthFromEnd(self, head, n): &quot;&quot;&quot; :type head: ListNode :type n: int :rtype: ListNode &quot;&quot;&quot; dummy = ListNode(0) dummy.next = head p = q = dummy count = 0 while p: p = p.next if count &gt;= n+1: q = q.next count += 1 q.next = q.next.next return dummy.next 复制带随机指针的链表 问题:给定一个链表，每个节点包含一个额外增加的随机指针，该指针可以指向链表中的任何节点或空节点。要求返回这个链表的深度拷贝。 思路：第一次遍历，先创建所有节点；第二次遍历，再建立节点间的连接； 代码： 12345678910111213141516171819def copyRandomList(self, head): &quot;&quot;&quot; :type head: RandomListNode :rtype: RandomListNode &quot;&quot;&quot; if not head:return None dic = &#123;&#125; p = head while p: dic[p] = RandomListNode(p.label) p = p.next p = head while p: dic[p].next = dic.get(p.next,None) dic[p].random = dic.get(p.random,None) p = p.next return dic[head] 有序链表转换二叉搜索树 问题： 思路：中位数做根，递归地左链表作为左子树，右链表作为右子树 重排链表 问题：给定一个单链表 L：L0→L1→…→Ln-1→Ln ，将其重新排列后变为： L0→Ln→L1→Ln-1→L2→Ln-2→… 思路：找到中间位置的节点，将后半部分逆序，然后归并到前半部分 Split Linked List in Parts 问题：Given a (singly) linked list with head node root, write a function to split the linked list into k consecutive linked list “parts”.Input: root = [1, 2, 3], k = 5，Output: [[1],[2],[3],[],[]] 思路：先得到每个链表的长度x,y = divmod(n,k) 分为k个链表，其中前y个长度为x+1，后面的长度为x； Linked List Components 问题：给定一个链表head和列表G，返回head中有多少个连续的块，块中节点的值在G中；Input: head: 0-&gt;1-&gt;2-&gt;3 G = [0, 1, 3] Output: 2Explanation: 0 and 1 are connected, so [0, 1] and [3] are the two connected components 思路：在有哑元时只需要统计上一个节点不在G中当前节点在G中的个数； 删除排序链表中的重复元素 II 问题：给定一个排序链表，删除所有含有重复数字的节点，只保留原始链表中 没有重复出现 的数字。输入: 1-&gt;2-&gt;3-&gt;3-&gt;4-&gt;4-&gt;5 输出: 1-&gt;2-&gt;5 思路：pre代表可能重复的前一个节点，cur代表可能重复的第一个节点，pro代表与cur不同的第一个节点，统计pro和cur的距离，如果大于1则pre.next直接指向pro，cur移动至pro,重复统计，反之等于1，则pre,cur前进一步，直至cur为None]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构与算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scala 教程：Basics（一）—— 变量声明]]></title>
    <url>%2FScala%2FScala%2FScala%20%E6%95%99%E7%A8%8B%EF%BC%9ABasics%EF%BC%88%E4%B8%80%EF%BC%89%E2%80%94%E2%80%94%20%E5%8F%98%E9%87%8F%E5%A3%B0%E6%98%8E%2F</url>
    <content type="text"><![CDATA[变量声明语法： 1var/val &lt;identifier&gt; [: &lt;type&gt;] = &lt;data&gt; 变量声明三要素： 变量：标识指定存储空间和指定类型的标签； 类型：对存储空间中数据的解释方式； 对象：包含数据的存储空间/地址； 变量声明时，如果缺省变量类型，Scala 将通过值的类型来推断变量的类型： 1234567891011121314// 显式声明变量的类型scala&gt; val x: Int = 1x: Int = 1// 隐式类型推断scala&gt; val x = 1x: Int = 1// 同时为多个变量赋以相同的值scala&gt; val x, y = 1x: Int = 1y: Int = 1// 同时为多个变量赋以不同的值，多个变量必须用小括号包围，右侧元组对象中的元素个数必须与变量个数相同，Scala 会将元组解开，并将其中每个元素分别赋值给不同的变量scala&gt; val (x, y) = (1, 2)x: Int = 1y: Int = 2 变量按照是否可以被重新赋值，Scala 中的变量分为两种： value：初始化之后不能再被赋值的变量，虽然 value 一旦被初始化之后就无法再为其赋值，但是如果 value 指向的是一个可变对象，则可以改变对象内部的状态； variable：初始化之后可重新赋值的变量，变量可以被重新赋值，但是不能改变其指定的类型，除非前后类型可以进行隐式转化； 1234567891011121314151617181920212223242526// 创建一个 val scala&gt; val array: Array[String] = new Array(5)array: Array[String] = Array(null, null, null, null, null)// 无法对 val 重新赋值scala&gt; array = new Array(5)&lt;console&gt;:12: error: reassignment to val array = new Array(5) ^// 但如果 val 指向的是一个可变对象，那么可以改变对象内部域状态scala&gt; array(0) = &quot;a&quot;scala&gt; arrayres7: Array[String] = Array(a, null, null, null, null)// 创建一个 varscala&gt; var array: Array[String] = new Array(5)array: Array[String] = Array(null, null, null, null, null)// 可以为 var 重新赋值scala&gt; array = new Array(3)array: Array[String] = [Ljava.lang.String;@44f338ec// 但是不能改变 var 的类型scala&gt; array = new Array[Int](3)&lt;console&gt;:12: error: type mismatch; found : Array[Int] required: Array[String] array = new Array[Int](3) ^ Scala 建议只要有机会，尽量使用val，这有以下几个好处： 有助于减少变量修改所带来的副作用； 让你的代码更加简洁、易读和重构； val 支持等式推理（equational reasoning）：引入的变量等于计算出它的值的表达式，在任何你打算写变量名的地方，都可以直接用表达式来替换； 类型Scala 中所有值都有一个类型，包括数值和函数。与Java不同， Scala中没有基本类型的概念，Scala 中的所有类型，从数字到字符串以至集合，都属于一个类型层次体系： Any：是Scala中所有类型的超类型，也称为顶级类型，它定义了一些通用的方法如 equals、hashCode、toString，Any有两个直接子类AnyVal和AnyRef； AnyVal：所有数值类型的父类，有9个预定义的值类型：Double、Float、Long、Int、Short、Byte、Char、Unit和 Boolean； AnyRef：所有引用类型的父类； 数值类型：包括 Byte Short Int Long Float Double 以及Char Boolean 和Unit，可以在运行时作为对象在堆中分配内存，也可以作为JVM基本类型值在栈中分配内存； 引用类型：只能作为对象在堆中分配内存； Unit：代表空值，用来标识没有返回值的函数，类似于Java中的void；Unit只有一个实例()，这个实例也没有实质的意义； Null：所有引用类型的子类，相当于一个空指针，类似于Java中的null引用; 它有一个单例值由关键字 null 所定义。Null 主要是使得 Scala 满足和其他 JVM 语言的互操作性，但是几乎不应该在Scala代码中使用； Nothing：所有类型的子类，也称为底部类型，它的用途之一是给出非正常终止的信号，如抛出异常、程序退出或者一个无限循环（可以理解为它是一个不对值进行定义的表达式的类型，或者是一个不能正常返回的方法）； 对象Scala 采用了一种纯粹的面向对象的模型，一切值都是对象，一切操作符都是对象的方法调用，一切代码块都是表达式（有返回值）： 字面值对象：每个字面值都可以被当做一个对象来处理，操作符可以看做是该对象上的方法调用 12345scala&gt; 5210000 + 1 * 1024 / 1res11: Int = 5211024scala&gt; 5210000.+(1.*(1024./(1)))res10: Int = 5211024 类型对象： 函数对象：]]></content>
      <categories>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>教程</tag>
        <tag>Scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scala 教程：Collections（一）—— Seq]]></title>
    <url>%2FScala%2FScala%2FScala%20%E6%95%99%E7%A8%8B%EF%BC%9ACollections%EF%BC%88%E4%B8%80%EF%BC%89%E2%80%94%E2%80%94%20Seq%2F</url>
    <content type="text"><![CDATA[Seq 是所有序列（有序集合）的根类型，序列集合的 Seq 层次体系如图所示： 序列的操作有以下几种： 索引和长度的操作 apply、isDefinedAt、length、indices，及lengthCompare。序列的apply操作用于索引访问；因此，Seq[T]类型的序列也是一个以单个Int（索引下标）为参数、返回值类型为T的偏函数。换言之，Seq[T]继承自Partial Function[Int, T]。序列各元素的索引下标从0开始计数，最大索引下标为序列长度减一。序列的length方法是collection的size方法的别名。lengthCompare方法可以比较两个序列的长度，即便其中一个序列长度无限也可以处理。 索引检索操作（indexOf、lastIndexOf、indexofSlice、lastIndexOfSlice、indexWhere、lastIndexWhere、segmentLength、prefixLength）用于返回等于给定值或满足某个谓词的元素的索引。 加法运算（+:，:+，padTo）用于在序列的前面或者后面添加一个元素并作为新序列返回。 更新操作（updated，patch）用于替换原序列的某些元素并作为一个新序列返回 排序操作（sorted, sortWith, sortBy）根据不同的条件对序列元素进行排序。 反转操作（reverse, reverseIterator, reverseMap）用于将序列中的元素以相反的顺序排列。 比较（startsWith, endsWith, contains, containsSlice, corresponds）用于对两个序列进行比较，或者在序列中查找某个元素。 多集操作（intersect, diff, union, distinct）用于对两个序列中的元素进行类似集合的操作，或者删除重复元素。 Seq 在 Iterator 的基础上添加了其他一些操作 ： Seq trait 具有三个子特征（subtrait）： 线性序列（LinearSeq）：不添加新的操作，具有高效的 head 和 tail 操作，常用线性序列有 scala.collection.immutable.List 和 scala.collection.immutable.Stream； 索引序列（IndexedSeq）：不添加新的操作，具有高效的apply, length, 和 (如果可变) update操作，常用索引序列有 scala.collection.mutable.ArrayBuffer 和 scala.collection.immutable.Vector； 缓冲器（Buffer）：缓冲器允许对现有元素进行增、删、改操作，常用的Buffer序列有 ListBuffer 和 ArrayBuffer，Buffer 在 Seq 的基础上增加了一些添加和删除的操作； 列表（List）List 是一个不可变链表，它既不能调整大小也不能改变其内容/状态。 List 的创建创建列表的标准做法Scala中创建List或者其他类型集合时，标准做法是作为一个函数来调用这个集合，并提供必要的内容： 12345678910scala&gt; val colors = List(&quot;red&quot;, &quot;green&quot;, &quot;blue&quot;)colors: List[String] = List(red, green, blue)scala&gt; val nums: List[List[Int]] = | List( | List(1, 0, 0), | List(0, 1, 0), | List(0, 0, 1) | )nums: List[List[Int]] = List(List(1, 0, 0), List(0, 1, 0), List(0, 0, 1)) Scala 的列表类型是协变的（covariant），对每一组类型的S和T，如果S是T的子类型，那么List[S]就是List[T]的子类型。空列表的类型是List[Nothing]，对任何T来说，List[Nothing] 都是List[T]的子类型，这也是为什么编译器允许我们编写一下代码： 12345scala&gt; val x = List()x: List[Nothing] = List()scala&gt; val s: List[String] = List()s: List[String] = List() 创建列表的构建单元所有列表都构建自两个基础的构建单元：Nil 和 ::，Nil 表示空列表，中缀表达式::表示在列表前追加元素，List(…)不过是最终展开成这些定义的包装方法而已 123// 以:结束的操作符是右结合的，等价于val colors = (&quot;red&quot; :: (&quot;green&quot; :: (&quot;blue&quot; :: Nil)))scala&gt; val colors = &quot;red&quot; :: &quot;green&quot; :: &quot;blue&quot; :: Nilcolors: List[String] = List(red, green, blue) List 的基本操作列表原始操作List 的所有操作都可以用下面三个原始操作来完成： head 返回列表的第一个元素，对空列表将抛出异常 tail 返回列表中除第一个元素之外的所有元素，对空列表将抛出异常 isEmpty 返回列表是否为空列表 123456789101112131415161718192021scala&gt; val emptyList = NilemptyList: scala.collection.immutable.Nil.type = List()scala&gt; emptyList.headjava.util.NoSuchElementException: head of empty list at scala.collection.immutable.Nil$.head(List.scala:430) ... 28 elidedscala&gt; emptyList.tailjava.lang.UnsupportedOperationException: tail of empty list at scala.collection.immutable.Nil$.tail(List.scala:432) ... 28 elidedscala&gt; emptyList.isEmptyres6: Boolean = truescala&gt; colors.headres7: String = redscala&gt; colors.tailres8: List[String] = List(green, blue) 与 head 和 tail 方法对应，List还提供了相应的对偶方法，last 获取列表的最后一个元素，init返回除了最后一个元素之外的剩余部分，所不同的是后两种方法的时间复杂度为 O(n) 12345678scala&gt; val abc = List(&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;)abc: List[Char] = List(a, b, c)scala&gt; abc.lastres14: Char = cscala&gt; abc.initres3: List[Char] = List(a, b) 列表解构列表支持解构： 1234scala&gt; val List(a, b, c) = colorsa: String = redb: String = greenc: String = blue 列表解构可以用于模式匹配：x :: xs 是中缀操作模式的一个特例，中缀操作等同于一次方法调用，而::作为模式有着不同的规则，它是被当做构造方法模式处理的，x::xs相当于 ::(x, xs)，Scala中有一个类叫scala.:: 就是用来构建非空列表的。通常，对列表做模式匹配比用方法来解构更清晰，下面是一个使用列表的模式匹配实现插入排序的一个例子 123456789101112131415// 插入排序def isort(xs: List[Int]): List[Int] = xs match &#123; case List() =&gt; List() case x :: xs1 =&gt; insert(x, isort(xs1))&#125;def insert(x: Int, xs: List[Int]): List[Int] = xs match &#123; case List() =&gt; List(x) case y :: ys =&gt; if (x &lt;= y) x:: xs else y :: insert(x, ys)&#125;val l = List(2,3,1)println(isort(l))List(1, 2, 3) List 的初阶方法List 继承了 Seq 特质（LinearSeq 没有添加新的操作）中不可变类型的所有方法，此外 List 还增加了一些自身独有的操作。 索引和长度1234567891011121314151617181920212223242526272829303132scala&gt; val abc = List(&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;)abc: List[Char] = List(a, b, c)// 按索引获取元素：当对象出现在方法调用中函数出现的位置时，编译器会帮我们插入applyscala&gt; abc.apply(1)res4: Char = bscala&gt; abc(1)res5: Char = b// 获取列表长度：length 和 size 方法都可以返回列表长度scala&gt; abc.lengthres13: Int = 3scala&gt; abc.sizeres14: Int = 3// 获取索引范围：indices属性返回列表的索引范围(0~list.size-1)scala&gt; abc.indicesres43: scala.collection.immutable.Range = Range 0 until 3// 获取列表切片：`xs.slice(start, end)` 返回列表的一个连续部分，从第一个索引到第二个索引（不包含第二个索引）scala&gt; List(1,2,3,4,5).slice(1,3)res5: List[Int] = List(2, 3)// 获取列表前缀scala&gt; abc.take(2)res9: List[Char] = List(a, b)// 获取列表后缀scala&gt; abc.drop(2)res8: List[Char] = List(c)// splitAt(n) 将列表从指定下标切开，返回这两个列表组成的元组，元组的第一个列表长度为 nscala&gt; abc.splitAt(1)res16: (List[Char], List[Char]) = (List(a),List(b, c)) 按索引查找1234567891011121314scala&gt; val list = List(1,2,4,3,2,4)list: List[Int] = List(1, 2, 4, 3, 2, 4)// 返回序列xs中等于x的第一个元素的索引scala&gt; list.indexOf(2)res50: Int = 1// 返回序列xs中等于x的最后一个元素的索引scala&gt; list.lastIndexOf(2)res51: Int = 4// 查找子序列ys，返回xs中匹配的第一个索引scala&gt; list.indexOfSlice(List(2,4))res52: Int = 1// xs序列中满足p的第一个元素scala&gt; list.indexWhere(_ &gt; 2)res53: Int = 2 列表加法12345678910111213141516171819202122232425262728scala&gt; val y = List(4,5)y: List[Int] = List(4, 5)scala&gt; val xs = List(1,2,3)xs: List[Int] = List(1, 2, 3)scala&gt; val ys = List(4,5)ys: List[Int] = List(4, 5)scala&gt; val x = 0x: Int = 0// 在列表xs首部插入一个元素x，返回一个新的列表scala&gt; x +: xsres54: List[Int] = List(0, 1, 2, 3)// 在列表xs尾部插入一个元素x，返回一个新的列表scala&gt; xs :+ xres55: List[Int] = List(1, 2, 3, 0)// 在列表ys首部插入另一个集合，返回一个新的列表scala&gt; xs ++: ysres56: List[Int] = List(1, 2, 3, 4, 5)// 在列表xs尾部插入另一个集合，返回一个新的列表scala&gt; xs ++ ysres58: List[Int] = List(1, 2, 3, 4, 5)// 拼接列表xs和ys，以上三种形式最终效果等价，但:::要求左右两个操作数均为列表scala&gt; xs ::: ysres62: List[Int] = List(1, 2, 3, 4, 5) 更新123456789101112131415scala&gt; xsres71: List[Int] = List(1, 2, 3)scala&gt; ysres72: List[Int] = List(4, 5)// 将xs中索引为i的元素替换为x，返回一个新的列表scala&gt; xs.updated(1, 9)res75: List[Int] = List(1, 9, 3)// 将xs中slice(i,j)的切片替换为序列 Seq ysscala&gt; xs.patch(0, ys, 3)res76: List[Int] = List(4, 5)scala&gt; xs.patch(0, ys, 0)res78: List[Int] = List(4, 5, 1, 2, 3) 排序 xs.sorted：按自然值对核心类型的列表排序（升序），返回一个新列表 12345678910// 对数值型来说就是值的大小scala&gt; List(2,1,3).sortedres8: List[Int] = List(1, 2, 3)scala&gt; List(&#x27;a&#x27;,&#x27;A&#x27;,&#x27;b&#x27;).sortedres80: List[Char] = List(A, a, b)// 对字符串来说是其字典序scala&gt; List(&quot;w&quot;, &quot;da&quot;, &quot;h&quot;).sortedres9: List[String] = List(da, h, w) sortBy：xs.sortBy(f) 接收一个列表xs，对每个元素调用函数 f，按照函数返回值的自然值对列表中的元素进行排序（升序） 12scala&gt; List(&quot;w&quot;, &quot;da&quot;, &quot;h&quot;).sortBy(_.size)res10: List[String] = List(w, h, da) sortWith：xs.sortWith(before) 接收一个列表 xs，before是一个用来比较两个元素的布尔函数，x before y 返回true则代表x应该排列在y前面 12scala&gt; List(1, -3 ,4, 2).sortWith(_ &lt; _)res77: List[Int] = List(-3, 1, 2, 4) 列表反转：由于列表是一个链表，当需要频繁访问列表的末尾时，先将列表反转，操作结束再对反转后的列表进行反转通常是更好的选择 12345678scala&gt; abc.reverseres10: List[Char] = List(c, b, a)scala&gt; abc.reverse.headres11: Char = cscala&gt; abc.reverse.tailres12: List[Char] = List(b, a) 比较123456789101112// 测试序列xs是否以序列ys开头scala&gt; xs.startsWith(ys)res85: Boolean = false// 测试序列xs是否以序列ys结尾scala&gt; xs.endsWith(ys)res86: Boolean = true// 测试xs序列中是否存在一个与x相等的元素scala&gt; xs.contains(1)res88: Boolean = true// 测试xs序列中是否存在一个与ys相同的连续子序列scala&gt; xs.containsSlice(ys)res89: Boolean = true 多集操作 集合操作：并非真的”集合“操作，因为不去重 123456789101112131415161718scala&gt; val xs = List(1,1,2,1,3,2,3)xs: List[Int] = List(1, 1, 2, 1, 3, 2, 3)scala&gt; val ys = List(1,2,1,4,5)ys: List[Int] = List(1, 2, 1, 4, 5)// 序列xs和ys的交集，并保留序列xs中的顺序，遍历xs中的元素，在ys中查找，找不到就pass，找到保留，并从ys中去掉这个元素，重复以上过程scala&gt; xs intersect ysres97: List[Int] = List(1, 1, 2)// 序列xs和ys的差集，并保留序列xs中的顺序，遍历xs中的元素，在ys中查找，找到就pass，找不到保留，并从ys中去掉这个元素，重复以上过程scala&gt; xs diff ysres98: List[Int] = List(1, 3, 2, 3)// 并集；同xs ++ ysscala&gt; xs union ysres96: List[Int] = List(1, 1, 2, 1, 3, 2, 3, 1, 2, 1, 4, 5)// 去重scala&gt; xs.distinctres99: List[Int] = List(1, 2, 3) 列表扁平化：flatten 方法接收一个列表的列表并将它扁平化，返回单个列表 12345scala&gt; val efg = List(&#x27;e&#x27;, &#x27;f&#x27;, &#x27;g&#x27;)efg: List[Char] = List(e, f, g)scala&gt; List(abc, efg).flattenres18: List[Char] = List(a, b, c, e, f, g) 列表拉链/解拉链：zip 方法接收两个列表，一一匹配两个列表中的元素，返回一个元组列表，如果两个列表长度不同则会丢弃没有匹配上的元素； 12scala&gt; abc.zip(efg)res19: List[(Char, Char)] = List((a,e), (b,f), (c,g)) 列表解拉链：unzip 方法是zip的逆方法，接收一个元组列表，返回一个列表元组 12scala&gt; abc.zip(efg).unzipres20: (List[Char], List[Char]) = (List(a, b, c),List(e, f, g)) var列表赋值 不可变集合同样提供了 += 和 -= 操作，虽然效果相同，但它们在实现上是不同的。可变集合的+=是在可变集合上调用+=方法，它会改变s的内容；但不可变类型的+=却是赋值操作的简写，它是在集合上应用方法+，并把结果赋值给集合变量。这体现了一个重要的原则：我们通常能用一个非不可变集合的变量(var)来替换可变集合的常量(val)。 列表虽然是不可变的，但是可以通过重新赋值来改变列表变量的值，从最终结果来看与使用可变类型相同，但有效率上的差异。 1234567891011121314151617181920212223242526272829303132333435363738scala&gt; var xs = List[Int]()xs: List[Int] = List()scala&gt; val ys = List(1,2)ys: List[Int] = List(1, 2)// += 操作scala&gt; xs +:= 0scala&gt; xsres112: List[Int] = List(0)scala&gt; xs :+= 3scala&gt; xsres114: List[Int] = List(0, 3)scala&gt; xs ++= ysscala&gt; xsres116: List[Int] = List(0, 3, 1, 2)scala&gt; xs :::= ysscala&gt; xsres118: List[Int] = List(1, 2, 0, 3, 1, 2)// 普通赋值scala&gt; xs = xs.updated(0,9)xs: List[Int] = List(9, 2, 0, 3, 1, 2)scala&gt; xsres119: List[Int] = List(9, 2, 0, 3, 1, 2)scala&gt; xs = xs.sortedxs: List[Int] = List(0, 1, 2, 2, 3, 9)scala&gt; xsres120: List[Int] = List(0, 1, 2, 2, 3, 9) 转换列表列表无处不在，需要集合时推荐优先使用List，Scala的集合类型可以很容易的进行相互转换 toString 返回列表的标准字符串表现形式； 12scala&gt; abc.toStringres16: String = List(a, b, c) mkString(pre, sep, post) 通过分隔符seq拼接列表中的元素，然后再首位添加前缀pre和后缀； 12345678scala&gt; abc.mkString(&quot;[&quot;, &quot;,&quot;, &quot;]&quot;)res23: String = [a,b,c]scala&gt; abc.mkString(&quot;,&quot;)res24: String = a,b,cscala&gt; abc.mkString(&quot;&quot;)res26: String = abc toArray：将列表转化为数组（顺序表） 12scala&gt; abc.toArrayres36: Array[Char] = Array(a, b, c) copyToArray：将列表中的元素依次复制到目标数组的指定位置 1234567scala&gt; val arr = new Array[Char](10)?[1m?[34marr?[0m: ?[1m?[32mArray[Char]?[0m = Array(?, ?, ?, ?, ?, ?, ?, ?, ?, ?)scala&gt; abc.copyToArray(arr, 3)scala&gt; arr?[1m?[34mres35?[0m: ?[1m?[32mArray[Char]?[0m = Array(?, ?, ?, a, b, c, ?, ?, ?, ?) iterator：将列表转化为迭代器 12scala&gt; abc.iteratorres37: Iterator[Char] = &lt;iterator&gt; List 的高阶方法列表映射(map)： map：xs.map(f) 接收一个列表 xs，依次对列表中每个元素调用 f ，返回一个包含所有返回值的列表 12scala&gt; List(1,2,3).map(_ * 2)res38: List[Int] = List(2, 4, 6) flatMap：xs.flatMap(f) 接收一个列表 xs，依次对列表中的每个元素调用 f，f 必须要返回一个collection，然后将所有collection拼接起来，返回一个包含所有collection中各个元素的列表 12scala&gt; List(&quot;ab c&quot;, &quot;d e f&quot;).flatMap(_.split(&quot; &quot;))res45: List[String] = List(ab, c, d, e, f) foreach：xs.foreach(g)接收一个列表 xs， 依次对列表中的每个元素调用过程 g，返回Unit 1234567scala&gt; var sum = 0sum: Int = 0scala&gt; List(1, 2, 3, 4).foreach(sum += _)scala&gt; sumres49: Int = 10 列表过滤(filter)： filter：xs.filter(p) 接收一个列表xs，依次对列表中的每个元素调用布尔函数 p，返回包含所有返回值为 true 的元素的列表 12scala&gt; List(1,2,3,4).filter(_ % 2 == 0)res50: List[Int] = List(2, 4) partition：xs.partition(p) 接收一个列表 xs，依次对列表中的每个元素调用布尔函数 p，返回一个列表二元组，第一个列表包含了所有返回true的元素，第二个列表包含了所有返回false的元素 12scala&gt; List(1,2,3,4).partition( _ &lt; 3)res51: (List[Int], List[Int]) = (List(1, 2),List(3, 4)) find：xs.find(p) 接收一个列表xs，依次对列表中的每个元素调用布尔函数 p，返回一个可选值(Option)，如果存在返回值为true的元素x，则返回第一个匹配成功的元素Some(x)，否则返回None 12345scala&gt; List(1,2,3,4).find(_ % 2 == 0)res52: Option[Int] = Some(2)scala&gt; List(1,2,3,4).find(_ % 5 == 0)res53: Option[Int] = None 列表归约(reduce)：实现归约的三要素： 可迭代对象(iterator)：进行迭代归约的对象； 累加器变量(accumulator)：在迭代开始时拥有一个初始值，在迭代过程中基于归约函数和当前元素进行更新，在迭代结束时作为最终的返回值； 归约函数(op)：接收当前元素和累加器变量更新累加器变量； Scala 支持数学归约（如计算一个列表的总和）和逻辑归约（例如确定一个列表是否包含给定元素）。Scala支持三种归约的高阶操作：fold、reduce 和 scan，这三个归约操作实际并没有本质区别，可以相互实现。对于每种归约操作，Scala又划分了三个版本：默认版本（等价于从左到右归约）、从左到右归约、从右到左归约。 左折叠：(z /: xs)(op) 或者 xs.foldLeft(z)(op) 接收一个列表 xs 和初始化累加器 z，z作为左操作数，从左到右依次对列表中的每个元素调用二元操作 op 来更新 z，返回最终的z（/: 操作符形象地表示该操作符会产生一颗向向左靠的操作树） 123456789101112131415161718// foldLeft等价于foldscala&gt; val xs = List(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;)xs: List[String] = List(a, b, c)scala&gt; (&quot;0&quot; /: xs)(_ + _)res66: String = 0abcscala&gt; xs.fold(&quot;0&quot;)(_ + _)res67: String = 0abcscala&gt; xs.foldLeft(&quot;0&quot;)(_ + _)res67: String = 0abc// 使用左折叠实现列表反转scala&gt; def reverseLeft[T](xs: List[T]) = (List[T]() /: xs)&#123;(ys, y) =&gt; y :: ys&#125;reverseLeft: [T](xs: List[T])List[T]scala&gt; reverseLeft(abc)res76: List[Char] = List(c, b, a) 右折叠：(xs :\ z)(op) 或 xs.foldRight(z)(op)，接收一个列表 xs 和初始化累加器 z，z作为右操组数，从右到左依次对列表中的每个元素调用二元操作 op 来更新 z，返回最终的z（:\ 操作符形象地表示该操作符会产生一颗向右靠的操作树） 1234567891011121314151617181920scala&gt; xs.foldRight(&quot;0&quot;)(_ + _)res68: String = abc0scala&gt; (xs :\ &quot;0&quot;)(_ + _)res69: String = abc0``` - reduce：与 fold 用法类似，只不过不用提供初始值 - `xs.reduce(op)` / `xs.reduceLeft(op)` 给定一个归约函数，从列表中第一个元素开始从左到右归约列表； - `xs.reduceRight(op)` 给定一个归约函数，从列表中第一个元素开始从右到左归约列表；```scalascala&gt; abc.reduce(_ + _)res19: String = abcscala&gt; abc.reduceLeft(_ + _)res20: String = abcscala&gt; abc.reduceRight(_ + _)res21: String = abc scan：与 fold 用法相同，只不过返回的是归约过程中各个累积起的一个列表 xs.scan(z)(op) / xs.scanLeft(z)(op) 给定一个初始值和一个归约函数，从左到右返回各个累计值的一个列表 xs.scanRight(z)(op) 给定一个初始值和一个归约函数，从右到左返回各个累计值的一个列表 12345678scala&gt; abc.scan(&quot;0&quot;)(_ + _)res22: List[String] = List(0, 0a, 0ab, 0abc)scala&gt; abc.scanLeft(&quot;0&quot;)(_ + _)res23: List[String] = List(0, 0a, 0ab, 0abc)scala&gt; abc.scanRight(&quot;0&quot;)(_ + _)res24: List[String] = List(abc0, bc0, c0, 0) 列表检查 forall：xs.forall(p) 接收一个列表 xs，依次对每个元素应用布尔函数 p，如果所有元素返回值均为true则返回true，否则返回false 12scala&gt; List(1,2,3,4).forall(_ &lt; 5)res54: Boolean = true exists：xs.exists(p) 接收一个列表 xs，依次对每个元素应用布尔函数 p，只要有一个元素的返回值为true则返回true，否则返回false 12scala&gt; List(1,2,3,4).exists(_ == 4)res57: Boolean = true List 伴生对象方法上面介绍的方法都是List类的方法，还有些方法是定义在全局可访问对象scala.List上的，这是List类的伴生对象，某些操作是用于创建列表的工厂方法，另一些则是对特定形状的列表进行操作。 从元素创建列表：List(1,2,3) 这样的字面量只不过是简单地将对象应用到元素1，2，3而已，与List.apply(1,2,3) 是等效的 12345scala&gt; List(1,2,3)res81: List[Int] = List(1, 2, 3)scala&gt; List.apply(1,2,3)res82: List[Int] = List(1, 2, 3) 创建整数区间：List.range(from, until[, step]) 创建一个从 from 开始，到until结束（不包含until），步长为step的整数列表 12scala&gt; List.range(1,10,2)res86: List[Int] = List(1, 3, 5, 7, 9) 创建相同元素的列表：List.fill(n)(value) 创建一个包含 n 个 value 元素的列表 12scala&gt; List.fill(10)(&#x27;1&#x27;)res87: List[Char] = List(1, 1, 1, 1, 1, 1, 1, 1, 1, 1) 创建一个表格化的列表：List.tabulate(m, n)(f) 创建一个 m 行 n 列的二维列表xs，xs(i)(j)=f(i,j), 0 &lt;= i &lt; m, 0 &lt;= j &lt; n 12scala&gt; List.tabulate(2,3)(_ * _)res90: List[List[Int]] = List(List(0, 0, 0), List(0, 1, 2)) 向量（Vector）Vector 是一个不可变的索引序列，通过多分支树实现。Vector 是用来解决 List 不能随机存取的一种结构，它在快速随机选择和快速随机更新的性能方面做到很好的平衡，所以目前正被用作不可变索引序列的默认实现方式。当你不知道该选择什么时，Vector是你最好的选择，因为它在Scala集合中是最灵活和高效的。 Vector结构通常被表示成具有多分支的树，每个节点包含最多32个vector元素或者至多32个子树节点，一次间接引用则可以用来表示一个包含至多32*32=1024个元素的vector。从树的根节点经过两跳到达叶节点足够存下有2的15次方个元素的vector结构，经过3跳可以存2的20次方个，4跳2的25次方个，5跳2的30次方个。所以对于一般大小的vector数据结构，一般经过至多5次数组访问就可以访问到指定的元素，这也就是我们之前所提及的随机数据访问时“运行时间的相对高效”。 Vector 创建123456789// 标准形式scala&gt; val vec1 = Vector(1,2,3)vec1: scala.collection.immutable.Vector[Int] = Vector(1, 2, 3)// 创建一个空的Vectorscala&gt; val vec2 = Vector[Int]()vec2: scala.collection.immutable.Vector[Int] = Vector()// 使用 IndexedSeq 默认创建一个Vectorscala&gt; val vec3 = IndexedSeq(1, 2, 3)vec3: IndexedSeq[Int] = Vector(1, 2, 3) Vector 操作与 List 相比， Vector 的操作没有什么不同，只是效率上有所差异。 1234567891011scala&gt; vec1 ++ vec3res0: scala.collection.immutable.Vector[Int] = Vector(1, 2, 3, 1, 2, 3)scala&gt; vec1.updated(0,9)res2: scala.collection.immutable.Vector[Int] = Vector(9, 2, 3)scala&gt; vec1.take(2)res3: scala.collection.immutable.Vector[Int] = Vector(1, 2)scala&gt; vec1.filter(_ &gt; 2)res4: scala.collection.immutable.Vector[Int] = Vector(3) 数组缓冲器（ArrayBuffer）ArrayBuffer 是一个变长数组，目前正被用作可变索引序列的默认实现方式。当新元素总是添加到最后的时候，可以用 ArrayBuffer 来高效地构造一个大型的容器。 ArrayBuffer 创建在创建 ArrayBuffer 之前必须先引入它： 12345678910scala&gt; import scala.collection.mutable// 标准方式scala&gt; val arrb1 = mutable.ArrayBuffer(1,2,3)arrb1: scala.collection.mutable.ArrayBuffer[Int] = ArrayBuffer(1, 2, 3)// 创建一个空的 ArrayBuffer，必须显式知名元素类型scala&gt; val arrb2 = mutable.ArrayBuffer[Int]()arrb2: scala.collection.mutable.ArrayBuffer[Int] = ArrayBuffer()// 创建一个 Buffer，默认返回一个 ArrayBufferscala&gt; val arrb3 = mutable.Buffer(1,2,3)arrb3: scala.collection.mutable.Buffer[Int] = ArrayBuffer(1, 2, 3) ArrayBuffer 操作与 List 相比，ArrayBuffer 支持原地修改操作，ArrayBuffer 的大多数操作，其速度与数组本身无异，因为这些操作直接访问、修改底层数组。 ArrayBuffer 可以通过 += 和 ++= 添加元素或另一个集合中的元素，也可以通过 -= 和 --= 从数组中删除元素或另一个集合中的元素，在删除元素时会从后向前删除，如果找到就删找不到就跳过： 123456789101112131415161718192021scala&gt; arrb = mutable.ArrayBuffer(1,2,1,2,3)arrb: scala.collection.mutable.ArrayBuffer[Int] = ArrayBuffer(1, 2, 1, 2, 3)scala&gt; arrb += 4res11: scala.collection.mutable.ArrayBuffer[Int] = ArrayBuffer(1, 2, 1, 2, 3, 4)scala&gt; arrb -= 1res12: scala.collection.mutable.ArrayBuffer[Int] = ArrayBuffer(2, 1, 2, 3, 4)scala&gt; arrb -= 9res12: scala.collection.mutable.ArrayBuffer[Int] = ArrayBuffer(2, 1, 2, 3, 4)scala&gt; arrb ++= Array(4,5)res13: scala.collection.mutable.ArrayBuffer[Int] = ArrayBuffer(2, 1, 2, 3, 4, 4, 5)scala&gt; arrb --= Array(4,5)res14: scala.collection.mutable.ArrayBuffer[Int] = ArrayBuffer(2, 1, 2, 3, 4)scala&gt; arrb --= Array(4,5)res15: scala.collection.mutable.ArrayBuffer[Int] = ArrayBuffer(2, 1, 2, 3) ArrayBuffer 还有很多其它方法用来添加或者删除元素： 12345678910111213141516171819202122232425262728293031scala&gt; val a = ArrayBuffer(1, 2, 3) // ArrayBuffer(1, 2, 3)scala&gt; a.append(4) // ArrayBuffer(1, 2, 3, 4)scala&gt; a.append(5, 6) // ArrayBuffer(1, 2, 3, 4, 5, 6)scala&gt; a.appendAll(Seq(7,8)) // ArrayBuffer(1, 2, 3, 4, 5, 6, 7, 8)scala&gt; a.clear // ArrayBuffer()scala&gt; val a = ArrayBuffer(9, 10) // ArrayBuffer(9, 10)a: scala.collection.mutable.ArrayBuffer[Int] = ArrayBuffer(9, 10)scala&gt; a.insert(0, 8) // ArrayBuffer(8, 9, 10)scala&gt; a.insert(0, 6, 7) // ArrayBuffer(6, 7, 8, 9, 10)scala&gt; a.insertAll(0, Vector(4, 5)) // ArrayBuffer(4, 5, 6, 7, 8, 9, 10)scala&gt; a.prepend(3) // ArrayBuffer(3, 4, 5, 6, 7, 8, 9, 10)scala&gt; a.prepend(1, 2) // ArrayBuffer(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)scala&gt; a.prependAll(Array(0)) // ArrayBuffer(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10)scala&gt; val a = ArrayBuffer.range(&#x27;a&#x27;, &#x27;h&#x27;) // ArrayBuffer(a, b, c, d, e, f, g)a: scala.collection.mutable.ArrayBuffer[Char] = ArrayBuffer(a, b, c, d, e, f, g)scala&gt; a.remove(0) // ArrayBuffer(b, c, d, e, f, g)res17: Char = ascala&gt; a.remove(2, 3) // ArrayBuffer(b, c, g)scala&gt; val a = ArrayBuffer.range(&#x27;a&#x27;, &#x27;h&#x27;) // ArrayBuffer(a, b, c, d, e, f, g)a: scala.collection.mutable.ArrayBuffer[Char] = ArrayBuffer(a, b, c, d, e, f, g)scala&gt; a.trimStart(2) // ArrayBuffer(c, d, e, f, g)scala&gt; a.trimEnd(2) // ArrayBuffer(c, d, e) ArrayBuffer 还支持原地修改序列中的元素： 1234567891011// ArrayBuffer 的 updated方法是在原地修改scala&gt; arrb.updated(0,9)res17: scala.collection.mutable.Buffer[Int] = ArrayBuffer(9, 2, 3)scala&gt; arrbres18: scala.collection.mutable.Buffer[Int] = ArrayBuffer(1, 2, 3)// ArrayBuffer 支持下面快速修改语法scala&gt; arrb(0) = 0scala&gt; arrbres20: scala.collection.mutable.Buffer[Int] = ArrayBuffer(0, 2, 3) 列表缓冲器（ListBuffer）ListBuffer 是一个可变链表。如果你还需要像使用 List 那样使用可变序列，或者需要将可变序列转化为 List，使用ListBuffer是一个更好的选择。 ListBuffer 创建在创建 ListBuffer 之前必须先引入它： 12345678scala&gt; import scala.collection.mutablescala&gt; val listb = mutable.ListBuffer[Int]()listb: scala.collection.mutable.ListBuffer[Int] = ListBuffer()scala&gt; val listb = mutable.ListBuffer(1,2,3)listb: scala.collection.mutable.ListBuffer[Int] = ListBuffer(1, 2, 3) ListBuffer 操作ListBuffer 的操作和 ArrayBuffer类似，通过 toList 方法可以转化为 List： 12scala&gt; listb.toListres22: List[Int] = List(1, 2, 3, 4) 数组（Array）Array 是一个定长数组：Scala 2.8 中数组不再被看作是序列，因为本地数组的类型不是Seq的子类型，而是在数组和 scala.collection.mutable.WrappedArray 这个类的实例之间隐式转换，后者则是Seq的子类 12345678scala&gt; val a1 = Array(1, 2, 3)a1: Array[Int] = Array(1, 2, 3)scala&gt; val seq: Seq[Int] = a1seq: Seq[Int] = WrappedArray(1, 2, 3)scala&gt; val a4: Array[Int] = seq.toArraya4: Array[Int] = Array(1, 2, 3) 关于Array，需要知道： Scala 数组与 Java 数组是一一对应的：Array类型实际上只是数组类型的一个包装器，Scala数组Array[Int]可看作Java的Int[]，Array[Double]可看作Java的double[]，以及Array[String]可看作Java的String[] Scala 数组是一种泛型：可以定义一个Array[T]，T可以是一种类型参数或抽象类型 Scala 数组与 Scala 序列是兼容的：在需要Seq[T]的地方可由Array[T]代替，Scala 数组支持所有的序列操作 不建议经常使用Array类型，除非JVM代码中需要用到。 Array 创建 标准做法：作为一个函数来调用Array 12scala&gt; val a = Array[Int](5)a: Array[Int] = Array(5) new 语句：根据给定类型和长度创建数组 12scala&gt; val a = new Array[Int](5)a: Array[Int] = Array(0, 0, 0, 0, 0) Array 操作Array 可原地修改但不能改变其大小： 1234567891011scala&gt; ares17: Array[Int] = Array(999, 0, 0, 0, 0)scala&gt; a :+ 1res18: Array[Int] = Array(999, 0, 0, 0, 0, 1)scala&gt; a += 1&lt;console&gt;:13: error: value += is not a member of Array[Int] Expression does not convert to assignment because receiver is not assignable. a += 1 ^ Array[T] 泛型数组：类型参数 T.ClassTag，当Array[T] 构造时，在任何情况下会发生的是，编译器会寻找类型参数T的一个类声明，这就是说，它会寻找ClassTag[T]一个隐式类型的值。如果如此的一个值被发现，声明会用来构造正确的数组类型；否则，你就会看到一个错误信息 1234567891011import scala.reflect.ClassTag// 创建泛型数组def evenElems[T: ClassTag](xs: Vector[T]): Array[T] = &#123; val arr = new Array[T]((xs.length + 1) / 2) for (i &lt;- 0 until xs.length by 2) arr(i / 2) = xs(i) arr&#125;// 使用泛型数组scala&gt; evenElems(Vector(1, 2, 3, 4, 5))res6: Array[Int] = Array(1, 3, 5) Seq 选择选择一个 Seq 的时候，你需要考虑两件事： 选择线性序列还是索引序列：线性序列方便头尾操作、索引序列方便随机存取； 选择可变序列还是不可变序列：可变序列方便原地修改、不可变序列安全无副作用； 主要的不可变序列： IndexedSeq LinearSeq Description List ✓ A singly linked list. Suited for recursive algorithms that work by splitting the head from the remainder of the list. Queue ✓ A first-in, first-out data structure. Range ✓ A range of integer values. Stack ✓ A last-in, first-out data structure. Stream ✓ Similar to List, but it’s lazy and persistent. Good for a large or infinite sequence, similar to a Haskell List. String ✓ Can be treated as an immutable, indexed sequence of characters. Vector ✓ The “go to” immutable, indexed sequence. The Scaladoc describes it as, “Implemented as a set of nested arrays that’s efficient at splitting and joining.” 主要的可变序列： IndexedSeq LinearSeq Description Array ✓ Backed by a Java array, its elements are mutable, but it can’t change in size. ArrayBuffer ✓ The “go to” class for a mutable, sequential collection. The amortized cost for appending elements is constant. ArrayStack ✓ A last-in, first-out data structure. Prefer over Stack when performance is important. DoubleLinkedList ✓ Like a singly linked list, but with a prev method as well. The documentation states, “the additional links make element removal very fast.” LinkedList ✓ A mutable, singly linked list. ListBuffer ✓ Like an ArrayBuffer, but backed by a list. The documentation states, “If you plan to convert the buffer to a list, use ListBuffer instead of ArrayBuffer.” Offers constant-time prepend and append; most other operations are linear. MutableList ✓ A mutable, singly linked list with constant-time append. Queue ✓ A first-in, first-out data structure. Stack ✓ A last-in, first-out data structure. (The documentation suggests that an ArrayStack is slightly more efficient.) StringBuilder ✓ Used to build strings, as in a loop. Like the Java StringBuilder. 序列类型常用操作的性能特点： 类型 具体类型 head tail apply update prepend append insert immutable List C C L L C L - immutable Stream C C L L C L - immutable Vector eC eC eC eC eC eC - immutable Stack C C L L C L L immutable Queue aC aC L L C C - immutable Range C C C - - - - immutable String C L C L L L - mutable ArrayBuffer C L C C L aC L mutable ListBuffer C L L L C C L mutable StringBuilder C L C C L aC L mutable MutableList C L L L C C L mutable Queue C L L L C C L mutable ArraySeq C L C C - - - mutable Stack C L L L C L L mutable ArrayStack C L C C aC L L mutable Array C L C C - - - 其中，时间复杂度符号解释如下： 符号 说明 C 指操作的时间复杂度为常数 eC 指操作的时间复杂度实际上为常数，但可能依赖于诸如一个向量最大长度或是哈希键的分布情况等一些假设。 aC 该操作的均摊运行时间为常数。某些调用的可能耗时较长，但多次调用之下，每次调用的平均耗时是常数。 Log 操作的耗时与容器大小的对数成正比。 L 操作是线性的，耗时与容器的大小成正比。 - 操作不被支持。 序列操作说明： 操作 说明 head 选择序列的第一个元素。 tail 生成一个包含除第一个元素以外所有其他元素的新的列表。 apply 索引。 update 功能性更新不可变序列，同步更新可变序列。 prepend 添加一个元素到序列头。对于不可变序列，操作会生成个新的序列。对于可变序列，操作会修改原有的序列。 append 在序列尾部插入一个元素。对于不可变序列，这将产生一个新的序列。对于可变序列，这将修改原有的序列。 insert 在序列的任意位置上插入一个元素。只有可变序列支持该操作。 针对数组／链表，可变／不可变的组合，推荐使用以下四种通用容器类： Immutable Mutable Linear (Linked lists) List ListBuffer Indexed Vector ArrayBuffer 总结 Seq 的一般选择原则： 如果你需要一个不可变序列，那么首选 List； 如果你还需要快速访问不可变序列的中间元素，那么就选 Vector； 如果你需要一个可变序列，那么首选 ArrayBuffer； 如果你还需要像使用 List 那样使用可变序列，或者需要将可变序列转化为 List，那么就选 ListBuffer； 参考Scala CookBook Scala 官方文档]]></content>
      <categories>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>教程</tag>
        <tag>Scala</tag>
      </tags>
  </entry>
</search>
